# Verify connection

URL: https://developers.cloudflare.com/1.1.1.1/check/

import { PublicStats } from "~/components";

After setting up `1.1.1.1`, you can check if you are correctly connected to Cloudflare's resolver.

1. Open a web browser on a configured device (smartphone or computer) or on a device connected to your configured router.
2. Enter [https://1.1.1.1/help](https://one.one.one.one/help) on the browser address bar.

Wait for the page to load and run its tests. The page will present you a summary of the type of connection you have to `1.1.1.1`, as well as the Cloudflare data center you are connected to.

---

# FAQ

URL: https://developers.cloudflare.com/1.1.1.1/faq/

import { GlossaryTooltip } from "~/components"

Below you will find answers to our most commonly asked questions. If you cannot find the answer you are looking for, refer to the [community page](https://community.cloudflare.com/) to explore more resources.

## What is 1.1.1.1?

1.1.1.1 is Cloudflare's fast and secure DNS resolver. When you request to visit an application like `cloudflare.com`, your computer needs to know which server to connect you to so that it can load the application. Computers donâ€™t know how to do this name to address translation, so they ask a specialized server to do it for them.

This specialized server is called a DNS recursive resolver. The resolverâ€™s job is to find the address for a given name, like `2400:cb00:2048:1::c629:d7a2` for `cloudflare.com`, and return it to the computer that asked for it.

Computers are configured to talk to specific DNS resolvers, identified by IP address. Usually the configuration is managed by your ISP (like Comcast or AT\&T) if you are on your home or wireless Internet, and by your network administrator if youâ€™re connected to the office Internet. You can also change the configured DNS resolver your computer talks to yourself.


## How can I check if my computer / smartphone / tablet is connected to 1.1.1.1?


Visit [1.1.1.1/help](https://one.one.one.one/help) to make sure your system is connected to 1.1.1.1 and that it is working.


## What do DNS resolvers do?

DNS resolvers are like address books for the Internet. They translate the name of places to addresses so that your browser can figure out how to get there. DNS resolvers do this by working backwards from the top until they find the website your are looking for.

Every resolver knows how to find the invisible `.` at the end of domain names (for example, `cloudflare.com.`). There are [hundreds of root servers](http://www.root-servers.org/) all over the world that host the `.` file, and resolvers are [hard coded to know the IP addresses](http://www.internic.net/domain/named.root) of those servers. Cloudflare itself hosts [that file](http://www.internic.net/domain/root.zone) on all of its servers around the world through a [partnership with ISC](https://blog.cloudflare.com/f-root/).

The resolver asks one of the root servers where to find the next link in the chain â€” the top-level domain (abbreviated to TLD) or domain ending. An example of a TLD is `.com` or `.org`. Luckily, the root servers store the locations of all the TLD servers, so they can return which IP address the DNS resolver should go ask next.

The resolver then asks the TLDâ€™s servers where it can find the domain it is looking for. For example, a resolver might ask `.com` where to find `cloudflare.com`. TLDs host a file containing the location of every domain using the TLD.

Once the resolver has the final IP address, it returns the answer to the computer that asked.

This whole system is called the [Domain Name System (DNS)](https://www.cloudflare.com/learning/dns/what-is-dns/). This system includes the servers that host the information (called [authoritative DNS](https://www.cloudflare.com/learning/dns/dns-server-types/)) and the servers that seek the information (the DNS resolvers).

## Does 1.1.1.1 support ANY?

Cloudflare [stopped supporting the ANY query](https://blog.cloudflare.com/deprecating-dns-any-meta-query-type/) in 2015 as ANY queries are more often used to perpetuate large volumetric attacks against the DNS system than valid use. 1.1.1.1 returns `NOTIMPL` when asked for `qtype==ANY`.


## How does 1.1.1.1 work with DNSSEC?

1.1.1.1 is a DNSSEC validating resolver. 1.1.1.1 sends the `DO` (`DNSSEC OK`) bit on every query to convey to the authoritative server that it wishes to receive signed answers if available. 1.1.1.1 supports the signature algorithms specified in [Supported DNSKEY signature algorithms](/1.1.1.1/encryption/dnskey/).


## â€‹Does 1.1.1.1 send EDNS Client Subnet header?

1.1.1.1 is a privacy centric resolver so it does not send any client IP information and does not send the <GlossaryTooltip term="EDNS Client Subnet (ECS)">EDNS Client Subnet (ECS)</GlossaryTooltip> header to authoritative servers. The exception is the single Akamai debug domain `whoami.ds.akahelp.net` to aid in cross-provider debugging. However, Cloudflare does not send ECS to any of Akamai's production domains, such as `akamaihd.net`Â or similar.


## Does 1.1.1.1 support IPv6?

1.1.1.1 has full IPv6 support.


## What is Purge Cache?


1.1.1.1's Purge Cache tool allows you to refresh 1.1.1.1's DNS cache for domain names. To refresh the cache for a domain name, visit the [Purge Cache page](https://one.one.one.one/purge-cache/).


## What is query name minimization?

Cloudflare minimizes privacy leakage by only sending minimal query name to authoritative DNS servers. For example, if a client is looking for foo.bar.example.com, the only part of the query 1.1.1.1 discloses to .com is that we want to know whoâ€™s responsible for example.com and the zone internals stay hidden.


## What are root hints?

For decreased latency, reduced privacy leakage of queries and lower load on the DNS system, 1.1.1.1 upstreams to [locally hosted root zone files](https://blog.cloudflare.com/f-root/).

## Can IPs used by 1.1.1.1 be allowlisted?

Authoritative DNS providers may want to allowlist IP's 1.1.1.1 uses to query upstream DNS providers. The comprehensive list of IP's to allowlist is available at [https://www.cloudflare.com/ips/](https://www.cloudflare.com/ips/).

---

# 1.1.1.1 (DNS Resolver)

URL: https://developers.cloudflare.com/1.1.1.1/

import { Description, Feature, Plan, RelatedProduct } from "~/components"

<Description>

Speed up your online experience with Cloudflare's public DNS resolver.

</Description>

<Plan type="all" />

1.1.1.1 is Cloudflareâ€™s public DNS resolver. It offers a fast and private way to browse the Internet. [DNS resolvers](https://www.cloudflare.com/learning/dns/what-is-dns/) translate domains like `cloudflare.com` into the IP addresses necessary to reach the website (like `104.16.123.96`).

Unlike most DNS resolvers, 1.1.1.1 does not sell user data to advertisers. 1.1.1.1 has also been measured to be the [fastest DNS resolver available](https://www.dnsperf.com/#!dns-resolvers) â€” it is deployed in [hundreds of cities worldwide](https://www.cloudflare.com/network/), and has access to the addresses of millions of domain names on the same servers it runs on.

1.1.1.1 is completely free. Setting it up takes minutes and requires no special software.

***

## Features

<Feature header="1.1.1.1 for Families" href="/1.1.1.1/setup/#1111-for-families">
1.1.1.1 for Families has additional protection against malware and adult content.
</Feature>

<Feature header="Encrypted service" href="/1.1.1.1/encryption/">
1.1.1.1 offers an encrypted service through DNS over HTTPS (DoH) or DNS over TLS (DoT) for increased security and privacy. You can also access 1.1.1.1 [as a Tor hidden service](/1.1.1.1/other-ways-to-use-1.1.1.1/dns-over-tor/).
</Feature>

***

## Related products

<RelatedProduct header="WARP Client" href="/warp-client/" product="warp-client">
Access the Internet in a more secure and private way.
</RelatedProduct>

<RelatedProduct header="DNS" href="/dns/" product="dns">
Cloudflare's global DNS platform provides speed and resilience. DNS customers also benefit from free DNSSEC, and protection against route leaks and hijacking.
</RelatedProduct>

<RelatedProduct header="Cloudflare Spectrum" href="/spectrum/" product="spectrum">
Secure and accelerate your TCP or UDP based applications.
</RelatedProduct>

---

# IP addresses

URL: https://developers.cloudflare.com/1.1.1.1/ip-addresses/

import { Render } from "~/components"

Consider the tables below to know which IPv4 or IPv6 addresses are used by the different Cloudflare DNS resolver offerings.

For detailed guidance refer to [Set up](/1.1.1.1/setup/).

***

## 1.1.1.1

1.1.1.1 is Cloudflareâ€™s public DNS resolver. It offers a fast and private way to browse the Internet.

| IPv4                     | IPv6                                               |
| ------------------------ | -------------------------------------------------- |
| `1.1.1.1` <br/>`1.0.0.1` | `2606:4700:4700::1111` <br/>`2606:4700:4700::1001` |

Refer to [Encryption](/1.1.1.1/encryption/) to learn how to use 1.1.1.1 in an encrypted way.

***

## 1.1.1.1 for Families

1.1.1.1 for Families categorizes destinations on the Internet based on the potential threat they pose regarding malware, phishing, or other types of security risks.

For more information, refer to [1.1.1.1 for Families set up](/1.1.1.1/setup/#1111-for-families).

### Block malware

| IPv4                     | IPv6                                               |
| ------------------------ | -------------------------------------------------- |
| `1.1.1.2` <br/>`1.0.0.2` | `2606:4700:4700::1112` <br/>`2606:4700:4700::1002` |

### Block malware and adult content

| IPv4                     | IPv6                                               |
| ------------------------ | -------------------------------------------------- |
| `1.1.1.3` <br/>`1.0.0.3` | `2606:4700:4700::1113` <br/>`2606:4700:4700::1003` |

---

# Terms of use

URL: https://developers.cloudflare.com/1.1.1.1/terms-of-use/

By using 1.1.1.1 Public DNS Resolver or 1.1.1.1 for Families, you consent to be bound by the [Cloudflare Website and Online Services Terms of Use](https://www.cloudflare.com/website-terms/).

If you are an [Internet Service Provider (ISP) or network equipment provider](/1.1.1.1/infrastructure/network-operators/), you agree to provide proper attribution to Cloudflare in accordance with our Trademark Guidelines using our Public DNS Resolver. Please reach out to `resolver@cloudflare.com` for such logo requests.

---

# Cloudflare Aegis

URL: https://developers.cloudflare.com/aegis/

import { CardGrid, Description, GlossaryTooltip, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Description>

Leverage dedicated IPs to improve your origin security and implement Zero Trust.
</Description>

<Plan type="enterprise" />

Cloudflare Aegis provides dedicated egress IPs (from Cloudflare to your origin) for your layer 7 [WAF](/waf/) and <GlossaryTooltip term="content delivery network (CDN)">CDN</GlossaryTooltip> services, as well as [Spectrum](/spectrum/). The egress IPs are reserved exclusively for your account so that you can increase your origin security by only allowing traffic from a small list of IP addresses.

Both [BYOIP](/byoip) and Cloudflare-leased IPs are supported by Cloudflare Aegis.


:::caution[Availability]
Cloudflare Aegis is available in early access to Enterprise customers. Contact your account team to request access.
:::


***

## Related products

<RelatedProduct header="Cloudflare Access" href="/cloudflare-one/policies/access/" product="cloudflare-one">
Cloudflare Access determines who can reach your application by applying the Access policies you configure.
</RelatedProduct>

<RelatedProduct header="Cloudflare Tunnel" href="/cloudflare-one/connections/connect-networks/" product="cloudflare-one">
Cloudflare Tunnel provides you with a secure way to connect your resources to Cloudflare without a publicly routable IP address.
</RelatedProduct>

<RelatedProduct header="Authenticated Origin Pulls" href="/ssl/origin-configuration/authenticated-origin-pull/" product="ssl">
Authenticated Origin Pulls gives you the ability to perform mutual TLS between Cloudflare and your origin environment, providing cryptographically verifiable proof of the source of the traffic you receive.
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Introductory blog post" href="https://blog.cloudflare.com/cloudflare-aegis/" icon="open-book">

Deep dive into use cases where Aegis can help secure enterprise origins.
</LinkTitleCard>

<LinkTitleCard title="Multi-Vendor Architecture" href="/reference-architecture/architectures/multi-vendor/#connectivity-options" icon="document">

Reference Architecture for multi-vendor application security and performance.
</LinkTitleCard>

</CardGrid>

---

# Troubleshooting

URL: https://developers.cloudflare.com/1.1.1.1/troubleshooting/

import { Render } from "~/components"

This guide will help you diagnose and resolve common issues with Cloudflare's DNS Resolver. Before proceeding with manual troubleshooting steps, you can [verify your connection](/1.1.1.1/check/) to automatically gather relevant information.

## Name resolution issues

### Linux/macOS

```sh
# Test DNS resolution
dig example.com @1.1.1.1
dig example.com @1.0.0.1
dig example.com @8.8.8.8

# Check connected nameserver
dig +short CHAOS TXT id.server @1.1.1.1
dig +short CHAOS TXT id.server @1.0.0.1

# Optional: Network information
dig @ns3.cloudflare.com whoami.cloudflare.com txt +short
```

### Windows

```sh
# Test DNS resolution
nslookup example.com 1.1.1.1
nslookup example.com 1.0.0.1
nslookup example.com 8.8.8.8

# Check connected nameserver
nslookup -class=chaos -type=txt id.server 1.1.1.1
nslookup -class=chaos -type=txt id.server 1.0.0.1

# Optional: Network information
nslookup -type=txt whoami.cloudflare.com ns3.cloudflare.com
```

**Note:** The network information command reveals your IP address. Only include this in reports to Cloudflare if you are comfortable sharing this information.

For additional analysis, you can generate a [DNSViz](http://dnsviz.net/) report for the domain in question.

## Connectivity and routing issues

Before reporting connectivity issues:

1. Search for existing reports from your country and ISP.
2. Run traceroutes to both Cloudflare DNS resolvers.

### Linux/macOS

```sh
# Basic connectivity tests
traceroute 1.1.1.1
traceroute 1.0.0.1

# If reachable, check nameserver identity
dig +short CHAOS TXT id.server @1.1.1.1
dig +short CHAOS TXT id.server @1.0.0.1

# TCP connection tests
dig +tcp @1.1.1.1 id.server CH TXT
dig +tcp @1.0.0.1 id.server CH TXT
```

### Windows

```sh
# Basic connectivity tests
tracert 1.1.1.1
tracert 1.0.0.1

# If reachable, check nameserver identity
nslookup -class=chaos -type=txt id.server 1.1.1.1
nslookup -class=chaos -type=txt id.server 1.0.0.1

# TCP connection tests
nslookup -vc -class=chaos -type=txt id.server 1.1.1.1
nslookup -vc -class=chaos -type=txt id.server 1.0.0.1
```

## DNS-over-TLS (DoT) troubleshooting

### Linux/macOS

```sh
# Test TLS connectivity
openssl s_client -connect 1.1.1.1:853
openssl s_client -connect 1.0.0.1:853

# Test DNS resolution over TLS
kdig +tls @1.1.1.1 id.server CH TXT
kdig +tls @1.0.0.1 id.server CH TXT
```

### Windows

Windows does not include a standalone DoT client. You can test TLS connectivity using OpenSSL after installing it manually.

## DNS-over-HTTPS (DoH) troubleshooting

### Linux/macOS

```sh
curl -H 'accept: application/dns-json' 'https://cloudflare-dns.com/dns-query?name=cloudflare.com&type=AAAA'
```

### Windows

```powershell
(Invoke-WebRequest -Uri 'https://cloudflare-dns.com/dns-query?name=cloudflare.com&type=AAAA').RawContent
```

## Common issues

### First hop failures

If your traceroute fails at the first hop, the issue is likely hardware-related. Your router may have a hardcoded route for 1.1.1.1. When reporting this issue, include:

- Router make and model
- ISP name
- Any relevant router configuration details

## Additional resources

- [1.1.1.1 DNS Resolver homepage](https://1.1.1.1)
- [DNS over TLS documentation](/1.1.1.1/encryption/dns-over-tls/)
- [Diagnostic tool](https://one.one.one.one/help/)

---

# Setup

URL: https://developers.cloudflare.com/aegis/setup/

You can control Aegis enablement on your zones via API. If you are not familiar with how Cloudflare API works, refer to [Fundamentals](/fundamentals/api/).

:::caution[Availability]
Cloudflare Aegis is available in early access to Enterprise customers. Contact your account team to request access.
:::

## Requirements

- The Aegis zone setting endpoint is only available within Cloudflare accounts that own leased IPs, or accounts to which a [BYOIP prefix](/byoip/) has been delegated. If you wish to use Aegis for zones that do not meet this criteria, contact your account team.
- Each Aegis pool can consist of either IPs from a BYOIP prefix or Cloudflare-leased IPs. A single dedicated egress pool cannot contain both BYOIPs and leased IPs.

## Steps

1. Contact your account team to get the ID for your dedicated egress pool.
2. Make a `PATCH` request to the [Edit Zone Setting](/api/resources/zones/subresources/settings/methods/edit/) endpoint:
- Specify `aegis` as the setting ID in the URL.
- In the request body, set `enabled` to `true` and use the ID from the previous step as `pool_id`.

```bash
--data '{
  "id": "aegis",
  "value": {
    "enabled": true,
    "pool_id": "<YOUR_EGRESS_POOL_ID>"
  },
}'
```

---

# Build Agents on Cloudflare

URL: https://developers.cloudflare.com/agents/

import {
	CardGrid,
	Description,
	Feature,
	LinkButton,
	LinkTitleCard,
	PackageManagers,
	Plan,
	RelatedProduct,
	Render,
	TabItem,
	Tabs,
} from "~/components";

Build and deploy AI-powered Agents on Cloudflare that can autonomously perform tasks, communicate with clients in real time, persist state, execute long-running and repeat tasks on a schedule, send emails, run asynchronous workflows, browse the web, query data from your Postgres database, call AI models, support human-in-the-loop use-cases, and more.

#### Ship your first Agent

Use the agent started template to create your first Agent with the `agents-sdk`:

```sh
# install it
npm create cloudflare@latest agents-starter -- --template=cloudflare/agents-starter
# and deploy it
npx wrangler@latest deploy
```

Head to the guide on [building a chat agent](/agents/getting-started/build-a-chat-agent) to learn how to build and deploy an Agent to prod.

If you're already building on [Workers](/workers/), you can install the `agents-sdk` package directly into an existing project:

```sh
npm i agents-sdk
```

Dive into the [Agent SDK reference](/agents/api-reference/sdk/) to learn more about how to use the `agents-sdk` package and defining an `Agent`.

#### Why build agents on Cloudflare?

We built the `agents-sdk` with a few things in mind:

- **Batteries (state) included**: Agents come with [built-in state management](/agents/examples/manage-and-sync-state/), with the ability to automatically sync state between an Agent and clients, trigger events on state changes, and read+write to each Agent's SQL database.
- **Communicative**: You can connect to an Agent via [WebSockets](/agents/examples/websockets/) and stream updates back to client in real-time. Handle a long-running response from a reasoning model, the results of an [asynchronous workflow](/agents/examples/run-workflows/), or build a chat app that builds on the `useAgent` hook included in the `agents-sdk`.
- **Extensible**: Agents are code. Use the [AI models](/agents/examples/using-ai-models/) you want, bring-your-own headless browser service, pull data from your database hosted in another cloud, add your own methods to your Agent and call them.

Agents built with `agents-sdk` can be deployed directly to Cloudflare and run on top of [Durable Objects](/durable-objects/) â€” which you can think of as stateful micro-servers that can scale to tens of millions â€” and are able to run wherever they need to. Run your Agents close to a user for low-latency interactivity, close to your data for throughput, and/or anywhere in between.

***

#### Build on the Cloudflare Platform

<RelatedProduct header="Workers" href="/workers/" product="workers">

Build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale.

</RelatedProduct>

<RelatedProduct header="AI Gateway" href="/ai-gateway/" product="ai-gateway">

Observe and control your AI applications with caching, rate limiting, request retries, model fallback, and more.

</RelatedProduct>

<RelatedProduct header="Vectorize" href="/vectorize/" product="vectorize">

Build full-stack AI applications with Vectorize, Cloudflareâ€™s vector database. Adding Vectorize enables you to perform tasks such as semantic search, recommendations, anomaly detection or can be used to provide context and memory to an LLM.

</RelatedProduct>

<RelatedProduct header="Workers AI" href="/workers-ai/" product="workers-ai">

Run machine learning models, powered by serverless GPUs, on Cloudflare's global network.

</RelatedProduct>

<RelatedProduct header="Workflows" href="/workflows/" product="workflows">

Build stateful agents that guarantee executions, including automatic retries, persistent state that runs for minutes, hours, days, or weeks.

</RelatedProduct>

---

# Changelog

URL: https://developers.cloudflare.com/ai-gateway/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/ai-gateway.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Architectures

URL: https://developers.cloudflare.com/ai-gateway/demos/

import { GlossaryTooltip, ResourcesBySelector } from "~/components";

Learn how you can use AI Gateway within your existing architecture.

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use AI Gateway:

<ResourcesBySelector
	types={[
		"reference-architecture",
		"design-guide",
		"reference-architecture-diagram",
	]}
	products={["AI Gateway"]}
/>

---

# Get started

URL: https://developers.cloudflare.com/ai-gateway/get-started/

import { Details, DirectoryListing, LinkButton, Render } from "~/components";

In this guide, you will learn how to create your first AI Gateway. You can create multiple gateways to control different applications.

## Prerequisites

Before you get started, you need a Cloudflare account.

<LinkButton variant="primary" href="https://dash.cloudflare.com/sign-up">
	Sign up
</LinkButton>

## Create gateway

Then, create a new AI Gateway.

<Render file="create-gateway" />

## Choosing gateway authentication

When setting up a new gateway, you can choose between an authenticated and unauthenticated gateway. Enabling an authenticated gateway requires each request to include a valid authorization token, adding an extra layer of security. We recommend using an authenticated gateway when storing logs to prevent unauthorized access and protect against invalid requests that can inflate log storage usage and make it harder to find the data you need. Learn more about setting up an [Authenticated Gateway](/ai-gateway/configuration/authentication/).

## Connect application

Next, connect your AI provider to your gateway.

AI Gateway offers multiple endpoints for each Gateway you create - one endpoint per provider, and one Universal Endpoint. To use AI Gateway, you will need to create your own account with each  provider and provide your API key. AI Gateway acts as a proxy for these requests, enabling observability, caching, and more.

Additionally, AI Gateway has a [WebSockets API](/ai-gateway/configuration/websockets-api/) which provides a single persistent connection, enabling continuous communication. This API supports all AI providers connected to AI Gateway, including those that do not natively support WebSockets.

Below is a list of our supported model providers:

<DirectoryListing folder="ai-gateway/providers" />

If you do not have a provider preference, start with one of our dedicated tutorials:

- [OpenAI](/ai-gateway/integrations/aig-workers-ai-binding/)
- [Workers AI](/ai-gateway/tutorials/create-first-aig-workers/)

## View analytics

Now that your provider is connected to the AI Gateway, you can view analytics for requests going through your gateway.

<Render file="analytics-overview" /> <br />

<Render file="analytics-dashboard" />

:::note[Note]

The cost metric is an estimation based on the number of tokens sent and received in requests. While this metric can help you monitor and predict cost trends, refer to your providerâ€™s dashboard for the most accurate cost details.

:::

## Next steps

- Learn more about [caching](/ai-gateway/configuration/caching/) for faster requests and cost savings and [rate limiting](/ai-gateway/configuration/rate-limiting/) to control how your application scales.
- Explore how to specify model or provider [fallbacks](/ai-gateway/configuration/fallbacks/) for resiliency.
- Learn how to use low-cost, open source models on [Workers AI](/ai-gateway/providers/workersai/) - our AI inference service.

---

# Header Glossary

URL: https://developers.cloudflare.com/ai-gateway/glossary/

import { Glossary } from "~/components";

AI Gateway supports a variety of headers to help you configure, customize, and manage your API requests. This page provides a complete list of all supported headers, along with a short description

<Glossary product="ai-gateway" />

## Configuration hierarchy

Settings in AI Gateway can be configured at three levels: **Provider**, **Request**, and **Gateway**. Since the same settings can be configured in multiple locations, the following hierarchy determines which value is applied:

1. **Provider-level headers**:
   Relevant only when using the [Universal Endpoint](/ai-gateway/providers/universal/), these headers take precedence over all other configurations.
2. **Request-level headers**:
   Apply if no provider-level headers are set.
3. **Gateway-level settings**:
   Act as the default if no headers are set at the provider or request levels.

This hierarchy ensures consistent behavior, prioritizing the most specific configurations. Use provider-level and request-level headers for more fine-tuned control, and gateway settings for general defaults.

---

# Overview

URL: https://developers.cloudflare.com/ai-gateway/

import {
	CardGrid,
	Description,
	Feature,
	LinkTitleCard,
	Plan,
	RelatedProduct,
} from "~/components";

<Description>

Observe and control your AI applications.

</Description>

<Plan type="all" />

Cloudflare's AI Gateway allows you to gain visibility and control over your AI apps. By connecting your apps to AI Gateway, you can gather insights on how people are using your application with analytics and logging and then control how your application scales with features such as caching, rate limiting, as well as request retries, model fallback, and more. Better yet - it only takes one line of code to get started.

Check out the [Get started guide](/ai-gateway/get-started/) to learn how to configure your applications with AI Gateway.

## Features

<Feature header="Analytics" href="/ai-gateway/observability/analytics/" cta="View Analytics">

View metrics such as the number of requests, tokens, and the cost it takes to run your application.

</Feature>

<Feature header="Logging" href="/ai-gateway/observability/logging/" cta="View Logging">

Gain insight on requests and errors.

</Feature>

<Feature header="Caching" href="/ai-gateway/configuration/caching/">

Serve requests directly from Cloudflare's cache instead of the original model provider for faster requests and cost savings.

</Feature>

<Feature header="Rate limiting" href="/ai-gateway/configuration/rate-limiting">

Control how your application scales by limiting the number of requests your application receives.

</Feature>

<Feature header="Request retry and fallback" href="/ai-gateway/configuration/fallbacks/">

Improve resilience by defining request retry and model fallbacks in case of an error.

</Feature>

<Feature header="Your favorite providers" href="/ai-gateway/providers/">

Workers AI, OpenAI, Azure OpenAI, HuggingFace, Replicate, and more work with AI Gateway.

</Feature>

---

## Related products

<RelatedProduct header="Workers AI" href="/workers-ai/" product="workers-ai">

Run machine learning models, powered by serverless GPUs, on Cloudflareâ€™s global network.

</RelatedProduct>

<RelatedProduct header="Vectorize" href="/vectorize/" product="vectorize">

Build full-stack AI applications with Vectorize, Cloudflareâ€™s vector database. Adding Vectorize enables you to perform tasks such as semantic search, recommendations, anomaly detection or can be used to provide context and memory to an LLM.

</RelatedProduct>

## More resources

<CardGrid>

<LinkTitleCard
	title="Developer Discord"
	href="https://discord.cloudflare.com"
	icon="discord"
>
	Connect with the Workers community on Discord to ask questions, show what you
	are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="Use cases" href="/use-cases/ai/" icon="document">
	Learn how you can build and deploy ambitious AI applications to Cloudflare's
	global network.
</LinkTitleCard>

<LinkTitleCard
	title="@CloudflareDev"
	href="https://x.com/cloudflaredev"
	icon="x.com"
>
	Follow @CloudflareDev on Twitter to learn about product announcements, and
	what is new in Cloudflare Workers.
</LinkTitleCard>

</CardGrid>

---

# Analytics

URL: https://developers.cloudflare.com/analytics/

import { Feature, RelatedProduct } from "~/components"

Cloudflare visualizes the metadata collected by our products in the Cloudflare dashboard. Refer to [Types of analytics](/analytics/types-of-analytics/) for more information about the various types of analytics and where they exist in the dashboard.

***

## Features

<Feature header="Workers Analytics Engine" href="/analytics/analytics-engine/">
Send unlimited-cardinality data from your Worker to a time-series database. Query it with SQL. 
</Feature>

<Feature header="Account and zone analytics" href="/analytics/account-and-zone-analytics/">
Provides details about the requests and traffic related to your Cloudflare accounts and zones. 
</Feature>

<Feature header="Cloudflare Network Analytics" href="/analytics/network-analytics/">
Provides near real-time visibility into network and transport-layer traffic patterns and DDoS attacks. 
</Feature>

<Feature header="GraphQL Analytics API" href="/analytics/graphql-api/">
Provides all of your performance, security, and reliability data from one endpoint. Select exactly what you need, from one metric for a domain to multiple metrics aggregated for your account. 
</Feature>

***

## Related products

<RelatedProduct header="Workers" href="/workers/" product="workers">
Cloudflare Workers allows developers to build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale. 
</RelatedProduct>

<RelatedProduct header="Logs" href="/logs/" product="logs">
Detailed logs that contain metadata generated by Cloudflare products helpful for debugging, identifying configuration adjustments, and creating analytics. 
</RelatedProduct>

---

# API Gateway

URL: https://developers.cloudflare.com/api-shield/api-gateway/

API Gateway is a package of features that will do everything for your APIs, including:

- **Security**: Protect your API from malicious traffic with [API Discovery](/api-shield/security/api-discovery/), [Schema Validation](/api-shield/security/schema-validation/), [mTLS validation](/api-shield/security/mtls/), and more.
- **Management and monitoring**: Streamline API management with [Endpoint Management](/api-shield/management-and-monitoring/) and [tools](https://blog.cloudflare.com/api-gateway/) like analytics, routing, and authentication.
- **Logging, quota management, and more**: All of Cloudflare's [established features](https://blog.cloudflare.com/api-gateway/), like caching, load balancing, and log integrations work natively with API Gateway.

For more details about API Gateway, refer to the [introductory blog post](https://blog.cloudflare.com/api-gateway/).

---

# Types of analytics

URL: https://developers.cloudflare.com/analytics/types-of-analytics/

import { Badge } from "~/components";

Cloudflare Analytics is a comprehensive product that encompasses all metadata generated by the Cloudflare network. You can access these insights through the Cloudflare dashboard. Depending on where in the dashboard you are, it will show you different aspects from the collected metadata.

## Account-level analytics

### Account Analytics (beta)

Available under **Analytics & Logs** in your Cloudflare dashboard when you log in, Account Analytics (beta) shows you an [overview of traffic for all domains](/analytics/account-and-zone-analytics/account-analytics/) under your Cloudflare account, such as requests and bandwidth by country, information related to security, cache, and errors, among others. To access Account Analytics, [log in to the Cloudflare dashboard](https://dash.cloudflare.com/login), select the appropriate account, and go to **Analytics & Logs** > **Account Analytics**.

### Network Analytics

Network Analytics provides [visibility into network and transport-layer traffic patterns, and DDoS attacks](/analytics/network-analytics/).

The Network Analytics dashboard is only available for customers with an Enterprise domain plan who use [Spectrum](/spectrum/), [Magic Transit](/magic-transit/), or [Bring Your Own IP (BYOIP)](/byoip/).

### Web Analytics

Web Analytics (formerly known as Browser Insights) [provides free, privacy-first analytics for your website](/web-analytics/). Web Analytics does not collect your visitor's personal data, and allows you to have a detailed view into the performance of web pages as experienced by your visitors.

### Carbon Impact Report

Carbon Impact Report gives you a [report on carbon savings](https://blog.cloudflare.com/understand-and-reduce-your-carbon-impact-with-cloudflare/) from using Cloudflare services versus Internet averages for your usage volume.

Cloudflare is committed to use 100% renewable energy sources, but also to [remove all greenhouse gases emitted](https://blog.cloudflare.com/cloudflare-committed-to-building-a-greener-internet/) as a result of powering our network since 2010.

## Analytics related to specific properties

Access aggregated traffic, security, and performance metrics for each domain proxied through Cloudflare. To access these analytics, [log in to the Cloudflare dashboard](https://dash.cloudflare.com/login), select your account and domain, and go to the **Analytics & Logs** section.

Data available under the **Analytics & Logs** section includes:

* **HTTP Traffic** - Requests, Data transfer, Page views, Visits, and API requests.
* **Security** - Total Threats, Top Crawlers/Bots, Rate Limiting, Total Threats Stopped.
* **Performance** - Origin Performance, Bandwidth Saved.
* **Edge Reachability** - [Last mile insights](/network-error-logging/) for Enterprise customers.
* **Workers** - [Detailed information](/workers/observability/metrics-and-analytics/) related to your Workers per zone, and Workers KV per account.
* **Logs** - [Detailed logs](/logs/) of the metadata generated by Cloudflare products for Enterprise customers.
* **Instant logs** - [Live stream of traffic](/logs/instant-logs/) for your domain. Enterprise customers can access this live stream from the Cloudflare dashboard or from a command-line interface (CLI).

## Product analytics

Beyond the analytics provided for your properties, you can also access analytics related to specific products:

* [Bot Analytics](/bots/bot-analytics/) - Shows which requests are associated with known bots, likely automated traffic, likely human traffic, and more.
* [Cache Analytics](/cache/performance-review/cache-analytics/) - Insights to that help determine if resources are missing from cache, expired, or ineligible for caching.
* [Security Events](/waf/analytics/security-events/) - Highlights attack and mitigation metrics detected by the Cloudflare WAF and HTTP DDoS protection systems.
* [Security Analytics](/waf/analytics/security-analytics/) - Displays information about all incoming HTTP requests, including those not affected by security measures (for example, from the WAF and DDoS protection systems).
* [Load Balancing Analytics](/load-balancing/reference/load-balancing-analytics/) - Features metrics to help gain insights into traffic load balancer steering decisions.

## GraphQL APIs

If you would like to have more control over how you visualize the analytic and log information available on the Cloudflare dashboard, use the [GraphQL Analytics API](/analytics/graphql-api/) to build customized views. This API replaces and expands on the previous Zone Analytics API.

---

# Changelog

URL: https://developers.cloudflare.com/api-shield/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/api-shield.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# FAQ

URL: https://developers.cloudflare.com/api-shield/frequently-asked-questions/

## Why are my API endpoints not found by API Discovery?

In most cases, this is due to the system not observing enough valid requests over a continuous period.

API Discovery only looks at requests that satisfy all of the following criteria:

1. Requests must return `2XX` response codes from the edge.
2. Requests must not come directly from Cloudflare Workers.
3. At least 500 requests are made to the discovered endpoint within a 10 day period.

Endpoints discovered using session identifiers will be labeled as such in the dashboard. If the endpoints are not discovered through session identifiers, they will be discovered using our machine learning-based [API Discovery](/api-shield/security/api-discovery/).

---

## How does Cloudflare calculate the recommended rate limit for my endpoint?


Cloudflare uses both the volume and frequency of traffic to guide your recommended rate. We calculate the recommended rate value throughout the day, and the new calculation may equal the existing recommendation due to similar traffic profiles existing on your API. When we recalculate, we look at requests that happened in the last 24 hours.

You can view the `P50`/`95`/`99` of your request count for more details under an endpointâ€™s expanded view.

---

## Will I be able to access an endpointâ€™s data after I delete it?

No. Cloudflare will stop tracking performance data when you delete an endpoint and its previous data will not be stored. This means that if you save this endpoint again, the metrics will start tracking from the point that you save it.

---

### Why do I not receive threshold recommendations for my discovered API endpoints?

Thresholds can only be recommended for endpoints that receive sufficient levels of traffic that meet the following criteria:

- Only requests with the same criteria as API Discovery are considered.
- If traffic has been erratic or intermittent to this endpoint, the threshold might not show up. Cloudflare needs endpoints to receive sufficient valid traffic in any 24-hour period in the last 7 days or since the initial discovery of the endpoint to make statistically safe threshold suggestions.
- Cloudflare also requires at least 50 distinct sessions to have accessed the endpoint in any 24-hour period in the last 7 days or since the initial discovery of the endpoint. To detect sessions, you must set up [session identifiers](/api-shield/get-started/#session-identifiers).

If you do not receive threshold recommendations for a discovered endpoint, you will see one of the following error codes:

- `404 response`: Cloudflare has not seen sufficient valid traffic for this zone to generate recommendations.
- `551 response`: Cloudflare has successfully generated recommendations at some point in the past, but we have not seen sufficient recent valid traffic to provide up-to-date recommendations.

---

## Does API Shield work for JDCloud customers?

Not currently.

---

## What version of OpenAPI specification do you support?

The importing ([Schema Validation](/api-shield/security/schema-validation/)) and exporting ([Schema Learning](/api-shield/management-and-monitoring/#endpoint-schema-learning)) of OpenAPI schemas from our product to customers is done using **OpenAPI v3.0**. Any specifications using patched versions (3.0.x) are compatible as well.

---

## Why am I not seeing latency metrics?

Latency metrics currently are not supported when a Cloudflare Worker is running on the URL, as the requests are not passed directly to your origin.

Some Cloudflare products such as [Waiting Room](/waiting-room/) are built on top of Workers, so the same limitations apply to applications using these products.

---

# Glossary

URL: https://developers.cloudflare.com/api-shield/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's API Shield documentation.

<Glossary product="api-shield" />

---

# Get started with API Shield

URL: https://developers.cloudflare.com/api-shield/get-started/

import { GlossaryTooltip, Render } from "~/components"

This guide will help you set up API Shield to identify and address API security best practices.

:::note

Enabling API Shield features will have no impact on your traffic until you choose to move a setting from `log` to `block` mode. 
:::

## Session identifiers

<Render file="session-identifiers" />

<Render file="required-session-identifiers" />

### To set up session identifiers

<Render file="set-up-session-identifiers" />

## Upload a schema using Schema Validation (optional)

Schema Validation protects your APIs by ensuring only requests matching your <GlossaryTooltip term="API schema">API schema</GlossaryTooltip> are allowed to communicate with your origin.

While not strictly required, uploading a pre-existing schema will offer the chance to automatically add endpoints to Endpoint Management. If you already have a schema, you can upload it to [Schema Validation](/api-shield/security/schema-validation/).

:::note

It is recommended to start with Schema Validation rules set to `log` to review logged requests in **Security** > **Events**. When you are confident that only the correct requests are logged, you should switch the rule to `block`. 
:::

If you do not have a schema to upload, continue reading this guide to learn how to generate a schema with API Shield.

## Enable the Sensitive Data Detection ruleset and accompanying rules

API Shield works with Cloudflare WAFâ€™s [Sensitive Data Detection](/api-shield/management-and-monitoring/#sensitive-data-detection) ruleset to identify <GlossaryTooltip term="API endpoint">API endpoints</GlossaryTooltip> that return sensitive data such as social security or credit card numbers in their HTTP responses. Monitoring these endpoints can be critical to ensuring sensitive data is returned only when expected.

:::note

A subscription is required for Sensitive Data Detection. Contact your account team if you are not entitled for Sensitive Data Detection. 
:::

You can identify endpoints returning sensitive data by selecting the icon next to the path in a row. Expand the endpoint to see details on which rules were triggered and view more information by exploring events in **Firewall Events**.

## Add your discovered endpoints to Endpoint Management

Cloudflareâ€™s machine learning models have already inspected your existing traffic for the presence of API endpoints. By adding endpoints from API Discovery to Endpoint Management, you can unlock further security, visibility, and management features of the platform. Endpoint Management monitors the health of your API endpoints by saving, updating, and monitoring performance metrics.

:::note

Schema Validation, Schema Learning, JWT Validation, Sequence Analytics, Sequence Mitigation, and rate limit recommendations only run on endpoints saved to Endpoint Management. 
:::

You can save your endpoints directly from [API Discovery](/api-shield/management-and-monitoring/#add-endpoints-from-api-discovery), [Schema Validation](/api-shield/management-and-monitoring/#add-endpoints-from-schema-validation), or [manually](/api-shield/management-and-monitoring/#add-endpoints-manually) by method, path, and host.

This will add the specified endpoints to your list of managed endpoints. You can view your list of saved endpoints in the **Endpoint Management** page.

Cloudflare will aggregate [performance data](/api-shield/management-and-monitoring/#endpoint-analysis) and security data on your endpoint once it is saved.

### Allow the system to learn your traffic patterns

Cloudflare will inspect your API traffic and begin to learn its schema over the next 24 hours after adding an endpoint. Depending on how much traffic an individual endpoint sees, our confidence in the resulting schema may differ.

Cloudflare will also use the configured session identifiers to suggest rate limits per endpoint.

For best results, allow at least 24 hours after adding endpoints before proceeding to the following steps.

We recommend proceeding with [additional configurations](/api-shield/get-started/#additional-configuration) if this is your first time setting up API Shield and have added your first API endpoints to Endpoint Management.

## Add rate limits to your most sensitive endpoints

[Rate limiting rules](/waf/rate-limiting-rules/) allow you to define rate limits for requests matching an expression, and choose the action to perform when those rate limits are reached.

You can observe Cloudflare suggested rate limits in Endpoint Management for endpoints using session identifiers. Unlike many security tools, these recommended rate limits are per-endpoint and per-session, not site-wide and not based on IP address. When creating a rule, it will be based on only traffic to that specific endpoint from unique visitors during their session. This feature allows you to be very specific and targeted with your rate limit enforcement, both lowering abusive traffic and false positives due to broadly scoped rules.

## Import a learned schema to Schema Validation

Cloudflare learns schema parameters via traffic inspection for all endpoints stored in Endpoint Management. You can export OpenAPI schemas in OpenAPI v3.0.0 format by hostname.

By importing the learned schema, you can protect API endpoints found via API Discovery that were never previously possible to protect due to not knowing about their presence or schema.

You can import the learned schema of an entire hostname using the [Cloudflare dashboard](/api-shield/security/schema-validation/#add-validation-by-applying-a-learned-schema-to-an-entire-hostname). Alternatively, you can [apply learned schemas to individual endpoints](/api-shield/security/schema-validation/#add-validation-by-applying-a-learned-schema-to-a-single-endpoint). Before applying the learned schema, Cloudflare suggests exporting the schema to review what will validate your traffic.

## Export a learned schema from Endpoint Management

Learned schemas will always include the listed hostname in the servers section, all endpoints by host, method, and path, and detected path variables. They can also potentially include detected query parameters and their format. You can optionally include API Shieldâ€™s rate limit threshold recommendations.

You can export your learned schemas in the [Cloudflare dashboard](/api-shield/management-and-monitoring/#export-a-schema) or via the [API](/api/resources/api_gateway/subresources/schemas/methods/list/).

## View and configure Sequence Analytics

[Sequence Analytics](/api-shield/security/sequence-analytics/) surfaces a subset of important API request sequences found in your API traffic over time.

You can observe the top sequences in your API traffic that contain endpoints stored in Endpoint Management. We rank sequences by Correlation Score. High-scoring sequences contain API requests which are likely to occur together in order.

[Sequence Mitigation](/api-shield/security/sequence-mitigation/) allows you to enforce request patterns for authenticated clients communicating with your API. Use Sequence Analytics to better understand the request sequences used by your API clients.

You should apply all possible API Shield protections (rate limiting suggestions, Schema Validation, JWT Validation, and mTLS) to API endpoints found in high correlation score sequences that make up the critical request flows in your application. You should also check their specific endpoint order with your development team.

For more information, refer to [Detecting API abuse automatically using sequence analysis](https://blog.cloudflare.com/api-sequence-analytics) blog post.

## Additional configuration

### Set up JSON Web Tokens (JWT) Validation

Use the Cloudflare API to configure [JSON Web Tokens Validation](/api-shield/security/jwt-validation/), which validates the integrity and validity of JWTs sent by clients to your API or web application.

### Set up GraphQL Malicious Query Protection

If your origin uses GraphQL, you may consider setting limits on GraphQL query size and depth.

[GraphQL malicious query protection](/api-shield/security/graphql-protection/configure/) scans your GraphQL traffic for queries that could overload your origin and result in a denial of service. Customers can build rules that limit the query depth and size of incoming GraphQL queries in order to block suspiciously large or complex queries.

For more information, refer to the [blog post](https://blog.cloudflare.com/protecting-graphql-apis-from-malicious-queries/).

### Mutual TLS (mTLS) authentication

If you operate an API that requires or would benefit from an extra layer of protection, you may consider using Mutual TLS (mTLS).

[Mutual TLS (mTLS) authentication](/api-shield/security/mtls/) uses client certificates to ensure traffic between client and server is bidirectionally secure and trusted. mTLS also allows requests that do not authenticate via an identity provider, such as Internet-of-things (IoT) devices, to demonstrate they can reach a given resource.

---

# Overview

URL: https://developers.cloudflare.com/api-shield/

import { Description, Feature, Plan, RelatedProduct, Render } from "~/components"

<Description>

Identify and address your API vulnerabilities. 
</Description>

<Plan type="ent-add-on" />

<Render file="non-contract-enablement" product="fundamentals" />

## Why care about API security?

<Render file="why-care" product="api-shield" />

Refer to the [Get started](/api-shield/get-started/) guide to set up API Shield.

## Features

<Feature header="Security features" href="/api-shield/security/">
Secure your APIs using API Shield's security features. 
</Feature>

<Feature header="Management, monitoring, and more" href="/api-shield/api-gateway/">
A package of features that will do everything for your APIs. 
</Feature>

## Availability

Cloudflare API Security products are available to Enterprise customers only, though anyone can set up [Mutual TLS](/api-shield/security/mtls/) with a Cloudflare-managed certificate authority.

The full API Shield security suite is available as an Enterprise-only paid add-on, but all customers can access [Endpoint Management](/api-shield/management-and-monitoring/) and [Schema Validation](/api-shield/security/schema-validation/) functionalities.

## Related products

<RelatedProduct header="DDoS Protection" href="/ddos-protection/" product="ddos-protection">
Cloudflare DDoS protection secures websites, applications, and entire networks while ensuring the performance of legitimate traffic is not compromised. 
</RelatedProduct>

---

# Plans

URL: https://developers.cloudflare.com/api-shield/plans/

Free, Pro, Business, and Enterprise customers without an API Gateway subcription can access [Endpoint Management](/api-shield/management-and-monitoring/) and [Schema Validation](/api-shield/security/schema-validation/), but no other [API Gateway](/api-shield/api-gateway/) features.

To subscribe to API Gateway, upgrade to an Enterprise plan and contact your account team.

Limits to endpoints apply to Endpoint Management and Schema Validation. Refer to the table below for limits based on your zone plan. 

| Plan type | Saved endpoints | Uploaded schemas | Total uploaded schema size | Rule action |
| --- | --- | --- | --- | --- |
| **Free** | 100 | 5 | 200 kB | `Block` only |
| **Pro** | 250 | 5 | 500 kB | `Block` only |
| **Business** | 500 | 10 | 2 MB | `Block` only |
| **Enterprise without API Gateway** | 500 | 5 | 5 MB | `Log` or `Block` |
| **Enterprise with API Gateway** | 10,000 | 10+ | 10+ MB | `Log` or `Block` |

---

# Argo for Packets

URL: https://developers.cloudflare.com/argo-smart-routing/argo-for-packets/

Argo for Packets provides IP layer network optimizations to supercharge your Cloudflare network services products like [Magic Transit](/magic-transit/), [Magic WAN](/magic-wan/), and [Cloudflare for Offices](https://blog.cloudflare.com/cloudflare-for-offices/).

Argo for Packets dynamically chooses the best possible path through Cloudflare's network and looks at every path back from every Cloudflare data center back to your origin, down to the individual network path. Argo compares existing Layer 4 traffic and network analytics across all of these unique paths to determine the fastest, most available path.

Customers with multiple locations will especially benefit from Argo for Packets because it optimizes complex paths on the Internet and makes them just as fast as the other paths.

To begin using Argo for Packets, contact your account manager.

---

# Analytics

URL: https://developers.cloudflare.com/argo-smart-routing/analytics/

Cloudflare provides analytics to show the performance benefits of Argo Smart Routing.

You can access Argo analytics for your domain in the [Cloudflare dashboard](https://dash.cloudflare.com/) at **Analytics** > **Performance**. For information on all analytics in the dashboard, refer to [Analytics](/analytics/).

## How it works

Analytics collects data based on the time-to-first-byte (TTFB) from your origin to the Cloudflare network. TTFB is the delay between when Cloudflare sends a request to your server and when it receives the first byte in response. Argo Smart Routing optimizes your server's network transit time to minimize this delay.

:::note


Detailed performance data within **Origin Performance (Argo)** will only display if Argo has routed at least 500 origin requests within the last 48 hours.


:::

## Types of analytics

The dashboard displays two different views for performance data:

* **Origin Response Time**: A histogram shows response time from your origin to the Cloudflare network. The blue bars show TTFB without Argo, while the orange bars show TTFB where Argo found a Smart Route.

* **Geography**: A map shows the improvement in response time at each Cloudflare data center.

  * A negative value indicates that requests from that location would not have benefited from Argo Smart Routing, so instead would have been routed directly.

---

# Get started

URL: https://developers.cloudflare.com/argo-smart-routing/get-started/

import { Render, TabItem, Tabs } from "~/components";

Argo Smart Routing is a one-click solution to speed up your global traffic.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To enable [Argo Smart Routing](https://dash.cloudflare.com/?to=/:account/:zone/traffic) in the dashboard:

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Traffic** > **Argo Smart Routing**.
3. For **Argo Smart Routing**, switch the toggle to **On**.
4. Provide your billing information.

   - If you do not have a [billing profile](/fundamentals/subscriptions-and-billing/create-billing-profile/), enter your billing information.

   - If you have a billing profile, confirm your billing information.

</TabItem> <TabItem label="API">

To enable or disable Argo Smart Routing with the API, send a [`PATCH`](/api/resources/argo/subresources/smart_routing/methods/edit/) request with the `value` parameter set to your desired setting (`"on"` or `"off"`).

You will need to already have a [billing profile](/fundamentals/subscriptions-and-billing/create-billing-profile/) on your account to enable Argo Smart Routing.

</TabItem> </Tabs>

<Render file="non-contract-enablement" product="fundamentals" />

## Billing

If Cloudflare mitigates attacks on your site - whether through DDoS protection, the WAF, or other mechanisms - that traffic will not be included in any charges for Argo Smart Routing.

<Render file="ubb-recommendation" product="fundamentals" />

## Enable Tiered Cache

[Cache](/cache/) works by storing a copy of website content at Cloudflare's data centers. [Tiered Cache](/cache/how-to/tiered-cache/) divides these data centers into a hierarchy based on location. This behavior allows Cloudflare to deliver content from data centers closest to your visitor.

Argo Smart Routing and Tiered Cache work together to provide the most efficient connection for visitors to your site. For more information, go to [Tiered Cache](/cache/how-to/tiered-cache/).

---

# Argo Smart Routing

URL: https://developers.cloudflare.com/argo-smart-routing/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct, Render } from "~/components"

<Description>

Speed up your global traffic with a single click 
</Description>

<Plan id="traffic.argo.properties.availability.summary" />

Argo Smart Routing detects real-time network issues and routes your web traffic across the most efficient network path, avoiding congestion. This results in faster loading times, increased reliability, and reduced costs. These benefits are most apparent for users farthest from your origin server.

Learn more about the [benefits of Argo Smart Routing](https://www.cloudflare.com/application-services/products/argo-smart-routing/).

<Render file="non-contract-enablement" product="fundamentals" />

***

## Features

<Feature header="Analytics" href="/argo-smart-routing/analytics/">

Argo Smart Routing includes comprehensive analytics to compare performance improvements with and without Argo enabled.


</Feature>

***

## Related products

<RelatedProduct header="Tiered Cache" href="/cache/how-to/tiered-cache/" product="cache">
Increase cache hit ratios to reduce requests to your origin server. 
</RelatedProduct>

<RelatedProduct header="China Network" href="/china-network/" product="china-network">
Improve security and performance within mainland China. 
</RelatedProduct>

<RelatedProduct header="Magic Transit" href="/magic-transit/" product="magic-transit">
Reduce latency and protect from DDoS attacks using the Cloudflare network. 
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Plans" href="https://www.cloudflare.com/plans/#overview" icon="document">
Compare available Cloudflare plans
</LinkTitleCard>

<LinkTitleCard title="Pricing" href="https://dash.cloudflare.com/?to=/:account/:zone/traffic/" icon="seti:shell">
Explore pricing options for Argo in the dashboard
</LinkTitleCard>

</CardGrid>

---

# Welcome

URL: https://developers.cloudflare.com/automatic-platform-optimization/

Take your WordPress siteâ€™s performance to the next level with Automatic Platform Optimizations (APO). APO allows Cloudflare to serve your entire WordPress site from its edge network ensuring consistent, fast performance for visitors no matter where they are.

Automatic Platform Optimization is the result of using the power of Cloudflare Workers to intelligently cache dynamic content. By caching dynamic content, Cloudflare can serve the entire website from our edge network to make a site's time to first byte (TTFB) both fast and consistent.

To read more about the benefits of using APO with your site, see [The Benefits of Automatic Platform Optimization blog](https://blog.cloudflare.com/automatic-platform-optimizations-starting-with-wordpress/#the-benefits-of-automatic-platform-optimization). You must use the Cloudflare for WordPress plugin to begin using APO.

---

# Glossary

URL: https://developers.cloudflare.com/bots/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Bots documentation.

<Glossary product="bots" />

---

# Changelog

URL: https://developers.cloudflare.com/bots/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/bots.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Overview

URL: https://developers.cloudflare.com/bots/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct, Render } from "~/components"

<Description>

Identify and mitigate automated traffic to protect your domain from bad bots. 
</Description>

<Plan type="all" />

While Cloudflare offers several products that relate to bot traffic, this section reviews our bot-specific products, Bot Fight Mode, Super Bot Fight Mode, and Bot Management for Enterprise.

<Render file="non-contract-enablement" product="fundamentals" />

## Which bot solution do I need?

<Render file="which-solution-do-i-need" product="bots" />

## Features

<Feature header="Bot Fight Mode" href="/bots/get-started/free/">
Detect and mitigate bot traffic on your domain. 
</Feature>

<Feature header="Super Bot Fight Mode" href="/bots/get-started/pro/">
Identify traffic matching patterns of known bots, challenge or block bots, protect static resources, and view analytics to help you understand bot traffic using Super Bot Fight Mode. 
</Feature>

<Feature header="Bot Analytics" href="/bots/bot-analytics/">
Use Bot Analytics to dynamically examine bot traffic. 
</Feature>

<Feature header="Firewall variables" href="/bots/reference/bot-management-variables/">
Access several new variables within the Firewall expression builder. 
</Feature>

## Related products

<RelatedProduct header="API Shield" href="/api-shield/" product="api-shield">
Identify and address API vulnerabilities using API Shield. 
</RelatedProduct>

<RelatedProduct header="DDoS Protection" href="/ddos-protection/" product="ddos-protection">
Detect and mitigate Distributed Denial of Service (DDoS) attacks using Cloudflare's Autonomous Edge. 
</RelatedProduct>

<RelatedProduct header="Turnstile" href="/turnstile/" product="turnstile">
Use Cloudflare's smart CAPTCHA alternative to run less intrusive challenges. 
</RelatedProduct>

<RelatedProduct header="WAF" href="/waf/" product="waf">
Get automatic protection from vulnerabilities and the flexibility to create custom rules. 
</RelatedProduct>

## More resources

<CardGrid>

<LinkTitleCard title="Plans" href="https://www.cloudflare.com/plans/#overview" icon="document">
Compare available Cloudflare plans
</LinkTitleCard>

</CardGrid>

---

# FAQ

URL: https://developers.cloudflare.com/bots/troubleshooting/

import { Render, RuleID } from "~/components";

## Bots

## How does Cloudflare detect bots?

Cloudflare uses multiple methods to detect bots, but these vary by plan. For more details, refer to [Plans](/bots/plans).

---

## How do I know what's included in my plan?

To know what's included in your plan, refer to our [Plans](/bots/plans).

---

## How do I set up my bot product?

To learn how to set up your bot product, refer to [Get started](/bots/get-started).

---

## Yandex bot unexpectedly blocked by the WAF managed rule with ID `...f6cbb163`

Yandex updates their bots very frequently, you may see more false positives while these changes are propagated. New and recently updated bots will occasionally be blocked by a Cloudflare WAF managed rule, as the IP list of Yandex bots has not yet synced with Yandex's most recent changes.

**Workarounds:**

- Create an [exception](/waf/managed-rules/waf-exceptions/) to temporarily skip the managed rule with ID <RuleID id="2854e3f18ad946049e6d90ccf6cbb163" /> when a request is coming from the **Yandex IP** and the user-agent contains **Yandex.**
- Create a [WAF custom rule with the _Skip_ action](/waf/custom-rules/skip/) to temporarily bypass WAF Managed Rules when a request is coming from the **Yandex IP** and the user-agent contains **Yandex.**

If you are using the legacy WAF managed rules ([now deprecated](/waf/reference/migration-guides/waf-managed-rules-migration/)), disable the WAF managed rule with ID `100203` temporarily.

**Solution:**

Once the new Yandex IP is propagated to our system, the requests will not be blocked anymore and you can remove any workaround you configured. This can take up to 48 hours. If you see any Yandex bots still being blocked after 48 hours with no change to the bot, contact [Cloudflare Support](/support/contacting-cloudflare-support/).

---

## How does machine learning work?

Supervised machine learning takes certain variables (X) like gender and age and predicts another variable (Y) like income.

In Bot Management and Super Bot Fight Mode, the X variables are request features, while the Y variable represents the probability of solving a challenge based on X values.

Cloudflare uses data from millions of requests and re-train the system on a periodic basis. You can learn about this data from your own request logs such as Cloudflare Logpull and Logpush as well as the Firewall API.

---

## Why am I seeing a Managed Challenge action for WAF rules?

When you choose to challenge different bot categories with Bot Fight Mode or Super Bot Fight Mode, you will see Security Events with an **Action Taken** of **Managed Challenge**.

You may also see Managed Challenge due to a triggered [WAF custom rule](/waf/reference/cloudflare-challenges/#managed-challenge-recommended).

This does not mean that your traffic was blocked. It is the challenge sent to your user to determine whether they are likely human or likely bot.

To understand if the result of the challenge was a success or a failure, you can verify using [Logpush](/logs/about/).

## Does the WAF run before Super Bot Fight Mode?

Yes. WAF rules are executed before Super Bot Fight Mode. If a WAF custom rule performs a [terminating action](/ruleset-engine/rules-language/actions/) such as _Block_, your Super Bot Fight Mode configuration will not be evaluated.

---

## What is the difference between the threat score and bot management score?

The difference is significant:

- Threat score (_cf.threat_score_) is what Cloudflare uses to determine IP Reputation. It goes from 0 (good) to 100 (bad).
- Bot management score (_cf.bot_management.score_) is what Cloudflare uses in Bot Management to measure if the request is from a human or a script. The scores range from 1 (bot) to 99 (human). Lower scores indicate the request came from a script, API service, or an automated agent. Higher scores indicate that the request came from a human using a standard desktop or mobile web browser.

These fields are available via [WAF custom rules](/waf/custom-rules/) and other products based on the Ruleset Engine.

---

## What is cf.bot_management.verified_bot?

A request's _cf.bot_management.verified_bot_ value is a boolean indicating whether such request comes from a Cloudflare allowed bot.

Cloudflare has built an allowlist of good, automated bots, e.g. Google Search Engine, Pingdom, and more.

This allowlist is large based on reverse DNS verification, meaning that the IPs we allow really match the requesting service. In addition to this, Cloudflare uses multiple validation methods including ASN blocks and public lists. If none of these validation types are available for a customer, we use internal Cloudflare data and machine learning to identify legitimate IP addresses from good bots.

To allow traffic from good bots, use the [Verified Bot](/ruleset-engine/rules-language/fields/reference/cf.bot_management.verified_bot/) field in your WAF custom rule.

---

## Why might the ja3hash be empty in HTTP logs?

The JA3 Fingerprint can be null or empty in some cases. The most common case is for HTTP requests, because JA3 is calculated in TLS, but can also be empty due to the following:

- Orange to Orange zones (Cloudflare Zone proxied to another Cloudflare Zone).

- Worker sending requests within the same zone or to a zone that is not proxied (or a 3rd party).

---

## I run a good bot and want for it to be added to the allowlist (cf.bot_management.verified_bot). What should I do?

Cloudflare maintains a sample list of verified bots in [Cloudflare Radar](https://radar.cloudflare.com/verified-bots).

As a bot operator, in order to be listed by Cloudflare as a Verified Bot, your bot must conform with our [verified bot public policy](/bots/concepts/bot/verified-bots/policy/). If your bot meets this criteria, submit this [online application](https://docs.google.com/forms/d/e/1FAIpQLSdqYNuULEypMnp4i5pROSc-uP6x65Xub9svD27mb8JChA_-XA/viewform?usp=sf_link).

---

## What information do I need to troubleshoot my bot issues?

If you are experiencing errors with your bot solution and need to submit a Support request, include the following information:

:::caution

The following information gathering are required when you are experiencing issues (e.g. false positives) with Enterprise Bot Management only (Enterprise plan).

Because Bot Fight Mode (BFM) and Super Bot Fight Mode (SBFM) are set at a domain level, we often find that disabling these features is the best solution to false positives.

Please follow instructions in the following questions on how to disable BFM and SBFM features. We conduct more thorough investigations for Enterprise Bot Management.
:::

- RayIDs
- IP addresses
- WAF custom rule IDs, rule expression, Challenge solve rates
- Common user-agents among false positives
- Common ASNs among false positives
- Screenshots of strange activity from the WAF, such as a huge spike in challenged traffic on the graph
- Problematic URIs or paths
- Rough description of how your domain is configured.
  - Is one zone Cloudflare for SaaS while the others are not?
  - Is most API traffic sent to a particular URI?
  - How much mobile traffic do you expect?

---

## What should I do if I am getting False positives caused by Bot Fight Mode (BFM) or Super Bot Fight Mode (SBFM)?

:::caution[Important considerations you need to be aware of before turning on BFM or SBFM]

- BFM and SBFM are high security features intended to quickly help customers under active attack stop as many bots as possible. Due to the high security threshold, false positives do sometimes happen.
- BFM has limited control. You cannot bypass or skip BFM using the _Skip_ action in WAF custom rules or using Page Rules. BFM will be disabled if there are any IP Access rules present. If you turned on BFM during an attack, and the attack has subsided, we recommend either disabling the feature using IP Access rules to bypass BFM, or looking at [Bot Management for Enterprise](/bots/plans/bm-subscription/), which gives you the ability to precisely customize your security threshold and create exception rules as needed.
- SBFM can be bypassed with IP Access _Allow_ action rules. You can use the _Skip_ action in [WAF custom rules](/waf/custom-rules/skip/) to specify where Super Bot Fight Mode should not run.

:::

**How to disable BFM/SBFM feature?**

If you encounter any issues with BFM/SBFM feature (e.g. false positive), you can disable it under **Security** > **Bots**.

- For **Free** plans, toggle the **Bot Fight Mode** option to **Off**
- For **Pro** plans, click the **Configure Super Bot Fight Mode** link and set each of **Definitely automated** and **Verified bots** features to **Allow**, and toggle the **Static resource protection** and **JavaScript Detections** options to **Off**
- For **Business** and **Enterprise** (with no Bot Management add-on) plans, click the **Configure Super Bot Fight Mode** link and set each of **Definitely automated**, **Likely automated** and **Verified bots** features to **Allow**, and toggle the **Static resource protection** and **JavaScript Detections** options to **Off**

<Render file="flexible-sbfm" />

You cannot bypass or skip Bot Fight Mode using the _Skip_ action in WAF custom rules or using Page Rules. _Skip_, _Bypass_, and _Allow_ actions apply to rules or rulesets running on the [Ruleset Engine](/ruleset-engine/). While Super Bot Fight Mode rules are implemented in the Ruleset Engine, Bot Fight Mode checks are not. This is why you can skip Super Bot Fight Mode, but not Bot Fight Mode. If you need to skip Bot Fight Mode, consider using [Super Bot Fight Mode](/bots/get-started/pro/).

Bot Fight Mode can still trigger if you have IP Access rules, but it cannot trigger if an IP Access rule matches the request. For example, the IP Access rule matches the connecting IP.

---

## Super Bot Fight Mode feature (SBFM) is still blocking requests even though the feature is turned off, why?

This is a known issue the Bots team is working to resolve in the near future. In the meantime, there is a workaround to resolve such issue. You will need to run the following API command to check and remove the SBFM ruleset:

1. List the existing Rulesets at the zone level.

   ```bash
   curl "https://api.cloudflare.com/client/v4/zones/{zone_id}/rulesets" \
   --header "Authorization: Bearer <API_TOKEN>"
   ```

2. From the output in step 1, find the ruleset ID that is associated with the zone's SBFM configuration. You should be able to see `"kind": "zone"` and `"phase": "http_request_sbfm"` for that ruleset.

3. Use the ruleset ID you found to delete the SBFM ruleset.

   ```bash
   curl --request DELETE "https://api.cloudflare.com/client/v4/zones/{zone_id}/rulesets/{ruleset_id}" \
   --header "Authorization: Bearer <API_TOKEN>"
   ```

Note that you need to replace `<API_TOKEN>` with your own [API token](/fundamentals/api/get-started/create-token/).

---

# Changelog

URL: https://developers.cloudflare.com/browser-rendering/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/radar.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# FAQ

URL: https://developers.cloudflare.com/browser-rendering/faq/

import { GlossaryTooltip } from "~/components";

Below you will find answers to our most commonly asked questions. If you cannot find the answer you are looking for, refer to the [Discord](https://discord.cloudflare.com) to explore additional resources.

##### Uncaught (in response) TypeError: Cannot read properties of undefined (reading 'fetch')

Make sure that you are passing your Browser binding to the `puppeteer.launch` api and that you have [Workers for Platforms Paid plan](/cloudflare-for-platforms/workers-for-platforms/platform/pricing/).

##### Will browser rendering bypass Cloudflare's Bot Protection?

Browser rendering requests are always identified as bots by Cloudflare.

If you are trying to **scan** your **own zone**, you can create a [WAF skip rule](/waf/custom-rules/skip/) to bypass the bot protection using a header or a custom user agent.

## Puppeteer

##### Code generation from strings disallowed for this context while using an Xpath selector

Currently it's not possible to use Xpath to select elements since this poses a security risk to Workers.

As an alternative try to use a css selector or `page.evaluate` for example:

```ts
const innerHtml = await page.evaluate(() => {
	return (
		// @ts-ignore this runs on browser context
		new XPathEvaluator()
			.createExpression("/html/body/div/h1")
			// @ts-ignore this runs on browser context
			.evaluate(document, XPathResult.FIRST_ORDERED_NODE_TYPE).singleNodeValue
			.innerHTML
	);
});
```

:::note

Keep in mind that `page.evaluate` can only return primitive types like strings, numbers, etc.

Returning an `HTMLElement` will not work.

:::

---

# Get started

URL: https://developers.cloudflare.com/browser-rendering/get-started/

Browser rendering can be used in two ways:

- [Workers Binding API](/browser-rendering/workers-binding-api) for complex scripts.
- [REST API](/browser-rendering/rest-api/) for simple actions.

---

# Browser Rendering

URL: https://developers.cloudflare.com/browser-rendering/

import {
	CardGrid,
	Description,
	LinkTitleCard,
	Plan,
	RelatedProduct,
} from "~/components";

<Description>

Browser automation for [Cloudflare Workers](/workers/).

</Description>

<Plan type="workers-paid" />

The Workers Browser Rendering API allows developers to programmatically control and interact with a headless browser instance and create automation flows for their applications and products. Once you configure the service, Workers Browser Rendering gives you access to a WebSocket endpoint that speaks the [DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/). DevTools is what allows Cloudflare to instrument a Chromium instance running in the Cloudflare global network.

Use Browser Rendering to:

- Take screenshots of pages.
- Convert a page to a PDF.
- Test web applications.
- Gather page load performance metrics.
- Crawl web pages for information retrieval.
## Related products

<RelatedProduct header="Workers" href="/workers/" product="workers">

Build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale.

</RelatedProduct>

<RelatedProduct header="Durable Objects" href="/durable-objects/" product="durable-objects">

A globally distributed coordination API with strongly consistent storage.

</RelatedProduct>

## More resources

<CardGrid>

<LinkTitleCard
	title="Get started"
	href="/browser-rendering/get-started/"
	icon="open-book"
>
	Deploy your first Browser Rendering project using Wrangler and Cloudflare's
	version of Puppeteer.
</LinkTitleCard>

<LinkTitleCard
	title="Learning Path"
	href="/learning-paths/workers/concepts/"
	icon="pen"
>
	New to Workers? Get started with the Workers Learning Path.
</LinkTitleCard>

<LinkTitleCard
	title="Limits"
	href="/browser-rendering/platform/limits/"
	icon="document"
>
	Learn about Browser Rendering limits.
</LinkTitleCard>

<LinkTitleCard
	title="Developer Discord"
	href="https://discord.cloudflare.com"
	icon="discord"
>
	Connect with the Workers community on Discord to ask questions, show what you
	are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard
	title="@CloudflareDev"
	href="https://x.com/cloudflaredev"
	icon="x.com"
>
	Follow @CloudflareDev on Twitter to learn about product announcements, and
	what is new in Cloudflare Workers.
</LinkTitleCard>

</CardGrid>

---

# Changelog

URL: https://developers.cloudflare.com/byoip/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/byoip.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Get started

URL: https://developers.cloudflare.com/byoip/get-started/

import { GlossaryTooltip } from "~/components"

To bring your own IPs, you must work with your account team to understand everything you need to ensure a smooth transition during the onboarding process.

Cloudflare requires a service-specific configuration for your prefixes, as well as some requirements common to all BYOIP customers regardless of service type. These requirements are common to all products compatible with BYOIP, such as [Magic Transit](/magic-transit/), [Spectrum](/spectrum/), and [CDN services](/cache/).

## Prerequisites

There are two major prerequisites before Cloudflare can begin onboarding your IP space.

1. Cloudflare must receive a [Letter of Agency (LOA)](/byoip/concepts/loa/) to announce your prefixes, which we will share with our transit partners as evidence that we are allowed to announce the route.
2. You must verify that your [Internet Routing Registry (IRR)](/byoip/concepts/irr-entries/) records are up to date and contain:
    - `route` or `route6` objects matching the exact prefixes you want to onboard
    - `origin` matching the correct ASN you want to onboard

:::caution[RPKI validation]
You are not required to use <GlossaryTooltip term="Resource Public Key Infrastructure (RPKI)">Resource Public Key Infrastructure (RPKI)</GlossaryTooltip>. However, if you do, make sure your <GlossaryTooltip term="Route Origin Authorization (ROA)">ROAs</GlossaryTooltip> are accurate. You can use [Cloudflare's RPKI Portal](https://rpki.cloudflare.com/?view=validator) and a second source such as [Routinator](https://rpki-validator.ripe.net/ui/) to double check your prefixes.
:::

After onboarding, [Border Gateway Protocol (BGP)](https://www.cloudflare.com/learning/security/glossary/what-is-bgp/) announcements for customer prefixes can be controlled with the [Dynamic Advertisement](/byoip/concepts/dynamic-advertisement/) API or via the Cloudflare dashboard.

## Cloudflare IPs

If you are unable to bring your own IP to Cloudflare, you can use an IP address issued by Cloudflare.

Using a Cloudflare IP may be a good option if you:

* Have one or a few IPs allocated from home or business class ISPs.
* Are an online streamer who could be the target of a DoS attack if your IP is leaked.
* Are a business owner with a small number of locations with broadband Internet connections.
* Do not own an IP space with a /24 prefix length.
* Maintain a large number of locations with a combination of connectivity methods.
* Own an IP space with a /24 prefix length but do not advertise prefixes from every location.

To protect your network using a Cloudflare IP address, contact your account manager.

:::note

When you use a Cloudflare-managed IP space, you do not need to provide a Letter of Agency (LOA) and advertise your prefixes that are associated with bringing your own IP.
:::

---

# Glossary

URL: https://developers.cloudflare.com/byoip/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's BYOIP documentation.

<Glossary product="byoip" />

---

# Route Leak Detection

URL: https://developers.cloudflare.com/byoip/route-leak-detection/

import { AvailableNotifications } from "~/components"

Route Leak Detection protects your routes on the Internet by notifying you when your traffic is routed somewhere it should not go, which could indicate a possible attack. Route Leak Detection also reduces the amount of time needed to mitigate leaks by providing you with timely notifications.

Cloudflare detects route leaks by using several sources of routing data to create a synthesis of how the Internet sees routes to BYOIP users. Cloudflare then watches these views to track any sudden changes that occur on the Internet. If the changes can be correlated to actions Cloudflare has taken, no further action is required. However, if changes have not been made, Cloudflare notifies you to inform you that your routes and users may be at risk.

## Enable Route Leak Detection

<AvailableNotifications product="Route Leak Detection" />

You must be a user who has brought your own IP address to Cloudflare, which includes Magic Transit, Spectrum, and WAF users. Only prefixes advertised by Cloudflare qualify for Route Leak Detection.

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Select **Notifications** > **Add**.
3. Locate **Route Leak Detection** from the list > **Select**.
4. Enter a name and description for the notification.
5. Enter one or more email addresses to receive the notifications.
6. Select **Save**.

---

# Cloudflare BYOIP

URL: https://developers.cloudflare.com/byoip/

import { LinkButton, Plan } from "~/components";

<Plan type="enterprise" />

With **Bringing Your Own IPs** (BYOIP), Cloudflare announces your IPs in all our locations. Use your IPs with [Magic Transit](/magic-transit/), [Spectrum](/spectrum/), [CDN services](/cache/), or [Gateway DNS](/cloudflare-one/policies/gateway/dns-policies/).

Learn how to [get started](/byoip/get-started/).

---

# Troubleshooting

URL: https://developers.cloudflare.com/byoip/troubleshooting/

import { GlossaryTooltip } from "~/components";

The following topics are useful for troubleshooting BYOIP issues.

## uRPF filtering and packet loss

Routers receive IP packets and forward the packets to the destination IP address. Unicast Reverse Path Forwarding (uRPF) is a security feature that can prevent spoofing attacks. uRPF operates under two modes: strict and loose mode.

Under **strict mode**, the router performs two checks on incoming packets to look for a matching entry in the source routing table and to determine whether the interface that received the packet can be used to reach the source. If the incoming IP packets pass both checks, the packets are forwarded; if the checks do not pass, the packets are dropped.

When uRPF is set to loose mode, the router performs a single check when it receives an IP packet to look for a source's matching entry in the routing table.

If you are experiencing packet loss as a result of an upstream ISP implementing uRPF filtering, contact your ISP and request the link be set to **loose mode**.

## Non-SNI support

Currently, BYOIP cannot be used with [legacy custom certificates](/ssl/edge-certificates/custom-certificates/uploading/) to support <GlossaryTooltip term="Server Name Indication (SNI)" link="/ssl/reference/browser-compatibility/#non-sni-support">non-SNI</GlossaryTooltip> requests.

Instead, you can use Address Maps to set a default SNI for IPs on your account or zone. Refer to [Setup](/byoip/address-maps/setup/#non-sni-support) for further guidance.

---

# Calls vs regular SFUs

URL: https://developers.cloudflare.com/calls/calls-vs-sfus/

## Cloudflare Calls vs. Traditional SFUs

Cloudflare Calls represents a paradigm shift in building real-time applications by leveraging a distributed real-time data plane. It creates a seamless experience in real-time communication, transcending traditional geographical limitations and scalability concerns. Calls is designed for developers looking to integrate WebRTC functionalities in a server-client architecture without delving deep into the complexities of regional scaling or server management.

### The Limitations of Centralized SFUs

Selective Forwarding Units (SFUs) play a critical role in managing WebRTC connections by selectively forwarding media streams to participants in a video call. However, their centralized nature introduces inherent limitations:

* **Regional Dependency:** A centralized SFU requires a specific region for deployment, leading to latency issues for global users except for those in proximity to the selected region.

* **Scalability Concerns:** Scaling a centralized SFU to meet global demand can be challenging and inefficient, often requiring additional infrastructure and complexity.

### How is Cloudflare Calls different?

Cloudflare Calls addresses these limitations by leveraging Cloudflare's global network infrastructure:

* **Global Distribution Without Regions:** Unlike traditional SFUs, Cloudflare Calls operates on a global scale without regional constraints. It utilizes Cloudflare's extensive network of over 250 locations worldwide to ensure low-latency video forwarding, making it fast and efficient for users globally.

* **Decentralized Architecture:** There are no dedicated servers for Calls. Every server within Cloudflare's network contributes to handling Calls, ensuring scalability and reliability. This approach mirrors the distributed nature of Cloudflare's products such as 1.1.1.1 DNS or Cloudflare's CDN.

## How Cloudflare Calls Works

### Establishing Peer Connections

To initiate a real-time communication session, an end user's client establishes a WebRTC PeerConnection to the nearest Cloudflare location. This connection benefits from anycast routing, optimizing for the lowest possible latency.

### Signaling and Media Stream Management

* **HTTPS API for Signaling:** Cloudflare Calls simplifies signaling with a straightforward HTTPS API. This API manages the initiation and coordination of media streams, enabling clients to push new MediaStreamTracks or request these tracks from the server.

* **Efficient Media Handling:** Unlike traditional approaches that require multiple connections for different media streams from different clients, Cloudflare Calls maintains a single PeerConnection per client. This streamlined process reduces complexity and improves performance by handling both the push and pull of media through a singular connection.

### Application-Level Management

Cloudflare Calls delegates the responsibility of state management and participant tracking to the application layer. Developers are empowered to design their logic for handling events such as participant joins or media stream updates, offering flexibility to create tailored experiences in applications.

## Getting Started with Cloudflare Calls

Integrating Cloudflare Calls into your application promises a straightforward and efficient process, removing the hurdles of regional scalability and server management so you can focus on creating engaging real-time experiences for users worldwide.

---

# Changelog

URL: https://developers.cloudflare.com/calls/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/calls.yaml. --> */}

<ProductReleaseNotes />

---

# DataChannels

URL: https://developers.cloudflare.com/calls/datachannels/

DataChannels are a way to send arbitrary data, not just audio or video data, between client in low latency. DataChannels are useful for scenarios like chat, game state, or any other data that doesn't need to be encoded as audio or video but still needs to be sent between clients in real time.

While it is possible to send audio and video over DataChannels, it's not optimal because audio and video transfer includes media specific optimizations that DataChannels do not have, such as simulcast, forward error correction, better caching across the Cloudflare network for retransmissions.

```mermaid
graph LR
    A[Publisher] -->|Arbitrary data| B[Cloudflare Calls SFU]
    B -->|Arbitrary data| C@{ shape: procs, label: "Subscribers"}
```

DataChannels on Cloudflare Calls can scale up to many subscribers per publisher, there is no limit to the number of subscribers per publisher.

### How to use DataChannels

1. Create a two Calls sessions, one for the publisher and one for the subscribers.
2. Create a DataChannel by calling /datachannels/new with the location set to "local" and the dataChannelName set to the name of the DataChannel.
3. Create a DataChannel by calling /datachannels/new with the location set to "remote" and the sessionId set to the sessionId of the publisher.
4. Use the DataChannel to send data from the publisher to the subscribers.

### Unidirectional DataChannels

Cloudflare Calls SFU DataChannels are one way only. This means that you can only send data from the publisher to the subscribers. Subscribers cannot send data back to the publisher. While regular MediaStream WebRTC DataChannels are bidirectional, this introduces a problem for Cloudflare Calls because the SFU does not know which session to send the data back to. This is especially problematic for scenarios where you have multiple subscribers and you want to send data from the publisher to all subscribers at scale, such as distributing game score updates to all players in a multiplayer game.

To send data in a bidirectional way, you can use two DataChannels, one for sending data from the publisher to the subscribers and one for sending data the opposite direction.

## Example

An example of DataChannels in action can be found in the [Calls Examples github repo](https://github.com/cloudflare/calls-examples/tree/main/echo-datachannels).

---

# Demos

URL: https://developers.cloudflare.com/calls/demos/

import { ExternalResources, GlossaryTooltip } from "~/components"

Learn how you can use Calls within your existing architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Calls.

<ExternalResources type="apps" products={["Calls"]} />

---

# Quickstart guide

URL: https://developers.cloudflare.com/calls/get-started/

:::note[Before you get started:]


You must first [create a Cloudflare account](/fundamentals/setup/account/create-account/).


:::

## Create your first app

Every Calls App is a separate environment, so you can make one for development, staging and production versions for your product.
Either using [Dashboard](https://dash.cloudflare.com/?to=/:account/calls), or the [API](/api/resources/calls/subresources/sfu/methods/create/) create a Calls App. When you create a Calls App, you will get:

* App ID
* App Secret

These two combined will allow you to make API Calls from your backend server to Calls.

---

# Example architecture

URL: https://developers.cloudflare.com/calls/example-architecture/

<div class="full-img">

![Example Architecture](~/assets/images/calls/video-calling-application.png)

</div>

1. Clients connect to the backend service
2. Backend service manages the relationship between the clients and the tracks they should subscribe to
3. Backend service contacts the Cloudflare Calls API to pass the SDP from the clients to establish the WebRTC connection.
4. Calls API relays back the Calls API SDP reply and renegotiation messages.
5. If desired, headless clients can be used to record the content from other clients or publish content.
6. Admin manages the rooms and room members.

---

# Connection API

URL: https://developers.cloudflare.com/calls/https-api/

Cloudflare Calls simplifies the management of peer connections and media tracks through HTTPS API endpoints. These endpoints allow developers to efficiently manage sessions, add or remove tracks, and gather session information.

## API Endpoints

- **Create a New Session**: Initiates a new session on Cloudflare Calls, which can be modified with other endpoints below.
  - `POST /apps/{appId}/sessions/new`
- **Add a New Track**: Adds a media track (audio or video) to an existing session.
  - `POST /apps/{appId}/sessions/{sessionId}/tracks/new`
- **Renegotiate a Session**: Updates the session's negotiation state to accommodate new tracks or changes in the existing ones.
  - `PUT /apps/{appId}/sessions/{sessionId}/renegotiate`
- **Close a Track**: Removes a specified track from the session.
  - `PUT /apps/{appId}/sessions/{sessionId}/tracks/close`
- **Retrieve Session Information**: Fetches detailed information about a specific session.
  - `GET /apps/{appId}/sessions/{sessionId}`

[View full API and schema (OpenAPI format)](/calls/static/calls-api-2024-05-21.yaml)

## Handling Secrets

It is vital to manage App ID and its secret securely. While track and session IDs can be public, they should be protected to prevent misuse. An attacker could exploit these IDs to disrupt service if your backend server does not authenticate request origins properly, for example by sending requests to close tracks on sessions other than their own. Ensuring the security and authenticity of requests to your backend server is crucial for maintaining the integrity of your application.

## Using STUN and TURN Servers

Cloudflare Calls is designed to operate efficiently without the need for TURN servers in most scenarios, as Cloudflare exposes a publicly routable IP address for Calls. However, integrating a STUN server can be necessary for facilitating peer discovery and connectivity.

- **Cloudflare STUN Server**: `stun.cloudflare.com:3478`

Utilizing Cloudflare's STUN server can help the connection process for Calls applications.

## Lifecycle of a Simple Session

This section provides an overview of the typical lifecycle of a simple session, focusing on audio-only applications. It illustrates how clients are notified by the backend server as new remote clients join or leave, incorporating video would introduce additional tracks and considerations into the session.

```mermaid
sequenceDiagram
    participant WA as WebRTC Agent
    participant BS as Backend Server
    participant CA as Calls API

    Note over BS: Client Joins

    WA->>BS: Request
    BS->>CA: POST /sessions/new
    CA->>BS: newSessionResponse
    BS->>WA: Response

    WA->>BS: Request
    BS->>CA: POST /sessions/<ID>/tracks/new (Offer)
    CA->>BS: newTracksResponse (Answer)
    BS->>WA: Response

    WA-->>CA: ICE Connectivity Check
    Note over WA: iceconnectionstatechange (connected)
    WA-->>CA: DTLS Handshake
    Note over WA: connectionstatechange (connected)

    WA<<->>CA: *Media Flow*

    Note over BS: Remote Client Joins

    WA->>BS: Request
    BS->>CA: POST /sessions/<ID>/tracks/new
    CA->>BS: newTracksResponse (Offer)
    BS->>WA: Response

    WA->>BS: Request
    BS->>CA: PUT /sessions/<ID>/renegotiate (Answer)
    CA->>BS: OK
    BS->>WA: Response

    Note over BS: Remote Client Leaves

    WA->>BS: Request
    BS->>CA: PUT /sessions/<ID>/tracks/close
    CA->>BS: closeTracksResponse
    BS->>WA: Response

    Note over BS: Client Leaves

    WA->>BS: Request
    BS->>CA: PUT /sessions/<ID>/tracks/close
    CA->>BS: closeTracksResponse
    BS->>WA: Response
```

---

# Overview

URL: https://developers.cloudflare.com/calls/

import { Description, LinkButton } from "~/components";

<Description>

Build real-time serverless video, audio and data applications.

</Description>

Cloudflare Calls is infrastructure for real-time audio/video/data applications. It allows you to build real-time apps without worrying about scaling or regions. It can act as a selective forwarding unit (WebRTC SFU), as a fanout delivery system for broadcasting (WebRTC CDN) or anything in between.

Cloudflare Calls runs on [Cloudflare's global cloud network](https://www.cloudflare.com/network/) in hundreds of cities worldwide.

<LinkButton variant="primary" href="/calls/get-started/">
	Get started
</LinkButton>
<LinkButton
	variant="secondary"
	href="https://dash.cloudflare.com/?to=/:account/calls"
>
	Calls dashboard
</LinkButton>
<LinkButton variant="secondary" href="https://github.com/cloudflare/orange">
	Orange Meets demo app
</LinkButton>

---

# Introduction

URL: https://developers.cloudflare.com/calls/introduction/

Cloudflare Calls can be used to add realtime audio, video and data into your applications. Cloudflare Calls uses WebRTC, which is the lowest latency way to communicate across a broad range of platforms like browsers, mobile and native apps.

Calls integrates with your backend and frontend application to add realtime functionality.

## Why Cloudflare Calls exists

* **It's difficult to scale WebRTC**: Many struggle scaling WebRTC servers. Operators run into issues about how many users can be in the same "room" or want to build unique solutions that don't fit into the current concepts in high level APIs.

* **High egress costs**: WebRTC is expensive to use as managed solutions charge a high premium on cloud egress and running your own servers incur system administration and scaling overhead. Cloudflare already has 300+ locations with upwards of 1,000 servers in some locations. Cloudflare Calls scales easily on top of this architecture and can offer the lowest WebRTC usage costs.

* **WebRTC is growing**: Developers are realizing that WebRTC is not just for video conferencing. WebRTC is supported on many platforms, it mature and well understood.

## What makes Cloudflare Calls unique

* \**Unopinionated*: Cloudflare Calls does not offer a SDK. It instead allows you to access raw WebRTC to solve unique problems that might not fit into existing concepts. The API is deliberately simple.

* **No rooms**: Unlike other WebRTC products, Cloudflare Calls lets you be in charge of each track (audio/video/data) instead of offering abstractions such as rooms. You define the presence protocol on top of simple pub/sub. Each end user can publish and subscribe to audio/video/data tracks as they wish.

* **No lock-in**: You can use Cloudflare Calls to solve scalability issues with your SFU. You can use in combination with peer-to-peer architecture. You can use Cloudflare Calls standalone. To what extent you use Cloudflare Calls is up to you.

## What exactly does Cloudflare Calls do?

* **SFU**: Calls is a special kind of pub/sub server that is good at forwarding media data to clients that subscribe to certain data. Each client connects to Cloudflare Calls via WebRTC and either sends data, receives data or both using WebRTC. This can be audio/video tracks or DataChannels.

* **It scales**: All Cloudflare servers act as a single server so millions of WebRTC clients can connect to Cloudflare Calls. Each can send data, receive data or both with other clients.

## How most developers get started

1. Get started with the echo example, which you can download from the Cloudflare dashboard when you create a Calls App or from [demos](/calls/demos/). This will show you how to send and receive audio and video.

2. Understand how you can manipulate who can receive what media by passing around session and track ids. Remember, you control who receives what media. Each media track is represented by a unique ID. It's your responsibility to save and distribute this ID.

:::note[Calls is not a presence protocol]


Calls does not know what a room is. It only knows media tracks. It's up to you to make a room by saving who is in a room along with track IDs that unique identify media tracks. If each participant publishes their audio/video, and receives audio/video from each other, you've got yourself a video conference!


:::

3. Create an app where you manage each connection to Cloudflare Calls and the track IDs created by each connection. You can use any tool to save and share tracks. Check out the example apps at [demos](/calls/demos/), such as [Orange Meets](https://github.com/cloudflare/orange), which is a full-fledged video conferencing app that uses [Workers Durable Objects](/durable-objects/) to keep track of track IDs.

---

# Limits, timeouts and quotas

URL: https://developers.cloudflare.com/calls/limits/

Understanding the limits and timeouts of Cloudflare Calls is crucial for optimizing the performance and reliability of your applications. This section outlines the key constraints and behaviors you should be aware of when integrating Cloudflare Calls into your app.

## Free

* Each account gets 1,000GB/month of data transfer from Cloudflare to your client for free.
* Data transfer from your client to Cloudflare is always free of charge.

## Limits

* **API Calls per Session**: You can make up to 50 API calls per second for each session. There is no ratelimit on a App basis, just sessions.

* **Tracks per API Call**: Up to 64 tracks can be added with a single API call. If you need to add more tracks to a session, you should distribute them across multiple API calls.

* **Tracks per Session**: There's no upper limit to the number of tracks a session can contain, the practical limit is governed by your connection's bandwidth to and from Cloudflare.

## Inactivity Timeout

* **Track Timeout**: Tracks will automatically timeout and be garbage collected after 30 seconds of inactivity, where inactivity is defined as no media packets being received by Cloudflare. This mechanism ensures efficient use of resources and session cleanliness across all Sessions that use a track.

## PeerConnection Requirements

* **Session State**: For any operation on a session (e.g., pulling or pushing tracks), the PeerConnection state must be `connected`. Operations will block for up to 5 seconds awaiting this state before timing out. This ensures that only active and viable sessions are engaged in media transmission.

## Handling Connectivity Issues

* **Internet Connectivity Considerations**: The potential for internet connectivity loss between the client and Cloudflare is an operational reality that must be addressed. Implementing a detection and reconnection strategy is recommended to maintain session continuity. This could involve periodic 'heartbeat' signals to your backend server to monitor connectivity status. Upon detecting connectivity issues, automatically attempting to reconnect and establish a new session is advised. Sessions and tracks will remain available for reuse for 30 seconds before timing out, providing a brief window for reconnection attempts.

Adhering to these limits and understanding the timeout behaviors will help ensure that your applications remain responsive and stable while providing a seamless user experience.

---

# Pricing

URL: https://developers.cloudflare.com/calls/pricing/

Cloudflare Calls billing is based on data sent from Cloudflare edge to your application.

Cloudflare Calls SFU and TURN services cost $0.05 per GB of data egress.

There is a free tier of 1,000 GB before any charges start. This free tier includes usage from both SFU and TURN services, not two independent free tiers. Cloudflare Calls billing appears as a single line item on your Cloudflare bill, covering both SFU and TURN.

Traffic between Cloudflare Calls TURN and Cloudflare Calls SFU or Cloudflare Stream (WHIP/WHEP) does not get double charged, so if you are using both SFU and TURN at the same time, you will get charged for only one.

### TURN

Please see the [TURN FAQ page](/calls/turn/faq), where there is additional information on speficially which traffic path from RFC8656 is measured and counts towards billing.

### SFU

Only traffic originating from Cloudflare towards clients incurs charges. Traffic pushed to Cloudflare incurs no charge even if there is no client pulling same traffic from Cloudflare.

---

# Sessions and Tracks

URL: https://developers.cloudflare.com/calls/sessions-tracks/

Cloudflare Calls offers a simple yet powerful framework for building real-time experiences. At the core of this system are three key concepts: **Applications**,  **Sessions** and **Tracks**. Familiarizing yourself with these concepts is crucial for using Calls.

## Application

A Calls Application is an environment within different Sessions and Tracks can interact. Examples of this could be production, staging or different environments where you'd want separation between Sessions and Tracks. Cloudflare Calls usage can be queried at Application, Session or Track level.

## Sessions

A **Session** in Cloudflare Calls correlates directly to a WebRTC PeerConnection. It represents the establishment of a communication channel between a client and the nearest Cloudflare data center, as determined by Cloudflare's anycast routing. Typically, a client will maintain a single Session, encompassing all communications between the client and Cloudflare.

* **One-to-One Mapping with PeerConnection**: Each Session is a direct representation of a WebRTC PeerConnection, facilitating real-time media data transfer.
* **Anycast Routing**: The client connects to the closest Cloudflare data center, optimizing latency and performance.
* **Unified Communication Channel**: A single Session can handle all types of communication between a client and Cloudflare, ensuring streamlined data flow.

## Tracks

Within a Session, there can be one or more **Tracks**.

* **Tracks map to MediaStreamTrack**: Tracks align with the MediaStreamTrack concept, facilitating audio, video, or data transmission.
* **Globally Unique Ids**: When you push a track to Cloudflare, it is assigned a unique ID, which can then be used to pull the track into another session elsewhere.
* **Available globally**: The ability to push and pull tracks is central to what makes Calls a versatile tool for real-time applications. Each track is available globally to be retrieved from any Session within an App.

## Calls as a Programmable "Switchboard"

The analogy of a switchboard is apt for understanding Calls. Historically, switchboard operators connected calls by manually plugging in jacks. Similarly, Calls allows for the dynamic routing of media streams, acting as a programmable switchboard for modern real-time communication.

## Beyond "Rooms", "Users", and "Participants"

While many SFUs utilize concepts like "rooms" to manage media streams among users, this approach has scalability and flexibility limitations. Cloudflare Calls opts for a more granular and flexible model with Sessions and Tracks, enabling a wide range of use cases:

* Large-scale remote events, like 'fireside chats' with thousands of participants.
* Interactive conversations with the ability to bring audience members "on stage."
* Educational applications where an instructor can present to multiple virtual classrooms simultaneously.

### Presence Protocol vs. Media Flow

Calls distinguishes between the presence protocol and media flow, allowing for scalability and flexibility in real-time applications. This separation enables developers to craft tailored experiences, from intimate calls to massive, low-latency broadcasts.

---

# Changelog

URL: https://developers.cloudflare.com/cache/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/cache.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Get started

URL: https://developers.cloudflare.com/cache/get-started/

import { GlossaryTooltip } from "~/components"

Cloudflare makes customer websites faster by storing a copy of the website's content on the servers of our globally distributed data centers. Content can be either static or dynamic: static content is â€œcacheableâ€ or eligible for caching, and dynamic content is â€œuncacheableâ€ or ineligible for caching. The cached copies of content are stored physically closer to users, optimized to be fast, and do not require recomputing.

Cloudflare caches static content based on the following factors:

* [Caching levels](/cache/how-to/set-caching-levels/)
* [File extension](/cache/concepts/default-cache-behavior/#default-cached-file-extensions)
* Presence of [query strings](/cache/advanced-configuration/query-string-sort/)
* [Origin cache-control headers](/cache/concepts/cache-control/)
* Origin headers that indicate <GlossaryTooltip term="dynamic content">dynamic content</GlossaryTooltip>
* Cache rules that bypass cache on cookie

Cloudflare only caches resources within the Cloudflare data center that serve the request. Cloudflare does not cache off-site or third-party resources, such as Facebook or Flickr, or content hosted on [unproxied (grey-clouded)](/dns/proxy-status/) DNS records.

## Learn the basics

Discover the benefits of caching with Cloudflare's CDN and understand the default cache behavior.

* [Understand what is a CDN](https://www.cloudflare.com/learning/cdn/what-is-a-cdn/)
* [Understand default cache behavior](/cache/concepts/default-cache-behavior/)
* [Understand the default file types Cloudflare caches](/cache/concepts/default-cache-behavior/#default-cached-file-extensions)

## Make more resources cacheable

Configure your settings to cache static HTML or cache anonymous page views of dynamic content.

* [Customize Caching with Cache Rules](/cache/how-to/cache-rules/)
* [Specify which resources to cache](/cache/concepts/customize-cache/)
* [Understand Origin Cache Control](/cache/concepts/cache-control/)
* [Cache by device type (Enterprise only)](/cache/how-to/cache-rules/examples/cache-device-type/)

## Improve cache hit rates

Include or exclude query strings, optimize cache keys, or enable tiered cache to improve hit rates and reduce traffic to your origin.

* [Choose a cache level](/cache/how-to/set-caching-levels/)
* [Enable Tiered Cache with Argo](/cache/how-to/tiered-cache/#enable-tiered-cache)
* [Configure custom cache keys (Enterprise only)](/cache/how-to/cache-keys/)
* [Enable Prefetch URLs (Enterprise only)](/speed/optimization/content/prefetch-urls/)

## Secure your cache configuration

Control resources a client is allowed to load and set access permissions to allow different origins to access your originâ€™s resources. Protect your site from web cache deception attacks while still caching static assets.

* [Avoid web cache poisoning attacks](/cache/cache-security/avoid-web-poisoning/)
* [Configure Cross-Origin Resource Sharing (CORS)](/cache/cache-security/cors/)
* [Enable Cache Deception Armor](/cache/cache-security/cache-deception-armor/#enable-cache-deception-armor)

## Cloudflare features that can alter your HTML and cacheable objects

To provide Cloudflare services to our customers, we may need to alter your HTML or cached objects to enable the feature or provide optimization.

These code alterations only occur on the cacheable objects found at Cloudflare's edge and do not affect the original source. The changes will also be removed if the specific feature is disabled and the cache is purged.

Review the list of Cloudflare features that function in this manner:

* [Rocket Loader](/speed/optimization/content/rocket-loader/)
* [Polish](/images/polish/)
* [Mirage](/speed/optimization/images/mirage/)
* [Hotlink Protection](/waf/tools/scrape-shield/hotlink-protection/)
* [Email address obfuscation](/waf/tools/scrape-shield/email-address-obfuscation/)
* [Bot Management JavaScript Detections](/bots/reference/javascript-detections/)

## Troubleshoot

Resolve common caching concerns.

* [Learn about Cloudflare's cache response statuses](/cache/concepts/cache-responses/)
* [Investigate Cloudflare's cache response with cURL](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/#troubleshoot-requests-with-curl)
* [Diagnose Always Online issues](/cache/troubleshooting/always-online/)

---

# Glossary

URL: https://developers.cloudflare.com/cache/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Cache documentation.

<Glossary product="cache" />

---

# Cloudflare Cache

URL: https://developers.cloudflare.com/cache/

import { CardGrid, Description, Feature, GlossaryTooltip, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Description>

Cache content across Cloudflare's global server network.
</Description>

<Plan type="all" />

Cache stores copies of frequently accessed content (such as images, videos, or webpages) in geographically distributed data centers that are located closer to end users than origin servers, reducing server load and improving website performance.

***

## Features

<Feature header="Default cache behavior" href="/cache/concepts/default-cache-behavior/">

Learn about default cache behavior, default cached file extensions and cache responses.


</Feature>

<Feature header="Cache Rules" href="/cache/how-to/cache-rules/">

Configure Cache Rules to optimize your website by specifying which resources should be cached and for how long.


</Feature>

<Feature header="Tiered Cache" href="/cache/how-to/tiered-cache/">

Enable Tiered Cache to optimize content delivery by caching frequently accessed content in multiple locations for faster delivery and reduced origin traffic.


</Feature>

<Feature header="Cache Reserve" href="/cache/advanced-configuration/cache-reserve/">

Use Cloudflare's persistent storage to increase cache times.


</Feature>

<Feature header="Purge" href="/cache/how-to/purge-cache/">

Instantly purge cached files to force Cloudflare to fetch fresh versions from your web server files. You can purge specific files or all at once.


</Feature>

***

## Related products

<RelatedProduct header="Load Balancing" href="/load-balancing/" product="load-balancing">
Cloudflare Load Balancing distributes traffic across your <GlossaryTooltip term="endpoint">endpoints</GlossaryTooltip>, reducing endpoint strain and latency and improving the end users experience.
</RelatedProduct>

<RelatedProduct header="Images" href="/images/" product="images">
A suite of products tailored to your image-processing needs.
</RelatedProduct>

<RelatedProduct header="Workers" href="/workers/" product="workers">
Cloudflare Workers allows developers to build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale.
</RelatedProduct>

<RelatedProduct header="Rules" href="/rules/" product="rules">
Cloudflare Rules allows you to make adjustments to requests and responses, configure Cloudflare settings, and trigger specific actions for matching requests.
</RelatedProduct>

<RelatedProduct header="Cloudflare Network Interconnect" href="/network-interconnect/" product="network-interconnect">
Cloudflare Network Interconnect (CNI) allows you to connect your network infrastructure directly with Cloudflare â€“ rather than using the public Internet â€“ for a more reliable and secure experience.
</RelatedProduct>

<RelatedProduct header="R2" href="/r2/" product="r2">
Cloudflare R2 Storage allows developers to store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.
</RelatedProduct>

<RelatedProduct header="Aegis" href="/aegis/" product="aegis">
Cloudflare Aegis provides dedicated egress IPs (from Cloudflare to your origin) for your layer 7 WAF and CDN services, as well as Spectrum.
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Plans" href="https://www.cloudflare.com/cdn/" icon="document">
Compare available Cloudflare plans
</LinkTitleCard>

<LinkTitleCard title="Pricing" href="https://www.cloudflare.com/plans/#overview" icon="seti:shell">
Explore pricing options for Cache
</LinkTitleCard>

</CardGrid>

---

# Plans

URL: https://developers.cloudflare.com/cache/plans/

import { ProductFeatures } from "~/components"

Cloudflare provides the following features for different [plans](https://www.cloudflare.com/plans/).

## Features

<ProductFeatures id="cache" />

---

# FAQ

URL: https://developers.cloudflare.com/china-network/faq/

## Requirements

### What are the requirements to enable the Cloudflare China Network service from Cloudflare?

Refer to [Get started](/china-network/get-started/) for more information.

### Can I use my current account to access the Cloudflare China Network service or do I need a separate account?

Yes, you can use your current Cloudflare account and dashboard.

### What are the requirements for requesting a Cloudflare China Network PoC?

Cloudflare requires that you have a valid [ICP (Internet Content Provider)](/china-network/concepts/icp/) number and content vetting approval from JD Cloud to provide you with a Cloudflare China Network PoC (proof of concept). If you are interested in a PoC, contact your sales team.

## Data storage

### Will my Cloudflare account or configuration information be stored in China?

Cloudflare has taken numerous steps to ensure your security and the integrity of your data in China. Your identification information such as email addresses, password hashes, and billing information are never stored on the Cloudflare China Network or shared with the Cloudflare partner except for Zone configuration information and bindings with Cloudflareâ€™s Developer Suite which are stored on the China Network operated by our partners in China upon your enabling the China Service for a particular Zone.

## Licensing and onboarding

### Does Cloudflare have an MIIT license to provide CDN services in China?

As a US company, Cloudflare does not have a license from China's Ministry of Industry and Information Technology (MIIT). However, Cloudflare's partner JD Cloud has all the licenses required by the MIIT to operate and provide CDN services in China.

### Can Cloudflare or JD Cloud help me to get the ICP?

No, neither Cloudflare nor JD Cloud is responsible for [ICP (Internet Content Provider)](/china-network/concepts/icp/) applications. Cloudflare recommends you to reach out to local agents specialized in ICP applications. For more information, refer to [Obtain an ICP number](/china-network/concepts/icp/#obtain-an-icp-number).

### Why is my ICP filing/license revoked?

The application and revocation of ICP filings or licenses is managed by China's local authorities. Usually either the customer or the agency processing the ICP application receive a notification with more details. Cloudflare cannot provide the ICP revocation reasons.

### What would happen if my ICP filing/license got revoked?

Cloudflare's partner JD Cloud and the local authorities continuously track the status of the ICP. If your ICP gets revoked, JD Cloud may terminate or suspend your access to the China Service at any time and without liability, in accordance with China local regulations.
To mitigate the impact on your Internet properties, Cloudflare would reroute the traffic for the affected domains to the nearest data centers outside of China.

### What is content vetting and why do I need JD Cloud to vet my domain's content before onboarding?

The JD Cloud network is proxying content inside of China for customers who have purchased the Cloudflare China Network subscription. To ensure compliance with Chinaâ€™s regulation on Internet information services and with [JD Cloud's service terms](https://docs.jdcloud.com/cn/product-service-agreement/starshield-terms-of-service), JD Cloud must review the content of all the domains before onboarding those domains to their network. They can approve or reject any domain based on the nature of its content. For more information, contact your sales team.

## Technical questions

### How does IPv6 work on the Cloudflare China Network?

All sites hosted in mainland China must have IPv6 enabled. The Cloudflare China Network feature automatically enables IPv6 for domains to fulfill this requirement and it is not possible to disable it. According to Cloudflare's tests, IPv6 connections in mainland China are more reliable and offer better latency.

## Other services

### Can Cloudflare provide a private line for cross-border traffic?

No. Cloudflare is not licensed to provide such cross-border private line service.

---

# Cloudflare China Network

URL: https://developers.cloudflare.com/china-network/

import { Stream } from "~/components"

The [Cloudflare China Network](https://www.cloudflare.com/china-network/) is a package of selected Cloudflare's performance and security products running on data centers located in mainland China and operated by Cloudflare's partner JD Cloud.

The data centers cover most populated regions in China. Combining Cloudflare's technological leadership and JD Cloud's local operations expertise, the Cloudflare China Network is designed to meet the needs for secure, reliable, and fast-performing content delivery in China. You can use the same configurations that you use with Cloudflare everywhere else in the world and with the same dashboard experience.

## Main features

The Cloudflare China Network provides:

- A single solution for both performance improvement and security services such as [WAF](/waf/), [DDoS](/ddos-protection/), and [bot management](/bots/).
- An unified experience for managing network traffic and security posture. You can manage all configurations on the same dashboard.
- The same customer support capabilities as Cloudflare's global network. You may also have access to premium service and local language support.
- [In-China Authoritative DNS and in-China nameservers](/china-network/concepts/china-dns/) to improve the Time to First Byte (TTFB) performance.
- [Global Acceleration](/china-network/concepts/global-acceleration/) is a suite of connectivity, performance offerings and professional services designed to simplify your global assets' deployment in China.

### Video Introduction

<Stream id="b7933a5b3636ca29f834128ca92665b3" title="China Network Overview" thumbnail="1s" />

## Availability

The Cloudflare China Network is available as a separate subscription for customers on an [Enterprise plan](https://www.cloudflare.com/plans/enterprise/).

## Important notes

- Not all Cloudflare products are available in the Cloudflare China Network. Refer to [Available products and features](/china-network/reference/available-products/) for details.
- IPv6 support is mandatory for all Internet entities operating in mainland China. The Cloudflare China Network feature automatically enables IPv6 for domains to fulfill this requirement.
- All the content inside of mainland China is monitored by local authorities and has to comply with local regulations.
- You must have a valid [ICP (Internet Content Provider) filing or license](/china-network/concepts/icp/) for each apex domain you wish to onboard to Cloudflare.

---

# Get started

URL: https://developers.cloudflare.com/client-ip-geolocation/get-started/

:::note


Client IP Geolocation is currently in closed Beta testing.


:::

There are several things you can do to best handle traffic from Cloudflare VPN and forward-proxy users:

* **Origin operators**:
  * Do not block IP addresses associated with our VPN and proxy products (see the [About section](/client-ip-geolocation/about/) for more details)
  * To get even more accurate geolocation data, ensure your origin is [reachable via IPv6](/client-ip-geolocation/faq/)
* **Geolocation data providers**:
  * Regularly pull updated geolocation data from the [Cloudflare API](https://api.cloudflare.com/local-ip-ranges.csv)
* **Users of WARP and 1.1.1.1**:
  * Review the [FAQs](/client-ip-geolocation/faq/#cloudflare-vpn-users) and [About section](/client-ip-geolocation/about/) to learn exactly how, how much, and why we share geolocation data

---

# Get started

URL: https://developers.cloudflare.com/china-network/get-started/

## 1. Contract required services and agree to supplemental terms

1. Ensure that you have a Cloudflare Enterprise plan. If you do not have an Enterprise plan yet, you must upgrade.
2. Add the Cloudflare China Network package (a separate subscription) to your Enterprise plan.
3. Agree to the [China Service Supplemental Terms](https://www.cloudflare.com/supplemental-terms/#china-service).

Contact your sales team for more information on these steps.

## 2. Obtain ICP and vet domain content

1. [Obtain Internet Content Provider (ICP) filings or licenses](/china-network/concepts/icp/#obtain-an-icp-number) for all the apex domains you wish to onboard.

2. Present valid ICP filings or licenses for the zones you are onboarding.

3. Ensure that your websites [display their ICP number in the page footer](/china-network/concepts/icp/#display-your-icp-number).

4. Prepare the required information for JD Cloud to review the content on your domains. JD Cloud, a Cloudflare partner, is required to review and vet the content of all domains on their network before enabling them. You will need to provide the following information:

   - Customer and company name.
   - Domain name.
   - ICP license/filing number.
   - A general description of the content of each domain (for example, `Marketing website`).
   - A signed Self Attestation letter (provided by your sales team).

## 3. Onboard your domains to the Cloudflare China Network

After content vetting is complete, [add your domains to Cloudflare](/fundamentals/setup/manage-domains/add-site/).

It will take a minimum of 24 hours to enable user traffic (that is, until DNS starts answering with JD Cloud's network IP addresses).

---

# Overview

URL: https://developers.cloudflare.com/client-ip-geolocation/

import { LinkButton } from "~/components"

:::note


Client IP Geolocation is currently in closed Beta testing.


:::

Cloudflare designed [Cloudflare WARP](/warp-client/) and [1.1.1.1](/1.1.1.1/) to make Internet browsing more private and secure. These applications encrypt last-mile connections and make it more difficult for others to use client IP addresses in user fingerprinting.

However, unlike legacy VPN applications, we never designed WARP or 1.1.1.1 to hide user locations or allow users to misrepresent their true geographic location. As a web property operator, you can use **Client IP Geolocation** to map Cloudflare egress IP addresses to specific geolocations.

 <LinkButton variant="primary" href="/client-ip-geolocation/get-started/">Get started</LinkButton> <LinkButton variant="secondary" href="/client-ip-geolocation/about/">Learn more</LinkButton>


:::note


Client IP Geolocation is different from the [Cloudflare IP Geolocation setting](/network/ip-geolocation/), which helps you capture country codes for visitors.


:::

---

# Cloudflare for Platforms

URL: https://developers.cloudflare.com/cloudflare-for-platforms/

import { Description, Feature } from "~/components"

<Description>

Cloudflare's offering for SaaS businesses. 
</Description>

Extend Cloudflare's security, reliability, and performance services to your customers with Cloudflare for Platforms. Together with Cloudflare for SaaS and Workers for Platforms, your customers can build custom logic to meet their needs right into your application.

***

## Products

<Feature header="Cloudflare for SaaS" href="/cloudflare-for-platforms/cloudflare-for-saas/">
Cloudflare for SaaS allows you to extend the security and performance benefits of Cloudflareâ€™s network to your customers via their own custom or vanity domains. 
</Feature>

<Feature header="Workers for Platforms" href="/cloudflare-for-platforms/workers-for-platforms/">
Workers for Platforms help you deploy serverless functions programmatically on behalf of your customers.
</Feature>

---

# Account limits

URL: https://developers.cloudflare.com/cloudflare-one/account-limits/

This page lists the default account limits for rules, applications, fields, and other features. These limits may be increased on Enterprise accounts. To request a limit increase, contact your account team.

## Access

| Feature                     | Limit |
| --------------------------- | ----- |
| Applications count          | 500   |
| Audit Logpush jobs          | 5     |
| Email addresses per rule    | 1,000 |
| Group count                 | 300   |
| Group size                  | 1,000 |
| IP addresses per rule       | 1,000 |
| mTLS root certificates      | 50    |
| Service tokens count        | 50    |
| IdP count                   | 50    |
| Rules count per application | 1,000 |
| Rules count per group       | 1,000 |
| Domains per application     | 5     |
| Infrastructure targets      | 5,000 |

## Gateway

| Feature                                   | Limit |
| ----------------------------------------- | ----- |
| DNS policies per account                  | 500   |
| Network policies per account              | 500   |
| HTTP policies per account                 | 500   |
| Egress policies per account               | 500   |
| Resolver policies per account             | 500   |
| DNS locations                             | 250   |
| Concurrent streams for HTTP/2 connections | 256   |
| Proxy endpoints                           | 500   |
| Source IP CIDRs per proxy endpoint        | 2,000 |
| Lists                                     | 100   |
| Entries per list (Standard users)         | 1,000 |
| Entries per list (Enterprise users)       | 5,000 |
| DNS Logpush jobs                          | 5     |
| HTTP Logpush jobs                         | 5     |

## Data Loss Prevention (DLP)

| Feature                                  | Limit     |
| ---------------------------------------- | --------- |
| Custom entries                           | 25        |
| Exact Data Match cells per spreadsheet   | 100,000   |
| Custom Wordlist keywords per spreadsheet | 200       |
| Custom Wordlist keywords per account     | 1,000     |
| Dataset cells per account                | 1,000,000 |

## Cloudflare Tunnel

| Feature                                  | Limit |
| ---------------------------------------- | ----- |
| Tunnels per account                      | 1,000 |
| IP routes per account                    | 1,000 |
| Active `cloudflared` replicas per tunnel | 25    |

## Digital Experience Monitoring (DEX)

| Feature                                       | Limit                                                             |
| --------------------------------------------- | ----------------------------------------------------------------- |
| Tests per account                             | Free Plan: 10, <br/>Pro & Business Plans: 30, <br/>Enterprise: 50 |
| Remote captures per day (Free users)          | 100                                                               |
| Remote captures per day (Pay-as-you-go users) | 200                                                               |
| Remote captures per day (Enterprise users)    | 800                                                               |

## Certificates

| Feature                        | Limit |
| ------------------------------ | ----- |
| Active certificates            | 10    |
| Certificates generated per day | 3     |
| Custom certificates            | 5     |

## Maximum number of characters

| Feature                       | Character limit |
| ----------------------------- | --------------- |
| Application name              | 350             |
| Group name                    | 350             |
| mTLS certificates name        | 350             |
| Service token name            | 350             |
| IdP name                      | 350             |
| Target name                   | 255             |
| Application URL               | 63              |
| Team domain                   | 63              |
| Gateway API policy expression | 140,000         |

---

# Glossary

URL: https://developers.cloudflare.com/cloudflare-one/glossary/

import { Glossary } from "~/components"

Review definitions for Cloudflare Zero Trust terms.

<Glossary product="cloudflare-one" />

---

# Overview

URL: https://developers.cloudflare.com/cloudflare-one/

import { GlossaryTooltip, Render } from "~/components";

Cloudflare Zero Trust replaces legacy security perimeters with Cloudflare's global network, making the Internet faster and safer for teams around the world. Refer to our [reference architecture](/reference-architecture/architectures/sase/) to learn how to evolve your network and security architecture to our SASE platform.

<Render file="non-contract-enablement" product="fundamentals" />

**Zero Trust access for all of your applications.**

- Authenticate users on our global network
- Onboard third-party users seamlessly
- Log every event and request

**A Secure Web Gateway to protect users and devices.**

- Enforce your company's Acceptable Use Policy (AUP)
- Block risky sites with custom blocklists and built-in threat intel
- Enhance visibility and protection into SaaS applications

**A fast and reliable solution for remote browsing.**

- Execute all browser code in the cloud
- Mitigate the impact of attacks
- Seamless, lightning-fast end user experience

**A Cloud Access Security Broker to safeguard data in the cloud.**

- Protect users and sensitive data at rest in SaaS applications
- Detect insider threats and unsanctioned application usage, also known as <GlossaryTooltip term="shadow IT" link="https://www.cloudflare.com/learning/access-management/what-is-shadow-it/">shadow IT</GlossaryTooltip>
- Ensure best practices to prevent data leaks and compliance violations

**A Data Loss Prevention solution to safeguard data in transit.**

- Detect sensitive data as it moves to and from SaaS applications
- Predefined DLP Profiles to quickly get started
- Log or block DLP matches

**An Email Security solution to protect your email inbox.**

- Configure policies to manage your inbox.
- Automatically move emails based on a certain disposition.
- Use screen criteria to investigate your email.

![Illustrated laptop with a login screen with a data stream flowing to a cloud server.](~/assets/images/cloudflare-one/teams-no-background.png)

---

# Roles and permissions

URL: https://developers.cloudflare.com/cloudflare-one/roles-permissions/

When creating a Cloudflare Zero Trust account, you will be given the Super Administrator role. As a Super Administrator, you can invite members to join your Zero Trust account and assign them different roles. There is no limit to the number of members which can be added to a given account. Any members with the proper permissions will be able to make configuration changes while actively logged into Zero Trust (unless [read-only mode](/cloudflare-one/api-terraform/#set-dashboard-to-read-only) is enabled).

To check the list of members in your account, or to manage roles and permissions, refer to our [Account setup](/fundamentals/setup/manage-members/) documentation.

## Zero Trust roles

Only Super Administrators will be able to assign or remove the following roles from users in their account. Scroll to the right to see a full list of permissions for each role.

|                                 | Access Read | Access Edit | Gateway Read | Gateway Edit | Gateway Report | Billing Read | Billing Edit | DEX Read | DEX Edit |
| ------------------------------- | ----------- | ----------- | ------------ | ------------ | -------------- | ------------ | ------------ | -------- | -------- |
| Super Administrator             | âœ…          | âœ…          | âœ…           | âœ…           | âœ…             | âœ…           | âœ…           | âœ…       | âœ…       |
| Cloudflare Zero Trust           | âœ…          | âœ…          | âœ…           | âœ…           | âœ…             | âœ…           | âŒ           | âœ…       | âœ…       |
| Cloudflare Access               | âœ…          | âœ…          | âœ…           | âŒ           | âœ…             | âœ…           | âŒ           | âŒ       | âŒ       |
| Cloudflare Gateway              | âœ…          | âŒ          | âœ…           | âœ…           | âœ…             | âœ…           | âŒ           | âŒ       | âŒ       |
| Cloudflare Zero Trust Read Only | âœ…          | âŒ          | âœ…           | âŒ           | âœ…             | âœ…           | âŒ           | âŒ       | âŒ       |
| Cloudflare Zero Trust Reporting | âŒ          | âŒ          | âŒ           | âŒ           | âœ…             | âœ…           | âŒ           | âœ…       | âŒ       |
| Cloudflare DEX                  | âŒ          | âŒ          | âŒ           | âŒ           | âŒ             | âŒ           | âŒ           | âœ…       | âœ…       |

:::note
The Cloudflare Zero Trust role grants administrator access to all Zero Trust products including Access, Gateway, WARP, Tunnel, Browser Isolation, CASB, DLP, DEX, and Email Security.
:::

### Cloudflare Zero Trust PII

By default, only Super Administrators can view end users' PII in the Gateway activity logs, such as Device IDs, Source IPs, or user emails. No other roles will have the ability to read PII unless Super Administrators explicitly assign the **Cloudflare Zero Trust PII** role to them.

The Cloudflare Zero Trust PII role should be considered an add-on role, to be combined with any role from the table above. For example, Super Administrators may decide to assign the Cloudflare Gateway role to a user, and add the Cloudflare Zero Trust PII role to allow that user to access PII in the Gateway logs.

:::note
The Cloudflare Zero Trust PII role does not apply to Access audit logs. PII is always visible in Access logs.
:::

## Email Security roles

For more information on Email Security roles, refer to [Account-scoped roles](/fundamentals/setup/manage-members/roles/#account-scoped-roles).

|                           | Write access | Read access |
| ------------------------- | ------------ | ----------- |
| Email Configuration Admin | âœ…           | âŒ          |
| Email Integration Admin   | âœ…           | âŒ          |
| Email Security Analyst    | âœ…           | âŒ          |
| Email Security Read Only  | âŒ           | âœ…          |
| Email Security Reporting  | âŒ           | âœ…          |

---

# Get started

URL: https://developers.cloudflare.com/cloudflare-one/setup/

import { Render } from "~/components";

This guide covers the recommended steps to start securing your users and devices with Cloudflare Zero Trust.

:::note

To get started with a specific use case, refer to our [implementation guides](/cloudflare-one/implementation-guides/).
:::

## Prerequisites

Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up).

## Create a Zero Trust organization

<Render file="choose-team-name" product="cloudflare-one" />

Welcome to Cloudflare Zero Trust! You can now explore a list of one-click actions we have designed to help you kickstart your Zero Trust experience.

## Install the WARP client on your devices

If you want to enable security features such as Browser Isolation, HTTP filtering, AV scanning, and device posture, or connect networks to Cloudflare, here are the next steps you need to take:

1. **Set up a login method.** Configure [One-time PIN](/cloudflare-one/identity/one-time-pin/) or connect a [third-party identity provider](/cloudflare-one/identity/idp-integration/) in Zero Trust. This is the login method your users will utilize when authenticating to add a new device to your Zero Trust setup.

2. **Next, define [device enrollment permissions](/cloudflare-one/connections/connect-devices/warp/deployment/device-enrollment/)**. Create device enrollment rules to define which users in your organization should be able to connect devices to your organization's Zero Trust setup. As you create your rule, you will be asked to select which login method you would like users to authenticate with.

3. **Install the [Cloudflare root certificate](/cloudflare-one/connections/connect-devices/user-side-certificates/) on your devices.** Advanced security features including HTTP traffic inspection require users to install and trust the Cloudflare root certificate on their machine or device. If you are installing certificates manually on all your devices, these steps will need to be performed on each new device that is to be subject to HTTP filtering.

4. **[Download](/cloudflare-one/connections/connect-devices/warp/download-warp/) and deploy the WARP client to your devices**. Choose one of the [different ways](/cloudflare-one/connections/connect-devices/warp/deployment/) to deploy the WARP client, depending on what works best for your organization.

5. **Log in to your organization's Cloudflare Zero Trust instance from your devices**. On your device, go to the Settings section in the WARP client and insert your organization's team name.

Your devices are now connected to Cloudflare Zero Trust through the WARP client. You can go to **My Team** > **Devices** to find a list of your enrolled devices, when they were last seen, and the WARP client version they are running.

Next, [enforce security policies](/cloudflare-one/policies/) on your traffic and access requests.

---

# Videos

URL: https://developers.cloudflare.com/cloudflare-one/video-tutorials/

import { CardGrid, LinkCard } from "~/components";

<CardGrid>
    <LinkCard
        title="Build and secure your SASE corporate network"
        description="Dive into Cloudflare's Secure Access Service Edge (SASE) platform and learn how it's been designed to revolutionize the idea of the corporate network."
        href="/learning-paths/sase-overview-course/series/evolution-corporate-networks-1/"
    />
</CardGrid>

---

# Overview

URL: https://developers.cloudflare.com/constellation/

import { CardGrid, Description, LinkTitleCard } from "~/components"

<Description>

Run machine learning models with Cloudflare Workers.
</Description>

Constellation allows you to run fast, low-latency inference tasks on pre-trained machine learning models natively on Cloudflare Workers. It supports some of the most popular machine learning (ML) and AI runtimes and multiple classes of models.

Cloudflare provides a curated list of verified models, or you can train and upload your own.

Functionality you can deploy to your application with Constellation:

* Content generation, summarization, or similarity analysis
* Question answering
* Audio transcription
* Image or audio classification
* Object detection
* Anomaly detection
* Sentiment analysis

***

## More resources

<CardGrid>

<LinkTitleCard title="Developer Discord" href="https://discord.cloudflare.com" icon="discord">
Connect with the Workers community on Discord to ask questions, show what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="@CloudflareDev" href="https://x.com/cloudflaredev" icon="x.com">
Follow @CloudflareDev on Twitter to learn about product announcements, and what is new in Cloudflare Workers.
</LinkTitleCard>

</CardGrid>

---

# Demos and architectures

URL: https://developers.cloudflare.com/d1/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use D1 within your existing application and architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for D1.

<ExternalResources type="apps" products={["D1"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use D1:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["D1"]} />

---

# Get started

URL: https://developers.cloudflare.com/d1/get-started/

import {
	Render,
	PackageManagers,
	Steps,
	FileTree,
	Tabs,
	TabItem,
	TypeScriptExample,
	WranglerConfig
} from "~/components";

This guide instructs you through:

- Creating your first database using D1, Cloudflare's native serverless SQL database.
- Creating a schema and querying your database via the command-line.
- Connecting a [Cloudflare Worker](/workers/) to your D1 database to query your D1 database programmatically.

You can perform these tasks through the CLI or through the Cloudflare dashboard.

:::note
If you already have an existing Worker and an existing D1 database, follow this tutorial from [3. Bind your Worker to your D1 database](/d1/get-started/#3-bind-your-worker-to-your-d1-database).
:::

## Prerequisites

<Render file="prereqs" product="workers" />

## 1. Create a Worker

Create a new Worker as the means to query your database.

<Tabs syncKey='CLIvDash'> <TabItem label='CLI'>

<Steps>
1. Create a new project named `d1-tutorial` by running:

    <PackageManagers type="create" pkg="cloudflare@latest" args={"d1-tutorial"} />

    <Render
    	file="c3-post-run-steps"
    	product="workers"
    	params={{
    	category: "hello-world",
    	type: "Hello World Worker",
    	lang: "TypeScript",
    	}}
    />

    This creates a new `d1-tutorial` directory as illustrated below.

    <FileTree>
    - d1-tutorial
    	- node_modules/
    	- test/
    	- src
    		- **index.ts**
    	- package-lock.json
    	- package.json
    	- testconfig.json
    	- vitest.config.mts
    	- worker-configuration.d.ts
    	- **wrangler.jsonc**
    </FileTree>

    Your new `d1-tutorial` directory includes:

    - A `"Hello World"` [Worker](/workers/get-started/guide/#3-write-code) in `index.ts`.
    - A [Wrangler configuration file](/workers/wrangler/configuration/). This file is how your `d1-tutorial` Worker accesses your D1 database.

</Steps>

:::note

If you are familiar with Cloudflare Workers, or initializing projects in a Continuous Integration (CI) environment, initialize a new project non-interactively by setting `CI=true` as an environmental variable when running `create cloudflare@latest`.

For example: `CI=true npm create cloudflare@latest d1-tutorial --type=simple --git --ts --deploy=false` creates a basic "Hello World" project ready to build on.

:::

</TabItem> <TabItem label='Dashboard'>

<Steps>
1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to your account > **Workers & Pages** > **Overview**.
3. Select **Create**.
4. Select **Create Worker**.
5. Name your Worker. For this tutorial, name your Worker `d1-tutorial`.
6. Select **Deploy**.
</Steps>
</TabItem> </Tabs>

## 2. Create a database

A D1 database is conceptually similar to many other databases: a database may contain one or more tables, the ability to query those tables, and optional indexes. D1 uses the familiar [SQL query language](https://www.sqlite.org/lang.html) (as used by SQLite).

To create your first D1 database:

<Tabs syncKey='CLIvDash'> <TabItem label='CLI'>

<Steps>
1. Change into the directory you just created for your Workers project:

    ```sh
    cd d1-tutorial
    ```

2. Run the following `wrangler d1` command and give your database a name. In this tutorial, the database is named `prod-d1-tutorial`:

   ```sh
   npx wrangler d1 create prod-d1-tutorial
   ```

   ```sh output

   âœ… Successfully created DB 'prod-d1-tutorial'

   [[d1_databases]]
   binding = "DB" # available in your Worker on env.DB
   database_name = "prod-d1-tutorial"
   database_id = "<unique-ID-for-your-database>"
   ```

</Steps>

This creates a new D1 database and outputs the [binding](/workers/runtime-apis/bindings/) configuration needed in the next step.

:::note

The `wrangler` command-line interface is Cloudflare's tool for managing and deploying Workers applications and D1 databases in your terminal. It was installed when you used `npm create cloudflare@latest` to initialize your new project.

:::

</TabItem> <TabItem label='Dashboard'>

<Steps>
1. Go to **Storage & Databases** > **D1**.
2. Select **Create**.
3. Name your database. For this tutorial, name your D1 database `prod-d1-tutorial`.
4. (Optional) Provide a location hint. Location hint is an optional parameter you can provide to indicate your desired geographical location for your database. Refer to [Provide a location hint](/d1/configuration/data-location/#provide-a-location-hint) for more information.
5. Select **Create**.

</Steps>

</TabItem>
</Tabs>

:::note

For reference, a good database name:

- Uses a combination of ASCII characters, shorter than 32 characters, and uses dashes (-) instead of spaces.
- Is descriptive of the use-case and environment. For example, "staging-db-web" or "production-db-backend".
- Only describes the database, and is not directly referenced in code.

:::

## 3. Bind your Worker to your D1 database

You must create a binding for your Worker to connect to your D1 database. [Bindings](/workers/runtime-apis/bindings/) allow your Workers to access resources, like D1, on the Cloudflare developer platform.

To bind your D1 database to your Worker:

<Tabs syncKey='CLIvDash'> <TabItem label='CLI'>

You create bindings by updating your Wrangler file.

<Steps>

1. Copy the lines obtained from [step 2](/d1/get-started/#2-create-a-database) from your terminal.
2. Add them to the end of your Wrangler file.

   <WranglerConfig>

   ```toml
   [[d1_databases]]
   binding = "DB" # available in your Worker on env.DB
   database_name = "prod-d1-tutorial"
   database_id = "<unique-ID-for-your-database>"
   ```

   </WranglerConfig>

   Specifically:

   - The value (string) you set for `binding` is the **binding name**, and is used to reference this database in your Worker. In this tutorial, name your binding `DB`.
   - The binding name must be [a valid JavaScript variable name](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Grammar_and_types#variables). For example, `binding = "MY_DB"` or `binding = "productionDB"` would both be valid names for the binding.
   - Your binding is available in your Worker at `env.<BINDING_NAME>` and the D1 [Workers Binding API](/d1/worker-api/) is exposed on this binding.

</Steps>

:::note
When you execute the `wrangler d1 create` command, the client API package (which implements the D1 API and database class) is automatically installed. For more information on the D1 Workers Binding API, refer to [Workers Binding API](/d1/worker-api/).
:::

You can also bind your D1 database to a [Pages Function](/pages/functions/). For more information, refer to [Functions Bindings for D1](/pages/functions/bindings/#d1-databases).

</TabItem> <TabItem label='Dashboard'>

You create bindings by adding them to the Worker you have created.

<Steps>
1. Go to **Workers & Pages** > **Overview**.
2. Select the `d1-tutorial` Worker you created in [step 1](/d1/get-started/#1-create-a-worker).
3. Select **Settings**.
4. Scroll to **Bindings**, then select **Add**.
5. Select **D1 database**.
6. Name your binding in **Variable name**, then select the `prod-d1-tutorial` D1 database you created in [step 2](/d1/get-started/#2-create-a-database) from the dropdown menu. For this tutorial, name your binding `DB`.
7. Select **Deploy** to deploy your binding. When deploying, there are two options:
	- **Deploy:** Immediately deploy the binding to 100% of your audience.
	- **Save version:** Save a version of the binding which you can deploy in the future.

    For this tutorial, select **Deploy**.

</Steps>

</TabItem>
</Tabs>

## 4. Run a query against your D1 database

### Configure your D1 database

<Tabs syncKey='CLIvDash'> <TabItem label='CLI'>

After correctly preparing your [Wrangler configuration file](/workers/wrangler/configuration/), set up your database. Use the example `schema.sql` file below to initialize your database.

<Steps>
1. Copy the following code and save it as a `schema.sql` file in the `d1-tutorial` Worker directory you created in step 1:

    ```sql
    DROP TABLE IF EXISTS Customers;
    CREATE TABLE IF NOT EXISTS Customers (CustomerId INTEGER PRIMARY KEY, CompanyName TEXT, ContactName TEXT);
    INSERT INTO Customers (CustomerID, CompanyName, ContactName) VALUES (1, 'Alfreds Futterkiste', 'Maria Anders'), (4, 'Around the Horn', 'Thomas Hardy'), (11, 'Bs Beverages', 'Victoria Ashworth'), (13, 'Bs Beverages', 'Random Name');
    ```

2. Initialize your database to run and test locally first. Bootstrap your new D1 database by running:

   ```sh
   npx wrangler d1 execute prod-d1-tutorial --local --file=./schema.sql
   ```

3. Validate your data is in your database by running:

   ```sh
   npx wrangler d1 execute prod-d1-tutorial --local --command="SELECT * FROM Customers"
   ```

   ```sh output
   ðŸŒ€ Mapping SQL input into an array of statements
   ðŸŒ€ Executing on local database production-db-backend (5f092302-3fbd-4247-a873-bf1afc5150b) from .wrangler/state/v3/d1:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ CustomerId â”‚ CompanyName         â”‚ ContactName       â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 1          â”‚ Alfreds Futterkiste â”‚ Maria Anders      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 4          â”‚ Around the Horn     â”‚ Thomas Hardy      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 11         â”‚ Bs Beverages        â”‚ Victoria Ashworth â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 13         â”‚ Bs Beverages        â”‚ Random Name       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```

</Steps>

</TabItem> <TabItem label='Dashboard'>

Use the Dashboard to create a table and populate it with data.

<Steps>
1. Go to **Storage & Databases** > **D1**.
2. Select the `prod-d1-tutorial` database you created in [step 2](/d1/get-started/#2-create-a-database).
3. Select **Console**.
4. Paste the following SQL snippet.

    ```sql
    DROP TABLE IF EXISTS Customers;
    CREATE TABLE IF NOT EXISTS Customers (CustomerId INTEGER PRIMARY KEY, CompanyName TEXT, ContactName TEXT);
    INSERT INTO Customers (CustomerID, CompanyName, ContactName) VALUES (1, 'Alfreds Futterkiste', 'Maria Anders'), (4, 'Around the Horn', 'Thomas Hardy'), (11, 'Bs Beverages', 'Victoria Ashworth'), (13, 'Bs Beverages', 'Random Name');
    ```

5. Select **Execute**. This creates a table called `Customers` in your `prod-d1-tutorial` database.
6. Select **Tables**, then select the `Customers` table to view the contents of the table.

</Steps>

</TabItem>
</Tabs>

### Write queries within your Worker

After you have set up your database, run an SQL query from within your Worker.

<Tabs syncKey='CLIvDash'> <TabItem label='CLI'>

<Steps>
1. Navigate to your `d1-tutorial` Worker and open the `index.ts` file. The `index.ts` file is where you configure your Worker's interactions with D1.
2. Clear the content of `index.ts`.
3. Paste the following code snippet into your `index.ts` file:

    <TypeScriptExample filename="index.ts">
    ```typescript
    export interface Env {
    	// If you set another name in the Wrangler config file for the value for 'binding',
    	// replace "DB" with the variable name you defined.
    	DB: D1Database;
    }

    export default {
    	async fetch(request, env): Promise<Response> {
    		const { pathname } = new URL(request.url);

    		if (pathname === "/api/beverages") {
    			// If you did not use `DB` as your binding name, change it here
    			const { results } = await env.DB.prepare(
    				"SELECT * FROM Customers WHERE CompanyName = ?",
    			)
    				.bind("Bs Beverages")
    				.all();
    			return Response.json(results);
    		}

    		return new Response(
    			"Call /api/beverages to see everyone who works at Bs Beverages",
    		);
    	},
    } satisfies ExportedHandler<Env>;
    ```
    </TypeScriptExample>

    In the code above, you:

    1. Define a binding to your D1 database in your TypeScript code. This binding matches the `binding` value you set in the [Wrangler configuration file](/workers/wrangler/configuration/) under `[[d1_databases]]`.
    2. Query your database using `env.DB.prepare` to issue a [prepared query](/d1/worker-api/d1-database/#prepare) with a placeholder (the `?` in the query).
    3. Call `bind()` to safely and securely bind a value to that placeholder. In a real application, you would allow a user to define the `CompanyName` they want to list results for. Using `bind()` prevents users from executing arbitrary SQL (known as "SQL injection") against your application and deleting or otherwise modifying your database.
    4. Execute the query by calling `all()` to return all rows (or none, if the query returns none).
    5. Return your query results, if any, in JSON format with `Response.json(results)`.

</Steps>

After configuring your Worker, you can test your project locally before you deploy globally.

</TabItem> <TabItem label='Dashboard'>

You can query your D1 database using your Worker.

<Steps>
1. Go to **Workers & Pages** > **Overview**.
2. Select the `d1-tutorial` Worker you created.
3. Select **Edit Code**.
4. Clear the contents of the `worker.js` file, then paste the following code:

    ```js
    export default {
    	async fetch(request, env) {
    		const { pathname } = new URL(request.url);

    		if (pathname === "/api/beverages") {
    			// If you did not use `DB` as your binding name, change it here
    			const { results } = await env.DB.prepare(
    				"SELECT * FROM Customers WHERE CompanyName = ?"
    			)
    				.bind("Bs Beverages")
    				.all();
    			return new Response(JSON.stringify(results), {
    				headers: { 'Content-Type': 'application/json' }
    			});
    		}

    		return new Response(
    			"Call /api/beverages to see everyone who works at Bs Beverages"
    		);
    	},
    };
    ```

5. Select **Save**.

</Steps>
</TabItem>
</Tabs>

## 5. Deploy your database

Deploy your database on Cloudflare's global network.

<Tabs syncKey='CLIvDash'> <TabItem label='CLI'>

To deploy your Worker to production using Wrangler, you must first repeat the [database configuration](/d1/get-started/#configure-your-d1-database) steps after replacing the `--local` flag with the `--remote` flag to give your Worker data to read. This creates the database tables and imports the data into the production version of your database.

<Steps>
1. Bootstrap your database with the `schema.sql` file you created in step 4:

    ```sh
    npx wrangler d1 execute prod-d1-tutorial --remote --file=./schema.sql
    ```

2. Validate the data is in production by running:

   ```sh
   npx wrangler d1 execute prod-d1-tutorial --remote --command="SELECT * FROM Customers"
   ```

3. Deploy your Worker to make your project accessible on the Internet. Run:

   ```sh
   npx wrangler deploy
   ```

   ```sh output
   Outputs: https://d1-tutorial.<YOUR_SUBDOMAIN>.workers.dev
   ```

   You can now visit the URL for your newly created project to query your live database.

   For example, if the URL of your new Worker is `d1-tutorial.<YOUR_SUBDOMAIN>.workers.dev`, accessing `https://d1-tutorial.<YOUR_SUBDOMAIN>.workers.dev/api/beverages` sends a request to your Worker that queries your live database directly.

4. Test your database is running successfully. Add `/api/beverages` to the provided Wrangler URL. For example, `https://d1-tutorial.<YOUR_SUBDOMAIN>.workers.dev/api/beverages`.

</Steps>

</TabItem> <TabItem label='Dashboard'>
<Steps>

1. Go to **Workers & Pages** > **Overview**.
2. Select your `d1-tutorial` Worker.
3. Select **Deployments**.
4. From the **Version History** table, select **Deploy version**.
5. From the **Deploy version** page, select **Deploy**.

</Steps>

This deploys the latest version of the Worker code to production.

</TabItem></Tabs>

## 6. (Optional) Develop locally with Wrangler

If you are using D1 with Wrangler, you can test your database locally. While in your project directory:

<Steps>
1. Run `wrangler dev`:

    ```sh
    npx wrangler dev
    ```

    When you run `wrangler dev`, Wrangler provides a URL (most likely `localhost:8787`) to review your Worker.

2. Go to the URL.

   The page displays `Call /api/beverages to see everyone who works at Bs Beverages`.

3. Test your database is running successfully. Add `/api/beverages` to the provided Wrangler URL. For example, `localhost:8787/api/beverages`.

</Steps>

If successful, the browser displays your data.

:::note
You can only develop locally if you are using Wrangler. You cannot develop locally through the Cloudflare dashboard.
:::

## 7. (Optional) Delete your database

To delete your database:

<Tabs syncKey='CLIvDash'> <TabItem label='CLI'>

Run:

```sh
npx wrangler d1 delete prod-d1-tutorial
```

</TabItem><TabItem label='Dashboard'>

<Steps>
1. Go to **Storages & Databases** > **D1**.

2. Select your `prod-d1-tutorial` D1 database.

3. Select **Settings**.

4. Select **Delete**.

5. Type the name of the database (`prod-d1-tutorial`) to confirm the deletion.

</Steps>

</TabItem> </Tabs>

If you want to delete your Worker:

<Tabs syncKey='CLIvDash'> <TabItem label='CLI'>

Run:

```sh
npx wrangler delete d1-tutorial
```

</TabItem> <TabItem label='Dashboard'>

<Steps>
1. Go to **Workers & Pages** > **Overview**.

2. Select your `d1-tutorial` Worker.

3. Select **Settings**.

4. Scroll to the bottom of the page, then select **Delete**.

5. Type the name of the Worker (`d1-tutorial`) to confirm the deletion.

</Steps>

</TabItem></Tabs>

## Summary

In this tutorial, you have:

- Created a D1 database
- Created a Worker to access that database
- Deployed your project globally

## Next steps

If you have any feature requests or notice any bugs, share your feedback directly with the Cloudflare team by joining the [Cloudflare Developers community on Discord](https://discord.cloudflare.com).

- See supported [Wrangler commands for D1](/workers/wrangler/commands/#d1).
- Learn how to use [D1 Worker Binding APIs](/d1/worker-api/) within your Worker, and test them from the [API playground](/d1/worker-api/#api-playground).
- Explore [community projects built on D1](/d1/reference/community-projects/).

---

# Overview

URL: https://developers.cloudflare.com/d1/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Description>


Create new serverless SQL databases to query from your Workers and Pages projects.


</Description>

<Plan type="workers-all" />

D1 is Cloudflare's managed, serverless database with SQLite's SQL semantics, built-in disaster recovery, and Worker and HTTP API access.

D1 is designed for horizontal scale out across multiple, smaller (10 GB) databases, such as per-user, per-tenant or per-entity databases. D1 allows you to build applications with thousands of databases at no extra cost for isolating with multiple databases. D1 pricing is based only on query and storage costs.

Create your first D1 database by [following the Get started guide](/d1/get-started/), learn how to [import data into a database](/d1/best-practices/import-export-data/), and how to [interact with your database](/d1/worker-api/) directly from [Workers](/workers/) or [Pages](/pages/functions/bindings/#d1-databases).

***

## Features

<Feature header="Create your first D1 database" href="/d1/get-started/" cta="Create your D1 database">

Create your first D1 database, establish a schema, import data and query D1 directly from an application [built with Workers](/workers/).


</Feature>

<Feature header="SQLite" href="/d1/sql-api/sql-statements/" cta="Execute SQL queries">

Execute SQL with SQLite's SQL compatibility and D1 Client API.


</Feature>

<Feature header="Time Travel" href="/d1/reference/time-travel/" cta="Learn about Time Travel">

Time Travel is D1â€™s approach to backups and point-in-time-recovery, and allows you to restore a database to any minute within the last 30 days.


</Feature>

***

## Related products

<RelatedProduct header="Workers" href="/workers/" product="workers">

Build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale.


</RelatedProduct>

<RelatedProduct header="Pages" href="/pages/" product="pages">

Deploy dynamic front-end applications in record time.


</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Pricing" href="/d1/platform/pricing/" icon="seti:shell">
Learn about D1's pricing and how to estimate your usage.
</LinkTitleCard>

<LinkTitleCard title="Limits" href="/d1/platform/limits/" icon="document">
Learn about what limits D1 has and how to work within them.
</LinkTitleCard>

<LinkTitleCard title="Community projects" href="/d1/reference/community-projects/" icon="pen">
Browse what developers are building with D1.
</LinkTitleCard>

<LinkTitleCard title="Storage options" href="/workers/platform/storage-options/" icon="document">
Learn more about the storage and database options you can build on with Workers.
</LinkTitleCard>

<LinkTitleCard title="Developer Discord" href="https://discord.cloudflare.com" icon="discord">
Connect with the Workers community on Discord to ask questions, show what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="@CloudflareDev" href="https://x.com/cloudflaredev" icon="x.com">
Follow @CloudflareDev on Twitter to learn about product announcements, and what is new in Cloudflare Developer Platform.
</LinkTitleCard>

</CardGrid>

---

# Wrangler commands

URL: https://developers.cloudflare.com/d1/wrangler-commands/

import { Render, Type, MetaInfo } from "~/components"

D1 Wrangler commands use REST APIs to interact with the control plane. This page lists the Wrangler commands for D1.

<Render file="wrangler-commands/d1" product="workers" />

## Global commands

<Render file="wrangler-commands/global-flags" product="workers" />

## Experimental commands

### `insights`

Returns statistics about your queries.

```sh
npx wrangler d1 insights <database_name> --<option>
```

For more information, see [Query `insights`](/d1/observability/metrics-analytics/#query-insights).

---

# Changelog

URL: https://developers.cloudflare.com/data-localization/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/data-localization.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Product compatibility

URL: https://developers.cloudflare.com/data-localization/compatibility/

The table below provides a summary of the Data Localization Suite product's behavior with Cloudflare products. Refer to the table legend for guidance on interpreting the table.

âœ… Product works with no caveats <br/>
ðŸš§ Product can be used with some caveats <br/>
âœ˜ Product cannot be used <br/>
âš«ï¸ Not applicable

## Application Performance

| Product                | Geo Key Manager | Regional Services | Customer Metadata Boundary |
| ---------------------- | --------------- | ----------------- | -------------------------- |
| Caching/CDN            | âœ…              | âœ…                 | âœ…                       |
| Cache Reserve          | âš«ï¸              | ðŸš§                 | âœ… [^29]                 |
| DNS                    | âš«ï¸              | ðŸš§ [^33]           | ðŸš§ [^32]                 |
| HTTP/3 (with QUIC)     | âš«ï¸              | âœ˜                  | âš«ï¸                       |
| Image Resizing         | âœ…              | âœ… [^6]           | ðŸš§ [^1]                  |
| Load Balancing         | âœ…              | âœ…                 | ðŸš§ [^1]                  |
| Onion Routing          | âœ˜               | âœ˜                  | âœ˜                        |
| Orange-to-Orange (O2O) | âœ˜               | âœ˜                  | âœ˜                        |
| Stream Delivery        | âœ…              | âœ…                 | âœ…                       |
| Tiered Caching         | âœ…              | ðŸš§ [^2]            | ðŸš§ [^30]                 |
| Trace                  | âœ˜               | âœ˜                  | âœ˜                        |
| Waiting Room           | âš«ï¸              | âœ…                 | âœ…                       |
| Zaraz                  | âœ…              | âœ…                 | âœ…                       |

***

## Application Security

| Product                      | Geo Key Manager | Regional Services | Customer Metadata Boundary |
| ---------------------------- | --------------- | ----------------- | -------------------------- |
| Advanced Certificate Manager | âš«ï¸              | âš«ï¸                | âš«ï¸                        |
| Advanced DDoS Protection     | âœ…              | âœ…                | ðŸš§ [^3]                   |
| API Shield                   | âœ…              | âœ…                | ðŸš§ [^4]                    |
| Bot Management               | âœ…              | âœ…                | ðŸš§ [^5]                   |
| DNS Firewall                 | âš«ï¸              | âš«ï¸                | ðŸš§ [^22]                   |
| Page Shield                  | âœ…              | âœ…                | âœ…                        |
| Rate Limiting                | âœ…              | âœ…                | âœ… [^37]                   |
| SSL                          | âœ…              | âœ…                | âœ…                        |
| Cloudflare for SaaS          | âœ˜               | âœ…                | âœ…                        |
| Turnstile                    | âš«ï¸              | âœ˜                 | âœ… [^38]                       |
| WAF/L7 Firewall              | âœ…              | âœ…                | âœ…                        |
| DMARC Management             | âš«ï¸              | âš«ï¸                | âœ…                        |

***

## Developer Platform

| Product                      | Geo Key Manager | Regional Services | Customer Metadata Boundary |
| ---------------------------- | --------------- | ----------------- | -------------------------- |
| Cloudflare Images            | âš«ï¸              | âœ… [^36]          | ðŸš§ [^35]                    |
| AI Gateway                   | âœ˜               | âœ˜                | ðŸš§ [^39]                    |
| Cloudflare Pages             | âœ… [^11]        | âœ… [^11]          | ðŸš§ [^1]                   |
| Cloudflare D1                | âš«ï¸             |  âš«ï¸                | ðŸš§ [^40]                   |
| Durable Objects              | âš«ï¸              | âœ… [^7]           | ðŸš§ [^1]                   |
| Email Routing                | âš«ï¸              | âš«ï¸                | âœ…                        |
| R2                           | âœ… [^27]        | âœ… [^8]           | âœ… [^28]                  |
| Smart Placement              | âš«ï¸              | âœ˜                 | âœ˜                         |
| Stream                       | âš«ï¸              | âœ˜                 | ðŸš§ [^1]                    |
| Workers (deployed on a Zone) | âœ…              | âœ…                | ðŸš§ [^41]                   |
| Workers AI                   | âš«ï¸              | âœ˜                 | âœ…                        |
| Workers KV                   | âš«ï¸              | âœ˜                 | âœ… [^34]                   |
| Workers.dev                  | âœ˜              | âœ˜                 | âœ˜                         |
| Workers Analytics Engine (WAE) | âš«ï¸           | âš«ï¸                 | ðŸš§ [^1]                    |

***

## Network Services

| Product            | Geo Key Manager | Regional Services | Customer Metadata Boundary |
| ------------------ | --------------- | ----------------- | -------------------------- |
| Argo Smart Routing | âœ…              | âœ˜ [^9]           | âœ˜ [^10]                    |
| Static IP/BYOIP    | âš«ï¸              | âœ… [^26]         | âš«ï¸                         |
| Magic Firewall     | âš«ï¸              | âš«ï¸               | âœ…                         |
| Magic Network Monitoring | âš«ï¸        | âš«ï¸               | ðŸš§ [^1]                    |
| Magic Transit      | âš«ï¸              | âš«ï¸               | ðŸš§ [^1]                    |
| Magic WAN          | âš«ï¸              | âš«ï¸               | âœ…                         |
| Spectrum           | âœ…              | âœ… [^42]         | âœ…                         |

***

## Platform

| Product | Geo Key Manager | Regional Services | Customer Metadata Boundary |
| ------- | --------------- | ----------------- | -------------------------- |
| Logpull | âš«ï¸              | âš«ï¸               | ðŸš§ [^12]                   |
| Logpush | âš«ï¸              | âœ…               | ðŸš§ [^13]                   |

***

## Zero Trust

| Product           | Geo Key Manager | Regional Services | Customer Metadata Boundary |
| ----------------- | --------------- | ----------------- | -------------------------- |
| Access            | ðŸš§ [^14]        | ðŸš§ [^15]           | ðŸš§ [^16]                 |
| Browser Isolation | âš«ï¸              | ðŸš§ [^17]           | âœ…                       |
| CASB              | âš«ï¸              | âš«ï¸                 | âœ˜                        |
| Cloudflare Tunnel | âš«ï¸              | ðŸš§ [^18]           | âš«ï¸                       |
| DLP               | âš«ï¸ [^19]        | âš«ï¸ [^19]           | ðŸš§ [^31]                 |
| Gateway           | ðŸš§ [^20]        | ðŸš§ [^21]           | ðŸš§ [^22]                 |
| WARP              | âš«ï¸              | âš«ï¸                 | ðŸš§ [^1]                  |


[^1]: Logs / Analytics not available outside US region when using Customer Metadata Boundary.
[^2]: Regular and Custom Tiered Cache works; Smart Tiered Caching not available with Regional Services.
[^3]: Adaptive DDoS Protection is only supported for US CMB.
[^4]: Features such as API Discovery and Volumetric Abuse Detection will not work with CMB set to EU only.
[^5]: Some advanced Enterprise features, including the [Anomaly Detection engine](/bots/concepts/bot-score/#anomaly-detection), are not available.
[^6]: Only when using a Custom Domain set to a region, either through Workers or [Transform Rules](/images/transform-images/serve-images-custom-paths/) within the same zone.
[^7]: [Jurisdiction restrictions for Durable Objects](/durable-objects/reference/data-location/#restrict-durable-objects-to-a-jurisdiction).
[^8]: Only when using a [Custom Domain](/r2/buckets/public-buckets/#connect-a-bucket-to-a-custom-domain) set to a region and using [jurisdictions with the S3 API](/r2/reference/data-location/#using-jurisdictions-with-the-s3-api).
[^9]: Argo cannot be used with Regional Services.
[^10]: Argo cannot be used with Customer Metadata Boundary.
[^11]: Only when using [Custom Domain](/pages/configuration/custom-domains/) set to a region.
[^12]: Logpull not available when using Customer Metadata Boundary outside US region. Logs may be stored and retrieved with [Logs Engine](https://blog.cloudflare.com/announcing-logs-engine/) which is adding region support in 2025.
[^13]: Logpush available with Customer Metadata Boundary for [these datasets](/data-localization/metadata-boundary/logpush-datasets/). Contact your account team if you need another dataset.
[^14]: Access App SSL keys can use Geo Key Manager. [Access JWT](/cloudflare-one/identity/authorization-cookie/validating-json/) is not yet localized.
[^15]: Can be localized to US FedRAMP region only. More regions coming in 2024.
[^16]: Customer Metadata Boundary can be used to limit data transfer outside region, but Access User Logs will not be available outside US region.
[^17]: Currently may only be used with US FedRAMP region.
[^18]: The only connectivity option is [US FedRAMP region](/cloudflare-one/connections/connect-networks/configure-tunnels/cloudflared-parameters/run-parameters/#region). Regional Services only applies when using [Public Hostnames](/cloudflare-one/connections/connect-networks/routing-to-tunnel/) set to a region.
[^19]: Uses Gateway HTTP and CASB.
[^20]: You can [bring your own certificate](https://blog.cloudflare.com/bring-your-certificates-cloudflare-gateway/) to Gateway but these cannot yet be restricted to a specific region.
[^21]: Gateway HTTP supports Regional Services. Gateway DNS does not yet support regionalization. <br/> ICMP proxy and WARP-to-WARP proxy are not available to Regional Services users.
[^22]: Dashboard Analytics and Logs are empty when using CMB outside the US region. Use Logpush instead.
[^26]: Static IP/BYOIP can be used with the legacy Spectrum setup.
[^27]: Only when using a Custom Domain and a [Custom Certificate](/r2/reference/data-security/#encryption-in-transit) or [Keyless SSL](/ssl/keyless-ssl/).
[^28]: R2 Dashboard [Metrics and Analytics](/r2/platform/metrics-analytics/) are populated. Additionally, [Jurisdictional Restrictions](/r2/reference/data-location/#jurisdictional-restrictions) guarantee objects in a bucket are stored within a specific jurisdiction.
[^29]: You cannot yet specify region location for object storage itself.
[^30]: Regular/Generic and Custom Tiered Cache works; Smart Tiered Caching does not work with Customer Metadata Boundary (CMB). <br/> With CMB set to EU, the Zone Dashboard **Caching** > **Tiered Cache** > **Smart Tiered Caching** option will not populate the Dashboard Analytics.
[^31]: DLP is part of Gateway HTTP, however, [DLP datasets](/cloudflare-one/policies/data-loss-prevention/datasets/#use-dlp-datasets) are not available outside US region when using Customer Metadata Boundary.
[^32]: Dashboard Analytics are empty when using CMB outside the US region. Use Logpush instead.
[^33]: [Outgoing zone transfers](/dns/zone-setups/zone-transfers/cloudflare-as-primary/) will carry Earth region proxy IPs, thus making regional service dysfunctional when non-Cloudflare nameservers respond to the DNS queries.
[^34]: Jurisdictional Restrictions (storage) for Workers KV pairs is not supported today.
[^35]: Logs / Analytics not available outside US region when using Customer Metadata Boundary. Jurisdictional Restrictions (storage) options are not supported today.
[^36]: Only when using a [Custom Domain](/images/manage-images/serve-images/serve-from-custom-domains/) set to a region.
[^37]: Legacy Zone Analytics & Logs section not available outside US region when using CMB. Use [Security Analytics](/waf/analytics/security-analytics/) instead.
[^38]: [Turnstile Analytics](/turnstile/turnstile-analytics/) are available. However, there are no regionalization guarantees for the Siteverify API yet.
[^39]: Jurisdictional Restrictions (storage) options for [Logs](/ai-gateway/observability/logging/) are not supported today.
[^40]: Jurisdictional Restrictions ([data location](/d1/configuration/data-location/) / storage) options are not supported today.
[^41]: Logs / Analytics not available outside US region when using Customer Metadata Boundary. Use Logpush instead.
[^42]: Only applies to HTTP/S Spectrum applications.

---

# FAQs

URL: https://developers.cloudflare.com/data-localization/faq/

## Are DLP and DLS the same?

No, they are not. DLP stands for [Data Loss Prevention](/cloudflare-one/policies/data-loss-prevention/), and it is part of Cloudflareâ€™s Zero Trust offering (requiring Gateway). It allows customers to scan web traffic and SaaS apps for sensitive data like secret keys, financial information (credit card numbers), and other keywords.

[Data Localization Suite](/data-localization/) (DLS) is a suite of features that can provide localization and data residency features.


## Are Cloudflareâ€™s services GDPR compliant?

Yes, even without DLS, Cloudflare services are designed to satisfy the GDPRâ€™s requirements. Cloudflare services are also verified compliant with the EU Cloud CoC, Verification-ID: 2023LVL02SCOPE4316. For further information, visit EU Cloud CoC [public register](https://eucoc.cloud/en/public-register).


## How can I use DLS?

Once you have purchased DLS, the post-sales team will entitle DLS for you, and you will be able to configure all features by yourself via dashboard or API. You can find more specific information under the [Configuration guides](/data-localization/how-to/) section.

## Does Regional Services work with HTTP/3 / QUIC?

Not yet.


## Are there other options if I prefer not to have Cloudflare handle TLS termination (decryption)?

Yes, you have these options available:

* [Spectrum TCP/UDP Apps](/spectrum/) (without TLS Termination)
* [Magic Transit](/magic-transit/)
* [Privacy Gateway](/privacy-gateway/)

These options only offer L3/L4 DDoS protection and using them imply that no application / L7 security or performance services can be applied.

---

# Geo Key Manager

URL: https://developers.cloudflare.com/data-localization/geo-key-manager/

Geo Key Manager offers enhanced control over the storage location of private SSL/TLS keys, ensuring compliance with regional data regulations and security requirements.

## Customize key storage

By default, private keys will be encrypted and securely distributed to each data center, where they can be utilized for local SSL/TLS termination. Geo Key Manager allows you to choose where you want to store your private keys.

Geo Key Manager was restricted to the US, EU, and high-security data centers, but with the new version of Geo Key Manager, available in [Closed Beta](https://blog.cloudflare.com/configurable-and-scalable-geo-key-manager-closed-beta/), you can now create `allowlists` and `blocklists` of countries in which your private keys will be stored. That means that you will be able define specific geographic locations where to store keys, for instance you can store your private keys exclusively in Australia or limit private keys storage to the EU and the UK.

## Cloudflare data center flow example

The following diagram is a high-level example of the flow of the Cloudflare data centers without private TLS key. In this process, data centers have to request and create temporary Session Keys to perform TLS termination by reaching out to Cloudflare data centers which hold the private TLS keys:

<br/>

```mermaid
sequenceDiagram
    participant User as End user
    participant CloudflarePoP as Closest data center without TLS Key
    participant CloudflarePoPwTLS as Data center with TLS Key

    User->>CloudflarePoP: Initial request
    Note right of CloudflarePoP: Closest data center cannot decrypt
    CloudflarePoP-->>CloudflarePoPwTLS: Requests TLS Signature
    CloudflarePoPwTLS-->>CloudflarePoP: Sends TLS Signature in order to establish Session Key
    Note right of CloudflarePoP: Decrypts and performs business logic (for example, WAF, Configuration Rules, Load Balancing)
    CloudflarePoP-->>User: Subsequent requests use the Session Key
    User-->>CloudflarePoP: Subsequent requests use the Session Key
```

<br/>

For detailed information on setup and supported options, refer to [Geo Key Manager documentation](/ssl/edge-certificates/geokey-manager/).

---

# Data Localization Suite

URL: https://developers.cloudflare.com/data-localization/

import { CardGrid, Feature, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Plan type="ent-add-on" />

The Data Localization Suite (DLS) is a collection of tools that enable customers to choose the location where Cloudflare inspects and stores data, while maintaining the security and performance benefits of our global network.

***

## Features

<Feature header="Geo Key Manager" href="/data-localization/geo-key-manager/">

Keep your keys within a specified region, ensuring compliance and data sovereignty.

</Feature>

<Feature header="Customer Metadata Boundary" href="/data-localization/metadata-boundary/">

Ensure that any traffic metadata that can identify your end user stays in the region you selected.


</Feature>

<Feature header="Regional Services" href="/data-localization/regional-services/">

Comply with regional restrictions by choosing which subset of data centers decrypts and services your HTTPS traffic.


</Feature>

***

## Related products

<RelatedProduct header="SSL/TLS" href="/ssl/" product="ssl">
Cloudflare SSL/TLS encrypts your web traffic to prevent data theft and other tampering.
</RelatedProduct>

<RelatedProduct header="DNS" href="/dns/" product="dns">
Cloudflare's global DNS platform provides speed and resilience. DNS customers also benefit from free DNSSEC, and protection against route leaks and hijacking.
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Resource hub" href="https://www.cloudflare.com/resource-hub/?topic=Privacy" icon="document">
Refer to our latest resources to learn more about privacy.
</LinkTitleCard>

<LinkTitleCard title="Cloudflare blog" href="https://blog.cloudflare.com/tag/data-localization-suite" icon="open-book">
Read articles about the latest updates to the Data Localization Suite.
</LinkTitleCard>

</CardGrid>

---

# Limitations

URL: https://developers.cloudflare.com/data-localization/limitations/

import { Render } from "~/components"

There are some caveats and limitations when deploying Data Localization Suite features.

Cloudflare is working hard to improve this offering and fill the gaps. If you have a specific feature request, please contact your [Account Team](/support/contacting-cloudflare-support/).

## Key Management

When using Geo Key Manager or Keyless SSL, some caveats may apply.

The remote procedure call from the server to the key server can add latency to the handshake, slowing down connection establishment. The additional latency cost corresponds to the round-trip time from the server to the key server, which can be as much as a second if the key server is on the other side of the world. Luckily, this latency cost only applies to the first time you connect to a server. Once the handshake is complete, the key server is not involved. Furthermore, if you reconnect to a site you do not have to pay the latency cost either because resuming a connection with TLS Session Resumption does not require the private key. Hence, latency is only added for the initial connection.

Learn more about how it works in our [blog post](https://blog.cloudflare.com/geo-key-manager-how-it-works/).

## Regional Services

When using Regional Services, some caveats and limitations may apply.

For product-specific caveats, refer to [Cloudflare product compatibility](/data-localization/compatibility/) page.

The following features and protocols are not supported by Regional Services and might not work on regionalized hostnames:

* [ICMP](https://www.cloudflare.com/learning/ddos/glossary/internet-control-message-protocol-icmp/)
* [Encrypted Client Hello (ECH)](/ssl/edge-certificates/ech/)
* [Orange-to-Orange (O2O)](/cloudflare-for-platforms/cloudflare-for-saas/saas-customers/how-it-works/)
* [Onion Routing (Tor)](/network/onion-routing/)

Since Regional Services leverages Spectrum in the background, [Spectrum limitations](/spectrum/reference/limitations/) apply.

Regional Services does not apply to [Subrequests](/workers/platform/limits/#subrequests). Regional Services operates on your hostname's IPs. We recommend using [DNSSEC](/learning-paths/application-security/default-traffic-security/dnssec/) and/or [DNS over HTTPS](/1.1.1.1/encryption/dns-over-https/) to ensure that DNS responses are secure and correct.

## Customer Metadata Boundary

There are certain limitations and caveats when using Customer Metadata Boundary.

Specifically most of the Zone Analytics & Logs UI Tabs will be showing up as empty, when configuring Customer Metadata Boundary to EU only. It is recommended to use the UI [Security Analytics](/waf/analytics/security-analytics/) instead, or the [HTTP request](/logs/reference/log-fields/zone/http_requests/) logs via [Logpush](/logs/about/).

To configure Customer Metadata Boundary to EU only, you must disable Log Retention for all zones within your account. Log Retention is a legacy feature of [Logpull](/logs/logpull/).

For product-specific caveats, refer to [Cloudflare product compatibility](/data-localization/compatibility/) page.

### Data unavailability

<Render file="customer_metadata_boundary_error" product="analytics" />

### Dashboard UI Analytics

In some cases, when using Customer Metadata Boundary set to the EU, some Dashboard UI Analytics might show up empty.

---

# Region support

URL: https://developers.cloudflare.com/data-localization/region-support/

Support by product and region is summarized in the following table:

| Region                                          | Geo Key Manager | Regional Services | Customer Metadata Boundary    |
| ----------------------------------              | --------------- | ----------------- | ----------------------------- |
| US                                              | âœ…              | âœ…                | âœ…                            |
| EU                                              | âœ…              | âœ…                | âœ…                            |
| UK                                              | âœ…              | âœ…                | Can use EU metadata boundary. |
| Canada                                          | âœ…              | âœ…                | âœ˜                             |
| Australia                                       | âœ…              | âœ…                | âœ˜                             |
| Japan                                           | âœ…              | âœ…                | âœ˜                             |
| India                                           | âœ…              | âœ…                | âœ˜                             |
| ISO 27001 Certified European Union              | âœ…              | âœ…                | Can use EU metadata boundary. |
| Germany                                         | âœ…              | âœ…                | Can use EU metadata boundary. |
| Singapore                                       | âœ…              | âœ…                | âœ˜                             |
| South Korea                                     | âœ…              | âœ…                | âœ˜                             |
| Austria                                         | âœ˜               | âœ…                | Can use EU metadata boundary. |
| Brazil                                          | âœ˜               | âœ…                | âœ˜                             |
| Cloudflare Green Energy                         | âœ˜               | âœ…                | âœ˜                             |
| Exclusive of Hong Kong and Macau                | âœ˜               | âœ…                | âœ˜                             |
| Exclusive of Russia and Belarus                 | âœ˜               | âœ…                | âœ˜                             |
| France                                          | âœ˜               | âœ…                | Can use EU metadata boundary. |
| Hong Kong                                       | âœ˜               | âœ…                | âœ˜                             |
| Italy                                           | âœ˜               | âœ…                | Can use EU metadata boundary. |
| NATO                                            | âœ˜               | âœ…                | âœ˜                             |
| Netherlands                                     | âœ˜               | âœ…                | Can use EU metadata boundary. |
| Russia                                          | âœ˜               | âœ…                | âœ˜                             |
| Saudi Arabia                                    | âœ˜               | âœ…                | âœ˜                             |
| South Africa                                    | âœ˜               | âœ…                | âœ˜                             |
| Spain                                           | âœ˜               | âœ…                | Can use EU metadata boundary. |
| Switzerland                                     | âœ˜               | âœ…                | âœ˜                             |
| Taiwan                                          | âœ˜               | âœ…                | âœ˜                             |
| US State of California                          | âœ˜               | âœ…                | âœ˜                             |
| US State of Florida                             | âœ˜               | âœ…                | âœ˜                             |
| US State of Texas                               | âœ˜               | âœ…                | âœ˜                             |
| FedRAMP Moderate Compliant (Domestic) [^1]      | âœ…              | âœ…                | âœ…                            |
| FedRAMP Moderate Compliant (International) [^2] | âœ˜               | âœ…                | âœ…                            |


Only supported in [Geo Key Manager v2](/ssl/edge-certificates/geokey-manager/).
[^1]: FedRAMP Moderate Compliant (Domestic) was previously named FedRAMP Compliant. It consists of our FedRAMP Moderate compliant data centers in the US.
[^2]: FedRAMP Moderate Compliant (International) consists of all our FedRAMP Moderate compliant data centers globally.

---

# Botnet Threat Feed

URL: https://developers.cloudflare.com/ddos-protection/botnet-threat-feed/

The Cloudflare DDoS Botnet Threat Feed is a threat intelligence feed for service providers (SPs) such as hosting providers and Internet service providers (ISPs) that provides information about their own IP addresses that have participated in HTTP DDoS attacks as observed from Cloudflare's global network. The feed aims to help service providers stop the abuse and reduce DDoS attacks originating from within their networks.

Each offense is a mitigated HTTP request from the specific IP address. For example, if an IP has 3,000 offenses, it means that Cloudflare has mitigated 3,000 HTTP requests from that IP.

A service provider can only get information about IP addresses associated with their autonomous system numbers (ASNs). The affiliation of a service provider with their ASNs will be checked against [PeeringDB](https://www.peeringdb.com/), a reliable and globally recognized interconnection database.

To ensure the feed's accuracy, Cloudflare will only include IP addresses that have participated in multiple HTTP DDoS attacks and have triggered high-confidence rules.

## Context

A single DDoS attack consisting of thousands of bots can involve as little as one single IP per service provider. Service providers usually only see a small fraction of the attack traffic leaving their network, and it can be hard to correlate it to malicious activity, while trying to identify abusers.

In the case of HTTPS DDoS attacks, service providers only see encrypted payloads leaving their network without any possibility to decrypt or understand if it is malicious or legitimate traffic. However, Cloudflare can see an entire attack and all of its sources if the attack targets an Internet property that uses Cloudflare's services. This global view can help service providers stop the abusers.

For more details, refer to [How DDoS protection works](/ddos-protection/about/how-ddos-protection-works/).

## Availability

The Cloudflare DDoS Botnet Threat Feed is available for free to service providers. For more information, refer to the [Terms of Use](https://www.cloudflare.com/en-gb/service-specific-terms-application-services/#ddos-botnet-threat-feed).

---

## Before you begin

Make sure that:

- You have [created a Cloudflare account](/fundamentals/setup/account/).

## Get started

### 1. Authenticate your ASN via PeeringDB

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **Manage Account** > **Configurations**.
3. Select **DDoS Threat Feed ASNs**.
4. On the list of ASNs configured for your threat feed, select **Add ASN**.
5. You will be redirected to the PeeringDB authentication page, where you can log in and consent to share the affiliation data with us. You will be redirected back to the configuration page once it is successful.

:::note
You can add multiple ASNs to your threat feed.
:::

### 2. Obtain Cloudflare API token

You must [obtain a Cloudflare API token](/fundamentals/api/get-started/create-token/) with at least the following account-level permission:

- _DDoS Botnet Feed_ > _Read_

### 3. Call Botnet Threat Feed API

Invoke one of the Botnet Threat Feed API endpoints:

- [Get full report](#get-full-report)
- [Get day report](#get-day-report)

---

## Available API endpoints

:::caution[Important notes]

- The API URI path is planned to change from `.../botnet_feed/...` to `.../ddos_botnet_feed/...` in the future.
- Responses with no IP addresses in the results (empty state) will return an `HTTP 200` status code (success), with an empty list in the `result` property.
- When the response is a success but the result is `0` or `null`, this means that there are no detected offenses.
:::

To invoke an API endpoint, append the operation endpoint to the Cloudflare API base URL:

```txt
https://api.cloudflare.com/client/v4
```

### Get full report

Retrieves all the data in the botnet tracking database for a given ASN (currently two weeks worth of data).

- HTTP verb: `GET`
- Operation endpoint: `/accounts/{account_id}/botnet_feed/asn/{asn}/full_report`

The provided `{asn}` must be affiliated with your account.

```bash title="Example request"
curl "https://api.cloudflare.com/client/v4/accounts/{account_id}/botnet_feed/asn/{asn}/full_report" \
--header "Content-Type: application/json" \
--header "Authorization: Bearer <API_TOKEN>"
```

```json title="Example response"
{
  "result": [
    {
      "cidr": "127.0.0.1/32",
      "date": "1970-01-01T00:00:00Z",
      "offense_count": 10000
    },
    // ... other entries ...
  ],
  "success": true,
  "errors": [],
  "messages": []
}
```

### Get day report

Retrieves all the data the botnet tracking database has for a given ASN on a given date. This operation currently allows dates greater than two weeks prior, but in this case it will return an empty dataset (the database currently stores two-weeks worth of data).

- HTTP verb: `GET`
- Operation endpoint: `/accounts/{account_id}/botnet_feed/asn/{asn}/day_report?date={date}`

The provided `{asn}` must be affiliated with your account.

`{date}` must be an ISO 8601-formatted date: `YYYY-MM-DD`. If no date is specified, the API responds with the data from the day before.

```bash title="Example request"
curl "https://api.cloudflare.com/client/v4/accounts/{account_id}/botnet_feed/asn/{asn}/day_report?date=2024-05-05" \
--header "Content-Type: application/json" \
--header "Authorization: Bearer <API_TOKEN>"
```

```json title="Example response"
{
  "result": [
    {
      "cidr": "127.0.0.1/32",
      "date": "2024-05-05T00:00:00Z",
      "offense_count": 10000
    },
    // ... other entries ...
  ],
  "success": true,
  "errors": [],
  "messages": []
}
```

---

# Get started

URL: https://developers.cloudflare.com/ddos-protection/get-started/

## Free, Pro, and Business plans

The DDoS Attack Protection managed rulesets provided by Cloudflare are enabled by default on zones onboarded to Cloudflare, IP applications onboarded to Spectrum, and IP Prefixes onboarded to Magic Transit.

In some situations, the default protection offered by DDoS rules may need to be fine-tuned to your specific situation. You may also want to configure additional protection using other Cloudflare products.

### Adjust the provided DDoS rules

If one or more DDoS rules provided by Cloudflare affects legitimate traffic, you can adjust them so that they do not perform any mitigation action against this kind of traffic. Follow the steps in [Handle a false positive](/ddos-protection/managed-rulesets/adjust-rules/false-positive/) to reduce the sensitivity level of one or more DDoS rules and allow incoming legitimate traffic.

### Configure additional protection

To configure additional protection against DDoS attacks, refer to the related Cloudflare products listed in [Network-layer DDoS Attack Protection](/ddos-protection/managed-rulesets/network/#related-cloudflare-products) and [HTTP DDoS Attack Protection](/ddos-protection/managed-rulesets/http/#related-cloudflare-products).

## Enterprise plan

Cloudflare's DDoS protection systems automatically detect and mitigate DDoS attacks. Additionally, the systems may flag suspiciously-looking incoming traffic from legacy applications, Internet services, or faulty client applications as malicious and apply mitigation actions. If the traffic is in fact legitimate, the mitigation actions can cause service disruptions and outages in your Internet properties.

To prevent this situation, Cloudflare recommends that you perform these steps to get started:

1. Set the ruleset actions for all the [DDoS Attack Protection managed rulesets](/ddos-protection/managed-rulesets/) to _Log_.
2. Analyze the flagged traffic.
3. Adjust the sensitivity or action of individual managed ruleset rules, if required.
4. Switch ruleset actions from _Log_ back to the default.

### Prerequisites

You must have one of the following:

- [A zone onboarded to Cloudflare](/dns/zone-setups/full-setup/) but without updated DNS records.
- [An IP application onboarded to Spectrum](/spectrum/get-started/).
- [An IP Prefix onboarded to Magic Transit](/magic-transit/get-started/).

### 1. Configure ruleset actions to Log

:::note

The _Log_ action is only available to Enterprise customers.
:::

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. [Configure all the rules in the HTTP DDoS Attack Protection managed ruleset](/ddos-protection/managed-rulesets/http/configure-dashboard/#create-a-ddos-override), setting their action to _Log_.
3. [Configure all the rules in the Network-layer DDoS Attack Protection managed ruleset](/ddos-protection/managed-rulesets/network/configure-dashboard/#create-a-ddos-override), setting the action to _Log_.

Alternatively, if you are using the API, define an override at the ruleset level to set the action of all managed ruleset rules to `log` by following these instructions:

- [Configure an override for the HTTP DDoS Attack Protection managed ruleset](/ddos-protection/managed-rulesets/http/configure-api/#configure-an-override-for-the-http-ddos-attack-protection-managed-ruleset)
- [Configure an override for the Network-layer DDoS Attack Protection managed ruleset](/ddos-protection/managed-rulesets/network/configure-api/#configure-an-override-for-the-network-layer-ddos-attack-protection-managed-ruleset)

### 2. Review flagged traffic

1. Go to your [analytics dashboard](/ddos-protection/reference/analytics/) (the exact dashboard depends on your Cloudflare services).
2. Apply one or more filters, if required, and identify any rules that would have blocked legitimate traffic if _Log_ mode were disabled. Take note of the rule IDs.

### 3. Customize managed ruleset rules

Customize the specific managed ruleset rules you identified, changing their sensitivity or their action, using the Cloudflare dashboard or using the API.

If you are using the Cloudflare dashboard, refer to:

- [Configure HTTP DDoS Attack Protection in the dashboard](/ddos-protection/managed-rulesets/http/configure-dashboard/)
- [Configure Network-layer DDoS Attack Protection in the dashboard](/ddos-protection/managed-rulesets/network/configure-dashboard/)

If you are using the API, refer to:

- [Configure HTTP DDoS Attack Protection via API](/ddos-protection/managed-rulesets/http/configure-api/)
- [Configure Network-layer DDoS Attack Protection via API](/ddos-protection/managed-rulesets/network/configure-api/)

When using the API, ensure that you add any required rule overrides without removing the ruleset override you configured in [Step 1](#1-configure-ruleset-actions-to-log).

### 4. Switch ruleset actions back to the default

Revert the change you did in [Step 1](#1-configure-ruleset-actions-to-log), changing the action of each managed ruleset rule back to _Default_ in **Ruleset action**.

Alternatively, if you are using the API, [remove the override](/ddos-protection/managed-rulesets/http/configure-api/#configure-an-override-for-the-http-ddos-attack-protection-managed-ruleset) you previously configured at the ruleset level for each managed ruleset. Ensure that you only remove the ruleset override and not any of the rule overrides you may have configured in [Step 3](#3-customize-managed-ruleset-rules).

---

# Overview

URL: https://developers.cloudflare.com/ddos-protection/

import { Description, Feature, FeatureTable, GlossaryTooltip, Plan, RelatedProduct } from "~/components"

<Description>

Detect and mitigate distributed denial-of-service (DDoS) attacks automatically.
</Description>

<Plan type="all" />

Cloudflare automatically detects and mitigates <GlossaryTooltip term="distributed denial-of-service (DDoS) attack" link="https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/">distributed denial-of-service (DDoS) attacks</GlossaryTooltip> via our autonomous DDoS systems.

These systems include multiple dynamic mitigation rules exposed as [DDoS attack protection managed rulesets](/ddos-protection/managed-rulesets/). You can customize the mitigation rules included in these rulesets to optimize and tailor the protection to your needs.

---

## Features

<Feature header="Managed rulesets" href="/ddos-protection/managed-rulesets/">
Protect against a variety of DDoS attacks across layers 3/4 (network layer) and layer 7 (application layer) of the OSI model.
</Feature>

<Feature header="Adaptive DDoS protection" href="/ddos-protection/managed-rulesets/adaptive-protection/">
Get increased protection against sophisticated DDoS attacks on layer 7 and layers 3/4.
</Feature>

<Feature header="Advanced TCP protection" href="/ddos-protection/advanced-ddos-systems/overview/advanced-tcp-protection/">
Detect and mitigate sophisticated out-of-state TCP attacks such as randomized and spoofed ACK floods, or SYN and SYN-ACK floods.
</Feature>

<Feature header="Advanced DNS protection" href="/ddos-protection/advanced-ddos-systems/overview/advanced-dns-protection/">
Protect against DNS-based DDoS attacks, specifically sophisticated and fully randomized DNS attacks such as random prefix attacks.
</Feature>

---

## Availability

<div style="font-size:87%">

<FeatureTable id="security.ddos" />

</div>

---

## Related products

<RelatedProduct header="Spectrum" href="/spectrum/" product="spectrum">
Provides security and acceleration for any TCP or UDP based application.
</RelatedProduct>

<RelatedProduct header="Magic Transit" href="/magic-transit/" product="magic-transit">
A network security and performance solution that offers DDoS protection, traffic acceleration, and more for on-premise, cloud-hosted, and hybrid networks.
</RelatedProduct>

<RelatedProduct header="Web Application Firewall (WAF)" href="/waf/" product="waf">
Get automatic protection from vulnerabilities and the flexibility to create custom rules.
</RelatedProduct>

---

# Application guide

URL: https://developers.cloudflare.com/developer-spotlight/application-guide/

If you use Cloudflare's developer products and would like to share your expertise then Cloudflare's Developer Spotlight program is for you. Whether you use Cloudflare in your profession, as a student or as a hobby, let us spotlight your creativity. Write a tutorial for our documentation and earn credits for your Cloudflare account along with having your name credited on your work.

The Developer Spotlight program is open for applicants until Thursday, the 24th of October 2024.

## Who can apply?

The following is required in order to be an eligible applicant for the Developer Spotlight program:

- You must not be an employee of Cloudflare.
- You must be 18 or older.
- All participants must agree to the [Developer Spotlight terms](/developer-spotlight/terms/).

## Submission rules

Your tutorial must be:

1. Easy for anyone to follow.
2. Technically accurate.
3. Entirely original, written only by you.
4. Written following Cloudflare's documentation style guide. For more information, please visit our [style guide documentation](/style-guide/) and our [tutorial style guide documentation](/style-guide/documentation-content-strategy/content-types/tutorial/#template)
5. About how to use [Cloudflare's Developer Platform products](/products/?product-group=Developer+platform) to create a project or solve a problem.
6. Complete, not an unfinished draft.

## How to apply

To apply to the program, submit an application through the [Developer Spotlight signup form](https://forms.gle/anpTPu45tnwjwXsk8). Successful applicants will be contacted by email.

## Account credits

Account credits can be used towards recurring monthly charges for Cloudflare plans or add-on services. Once a tutorial submission has been approved and published, we can then add 350 credits to your Cloudflare account. Credits are only valid for three years. Valid payment details must be stored on the recieving account before credits can be added.

## FAQ

### How many tutorial topic ideas can I submit?

You may submit as many tutorial topics ideas as you like in your application.

### When will I be compensated for my tutorial?

We will add the account credits to your Cloudflare account after your tutorial has been approved and published under the Developer Spotlight program.

### If my tutorial is accepted and published on Cloudflare's Developer Spotlight program, can I republish it elsewhere?

We ask that you do not republish any tutorials that have been published under the Cloudflare Developer Spotlight program.

### Will I be credited for my work?

You will be credited as the author of any tutorial you submit that is successfully published through the Cloudflare Developer Spotlight program. We will add your details to your work after it has been approved.

### What happens If my topic of choice gets accepted but the tutorial submission gets rejected?

Our team will do our best to help you edit your tutorial's pull request to be ready for submission; however, in the unlikely chance that your tutorial's pull request is rejected, you are still free to publish your work elsewhere.

---

# Developer Spotlight program

URL: https://developers.cloudflare.com/developer-spotlight/

import { LinkTitleCard } from "~/components";

![Illustration of a laptop.](~/assets/images/developer_spotlight/developer_spotlight.png)

Find examples of how our community of developers are getting the most out of our products.

Applications are currently open until Thursday, the 24th of October 2024. To apply, please read the [application guide](/developer-spotlight/application-guide/)

## View latest contributions

<LinkTitleCard
	title="Setup Fullstack Authentication with Next.js, Auth.js, and Cloudflare D1"
	href="/developer-spotlight/tutorials/fullstack-authentication-with-next-js-and-cloudflare-d1/"
>
	By Mackenly Jones
</LinkTitleCard>

<LinkTitleCard
	title="Build a Voice Notes App with auto transcriptions using Workers AI"
	href="/workers-ai/tutorials/build-a-voice-notes-app-with-auto-transcription/"
>
	By Rajeev R. Sharma
</LinkTitleCard>

<LinkTitleCard
	title="Protect payment forms from malicious bots using Turnstile"
	href="/turnstile/tutorials/protecting-your-payment-form-from-attackers-bots-using-turnstile/"
>
	By Hidetaka Okamoto
</LinkTitleCard>

<LinkTitleCard
	title="Build Live Cursors with Next.js, RPC and Durable Objects"
	href="/workers/tutorials/live-cursors-with-nextjs-rpc-do/"
>
	By Ivan Buendia
</LinkTitleCard>

<LinkTitleCard
	title="Build an interview practice tool with Workers AI"
	href="/workers-ai/tutorials/build-ai-interview-practice-tool/"
>
	By Vasyl
</LinkTitleCard>

<LinkTitleCard
	title="Automate analytics reporting with Cloudflare Workers and email routing"
	href="/workers/tutorials/automated-analytics-reporting/"
>
	By Aleksej Komnenovic
</LinkTitleCard>

<LinkTitleCard
	title="Create a sitemap from Sanity CMS with Workers"
	href="/developer-spotlight/tutorials/create-sitemap-from-sanity-cms/"
>
	By John Siciliano
</LinkTitleCard>

<LinkTitleCard
	title="Recommend products on e-commerce sites using Workers AI and Stripe"
	href="/developer-spotlight/tutorials/creating-a-recommendation-api/"
>
	By Hidetaka Okamoto
</LinkTitleCard>

<LinkTitleCard
	title="Custom access control for files in R2 using D1 and Workers"
	href="/developer-spotlight/tutorials/custom-access-control-for-files/"
>
	By Dominik Fuerst
</LinkTitleCard>

<LinkTitleCard
	title="Send form submissions using Astro and Resend"
	href="/developer-spotlight/tutorials/handle-form-submission-with-astro-resend/"
>
	By Cody Walsh
</LinkTitleCard>

---

# Developer Spotlight Terms

URL: https://developers.cloudflare.com/developer-spotlight/terms/

These Developer Spotlight Terms (the â€œTermsâ€) govern your participation in the Cloudflare Developer Spotlight Program (the â€œProgramâ€). As used in these Terms, "Cloudflare", "us" or "we" refers to Cloudflare, Inc. and its affiliates.

THESE TERMS DO NOT APPLY TO YOUR ACCESS AND USE OF THE CLOUDFLARE PRODUCTS AND SERVICES THAT ARE PROVIDED UNDER THE [SELF-SERVE SUBSCRIPTION AGREEMENT](https://www.cloudflare.com/terms/), THE [ENTERPRISE SUBSCRIPTION AGREEMENT](https://www.cloudflare.com/enterpriseterms/), OR OTHER WRITTEN AGREEMENT SIGNED BETWEEN YOU AND CLOUDFLARE (IF APPLICABLE).

1. Eligibility. By agreeing to these Terms, you represent and warrant to us: (i) that you are at least eighteen (18) years of age; (ii) that you have not previously been suspended or removed from the Program and (iii) that your participation in the Program is in compliance with any and all applicable laws and regulations.

2. Submissions. From time-to-time, Cloudflare may accept certain tutorials, blogs, and other content submissions from its developer community (â€œDev Contentâ€) for consideration for publication on a Cloudflare blog, developer documentation, social media platform or other website. You grant us a worldwide, perpetual, irrevocable, non-exclusive, royalty-free license (with the right to sublicense) to use, copy, reproduce, process, adapt, modify, publish, transmit, display and distribute such Dev Content in any and all media or distribution methods now known or later developed.

a. Likeness. You hereby grant to Cloudflare the royalty free right to use your name and likeness and any trademarks you include in the Dev Content in any and all manner, media, products, means, or methods, now known or hereafter created, throughout the world, in perpetuity, in connection with Cloudflareâ€™s exercise of its rights under these Terms, including Cloudflareâ€™s use of the Dev Content. Notwithstanding any other provision of these Terms, nothing herein will obligate Cloudflare to use the Dev Content in any manner. You understand and agree that you will have no right to any proceeds derived by Cloudflare or any third party from the use of the Dev Content.

b. Representations & Warranties. By submitting Dev Content, you represent and warrant that (1) you are the author and sole owner of all rights to the Dev Content; (2) the Dev Content is original and has not in whole or in part previously been published in any form and is not in the public domain; (3) your Dev Content is accurate and not misleading; (4) your Dev Content, does not: (i) infringe, violate, or misappropriate any third-party right, including any copyright, trademark, patent, trade secret, moral right, privacy right, right of publicity, or any other intellectual property or proprietary right; or (ii) slander, defame, or libel any third-party; and (2) no payments will be due from Cloudflare to any third party for the exercise of any rights granted under these Terms.

c. Compensation. Unless otherwise agreed by Cloudflare in writing, you understand and agree that Cloudflare will have no obligation to you or any third-party for any compensation, reimbursement, or any other payments in connection with your participation in the Program or publication of Dev Content.

3. Termination. These Terms will continue in full force and effect until either party terminates upon 30 daysâ€™ written notice to the other party. The provisions of Sections 2, 4, and 5 shall survive any termination or expiration of this agreement.

4. Indemnification. You agree to defend, indemnify, and hold harmless Cloudflare and its officers, directors, employees, consultants, affiliates, subsidiaries and agents (collectively, the "Cloudflare Entities") from and against any and all claims, liabilities, damages, losses, and expenses, including reasonable attorneys' fees and costs, arising out of or in any way connected with your violation of any third-party right, including without limitation any intellectual property right, publicity, confidentiality, property or privacy right. We reserve the right, at our own expense, to assume the exclusive defense and control of any matter otherwise subject to indemnification by you (and without limiting your indemnification obligations with respect to such matter), and in such case, you agree to cooperate with our defense of such claim.

5. Limitation of Liability. IN NO EVENT WILL THE CLOUDFLARE ENTITIES BE LIABLE TO YOU OR ANY THIRD PARTY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING OUT OF OR RELATING TO YOUR PARTICIPATION IN THE PROGRAM, WHETHER BASED ON WARRANTY, CONTRACT, TORT (INCLUDING NEGLIGENCE), STATUTE, OR ANY OTHER LEGAL THEORY, WHETHER OR NOT THE CLOUDFLARE ENTITIES HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGE.

6. Independent Contractor. The parties acknowledge and agree that you are an independent contractor, and nothing in these Terms will create a relationship of employment, joint venture, partnership or agency between the parties. Neither party will have the right, power or authority at any time to act on behalf of, or represent the other party. Cloudflare will not obtain workersâ€™ compensation or other insurance on your behalf, and you are solely responsible for all payments, benefits, and insurance required for the performance of services hereunder, including, without limitation, taxes or other withholdings, unemployment, payroll disbursements, and other related expenses. You hereby acknowledge and agree that these Terms are not governed by any union or collective bargaining agreement and Cloudflare will not pay you any union-required residuals, reuse fees, pension, health and welfare benefits or other benefits/payments.

7. Governing Law. These Terms will be governed by the laws of the State of California without regard to conflict of law principles. To the extent that any lawsuit or court proceeding is permitted hereunder, you and Cloudflare agree to submit to the personal and exclusive jurisdiction of the state and federal courts located within San Francisco County, California for the purpose of litigating all such disputes.

8. Modifications. Cloudflare reserves the right to make modifications to these Terms at any time. Revised versions of these Terms will be posted publicly online. Unless otherwise specified, any modifications to the Terms will take effect the day they are posted publicly online. If you do not agree with the revised Terms, your sole and exclusive remedy will be to discontinue your participation in the Program.

9. General. These Terms, together with any applicable product limits, disclaimers, or other terms presented to you on a Cloudflare controlled website (e.g., www.cloudflare.com, as well as the other websites that Cloudflare operates and that link to these Terms) or documentation, each of which are incorporated by reference into these Terms, constitute the entire and exclusive understanding and agreement between you and Cloudflare regarding your participation in the Program. Use of section headers in these Terms is for convenience only and will not have any impact on the interpretation of particular provisions. You may not assign or transfer these Terms or your rights hereunder, in whole or in part, by operation of law or otherwise, without our prior written consent. We may assign these Terms at any time without notice. The failure to require performance of any provision will not affect our right to require performance at any time thereafter, nor will a waiver of any breach or default of these Terms or any provision of these Terms constitute a waiver of any subsequent breach or default or a waiver of the provision itself. In the event that any part of these Terms is held to be invalid or unenforceable, the unenforceable part will be given effect to the greatest extent possible and the remaining parts will remain in full force and effect. Upon termination of these Terms, any provision that by its nature or express terms should survive will survive such termination or expiration.

---

# DNS lookup limit

URL: https://developers.cloudflare.com/dmarc-management/dns-lookup-limits/

The [Sender Policy Framework (SPF)](https://datatracker.ietf.org/doc/rfc4408/) specification has a limit on the number of DNS lookups required to fully resolve an SPF record. According to the specification, SPF must limit the number of DNS lookups to 10 per SPF check. If your SPF records exceed this number, your emails might not reach their destination.

To check if your SPF records are compliant with the SPF specification:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Email** > **DMARC Management**.
3. In **Email record overview**, select **View records**.
4. Find your SPF record, and select the three dots next to it > **Inspect**.
5. DMARC Management will inspect your records and check for the total number of DNS lookups. If the record exceeds the maximum number of DNS lookups, DMARC Management will warn you about this. You should edit and remove unnecessary records in the DNS page. Refer to [Manage DNS records](/dns/manage-dns-records/how-to/create-dns-records/#delete-dns-records) for more information on how to delete DNS records.

---

# Enable DMARC Management

URL: https://developers.cloudflare.com/dmarc-management/enable/

import { GlossaryTooltip } from "~/components"

You need to enable DMARC Management to allow Cloudflare to process DMARC reports on your behalf. DMARC Management only works with <GlossaryTooltip term="apex domain">apex domains</GlossaryTooltip> and not domains in [subdomain setups](/dns/zone-setups/subdomain-setup/).

:::caution[A warning on DMARC Management and SPF records]

DMARC Management does not support actions on SPF records when your zone has a CNAME record that points to a different domain. Changing the SPF record would make DMARC rules invalid, as Cloudflare cannot change other DNS records to reflect your updates.
:::

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Email** > **DMARC Management**.
3. Select **Enable DMARC Management**.
4. DMARC Management will scan your zone for DMARC records.

   1. If no record is found, Cloudflare will automatically invite you to add one that you can edit later. Select **Add** to continue.
   2. If there is a DMARC record in your zone, Cloudflare will add another `rua` entry to it. This additional `rua` tag has a Cloudflare email address and is needed for Cloudflare to be able to start processing DMARC reports on your behalf. Select **Next** to continue.

DMARC Management (beta) is now active. However, it may take up to 24 hours to receive your first DMARC report and to display this information in DMARC Management.

---

# Overview

URL: https://developers.cloudflare.com/dmarc-management/

import { Description, Plan, RelatedProduct } from "~/components"

<Description>

Stop brand impersonation.
</Description>

<Plan type="all" />

Cloudflare DMARC Management (beta) helps you track every source that is sending emails from your domain and review [Domain-based Message Authentication Reporting and Conformance (DMARC)](https://www.cloudflare.com/learning/dns/dns-records/dns-dmarc-record/) reports for each source. DMARC reports will help you understand if messages sent from your domain are passing DMARC authentication, [DomainKeys Identified Mail (DKIM)](https://www.cloudflare.com/learning/dns/dns-records/dns-dkim-record/) authentication, and [Sender Policy Framework (SPF)](https://www.cloudflare.com/learning/dns/dns-records/dns-spf-record/) policies.

DMARC Management (beta) is available to all Cloudflare customers with [Cloudflare DNS](/dns/).

***

## Related products

<RelatedProduct header="Area 1 Email Security" href="/email-security/" product="email-security">
Stop phishing attacks with Area 1 cloud-native email security service.
</RelatedProduct>

<RelatedProduct header="Cloudflare DNS" href="/dns/" product="dns">
Fast, resilient and easy-to-manage DNS service.
</RelatedProduct>

---

# Security records

URL: https://developers.cloudflare.com/dmarc-management/security-records/

import { Render } from "~/components"

<Render file="domain-spoofing" />

## Create security records

To set up email security records:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Email** > **DMARC Management**.
3. In **Email record overview**, select **View records**.
4. Use the available options to set up SPF, DKIM, and DMARC records. This page will also list any previous records you might already have in your account.

## Edit or delete records

Refer to [Manage DNS records](/dns/manage-dns-records/how-to/create-dns-records/) for more information.

---

# Statistics and details

URL: https://developers.cloudflare.com/dmarc-management/statistics/

DMARC Management (beta) allows you to review how emails sent on your behalf have fared regarding security policies such as DMARC, SFP, and DKIM.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Email** > **DMARC Management**.
3. The graph shows you the volume of emails during a certain period of time. Select the dropdown to select a period of time up to 30 days.
4. Moving your mouse through the graph gives you details for a particular day. Select **View reports** for a list of DMARC reports by date.
5. Select one of the dates shown to open a window with more details.

## Source details

The Top 10 sources section shows you details about the top sources sending emails on your behalf, with information such as total volume of emails and how these sources fared regarding security policies.

You also have access to information about all third parties, and can drill down for further details on each of them:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Email** > **DMARC Management**.
3. Select **View all**.
4. The next page shows you a list of all sources sending email on your behalf. You can filter this list by time period.
5. Find a source you want to inspect further, and select the three dots in front of it > **Details** to learn more about that third party.

---

# Overview

URL: https://developers.cloudflare.com/docs-guide/

This section provides guidance on creating and leveling up your docs website.

It's also still in progress, so check back as we add more resources over time.

---

# Changelog

URL: https://developers.cloudflare.com/dns/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/dns.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Concepts

URL: https://developers.cloudflare.com/dns/concepts/

import { Details, Example, GlossaryTooltip } from "~/components";

This page defines and articulates key concepts that are relevant to the Cloudflare DNS service and are used in this documentation. For more concepts and broader descriptions, refer to the [Cloudflare Learning Center](https://www.cloudflare.com/learning/dns/what-is-dns/).
## Domain

Also known as domain name, a domain is the string of text that identifies a specific website, such as `google.com` or `facebook.com`. Every time you access a website from your web browser, a DNS query takes place and the DNS service maps the domain to the actual IP address where the website is [hosted](/fundamentals/setup/manage-domains/).

## Registrar

Before you can start using the Cloudflare DNS service, you must first have a domain. This is achieved by using a service called registrar. As explained in our [Learning Center](https://www.cloudflare.com/learning/dns/glossary/what-is-a-domain-name-registrar/), a registrar handles the reservation of domain names.

Very often the same company that offers domain registration also offers web hosting and DNS management.

You can register a domain name at cost through [Cloudflare Registrar](/registrar/). Every domain acquired through Cloudflare Registrar must also use Cloudflare as their [primary authoritative DNS](#authoritative-dns).

## Nameserver

Although the resolution of a DNS query involves a number of different servers, in this documentation nameserver usually refers to the Cloudflare authoritative nameservers. As explained in the [article about DNS server types](https://www.cloudflare.com/learning/dns/dns-server-types/), the authoritative nameserver is the last stop in the resolution of a DNS query.

Refer to [Nameservers](/dns/nameservers/) for details on the different nameserver offerings.

## Authoritative DNS

Authoritative DNS refers to the service whose nameservers provide the final information mapping a hostname (such as `example.com` or `blog.example.com`) to the IP address that hosts the corresponding content or resources.

This is important because the performance of such authoritative DNS services determine how available, resilient, and performant your website or application is. Cloudflare DNS is an authoritative DNS service leveraging Cloudflare's global network. Refer to [How Cloudflare works](/fundamentals/concepts/how-cloudflare-works/) for details.

## DNS setups

It is also possible that one same company will use more than one DNS provider. Usually, this relates to making a domain more resilient - if one provider faces an outage, the nameservers operated by the other DNS provider will most likely still be available.

In this context, you can have a primary DNS setup, when you use Cloudflare to manage your [DNS records](#dns-records), or a [secondary DNS setup](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/), when your DNS records are managed on a different provider and Cloudflare simply receives zone transfers containing your DNS records.

When you have a primary DNS setup, you can either use only Cloudflare (also known as [Full setup](/dns/zone-setups/full-setup/)), or you can use Cloudflare and another provider, where the other provider is the one to receive [outgoing zone transfers](/dns/zone-setups/zone-transfers/cloudflare-as-primary/) from Cloudflare.

Finally, as Cloudflare also works as a [reverse proxy](/fundamentals/concepts/how-cloudflare-works/#cloudflare-as-a-reverse-proxy), [partial (CNAME) setups](/dns/zone-setups/partial-setup/) can be used when you do not want Cloudflare to be [authoritative](#authoritative-dns) for your domain but you still want to proxy individual subdomains through Cloudflare.

## DNS records

DNS records are instructions that live in the authoritative DNS servers and provide information about a [zone](#zone). This includes what IP address is associated with a particular domain, but can also cover many other use cases, such as directing emails to a mail server or validating ownership of a domain.

For more details about using DNS records within Cloudflare, refer to [Manage DNS records](/dns/manage-dns-records/how-to/create-dns-records/) and [DNS record types](/dns/manage-dns-records/reference/dns-record-types/).

## Zone

DNS zone is an administrative concept used for delegating control over a given domain and its subdomains. Read more in the ["What is a DNS zone?" Learning Center article](https://www.cloudflare.com/learning/dns/glossary/dns-zone/).

For the purpose of this documentation, keep in mind that each domain added to a Cloudflare account is listed in the account home page as a zone. The exact properties and behaviors of your zone depend on its [DNS setup](/dns/zone-setups/).

Also, different Cloudflare products and features are configurable at the zone level. Refer to [Fundamentals](/fundamentals/setup/manage-domains/connect-your-domain/#domain-configurations) for details.

### Zone apex

Zone apex refers to the domain or subdomain on which the control of DNS records starts.

<Details header="Example 1">
<Example>
DNS management for **example.com**:

| Type | Name    | Content      | Proxy status | TTL    |
| ---- | ------- | ------------ | ------------ | ------ |
| A    | `blog`  | `192.0.2.1`  | Proxied      | Auto   |

Zone apex: `example.com`

Full record name: `blog.example.com`

</Example>
</ Details>

<Details header="Example 2">
<Example>
DNS management for **sub.example.com**:

| Type | Name    | Content      | Proxy status | TTL    |
| ---- | ------- | ------------ | ------------ | ------ |
| A    | `blog`  | `192.0.2.1`  | Proxied      | Auto   |

Zone apex: `sub.example.com`

Full record name: `blog.sub.example.com`

</Example>
</ Details>

Usually, the zone apex coincides with the <GlossaryTooltip term="apex domain">apex domain</GlossaryTooltip>, as shown in Example 1. Example 2 refers to [subdomain delegation](/dns/zone-setups/subdomain-setup/), which is only available to Enterprise plans.

To create a DNS record at the zone apex, use `@` for the record **Name**. For details, refer to [How to](/dns/manage-dns-records/how-to/create-zone-apex/).

<Details header="Record at the zone apex">
<Example>
DNS management for **example.com**:

| Type | Name    | Content      | Proxy status | TTL    |
| ---- | ------- | ------------ | ------------ | ------ |
| A    | `@`  | `192.0.2.1`  | Proxied      | Auto   |

Zone apex: `example.com`

Full record name: `example.com`
</Example>

<Example>
DNS management for **sub.example.com**:

| Type | Name    | Content      | Proxy status | TTL    |
| ---- | ------- | ------------ | ------------ | ------ |
| A    | `@`  | `192.0.2.1`  | Proxied      | Auto   |

Zone apex: `sub.example.com`

Full record name: `sub.example.com`
</Example>
</Details>

## DNSSEC

DNSSEC stands for DNS Security Extensions. It increases security by adding cryptographic signatures to DNS records. These signatures can then be checked to verify that a record came from the correct DNS server, preventing anyone else from issuing false DNS records on your behalf and redirecting traffic intended for your domain. You can read more about it in the [article about DNS security](https://www.cloudflare.com/learning/dns/dns-security/).

For help setting up DNSSEC in Cloudflare, refer to [Enable DNSSEC](/dns/dnssec/).

---

# Get started

URL: https://developers.cloudflare.com/dns/get-started/

import { GlossaryDefinition, Render } from "~/components";

You can use Cloudflare DNS with a variety of [setups](/dns/zone-setups/). For an overview of what these setups are and an introduction to specific DNS terminology, refer to [Concepts](/dns/concepts/).

In the most common setup (full), you [add your domain](/fundamentals/setup/manage-domains/add-site/), import your [DNS records](/dns/manage-dns-records/), and [update your nameservers](/dns/nameservers/update-nameservers/) to make Cloudflare your primary authoritative DNS provider.

:::note
Make sure to [review your DNS records](/dns/zone-setups/full-setup/setup/#review-dns-records) before updating your nameservers. If you activate your domain on Cloudflare *without* setting up the correct DNS records, your domain may not be reachable.
:::

Once the setup is completed:

- You [manage DNS records](/dns/manage-dns-records/how-to/create-dns-records/) through the Cloudflare dashboard or API. This is how you control which resources are available on the apex domain (`example.com`) or specific subdomains (`blog.example.com`) of your website, as well as control other configurations.

- Cloudflare [responds to all DNS queries](/fundamentals/concepts/how-cloudflare-works/) for your hostnames and your DNS records are propagated across the [Cloudflare global network](https://www.cloudflare.com/network/), speeding up your domain.

## Resources

The following links introduce important concepts and will guide you through actions you may need to take while having your website or application on Cloudflare.

- [DNS records](/dns/manage-dns-records/): DNS records contain information about your domain and are used to make your website or application available to visitors and other web services.

- [Nameservers](/dns/nameservers/): In the context of Cloudflare DNS, nameservers refer to authoritative nameservers. When a nameserver is authoritative for `example.com`, it means that DNS resolvers will consider responses from this nameserver when a user tries to access `example.com`.

- [Proxy status](/dns/proxy-status/): Proxy status affects how Cloudflare treats incoming HTTP/S requests to A, AAAA, and CNAME records. When a record is proxied, Cloudflare responds with [anycast IPs](/fundamentals/concepts/cloudflare-ip-addresses/), which speeds up and protects HTTP/S traffic with our [cache](/cache/)/[CDN](https://www.cloudflare.com/learning/cdn/what-is-a-cdn/), [DDoS protection](/ddos-protection/), [WAF](/waf/), and [more](/products/?product-group=Application+performance%2CApplication+security).

## Further reading

- [How Cloudflare works](/fundamentals/concepts/how-cloudflare-works/): An overview of how Cloudflare works as a DNS provider and as a reverse proxy.

- [DNS analytics](/dns/additional-options/analytics/): An overview of the different data sources and insights you can get when using Cloudflare DNS.

- [Troubleshooting](/dns/troubleshooting/): A full resources list for when something is not working.

---

# Glossary

URL: https://developers.cloudflare.com/dns/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's DNS documentation.

<Glossary product="dns" />

---

# Changelog

URL: https://developers.cloudflare.com/durable-objects/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/durable-objects.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Cloudflare DNS

URL: https://developers.cloudflare.com/dns/

import { Description, Feature, Plan, RelatedProduct } from "~/components"

<Description>

Leverage Cloudflare's global network to deliver excellent performance and reliability to your domain.
</Description>

<Plan type="all" />

Cloudflare DNS is a fast, resilient and easy-to-manage authoritative DNS service. It delivers excellent performance and reliability to your domain while also protecting your business from [DDoS attacks](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/) and [route leaks and hijacking](https://www.cloudflare.com/learning/security/glossary/bgp-hijacking/). To know where to begin, refer to [Get started](/dns/get-started/).

***

## Features

<Feature header="DNS records" href="/dns/manage-dns-records/how-to/create-dns-records/">
DNS records make resources available on your domain, and allow you to configure services such as email.
</ Feature>

<Feature header="DNSSEC" href="/dns/dnssec/">
DNS Security Extensions (DNSSEC) adds cryptographic signatures to your DNS records, preventing anyone else from redirecting traffic intended for your domain.

Cloudflare also supports [Multi-signer DNSSEC](/dns/dnssec/multi-signer-dnssec/).
</Feature>

<Feature header="CNAME flattening" href="/dns/cname-flattening/">
CNAME flattening delivers better performance and allows you to add a CNAME record at your apex domain (`example.com`). Paid accounts can choose to flatten all CNAME records on their domain.
</Feature>

<br />

Refer to [DNS features and availability](/dns/reference/all-features/) for a complete list of features and their availability according to different Cloudflare plans.

***

## Related products

<RelatedProduct header="Registrar" href="/registrar/" product="registrar">
Before you can start using Cloudflare DNS you must first have a domain. Buy and renew your domain at cost with Cloudflare Registrar.
</RelatedProduct>

<RelatedProduct header="DNS Resolver" href="/1.1.1.1/" product="1.1.1.1">
Cloudflare DNS focuses on businesses and their domain administration. If you are a consumer and want a more private way to browse the Internet, check out 1.1.1.1, Cloudflare's public DNS Resolver.
</RelatedProduct>

---

# Demos and architectures

URL: https://developers.cloudflare.com/durable-objects/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use a <GlossaryTooltip term = "Durable Object">Durable Object</GlossaryTooltip> within your existing application and architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Durable Objects.

<ExternalResources type="apps" products={["Durable Objects"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use Durable Objects:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["Durable Objects"]} />

---

# Overview

URL: https://developers.cloudflare.com/durable-objects/

import { Render, CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct, LinkButton } from "~/components"

<Description>
Create collaborative applications, real-time chat, multiplayer games and more without needing to coordinate state or manage infrastructure.
</Description>

<Plan type="paid" />

Durable Objects provide a building block for stateful applications and distributed systems.

Use Durable Objects to build applications that need coordination among multiple clients, like collaborative editing tools, interactive chat, multiplayer games, and deep distributed systems, without requiring you to build serialization and coordination primitives on your own.

### What are Durable Objects?

<Render file="what-are-durable-objects"/>

For more information, refer to the full [What are Durable Objects?](/durable-objects/what-are-durable-objects/) page.

<LinkButton href="/durable-objects/get-started/tutorial/">Get started</LinkButton>

:::note[SQLite in Durable Objects Beta]

The new beta version of Durable Objects is available where each Durable Object has a private, embedded SQLite database. When deploying a new Durable Object class, users can [opt-in to a SQLite storage backend](/durable-objects/best-practices/access-durable-objects-storage/#sqlite-storage-backend) in order to access new [SQL API](/durable-objects/api/sql-storage/#exec) and [point-in-time-recovery API](/durable-objects/api/sql-storage/#point-in-time-recovery), part of Durable Objects Storage API.

Storage API billing is not enabled for Durable Object classes using SQLite storage backend. SQLite-backed Durable Objects will incur [charges for requests and duration](/durable-objects/platform/pricing/#billing-metrics). We plan to enable Storage API billing for Durable Objects using SQLite storage backend in the first half of 2025 after advance notice with the following [pricing](/durable-objects/platform/pricing/#sqlite-storage-backend).

:::

***

## Features

<Feature header="In-memory State" href="/durable-objects/reference/in-memory-state/">

Learn how Durable Objects coordinate connections among multiple clients or events.


</Feature>

<Feature header="Storage API" href="/durable-objects/api/storage-api/">

Learn how Durable Objects provide transactional, strongly consistent, and serializable storage.


</Feature>

<Feature header="WebSocket Hibernation" href="/durable-objects/best-practices/websockets/#websocket-hibernation-api">

Learn how WebSocket Hibernation allows you to manage the connections of multiple clients at scale.


</Feature>

<Feature header="Durable Objects Alarms" href="/durable-objects/api/alarms/">

Learn how to use alarms to trigger a Durable Object and perform compute in the future at customizable intervals.


</Feature>

***

## Related products

<RelatedProduct header="Workers" href="/workers/" product="workers">

Cloudflare Workers provides a serverless execution environment that allows you to create new applications or augment existing ones without configuring or maintaining infrastructure.


</RelatedProduct>

<RelatedProduct header="D1" href="/d1/" product="d1">

D1 is Cloudflareâ€™s SQL-based native serverless database. Create a database by importing data or defining your tables and writing your queries within a Worker or through the API.


</RelatedProduct>

<RelatedProduct header="R2" href="/r2/" product="r2">

Cloudflare R2 Storage allows developers to store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.


</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Built with Durable Objects" href="https://workers.cloudflare.com/built-with/collections/durable-objects/" icon="pen">
Browse what other developers are building with Durable Objects.
</LinkTitleCard>

<LinkTitleCard title="Limits" href="/durable-objects/platform/limits/" icon="document">
Learn about Durable Objects limits.
</LinkTitleCard>

<LinkTitleCard title="Pricing" href="/durable-objects/platform/pricing/" icon="pen">
Learn about Durable Objects pricing.
</LinkTitleCard>

<LinkTitleCard title="Storage options" href="/workers/platform/storage-options/" icon="document">
Learn more about storage and database options you can build with Workers.
</LinkTitleCard>

<LinkTitleCard title="Developer Discord" href="https://discord.cloudflare.com" icon="discord">
Connect with the Workers community on Discord to ask questions, show what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="@CloudflareDev" href="https://x.com/cloudflaredev" icon="x.com">
Follow @CloudflareDev on Twitter to learn about product announcements, and what is new in Cloudflare Developer Platform.
</LinkTitleCard>

</CardGrid>

---

# What are Durable Objects?

URL: https://developers.cloudflare.com/durable-objects/what-are-durable-objects/

import { Render } from "~/components";

<Render file="what-are-durable-objects"/>

## Durable Objects highlights

Durable Objects have properties that make them a great fit for distributed stateful scalable applications.

**Serverless compute, zero infrastructure management**

- Durable Objects are built on-top of the Workers runtime, so they support exactly the same code (JavaScript and WASM), and similar memory and CPU limits.
- Each Durable Object is [implicitly created on first access](/durable-objects/api/namespace/#get). User applications are not concerned with their lifecycle, creating them or destroying them. Durable Objects migrate among healthy servers, and therefore applications never have to worry about managing them.
- Each Durable Object stays alive as long as requests are being processed, and remains alive for several seconds after being idle before hibernating, allowing applications to [exploit in-memory caching](/durable-objects/reference/in-memory-state/) while handling many consecutive requests and boosting their performance.

**Storage colocated with compute**

- Each Durable Object has its own [durable, transactional, and strongly consistent storage](/durable-objects/api/storage-api/) (up to 10 GB[^1]), persisted across requests, and accessible only within that object.

**Single-threaded concurrency**

- Each [Durable Object instance has an identifier](/durable-objects/api/id/), either randomly-generated or user-generated, which allows you to globally address which Durable Object should handle a specific action or request.
- Durable Objects are single-threaded and cooperatively multi-tasked, just like code running in a web browser. For more details on how safety and correctness are achieved, refer to the blog post ["Durable Objects: Easy, Fast, Correct â€” Choose three"](https://blog.cloudflare.com/durable-objects-easy-fast-correct-choose-three/).

**Elastic horizontal scaling across Cloudflare's global network**

- Durable Objects can be spread around the world, and you can [optionally influence where each instance should be located](/durable-objects/reference/data-location/#provide-a-location-hint). Durable Objects are not yet available in every Cloudflare data center; refer to the [where.durableobjects.live](https://where.durableobjects.live/) project for live locations.
- Each Durable Object type (or ["Namespace binding"](/durable-objects/api/namespace/) in Cloudflare terms) corresponds to a JavaScript class implementing the actual logic. There is no hard limit on how many Durable Objects can be created for each namespace.
- Durable Objects scale elastically as your application creates millions of objects. There is no need for applications to manage infrastructure or plan ahead for capacity.

## Durable Objects features

### In-memory state

Each Durable Object has its own [in-memory state](/durable-objects/reference/in-memory-state/). Applications can use this in-memory state to optimize the performance of their applications by keeping important information in-memory, thereby avoiding the need to access the durable storage at all.

Useful cases for in-memory state include batching and aggregating information before persisting it to storage, or for immediately rejecting/handling incoming requests meeting certain criteria, and more.

In-memory state is reset when the Durable Object hibernates after being idle for some time. Therefore, it is important to persist any in-memory data to the durable storage if that data will be needed at a later time when the Durable Object receives another request.

### Storage API

The [Durable Object Storage API](/durable-objects/api/storage-api/) allows Durable Objects to access fast, transactional, and strongly consistent storage. A Durable Object's attached storage is private to its unique instance and cannot be accessed by other objects.

There are two flavors of the storage API, a [key-value (KV) API](/durable-objects/api/storage-api/#methods) and an [SQL API](/durable-objects/api/sql-storage/).

When using the [new SQLite in Durable Objects storage backend](/durable-objects/reference/durable-objects-migrations/#enable-sqlite-storage-backend-on-new-durable-object-class-migration), you have access to both the APIs. However, if you use the previous storage backend you only have access to the key-value API.

### Alarms API

Durable Objects provide an [Alarms API](/durable-objects/api/alarms/) which allows you to schedule the Durable Object to be woken up at a time in the future. This is useful when you want to do certain work periodically, or at some specific point in time, without having to manually manage infrastructure such as job scheduling runners on your own.

You can combine Alarms with in-memory state and the durable storage API to build batch and aggregation applications such as queues, workflows, or advanced data pipelines.

### WebSockets

WebSockets are long-lived TCP connections that enable bi-directional, real-time communication between client and server. Because WebSocket sessions are long-lived, applications commonly use Durable Objects to accept either the client or server connection.

Because Durable Objects provide a single-point-of-coordination between Cloudflare Workers, a single Durable Object instance can be used in parallel with WebSockets to coordinate between multiple clients, such as participants in a chat room or a multiplayer game.

Durable Objects support the [WebSocket Standard API](/durable-objects/best-practices/websockets/#websocket-standard-api), as well as the [WebSockets Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api) which extends the Web Standard WebSocket API to reduce costs by not incurring billing charges during periods of inactivity.

### RPC

Durable Objects support Workers [Remote-Procedure-Call (RPC)](/workers/runtime-apis/rpc/) which allows applications to use JavaScript-native methods and objects to communicate between Workers and Durable Objects.

Using RPC for communication makes application development easier and simpler to reason about, and more efficient.

## Actor programming model

Another way to describe and think about Durable Objects is through the lens of the [Actor programming model](https://en.wikipedia.org/wiki/Actor_model). There are several popular examples of the Actor model supported at the programming language level through runtimes or library frameworks, like [Erlang](https://www.erlang.org/), [Elixir](https://elixir-lang.org/), [Akka](https://akka.io/), or [Microsoft Orleans for .NET](https://learn.microsoft.com/en-us/dotnet/orleans/overview).

The Actor model simplifies a lot of problems in distributed systems by abstracting away the communication between actors using RPC calls (or message sending) that could be implemented on-top of any transport protocol, and it avoids most of the concurrency pitfalls you get when doing concurrency through shared memory such as race conditions when multiple processes/threads access the same data in-memory.

Each Durable Object instance can be seen as an Actor instance, receiving messages (incoming HTTP/RPC requests), executing some logic in its own single-threaded context using its attached durable storage or in-memory state, and finally sending messages to the outside world (outgoing HTTP/RPC requests or responses), even to another Durable Object instance.

Each Durable Object has certain capabilities in terms of [how much work it can do](/durable-objects/platform/limits/#how-much-work-can-a-single-durable-object-do), which should influence the application's [architecture to fully take advantage of the platform](/reference-architecture/diagrams/storage/durable-object-control-data-plane-pattern/).

Durable Objects are natively integrated into Cloudflare's infrastructure, giving you the ultimate serverless platform to build distributed stateful applications exploiting the entirety of Cloudflare's network.

## Durable Objects in Cloudflare

Many of Cloudflare's products use Durable Objects. Some of our technical blog posts showcase real-world applications and use-cases where Durable Objects make building applications easier and simpler.

These blog posts may also serve as inspiration on how to architect scalable applications using Durable Objects, and how to integrate them with the rest of Cloudflare Developer Platform.

- [Durable Objects aren't just durable, they're fast: a 10x speedup for Cloudflare Queues](https://blog.cloudflare.com/how-we-built-cloudflare-queues/)
- [Behind the scenes with Stream Live, Cloudflare's live streaming service](https://blog.cloudflare.com/behind-the-scenes-with-stream-live-cloudflares-live-streaming-service/)
- [DO it again: how we used Durable Objects to add WebSockets support and authentication to AI Gateway](https://blog.cloudflare.com/do-it-again/)
- [Workers Builds: integrated CI/CD built on the Workers platform](https://blog.cloudflare.com/workers-builds-integrated-ci-cd-built-on-the-workers-platform/)
- [Build durable applications on Cloudflare Workers: you write the Workflows, we take care of the rest](https://blog.cloudflare.com/building-workflows-durable-execution-on-workers/)
- [Building D1: a Global Database](https://blog.cloudflare.com/building-d1-a-global-database/)
- [Billions and billions (of logs): scaling AI Gateway with the Cloudflare Developer Platform](https://blog.cloudflare.com/billions-and-billions-of-logs-scaling-ai-gateway-with-the-cloudflare/)
- [Indexing millions of HTTP requests using Durable Objects](https://blog.cloudflare.com/r2-rayid-retrieval/)

Finally, the following blog posts may help you learn some of the technical implementation aspects of Durable Objects, and how they work.

- [Durable Objects: Easy, Fast, Correct â€” Choose three](https://blog.cloudflare.com/durable-objects-easy-fast-correct-choose-three/)
- [Zero-latency SQLite storage in every Durable Object](https://blog.cloudflare.com/sqlite-in-durable-objects/)
- [Workers Durable Objects Beta: A New Approach to Stateful Serverless](https://blog.cloudflare.com/introducing-workers-durable-objects/)

## Get started

Get started now by following the ["Tutorial with SQL API"](/durable-objects/get-started/tutorial-with-sql-api/) to create your first application using Durable Objects.

[^1]: Storage per Durable Object with SQLite is currently 1 GB. This will be raised to 10 GB for general availability.

---

# Limits

URL: https://developers.cloudflare.com/email-routing/limits/

import { Render } from "~/components"

## Email Workers size limits

When you process emails with Email Workers and you are on [Workersâ€™ free pricing tier](/workers/platform/pricing/) you might encounter an allocation error. This may happen due to the size of the emails you are processing and/or the complexity of your Email Worker. Refer to [Worker limits](/workers/platform/limits/#worker-limits) for more information.

You can use the [log functionality for Workers](/workers/observability/logs/) to look for messages related to CPU limits (such as `EXCEEDED_CPU`) and troubleshoot any issues regarding allocation errors.

If you encounter these error messages frequently, consider upgrading to the [Workers Paid plan](/workers/platform/pricing/) for higher usage limits.

## Message size

Currently, Email Routing does not support messages bigger than 25 MiB.

## Rules and addresses

| Feature                                                                          | Limit |
| -------------------------------------------------------------------------------- | ----- |
| [Rules](/email-routing/setup/email-routing-addresses/)                           | 200   |
| [Addresses](/email-routing/setup/email-routing-addresses/#destination-addresses) | 200   |

<Render file="limits_increase" product="workers" />

## Email Routing summary for emails sent through Workers

Emails sent through Workers will show up in the Email Routing summary page as dropped even if they were successfully delivered.

---

# Overview

URL: https://developers.cloudflare.com/email-routing/

import { Description, Feature, Plan, RelatedProduct, Render } from "~/components"

<Description>

Create custom email addresses for your domain and route incoming emails to your preferred mailbox. 
</Description>

<Plan id="email.email_routing.properties.availability.summary" />

<Render file="email-routing-definition" />

It is available to all Cloudflare customers [using Cloudflare as an authoritative nameserver](/dns/zone-setups/full-setup/).

***

## Features

<Feature header="Email Workers" href="/email-routing/email-workers/">
Leverage the power of Cloudflare Workers to implement any logic you need to process your emails. Create rules as complex or simple as you need. 
</Feature>

<Feature header="Custom addresses" href="/email-routing/get-started/enable-email-routing/">
With Email Routing you can have many custom email addresses to use for specific situations. 
</Feature>

<Feature header="Analytics" href="/email-routing/get-started/email-routing-analytics/">
Email Routing includes metrics to help you check on your email traffic history. 
</Feature>

***

## Related products

<RelatedProduct header="Area 1 Email Security" href="/email-security/" product="email-security">
Cloudflare Area 1 Email Security is a cloud-native service that stops phishing attacks, the biggest cybersecurity threat, across all threat vectors - email, web, and network - either at the edge or in the cloud. 
</RelatedProduct>

<RelatedProduct header="DNS" href="/dns/" product="dns">
Email Routing is available to customers using Cloudflare as an authoritative nameserver. 
</RelatedProduct>

---

# Postmaster

URL: https://developers.cloudflare.com/email-routing/postmaster/

This page provides technical information about Email Routing to professionals who administer email systems, and other email providers.

Here you will find information regarding Email Routing, along with best practices, rules, guidelines, troubleshooting tools, as well as known limitations for Email Routing.

## Postmaster

### Authenticated Received Chain (ARC)

Email Routing supports [Authenticated Received Chain (ARC)](http://arc-spec.org/). ARC is an email authentication system designed to allow an intermediate email server (such as Email Routing) to preserve email authentication results. Google also supports ARC.

### Contact information

The best way to contact us is using our [community forum](https://community.cloudflare.com/new-topic?category=Feedback/Previews%20%26%20Betas&tags=email) or our [Discord server](https://discord.com/invite/cloudflaredev).

### DKIM signature

[DKIM (DomainKeys Identified Mail)](https://en.wikipedia.org/wiki/DomainKeys_Identified_Mail) ensures that email messages are not altered in transit between the sender and the recipient's SMTP servers through public-key cryptography.

Through this standard, the sender publishes its public key to a domain's DNS once, and then signs the body of each message before it leaves the server. The recipient server reads the message, gets the domain public key from the domain's DNS, and validates the signature to ensure the message was not altered in transit.

Email Routing signs email on behalf of `email.cloudflare.net`. If the sender did not sign the email, the receiver will likely use Cloudflare's signature for authentication.

Below is the DKIM key for `email.cloudflare.net`:

```sh
dig TXT 2022._domainkey.email.cloudflare.net +short
```

```sh output

"v=DKIM1; h=sha256; k=rsa; p=MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAnraPy1d8e6+lzeE1HIoUvYWoAOUSREkNHcwxA/ueVM8f6FKXvPu/9gVpgkn8iUyaCfk2z1MW+OVLuFeH64YRMa39mkaQalgke2tZ05SnjRUtYEHYvfrqPuMT+Ouk+GecpgvrtMq5gMXm6ZfeUhQkdWxmMQJGf4fdW5I0piUQJMhK/Qc1dNRSskk" "TiUtXKnsEdjTN2xcnHhyj985S0xOEAxm9Uj1rykPqVvKpqEdjUkujbXOwR0KmHTvPyFpBjCCfxAVqOwwo9zBYuvk/nh0qlDgLIpy0SimrYhNFCq2XBxIj4tdUzIl7qZ5Ck6zLCQ+rjzJ4sm/zA+Ov9kDkbcmyrwIDAQAB"
```

### DMARC enforcing

Email Routing enforces Domain-based Message Authentication, Reporting & Conformance (DMARC). Depending on the sender's DMARC policy, Email Routing will reject emails when there is an authentication failure. Refer to [dmarc.org](https://dmarc.org/) for more information on this protocol.

### IPv6 support

Currently, Email Routing will connect to the upstream SMTP servers using IPv6 if they provide AAAA records for their MX servers, and fall back to IPv4 if that is not possible.

Below is an example of a popular provider that supports IPv6:

```sh
dig mx gmail.com
```

```sh output

gmail.com. 3084 IN MX 5 gmail-smtp-in.l.google.com.
gmail.com. 3084 IN MX 20 alt2.gmail-smtp-in.l.google.com.
gmail.com. 3084 IN MX 40 alt4.gmail-smtp-in.l.google.com.
gmail.com. 3084 IN MX 10 alt1.gmail-smtp-in.l.google.com.
gmail.com. 3084 IN MX 30 alt3.gmail-smtp-in.l.google.com.
```

```sh
dig AAAA gmail-smtp-in.l.google.com
```

```sh output
gmail-smtp-in.l.google.com. 17 IN AAAA 2a00:1450:400c:c09::1b
```

Email Routing also supports IPv6 through Cloudflareâ€™s inbound MX servers.

### MX, SPF, and DKIM records

Email Routing automatically adds a few DNS records to the zone when our customers enable Email Routing. If we take `example.com` as an example:

```txt
example.com. 300 IN MX 13 amir.mx.cloudflare.net.
example.com. 300 IN MX 86 linda.mx.cloudflare.net.
example.com. 300 IN MX 24 isaac.mx.cloudflare.net.

example.com. 300 IN TXT "v=spf1 include:_spf.mx.cloudflare.net ~all"
```

[The MX (mail exchange) records](https://www.cloudflare.com/learning/dns/dns-records/dns-mx-record/) tell the Internet where the inbound servers receiving email messages for the zone are. In this case, anyone who wants to send an email to `example.com` can use the `amir.mx.cloudflare.net`, `linda.mx.cloudflare.net`, or `isaac.mx.cloudflare.net` SMTP servers.

### Outbound hostnames

In addition to the outbound prefixes, Email Routing will use the domain `email.cloudflare.net` for the `HELO/EHLO` command.

PTR records (reverse DNS) ensure that each hostname has an corresponding IP. For example:

```sh
dig a0-7.email.cloudflare.net +short
```

```sh output
104.30.0.7
```

```sh
dig -x 104.30.0.7 +short
```

```sh output
a0-7.email.cloudflare.net.
```

### Outbound prefixes

Email Routing sends its traffic using both IPv4 and IPv6 prefixes, when supported by the upstream SMTP server.

If you are a postmaster and are having trouble receiving Email Routing's emails, allow the following outbound IP addresses in your server configuration:

**IPv4**

`104.30.0.0/20`

**IPv6**

`2405:8100:c000::/38`

_Ranges last updated: December 13th, 2023_

### Sender rewriting

Email Routing rewrites the SMTP envelope sender (`MAIL FROM`) to the forwarding domain to avoid issues with [SPF](#spf-record). Email Routing uses the [Sender Rewriting Scheme](https://en.wikipedia.org/wiki/Sender_Rewriting_Scheme) to achieve this.

This has no effect to the end user's experience, though. The message headers will still report the original sender's `From:` address.

### SMTP errors

In most cases, Email Routing forwards the upstream SMTP errors back to the sender client in-session.

### Spam and abusive traffic

Handling spam and abusive traffic is essential to any email provider. Email Routing filters emails based on advanced anti-spam criteria, [powered by Email Security (formerly Area 1)](/email-security/). When Email Routing detects and blocks a spam email, you will receive a message with details explaining what happened. For example:

```txt
554 <YOUR_IP_ADDRESS> found on one or more DNSBLs (abusixip). Refer to https://developers.cloudflare.com/email-routing/postmaster/#spam-and-abusive-traffic/
```

### SPF record

A SPF DNS record is an anti-spoofing mechanism that is used to specify which IP addresses and domains are allowed to send emails on behalf of your zone.

The Internet Engineering Task Force (IETF) tracks the SPFv1 specification [in RFC 7208](https://datatracker.ietf.org/doc/html/rfc7208). Refer to the [SPF Record Syntax](http://www.open-spf.org/SPF_Record_Syntax/) to learn the SPF syntax.

Email Routing's SPF record contains the following:

```txt
v=spf1 include:_spf.mx.cloudflare.net ~all
```

In the example above:

- `spf1`: Refers to SPF version 1, the most common and more widely adopted version of SPF.
- `include`: Include a second query to `_spf.mx.cloudflare.net` and allow its contents.
- `~all`: Otherwise [`SoftFail`](http://www.open-spf.org/SPF_Record_Syntax/) on all other origins. `SoftFail` means NOT allowed to send, but in transition. This instructs the upstream server to accept the email but mark it as suspicious if it came from any IP addresses outside of those defined in the SPF records.

If we do a TXT query to `_spf.mx.cloudflare.net`, we get:

```txt
_spf.mx.cloudflare.net. 300 IN TXT "v=spf1 ip4:104.30.0.0/20 ~all"
```

This response means:

- Allow all IPv4 IPs coming from the `104.30.0.0/20` subnet.
- Otherwise, `SoftFail`.

You can read more about SPF, DKIM, and DMARC in our [Tackling Email Spoofing and Phishing](https://blog.cloudflare.com/tackling-email-spoofing/) blog.

---

## Known limitations

Below, you will find information regarding known limitations for Email Routing.

### Email address internationalization (EAI)

Email Routing does not support [internationalized email addresses](https://en.wikipedia.org/wiki/International_email). Email Routing only supports [internationalized domain names](https://en.wikipedia.org/wiki/Internationalized_domain_name).

This means that you can have email addresses with an internationalized domain, but not an internationalized local-part (the first part of your email address, before the `@` symbol). Refer to the following examples:

- `info@piÃ±ata.es` - Supported.
- `piÃ±ata@piÃ±ata.es` - Not supported.

### Non-delivery reports (NDRs)

Email Routing does not forward non-delivery reports to the original sender. This means the sender will not receive a notification indicating that the email did not reach the intended destination.

### Restrictive DMARC policies can make forwarded emails fail

Due to the nature of email forwarding, restrictive DMARC policies might make forwarded emails fail to be delivered. Refer to [dmarc.org](https://dmarc.org/wiki/FAQ#My_users_often_forward_their_emails_to_another_mailbox.2C_how_do_I_keep_DMARC_valid.3F) for more information.

### Sending or replying to an email from your Cloudflare domain

Email Routing does not support sending or replying from your Cloudflare domain. When you reply to emails forwarded by Email Routing, the reply will be sent from your destination address (like `my-name@gmail.com`), not your custom address (like `info@my-company.com`).

### Signs such "`+`" and "`.`" are treated as normal characters for custom addresses

Email Routing does not have advanced routing options. Characters such as `+` or `.`, which perform special actions in email providers like Gmail and Outlook, are currently treated as normal characters on custom addresses. More flexible routing options are in our roadmap.

---

# Cloudflare Firewall Rules

URL: https://developers.cloudflare.com/firewall/

import { FeatureTable, Render } from "~/components";

Cloudflare Firewall Rules allows you to create rules that inspect incoming traffic and block, challenge, log, or allow specific requests.

<Render file="deprecation-notice" />

## Main features

- **Rule-based protection**: Use pre-defined rulesets provided by Cloudflare, or define your own firewall rules. Create rules in the Cloudflare dashboard or via API.
- **Complex custom rules**: Each rule's expression can reference multiple fields from all the available HTTP request parameters and fields, allowing you to create complex rules.

## Availability

This table outlines the Firewall Rules features and entitlements available with each customer plan:

<FeatureTable id="security.x_firewall_rules" />

## Next steps

- Unless you are already an advanced user, refer to [Expressions](/ruleset-engine/rules-language/expressions/) and [Actions](/firewall/cf-firewall-rules/actions/) to learn more about the basic elements of firewall rules.

- To start building your own firewall rules, refer to one of the following pages:

  - [Manage firewall rules in the dashboard](/firewall/cf-dashboard/create-edit-delete-rules/)
  - [Manage firewall rules via the APIs](/firewall/api/)

- You can also manage firewall rules through Terraform. For more information, refer to [Getting Started with Terraform](https://blog.cloudflare.com/getting-started-with-terraform-and-cloudflare-part-1/).

## Related resources

- [Cloudflare Rules language](/ruleset-engine/rules-language/)

---

# Glossary

URL: https://developers.cloudflare.com/email-security/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Email Security (formerly Area 1) documentation.

<Glossary product="email-security" />

---

# Overview

URL: https://developers.cloudflare.com/email-security/

import { CardGrid, Description, Feature, GlossaryTooltip, LinkTitleCard, Plan, RelatedProduct, Render } from "~/components"

<Description>

Stop <GlossaryTooltip term="phishing">phishing</GlossaryTooltip> attacks with Email Security (formerly Area 1) cloud-native email security service. 
</Description>

<Plan id="email.email_security.properties.availability.summary" />

:::caution[Area 1 has been renamed]


Area 1 is now **Email Security (formerly Area 1)**. Customers who purchased the new Email Security can access the documentation by going to Cloudflare Zero Trust > [Email Security](/cloudflare-one/email-security/).


:::

<Render file="email-security-description" />

***

## Features

<Feature header="Deployment" href="/email-security/deployment/" cta="Evaluate deployment options">
Email Security (formerly Area 1) provides two architectures to protect your organization: inline or API setup. Inline architecture evaluates email messages before they reach a user's inbox. API architecture evaluates emails when they have already reached a user's inbox. 
</Feature>

<Feature header="SSO integration" href="/email-security/account-setup/sso/">
In addition to standard logins, Email Security (formerly Area 1) offers support for <GlossaryTooltip term="SAML">SAML</GlossaryTooltip> based single sign-on (SSO) logins to your dashboard. 
</Feature>

<Feature header="Business email compromise" href="/email-security/email-configuration/enhanced-detections/business-email-compromise/">
Attackers often try to impersonate executives within an organization when sending malicious emails. The Business email compromise (BEC) feature protects against these attacks. 
</Feature>

***

## Related products

<RelatedProduct header="Cloudflare Zero Trust" href="/cloudflare-one/" product="cloudflare-one">
Cloudflare Zero Trust replaces legacy security perimeters with Cloudflare's global network, making the Internet faster and safer for teams around the world. 
</RelatedProduct>

<RelatedProduct header="Email Routing" href="/email-routing/" product="email-routing">
Email Routing simplifies the way you create and manage custom email addresses. Email Security (formerly Area 1) helps secure your mail infrastructure from phishing attacks. 
</RelatedProduct>

<RelatedProduct header="DMARC Management" href="/dmarc-management/" product="dmarc-management">
Stop brand impersonation. Track and manage every source that is sending emails from your domain. 
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Pricing" href="https://www.cloudflare.com/products/zero-trust/email-security/" icon="seti:shell">

Email Security (formerly Area 1) is available as a standalone product purchase. 
</LinkTitleCard>

</CardGrid>

---

# Channel and Alliance Partners

URL: https://developers.cloudflare.com/email-security/partners/

import { GlossaryTooltip, Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

Email Security Channel and Alliance partners have the option to set up accounts for themselves and their customers.

## Create accounts

Start by creating parent and child accounts.

### Create a parent account

Parent accounts are treated as containers with no services provisioned. User accounts created at the parent level will allow them to access any child account.

:::note
This is only required for administrators that manage multiple accounts. For example, Managed Security Service Providers (MSSP) managing multiple customer accounts.
:::

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. In **Delegated Accounts** > **Accounts**, select **Create new customer**.
4. Enter their information, and make sure you select *Parent* in **Account Type**.
5. Select **Save**.

Your newly created account should show up in the list. If not, refresh the page.

### Create a child account

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. In **Delegated Accounts** > **Accounts**, select the parent account where you want to create a child account.
4. Select **Create New customer**.
5. Enter their information, and make sure you select *Advantage* in **Account Type**.
6. Scroll down to the **Email Traffic Related Information** section, and enter the information related to your email provider. The number to enter in **Loopback Hops** will depend on your email configuration and where Email Security is in the chain of events. Refer to [Inline deployment](/email-security/deployment/inline/) and [API deployment](/email-security/deployment/api/) for more information.
7. For **Daily Email Volume** and **Number of Email Users** make sure you enter the appropriate values for your organization.
8. Select **Save**.

## Create users and assign permissions

You can create users at both the parent and child account level. Users created at parent level will have access to all its child accounts. Users created at child level will only have access to the assigned child account.

Child accounts can [limit or disable](/email-security/account-setup/manage-parent-permissions/) the level of access allowed from their parent account.

If you modify the Delegated Access controls, make sure you create an administrator account in the child first.

To create an account at parent level or child level:

<Render file="add-user" params={{ one: "with a parent account or child account depending on what you are trying to create" }} />

## Escalation contacts

You should add escalation contacts so Email Security can send notifications regarding detection events and critical service related issues. Email Security highly recommends that these contacts have both phone and email contacts.

Refer to [Escalation contacts](/email-security/account-setup/escalation-contacts/) for more information.

## Status alerts

Subscribe to incident status alerts [from Email Security](https://status.area1security.com/).

## Domains setup (inline/API)

Refer to the [setup options](/email-security/deployment/) for Email Security to learn about the best way of deploying Email Security in your organization. You can choose between two main setup architectures:

* Inline deployment
* API deployment

With an [inline deployment](/email-security/deployment/inline/), Email Security evaluates email messages before they reach a userâ€™s inbox. When you choose an [API deployment](/email-security/deployment/api/), email messages only reach Email Security after they have already reached a userâ€™s inbox.

## Classification actions

Email Security recommends that you quarantine `MALICIOUS` and `SPAM` <GlossaryTooltip term="disposition">dispositions</GlossaryTooltip>. You can configure this directly in [Office 365](/email-security/deployment/inline/setup/office-365-area1-mx/) and [Gsuite](/email-security/deployment/inline/setup/gsuite-area1-mx/), as well as [Email Security](/email-security/email-configuration/domains-and-routing/domains/).

## Message retraction

You can configure message retraction to take post-delivery actions against suspicious email messages. You can retract messages manually or automatically. Refer to [Retract settings](/email-security/email-configuration/retract-settings/) for more information.

## TLS enforcement for domains

To add additional TLS requirements for emails coming from certain domains, you can enforce higher levels of SSL/TLS inspection. Refer to [Partner Domains TLS](/email-security/email-configuration/domains-and-routing/partner-domains-tls/) for more information.

## Reports

You can subscribe to [daily and weekly email reports](https://horizon.area1security.com/settings/subscriptions/email-subscriptions), as well as <GlossaryTooltip term="SIEM" link="https://horizon.area1security.com/settings/email/routing/webhooks">SIEM events</GlossaryTooltip>. For SIEM events, you will need to [configure your SIEM tool](/email-security/reporting/siem-integration/) into Email Security first.

## Whitelisting and blocklisting senders

If you need to whitelist of blocklist senders, refer to [Allow and block lists](/email-security/email-configuration/lists/).

## Submitting false positives and false negatives

There are several ways of dealing with missed <GlossaryTooltip term="phishing">phish</GlossaryTooltip> or messages flagged as such that are not. Refer to [Phish submissions](/email-security/email-configuration/phish-submissions/) to learn more.

## Best practices

Refer to the following pages to learn more:

1. [Business Email compromise (BEC)](/email-security/email-configuration/enhanced-detections/business-email-compromise/)
2. [Text add-ons](/email-security/email-configuration/email-policies/text-addons/)
3. [Search and reports](/email-security/reporting/)

---

# Changelog

URL: https://developers.cloudflare.com/fundamentals/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/fundamentals.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Overview

URL: https://developers.cloudflare.com/fundamentals/

import { Render } from "~/components"

Cloudflare helps connect and protect millions of customers globally. Everyone from individuals to the world's largest enterprises use our unified platform of networking, security, and developer services to succeed everywhere in the world.

Before you get started, we recommend reviewing [Concepts](/fundamentals/concepts/) to learn about key concepts related to using different Cloudflare products.

## Additional resources

<Render file="cloudflare-resources" product="fundamentals" />

---

# Google tag in first-party mode

URL: https://developers.cloudflare.com/google-tag-first-party-mode/

## Get started

Google tag in first-party mode allows website owners using Cloudflare as a CDN to get the most out of ad measurement tools with just a few clicks.

At the moment, Google tag in first-party mode is in closed beta. You can [sign up](https://www.cloudflare.com/en-gb/lp/google-tag-first-party-mode/) to participate in the closed beta through [Google Ads](http://ads.google.com), [Google Analytics](http://analytics.google.com) or [Google Marketing Platform](https://marketingplatform.google.com/).

### Configure Google tag in first-party mode in the dashboard

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Go to **Google tag in first-party mode** > **Configuration**.
3. Enable the toggle for **Turn on and configure Google tag**. 

![Google tag in first-party mode configuration](~/assets/images/google-tag-first-party-mode/google-tag-configuration.png)

4. Add your Google tag ID and the path on your website reserved for the Google tag. 
The [Google tag ID](https://support.google.com/analytics/answer/9539598?hl=en) can be found in the Google Tag Experience dashboard. The measurement path is an unused path on your site that will load Google Tag Manager and all subsequent measurement requests.  

![Add to ID and path](~/assets/images/google-tag-first-party-mode/google-tag-id-path.png)

5. Once you click **Save**, Google Tag in First Party Mode will be enabled on your zone. If you already have a GTM script on your website, this First Party Tag will override the existing script.

Now that you have authenticated into your Cloudflare account and configured GTM in first-party mode, your Google Tags will be loaded using `https://your-domain/measurement-path/...`and subsequent measurement requests will be served by Cloudflare.

---

# Get started

URL: https://developers.cloudflare.com/health-checks/get-started/

This guide will get you started with creating and managing configured Health Checks.

## Create a Health Check

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Traffic** > **Health Checks**.
3. Select **Create** and fill out the form, paying special attention to:
   * The values for **Interval** and **Check regions**, because decreasing the **Interval** and increasing **Check regions** may increase the load on your origin server.
   * **Retries**, which specify the number of retries to attempt in case of a timeout before marking the origin as unhealthy.
4. Select **Save and Deploy**.

## Manage Health Checks

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account and domain.
2. Go to **Traffic** > **Health Checks**.
3. Navigate to your health check and select **Edit**.
4. Edit your Health Check.
5. Select **Save**.

:::note


You can also enable, disable, or delete configured Health Checks.


:::

:::note


Authenticated origin pull is not supported by Standalone Health Checks.


:::

---

# Health Checks

URL: https://developers.cloudflare.com/health-checks/

import { Feature, FeatureTable, GlossaryTooltip, RelatedProduct } from "~/components"

Standalone Health Checks monitors an IP address or hostname for origin servers or applications and notifies you in near real-time if there happens to be a problem.

A Health Check is a service that runs on Cloudflareâ€™s edge network to monitor whether an origin server is online. This allows you to view the health of your origin servers even if there is only one origin or you do not yet need to balance traffic across your infrastructure.

Standalone Health Checks support various configurations to hone in on what you can check, including response codes, protocol types, and intervals. You can specify a particular path if an origin server serves multiple applications or check a larger subset of response codes for your staging environment. All of these options allow you to properly target your Health Check, providing a precise picture of what is wrong with an origin server.

:::note


Standalone Health Checks are different from health monitors associated with load balancers. For more details about health monitors, refer to the [Load Balancing documentation](/load-balancing/monitors/).


:::

***

## Features

<Feature header="Health Checks Analytics" href="/health-checks/health-checks-analytics/">

You can use Health Checks Analytics to evaluate origin uptime, latency, failure reason, and specific event logs to debug possible origin issues.


</Feature>

***

## Related products

<RelatedProduct header="Load Balancing" href="/load-balancing/" product="load-balancing">

Cloudflare Load Balancing distributes traffic across your <GlossaryTooltip term="endpoint" link="/load-balancing/understand-basics/load-balancing-components/">endpoints</GlossaryTooltip>, which reduces endpoint strain and latency and improves the experience for end users.


</RelatedProduct>

***

## Availability

<FeatureTable id="traffic.health_checks" />

---

# Health Checks Analytics

URL: https://developers.cloudflare.com/health-checks/health-checks-analytics/

import { AvailableNotifications, Render } from "~/components"

Once you have set up a standalone Health Check including notification emails, use Health Check Analytics to debug possible origin issues.

To access health check analytics:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account and domain.
2. Go to **Traffic** > **Health Check Analytics**.

You can evaluate origin uptime, latency, failure reason, and specific event logs:

* **Health Checks By Uptime**: Shows the percentage of uptime for individual origins over time.
* **Health Checks By Failure Reason**: Shows a breakdown of failures by the specific reason. See [common error code causes and solutions below](/health-checks/health-checks-analytics/#common-error-codes).
* **Health Checks By Latency**: Shows average latency â€“ measured in round trip time â€” for individual origins over time.
* **Event Log**: Shows individual health check data.
  * Select each record for additional details on **Round trip time**, the **Failure Reason**, the **Average Waterfall** (showing chronological data about request stages), **Response status code**, and more.

## Set up alerts

You can configure alerts to notify you of any changes in your health check status.

<AvailableNotifications product="Health Checks" />

<Render file="get-started" product="notifications" />

## Common error codes

### TCP connection failed

#### Cause

Health Checks failed to establish a TCP connection to your origin server.

#### Solution

This typically occurs when there is a network failure between Cloudflare and your origin, and/or a firewall refuses to allow our connection. Ensure your network and firewall configurations are not interfering with traffic.

### HTTP timeout occurred

#### Cause

The origin failed to return an HTTP response within the timeout configured. This happens if you have the timeout set to a low number. For example, one to two seconds.

#### Solution

Cloudflare recommends increasing the HTTP response timeout to allow the origin server to respond.

### Response code mismatch error

#### Cause

Cloudflare receives an HTTP status code that does not match the values defined in the `expected_codes` property of your Health Check configuration.

#### Solution

Response codes must match the `expected_codes`. Confirm the values are correct by comparing the expected response codes and the status code received in the Event Log.

#### â€‹â€‹Alternate cause

You may also see this issue if you have a Health Check configured to use HTTP connections and your origin server is redirecting to HTTPS. In this case, the response code will often be `301`, `302`, or `303`.

#### Solution

Change your Cloudflare Health Check configuration to use HTTPS or set the value of `follow_redirect` to `true` so that Cloudflare can resolve the correct status code.

### Response body mismatch error

#### Cause

The response body returns from your origin server and does not include the (case-insensitive) value of `expected_body` configured in your Health Check. 

:::note
We only read the first 10 KB of the response. If you return a larger response, and the `expected_body` is not in the first 10 KB, the Health Check will fail. 
:::

#### Solution

Ensure the `expected_body` is in the first 10 KB of the response body.
â€‹â€‹

### TLS untrusted certificate error

#### Cause

The certificate is not trusted by a public Certificate Authority (CA).

#### Solution

If youâ€™re using a self-signed certificate, Cloudflare recommends either using a publicly trusted certificate or setting the `allow_insecure` property on your Health Check to `true`.

### TLS name mismatch error

#### Cause

Our Health Check (client) was not able to match a name on the server certificate to the hostname of the request.

#### Solution

Inspect your Health Check configuration to confirm that the `header` value set in the Cloudflare Health Check is correct.

### TLS protocol error

#### Cause

This error can occur if you are using an older version of TLS or your origin server is not configured for HTTPS.

#### Solution

Ensure that your origin server supports TLS 1.2 or greater and is configured for HTTPS.

### TLS unrecognized name error

#### Cause

The server did not recognize the name provided by the client. When a host header is set, this is set as the ServerName in the initial TLS handshake. If it is not set, Cloudflare will not provide a ServerName, which can cause this error.

#### Solution

Set the host header in your Health Check object.

### â€‹â€‹No route to host error

#### Cause

The IP address cannot be reached from Cloudflareâ€™s network. Common causes are ISP or hosting provider network issues (e.g. BGP level), or that the IP does not exist.

#### Solution

Ensure IP is accurate, and check if there is an ISP or hosting provider network issue.

### TCP Timeout

#### Cause

Data transmission was not acknowledged and the retransmit of data did not succeed.

#### Solution

Confirm whether the SYN-ACK for the handshake takes place at your origin and contact [Cloudflare support](/support/contacting-cloudflare-support/).

### â€‹â€‹Network Unreachable

#### Cause

Cloudflare cannot connect to the origin web server due to network unavailability. This is usually caused by a network issue or incorrect origin IP.

#### Solution

Check the IP entered for the origin in Cloudflareâ€™s Health Checks configuration or the IP returned via DNS for the origin hostname.

### HTTP Invalid Response

#### Cause

Usually caused by an HTTP 502 error or bad gateway.

#### Solution

Ensure the origin web server responds to requests and that no applications have crashed or are under high load.

### DNS Unknown Host

#### Cause

The origin web server hostname does not exist.

#### Solution

Confirm the origin web server resolves to an IP address.

### Connection Reset by Peer

#### Cause

A network error occurred while the client received data from the origin web server.

#### Solution

Confirm whether the origin web server is experiencing a high amount of traffic or an error.

### Monitor Configuration Error

#### Cause

There was a configuration error in the Health Check and no checks were run against the origin.

#### Solution

Review your Health Check configuration to ensure it matches an expected request to your origin.

### â€‹â€‹DNS Internal

#### Cause

The origin web serverâ€™s hostname resolves to an internal or restricted address. No checks are run against this origin.

#### Solution

Cloudflare does not allow use of an origin web server hostname that resolves to a Cloudflare IP.

### Other Failure

#### Cause

If the failure cannot be classified as any other type of failure mentioned above.

#### Solution

Contact [Cloudflare support](/support/contacting-cloudflare-support/).

---

# Demos and architectures

URL: https://developers.cloudflare.com/hyperdrive/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use Hyperdrive within your existing application and architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Hyperdrive.

<ExternalResources type="apps" products={["Hyperdrive"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use Hyperdrive:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["Hyperdrive"]} />

---

# Get started

URL: https://developers.cloudflare.com/hyperdrive/get-started/

import { Render, PackageManagers } from "~/components";

Hyperdrive accelerates access to your existing databases from Cloudflare Workers, making even single-region databases feel globally distributed.

By maintaining a connection pool to your database within Cloudflare's network, Hyperdrive reduces seven round-trips to your database before you can even send a query: the TCP handshake (1x), TLS negotiation (3x), and database authentication (3x).

Hyperdrive understands the difference between read and write queries to your database, and can cache the most common read queries, improving performance and reducing load on your origin database.

This guide will instruct you through:

- Creating your first Hyperdrive configuration.
- Creating a [Cloudflare Worker](/workers/) and binding it to your Hyperdrive configuration.
- Establishing a database connection from your Worker to a public database.

## Prerequisites

:::note[Workers Paid plan required]

Hyperdrive is available to all users on the [Workers Paid plan](/workers/platform/pricing/#workers).

:::

To continue:

1. Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up/workers-and-pages) if you have not already.
2. Install [`Node.js`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm). Use a Node version manager like [Volta](https://volta.sh/) or [nvm](https://github.com/nvm-sh/nvm) to avoid permission issues and change Node.js versions. [Wrangler](/workers/wrangler/install-and-update/) requires a Node version of `16.17.0` or later.
3. Have **a publicly accessible PostgreSQL (or PostgreSQL compatible) database**. Cloudflare recommends [Neon](https://neon.tech/) if you do not have an existing database. Read the [Neon documentation](https://neon.tech/docs/introduction) to create your first database.

## 1. Log in

Before creating your Hyperdrive binding, log in with your Cloudflare account by running:

```sh
npx wrangler login
```

You will be directed to a web page asking you to log in to the Cloudflare dashboard. After you have logged in, you will be asked if Wrangler can make changes to your Cloudflare account. Scroll down and select **Allow** to continue.

## 2. Create a Worker

:::note[New to Workers?]

Refer to [How Workers works](/workers/reference/how-workers-works/) to learn about the Workers serverless execution model works. Go to the [Workers Get started guide](/workers/get-started/guide/) to set up your first Worker.

:::

Create a new project named `hyperdrive-tutorial` by running:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"hyperdrive-tutorial"}
/>

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "hello-world",
		type: "Hello World Worker",
		lang: "TypeScript",
	}}
/>

This will create a new `hyperdrive-tutorial` directory. Your new `hyperdrive-tutorial` directory will include:

- A `"Hello World"` [Worker](/workers/get-started/guide/#3-write-code) at `src/index.ts`.
- A [`wrangler.jsonc`](/workers/wrangler/configuration/) configuration file. `wrangler.jsonc` is how your `hyperdrive-tutorial` Worker will connect to Hyperdrive.

### Enable Node.js compatibility

[Node.js compatibility](/workers/runtime-apis/nodejs/) is required for database drivers, including Postgres.js, and needs to be configured for your Workers project.

<Render file="nodejs_compat" product="workers" />

## 3. Connect Hyperdrive to a database

:::note

Hyperdrive currently works with PostgreSQL and PostgreSQL compatible databases, including CockroachDB and Materialize.

Support for other database engines, including MySQL, is on the roadmap.

:::

Hyperdrive works by connecting to your database.

To create your first Hyperdrive database configuration, change into the directory you just created for your Workers project:

```sh
cd hyperdrive-tutorial
```

:::note

Support for the new `hyperdrive` commands in the wrangler CLI requires a wrangler version of `3.10.0` or later. You can use `npx wrangler@latest` to always ensure you are using the latest version of Wrangler.

:::

To create your first Hyperdrive, you will need:

- The IP address (or hostname) and port of your database.
- The database username (for example, `hyperdrive-demo`).
- The password associated with that username.
- The name of the database you want Hyperdrive to connect to. For example, `postgres`.

Hyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:

```txt
postgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name
```

Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.

To create a Hyperdrive connection, run the `wrangler` command, replacing the placeholder values passed to the `--connection-string` flag with the values of your existing database:

```sh
npx wrangler hyperdrive create <YOUR_CONFIG_NAME> --connection-string="postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name"
```

If successful, the command will output your new Hyperdrive configuration:

```json
{
  "hyperdrive": [
    {
      "binding": "HYPERDRIVE",
			"id": "<example id: 57b7076f58be42419276f058a8968187>"
    }
  ]
}

```

Copy the `id` field: you will use this in the next step to make Hyperdrive accessible from your Worker script.

:::note

Hyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](/hyperdrive/observability/troubleshooting/) to debug possible causes.

:::


## 4. Bind your Worker to Hyperdrive

<Render file="create-hyperdrive-binding" product="hyperdrive" />

## 5. Run a query against your database

### Install a database driver

To connect to your database, you will need a database driver which allows you to authenticate and query your database. For this tutorial, you will use [Postgres.js](https://github.com/porsager/postgres), one of the most widely used PostgreSQL drivers.

To install `postgres`, ensure you are in the `hyperdrive-tutorial` directory. Open your terminal and run the following command:

<PackageManagers pkg="postgres" comment="This should install v3.4.5 or later" />

With the driver installed, you can now create a Worker script that queries your database.

### Write a Worker

After you have set up your database, you will run a SQL query from within your Worker.

Go to your `hyperdrive-tutorial` Worker and open the `index.ts` file.

The `index.ts` file is where you configure your Worker's interactions with Hyperdrive.

Populate your `index.ts` file with the following code:

```typescript
// Postgres.js 3.4.5 or later is recommended
import postgres from "postgres";

export interface Env {
	// If you set another name in the Wrangler config file as the value for 'binding',
	// replace "HYPERDRIVE" with the variable name you defined.
	HYPERDRIVE: Hyperdrive;
}

export default {
	async fetch(request, env, ctx): Promise<Response> {
		console.log(JSON.stringify(env));
		// Create a database client that connects to your database via Hyperdrive.
		//
		// Hyperdrive generates a unique connection string you can pass to
		// supported drivers, including node-postgres, Postgres.js, and the many
		// ORMs and query builders that use these drivers.
		const sql = postgres(
            env.HYPERDRIVE.connectionString,
            {
                // Workers limit the number of concurrent external connections, so be sure to limit
                // the size of the local connection pool that postgres.js may establish.
                max: 5,

                // If you are using array types in your Postgres schema, it is necessary to fetch
                // type information to correctly de/serialize them. However, if you are not using
                // those, disabling this will save you an extra round-trip every time you connect.
                fetch_types: false,
            },
        );

		try {
			// Test query
			const results = await sql`SELECT * FROM pg_tables`;

			// Clean up the client, ensuring we don't kill the worker before that is
			// completed.
			ctx.waitUntil(sql.end());

			// Return result rows as JSON
			return Response.json(results);
		} catch (e) {
			console.error(e);
			return Response.json(
				{ error: e instanceof Error ? e.message : e },
				{ status: 500 },
			);
		}
	},
} satisfies ExportedHandler<Env>;
```

Upon receiving a request, the code above does the following:

1. Creates a new database client configured to connect to your database via Hyperdrive, using the Hyperdrive connection string.
2. Initiates a query via `await sql` that outputs all tables (user and system created) in the database (as an example query).
3. Returns the response as JSON to the client.

## 6. Deploy your Worker

You can now deploy your Worker to make your project accessible on the Internet. To deploy your Worker, run:

```sh
npx wrangler deploy
# Outputs: https://hyperdrive-tutorial.<YOUR_SUBDOMAIN>.workers.dev
```

You can now visit the URL for your newly created project to query your live database.

For example, if the URL of your new Worker is `hyperdrive-tutorial.<YOUR_SUBDOMAIN>.workers.dev`, accessing `https://hyperdrive-tutorial.<YOUR_SUBDOMAIN>.workers.dev/` will send a request to your Worker that queries your database directly.

By finishing this tutorial, you have created a Hyperdrive configuration, a Worker to access that database and deployed your project globally.

## Next steps

- Learn more about [how Hyperdrive works](/hyperdrive/configuration/how-hyperdrive-works/).
- How to [configure query caching](/hyperdrive/configuration/query-caching/).
- [Troubleshooting common issues](/hyperdrive/observability/troubleshooting/) when connecting a database to Hyperdrive.

If you have any feature requests or notice any bugs, share your feedback directly with the Cloudflare team by joining the [Cloudflare Developers community on Discord](https://discord.cloudflare.com).

---

# Overview

URL: https://developers.cloudflare.com/hyperdrive/

import {
	CardGrid,
	Description,
	Feature,
	LinkTitleCard,
	Plan,
	RelatedProduct,
	Tabs,
	TabItem,
	LinkButton
} from "~/components";

<Description>

Turn your existing regional database into a globally distributed database.

</Description>

<Plan type="workers-paid" />

Hyperdrive is a service that accelerates queries you make to existing databases, making it faster to access your data from across the globe from [Cloudflare Workers](/workers/), irrespective of your users' location.

Hyperdrive supports any Postgres database, including those hosted on AWS, Google Cloud and Neon, as well as Postgres-compatible databases like CockroachDB and Timescale, with MySQL coming soon. You do not need to write new code or replace your favorite tools: Hyperdrive works with your existing code and tools you use.

Use Hyperdrive's connection string from your Cloudflare Workers application with your existing Postgres drivers and object-relational mapping (ORM) libraries:

<Tabs>
	<TabItem label="Workers Binding API">
	<Tabs>
	<TabItem label="index.ts">
```ts
import postgres from 'postgres';

export default {
	async fetch(request, env, ctx): Promise<Response> {
		// Hyperdrive provides a unique generated connection string to connect to
		// your database via Hyperdrive that can be used with your existing tools
		const sql = postgres(env.HYPERDRIVE.connectionString);

    	try {
    		// Sample SQL query
    		const results = await sql`SELECT * FROM pg_tables`;

    		// Close the client after the response is returned
    		ctx.waitUntil(sql.end());

    		return Response.json(results);
    	} catch (e) {
    		return Response.json({ error: e instanceof Error ? e.message : e }, { status: 500 });
    	}
    },
} satisfies ExportedHandler<{ HYPERDRIVE: Hyperdrive }>;
```

    </TabItem>
    <TabItem label="wrangler.jsonc">
```json
	{
		"$schema": "node_modules/wrangler/config-schema.json",
		"name": "WORKER-NAME",
		"main": "src/index.ts",
		"compatibility_date": "2025-02-04",
		"compatibility_flags": [
			"nodejs_compat"
		],
		"observability": {
			"enabled": true
		},
		"hyperdrive": [
			{
				"binding": "HYPERDRIVE",
				"id": "<YOUR_HYPERDRIVE_ID>",
				"localConnectionString": "<ENTER_LOCAL_CONNECTION_STRING_FOR_LOCAL_DEVELOPMENT_HERE>"
			}
		]
	}
```

</TabItem>
</Tabs>

<LinkButton href="/hyperdrive/get-started/">Get started</LinkButton>

</TabItem>
</Tabs>

---

## Features

<Feature header="Connect your database" href="/hyperdrive/get-started/" cta="Connect Hyperdrive to your database">

Connect Hyperdrive to your existing database and deploy a [Worker](/workers/) that queries it.

</Feature>

<Feature header="PostgreSQL support" href="/hyperdrive/configuration/connect-to-postgres/" cta="Connect Hyperdrive to your PostgreSQL database">

Hyperdrive allows you to connect to any PostgreSQL or PostgreSQL-compatible database.

</Feature>

<Feature header="Query Caching" href="/hyperdrive/configuration/query-caching/" cta="Learn about Query Caching">

Use Hyperdrive to cache the most popular queries executed against your database.

</Feature>

---

## Related products

<RelatedProduct header="Workers" href="/workers/" product="workers">

Build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale.

</RelatedProduct>

<RelatedProduct header="Pages" href="/pages/" product="pages">

Deploy dynamic front-end applications in record time.

</RelatedProduct>

---

## More resources

<CardGrid>

<LinkTitleCard
	title="Pricing"
	href="/hyperdrive/platform/pricing/"
	icon="seti:shell"
>
	Learn about Hyperdrive's pricing.
</LinkTitleCard>

<LinkTitleCard
	title="Limits"
	href="/hyperdrive/platform/limits/"
	icon="document"
>
	Learn about Hyperdrive limits.
</LinkTitleCard>

<LinkTitleCard
	title="Storage options"
	href="/workers/platform/storage-options/"
	icon="document"
>
	Learn more about the storage and database options you can build on with
	Workers.
</LinkTitleCard>

<LinkTitleCard
	title="Developer Discord"
	href="https://discord.cloudflare.com"
	icon="discord"
>
	Connect with the Workers community on Discord to ask questions, show what you
	are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard
	title="@CloudflareDev"
	href="https://x.com/cloudflaredev"
	icon="x.com"
>
	Follow @CloudflareDev on Twitter to learn about product announcements, and
	what is new in Cloudflare Developer Platform.
</LinkTitleCard>

</CardGrid>
````

---

# Demos and architectures

URL: https://developers.cloudflare.com/images/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use Images within your existing architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Images.

<ExternalResources type="apps" products={["Images"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use Images:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["Images"]} />

---

# Get started

URL: https://developers.cloudflare.com/images/get-started/

In this guide, you will get started with Cloudflare Images and make your first API request.

## Prerequisites

Before you make your first API request, ensure that you have a Cloudflare Account ID and an API token.

Refer to [Find zone and account IDs](/fundamentals/setup/find-account-and-zone-ids/) for help locating your Account ID and [Create an API token](/fundamentals/api/get-started/create-token/) to learn how to create an access your API token.

## Make your first API request

```bash
curl --request POST \
  --url https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/images/v1 \
  --header 'Authorization: Bearer <API_TOKEN>' \
  --header 'Content-Type: multipart/form-data' \
  --form file=@./<YOUR_IMAGE.IMG>
```

## Enable transformations on your zone

You can dynamically optimize images that are stored outside of Cloudflare Images and deliver them using [transformation URLs](/images/transform-images/transform-via-url/).

Cloudflare will automatically cache every transformed image on our global network so that you store only the original image at your origin.

To enable transformations on your zone:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Go to **Images** > **Transformations**.
3. Go to the specific zone where you want to enable transformations.
4. Select **Enable for zone**. This will allow you to optimize and deliver remote images.

:::note


With **Resize images from any origin** unchecked, only the initial URL passed will be checked. Any redirect returned will be followed, including if it leaves the zone, and the resulting image will be transformed.


:::

:::note


If you are using transformations in a Worker, you need to include the appropriate logic in your Worker code to prevent resizing images from any origin. Unchecking this option in the dash does not apply to transformation requests coming from Cloudflare Workers.


:::

---

# Overview

URL: https://developers.cloudflare.com/images/

import { CardGrid, Description, Feature, LinkTitleCard, Plan } from "~/components"

<Description>

Store, transform, optimize, and deliver images at scale 
</Description>

<Plan type="all" />

Cloudflare Images provides an end-to-end solution designed to help you streamline your image infrastructure from a single API and runs on [Cloudflare's global network](https://www.cloudflare.com/network/).

There are two different ways to use Images:

- **Efficiently store and deliver images.** You can upload images into Cloudflare Images and dynamically deliver multiple variants of the same original image.
- **Optimize images that are stored outside of Images** You can make transformation requests to optimize any publicly available image on the Internet.

Cloudflare Images is available on both [Free and Paid plans](/images/pricing/). By default, all users have access to the Images Free plan, which includes limited usage of the transformations feature to optimize images in remote sources.

:::note[Image Resizing is now available as transformations]

All Image Resizing features are available as transformations with Images. Each unique transformation is billed only once per 30 days. 

If you are using a legacy plan with Image Resizing, visit the [dashboard](https://dash.cloudflare.com/) to switch to an Imagesplan.


:::

***

## Features

<Feature header="Storage" href="/images/upload-images/">
Use Cloudflareâ€™s edge network to store your images.


</Feature>

<Feature header="Direct creator upload" href="/images/upload-images/direct-creator-upload/">
Accept uploads directly and securely from your users by generating a one-time token. 
</Feature>

<Feature header="Variants" href="/images/transform-images" cta="Create variants by transforming images">
Add up to 100 variants to specify how images should be resized for various use cases. 
</Feature>

<Feature header="Signed URLs" href="/images/manage-images/serve-images/serve-private-images" cta="Serve private images">
Control access to your images by using signed URL tokens. 
</Feature>

***

## More resources

<CardGrid>

<LinkTitleCard title="Community Forum" href="https://community.cloudflare.com/c/developers/images/63" icon="open-book">

Engage with other users and the Images team on Cloudflare support forum. 
</LinkTitleCard>

</CardGrid>

---

# Overview

URL: https://developers.cloudflare.com/key-transparency/

import { CardGrid, Description, Feature, LinkTitleCard, RelatedProduct } from "~/components"

<Description>

Secure the distribution of public keys in your end-to-end encrypted (E2EE) messaging systems

</Description>

Cloudflare's Key Transparency Auditor aims to secure the distribution of public keys for end-to-end encrypted (E2EE) messaging systems like [WhatsApp](https://engineering.fb.com/2023/04/13/security/whatsapp-key-transparency/). It achieves this by building a verifiable append-only data structure called a Log, similar to [Certificate Transparency](https://developer.mozilla.org/en-US/docs/Web/Security/Certificate_Transparency).

Cloudflare acts as an auditor of Key Transparency Logs to ensure the transparency of end-to-end encrypted messaging public keys. Cloudflare provides an API for anyone to monitor the verification work we perform, and verify the state of its associated Logs locally.

## Related products

<RelatedProduct header="Certificate Transparency Monitoring" href="/ssl/edge-certificates/additional-options/certificate-transparency-monitoring/" product="ssl">

Certificate Transparency (CT) Monitoring is an opt-in feature in public beta that aims to improve security by allowing you to double-check any SSL/TLS certificates issued for your domain.

</RelatedProduct>

<RelatedProduct header="Privacy Gateway" href="/privacy-gateway/" product="privacy-gateway">

Privacy Gateway is a managed service deployed on Cloudflare's global network that implements part of the [Oblivious HTTP (OHTTP) IETF](https://www.ietf.org/archive/id/draft-thomson-http-oblivious-01.html) standard. The goal of Privacy Gateway and Oblivious HTTP is to hide the client's IP address when interacting with an application backend.

</RelatedProduct>

---

# Pricing

URL: https://developers.cloudflare.com/images/pricing/

By default, all users are on the Images Free plan. The Free plan includes access to the transformations feature, which lets you optimize images stored outside of Images, like in R2. 

The Paid plan allows transformations, as well as access to storage in Images.

Pricing is dependent on which features you use. The table below shows which metrics are used for each use case.

| Use case | Metrics | Availability |
|----------|---------|--------------|
| Optimize images stored outside of Images | Images Transformed | Free and Paid plans |
| Optimized images that are stored in Cloudflare Images | Images Stored, Images Delivered | Only Paid plans |

## Images Free

On the Free plan, you can request up to 5,000 unique transformations each month for free.

Once you exceed 5,000 unique transformations:

- Existing transformations in cache will continue to be served as expected.
- New transformations will return a `9422` error. If your source image is from the same domain where the transformation is served, then you can use the [`onerror` parameter](/images/transform-images/transform-via-url/#onerror) to redirect to the original image.
- You will not be charged for exceeding the limits in the Free plan.

To request more than 5,000 unique transformations each month, you can purchase an Images Paid plan.

## Images Paid 

When you purchase an Images Paid plan, you can choose your own storage or add storage in Images.

| Metric | Pricing |
|--------|---------|
| Images Transformed | First 5,000 unique transformations included + $0.50 / 1,000 unique transformations / month |
| Images Stored | $5 / 100,000 images stored / month |
| Images Delivered | $1 / 100,000 images delivered / month |

If you optimize an image stored outside of Images, then you will be billed only for Images Transformed.

Alternatively, Images Stored and Images Delivered apply only to images that are stored in your Images bucket. When you optimize an image that is stored in Images, then this counts toward Images Delivered â€” not Images Transformed.

## Metrics

### Images Transformed

A unique transformation is a request to transform an original image based on a set of [supported parameters](/images/transform-images/transform-via-url/#options). This metric is used only when optimizing images that are stored outside of Images.

For example, if you transform `thumbnail.jpg` as 100x100, then this counts as 1 unique transformation. If you transform the same `thumbnail.jpg` as 200x200, then this counts as a separate unique transformation.

You are billed for the number of unique transformations that are counted during each billing period.

Unique transformations are counted over a 30-day sliding window. For example, if you request `width=100/thumbnail.jpg` on June 30, then this counts once for that billing period. If you request the same transformation on July 1, then this will not count as a billable request, since the same transformation was already requested within the last 30 days.

The `format` parameter counts as only 1 billable transformation, even if multiple copies of an image are served. In other words, if `width=100,format=auto/thumbnail.jpg` is served to some users as AVIF and to others as WebP, then this counts as 1 unique transformation instead of 2.

#### Example

A retail website has 1,000 original product images that get served in 5 different sizes each month. This results in 5,000 unique transformations â€” or a cost of $2.50 per month.

### Images Stored 

Storage in Images is available only with an Images Paid plan. You can purchase storage in increments of $5 for every 100,000 images stored per month.

You can create predefined variants to specify how an image should be resized, such as `thumbnail` as 100x100 and `hero` as 1600x500.

Only uploaded images count toward Images Stored; defining variants will not impact your storage limit.

### Images Delivered 

For images that are stored in Images, you will incur $1 for every 100,000 images delivered per month. This metric does not include transformed images that are stored in remote sources.

Every image requested by the browser counts as 1 billable request.

#### Example

A retail website has a product page that uses Images to serve 10 images. If the page was visited 10,000 times this month, then this results in 100,000 images delivered â€” or $1.00 in billable usage.

---

# Demos and architectures

URL: https://developers.cloudflare.com/kv/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use KV within your existing application and architecture.

## Demo applications

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for KV.

<ExternalResources type="apps" products={["KV"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use KV:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["KV"]} />

---

# Get started

URL: https://developers.cloudflare.com/kv/get-started/

import { Render, PackageManagers, Steps, FileTree, Details, Tabs, TabItem, WranglerConfig } from "~/components";

Workers KV provides low-latency, high-throughput global storage to your [Cloudflare Workers](/workers/) applications. Workers KV is ideal for storing user configuration data, routing data, A/B testing configurations and authentication tokens, and is well suited for read-heavy workloads.

This guide instructs you through:

- Creating a KV namespace.
- Writing key-value pairs to your KV namespace from a Cloudflare Worker.
- Reading key-value pairs from a KV namespace.

You can perform these tasks through the CLI or through the Cloudflare dashboard.

## Prerequisites

<Render file="prereqs" product="workers" />

## 1. Create a Worker project

:::note[New to Workers?]

Refer to [How Workers works](/workers/reference/how-workers-works/) to learn about the Workers serverless execution model works. Go to the [Workers Get started guide](/workers/get-started/guide/) to set up your first Worker.

:::

<Tabs syncKey = 'CLIvsDash'> <TabItem label='CLI'>

Create a new Worker to read and write to your KV namespace.

<Steps>
1. Create a new project named `kv-tutorial` by running:

    <PackageManagers type="create" pkg="cloudflare@latest" args={"kv-tutorial"} />

    <Render
    	file="c3-post-run-steps"
    	product="workers"
    	params={{
    	category: "hello-world",
    	type: "Hello World Worker",
    	lang: "TypeScript",
    	}}
    />

    This creates a new `kv-tutorial` directory, illustrated below.

    <FileTree>
    	- kv-tutorial/
    		- node_modules/
    		- test/
    		- src
    			- **index.ts**
    		- package-lock.json
    		- package.json
    		- testconfig.json
    		- vitest.config.mts
    		- worker-configuration.d.ts
    		- **wrangler.jsonc**
    </FileTree>

    Your new `kv-tutorial` directory includes:

    - A `"Hello World"` [Worker](/workers/get-started/guide/#3-write-code) in `index.ts`.
    - A [`wrangler.jsonc`](/workers/wrangler/configuration/) configuration file. `wrangler.jsonc` is how your `kv-tutorial` Worker accesses your kv database.

2. Change into the directory you just created for your Worker project:

	```sh
	cd kv-tutorial
	```

	:::note

	If you are familiar with Cloudflare Workers, or initializing projects in a Continuous Integration (CI) environment, initialize a new project non-interactively by setting `CI=true` as an environmental variable when running `create cloudflare@latest`.

	For example: `CI=true npm create cloudflare@latest kv-tutorial --type=simple --git --ts --deploy=false` creates a basic "Hello World" project ready to build on.

	:::

</Steps>
</TabItem> <TabItem label = 'Dashboard'>

<Steps>
1. Log in to your Cloudflare dashboard and select your account.
2. Go to [your account > **Workers & Pages** > **Overview**](https://dash.cloudflare.com/?to=/:account/workers-and-pages).
3. Select **Create**.
4. Select **Create Worker**.
5. Name your Worker. For this tutorial, name your Worker `kv-tutorial`.
6. Select **Deploy**.
</Steps>
</TabItem>
</Tabs>

## 2. Create a KV namespace

A [KV namespace](/kv/concepts/kv-namespaces/) is a key-value database replicated to Cloudflareâ€™s global network.

<Tabs syncKey = 'CLIvsDash'> <TabItem label='CLI'>

[Wrangler](/workers/wrangler/) allows you to put, list, get, and delete entries within your KV namespace.

:::note

KV operations are scoped to your account.
:::

To create a KV namespace via Wrangler:

<Steps>
1. Open your terminal and run the following command:

	```sh
	npx wrangler kv namespace create <BINDING_NAME>
	```

	The `npx wrangler kv namespace create <BINDING_NAME>` subcommand takes a new binding name as its argument. A KV namespace is created using a concatenation of your Workerâ€™s name (from your Wrangler file) and the binding name you provide. A `BINDING_ID` is randomly generated for you.

	For this tutorial, use the binding name `BINDING_NAME`.

	```sh
	npx wrangler kv namespace create BINDING_NAME
	```

	```sh output
	ðŸŒ€  Creating namespace with title kv-tutorial-BINDING_NAME
	âœ¨  Success!
	Add the following to your configuration file:
	[[kv_namespaces]]
	binding = "BINDING_NAME"
	id = "<BINDING_ID>"
	```

</Steps>

</TabItem><TabItem label = 'Dashboard'>

<Steps>
1. Go to [**Storage & Databases** > **KV**](https://dash.cloudflare.com/?to=/:account/workers/kv/namespaces).
2. Select **Create a namespace**.
3. Enter a name for your namespace. For this tutorial, use `kv_tutorial_namespace`.
4. Select **Add**.
</Steps>

:::note

:::

</TabItem></Tabs>

## 3. Bind your Worker to your KV namespace

You must create a binding to connect your Worker with your KV namespace. [Bindings](/workers/runtime-apis/bindings/) allow your Workers to access resources, like KV, on the Cloudflare developer platform.

To bind your KV namespace to your Worker:

<Tabs syncKey='CLIvsDash'><TabItem label='CLI'>
<Steps>
1. In your Wrangler file, add the following with the values generated in your terminal from [step 2](/kv/get-started/#2-create-a-kv-namespace):

	<WranglerConfig>

	```toml
	[[kv_namespaces]]
	binding = "<BINDING_NAME>"
	id = "<BINDING_ID>"
	```

	</WranglerConfig>

   Binding names do not need to correspond to the namespace you created. Binding names are only a reference. Specifically:

	- The value (string) you set for `<BINDING_NAME>` is used to reference this KV namespace in your Worker. For this tutorial, this should be `BINDING_NAME`.
	- The binding must be [a valid JavaScript variable name](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Grammar_and_types#variables). For example, `binding = "MY_KV"` or `binding = "routingConfig"` would both be valid names for the binding.
	- Your binding is available in your Worker at `env.<BINDING_NAME>` from within your Worker.
</Steps>

:::note[Bindings]

A binding is how your Worker interacts with external resources such as [KV namespaces](/kv/concepts/kv-namespaces/). A binding is a runtime variable that the Workers runtime provides to your code. You can declare a variable name in your Wrangler file that binds to these resources at runtime, and interact with them through this variable. Every binding's variable name and behavior is determined by you when deploying the Worker.

Refer to [Environment](/kv/reference/environments/) for more information.

:::

</TabItem><TabItem label='Dashboard'>

<Steps>
1. Go to [**Workers & Pages** > **Overview**](https://dash.cloudflare.com/?to=/:account/workers-and-pages).
2. Select the `kv-tutorial` Worker you created in [step 1](/kv/get-started/#1-create-a-worker-project).
3. Select **Settings**.
4. Scroll to **Bindings**, then select **Add**.
5. Select **KV namespace**.
6. Name your binding (`BINDING_NAME`) in **Variable name**, then select the KV namespace (`kv_tutorial_namespace`) you created in [step 2](/kv/get-started/#2-create-a-kv-namespace) from the dropdown menu.
7. Select **Deploy** to deploy your binding.
</Steps>
</TabItem></Tabs>

## 4. Interact with your KV namespace

You can interact with your KV namespace via [Wrangler](/workers/wrangler/install-and-update/) or directly from your [Workers](/workers/) application.

### Write a value

<Tabs syncKey='CLIvsDash'><TabItem label = 'CLI'>

To write a value to your empty KV namespace using Wrangler:

<Steps>
1. Run the `wrangler kv key put` subcommand in your terminal, and input your key and value respectively. `<KEY>` and `<VALUE>` are values of your choice.

	```sh
	npx wrangler kv key put --binding=<BINDING_NAME> "<KEY>" "<VALUE>"
	```

	```sh output
	Writing the value "<VALUE>" to key "<KEY>" on namespace <BINDING_ID>.
	```
</Steps>

Instead of using `--binding`, you can also use `--namespace-id` to specify which KV namespace should receive the operation:

```sh
npx wrangler kv key put --namespace-id=<BINDING_ID> "<KEY>" "<VALUE>"
```

```sh output
Writing the value "<VALUE>" to key "<KEY>" on namespace <BINDING_ID>.
```

To create a key and a value in local mode, add the `--local` flag at the end of the command:

```sh
npx wrangler kv key put --namespace-id=xxxxxxxxxxxxxxxx "<KEY>" "<VALUE>" --local
```

</TabItem><TabItem label = 'Dashboard'>
<Steps>
1. Go to [**Storage & Databases** > **KV**](https://dash.cloudflare.com/?to=/:account/workers/kv/namespaces).
2. Select the KV namespace you created (`kv_tutorial_namespace`), then select **View**.
3. Select **KV Pairs**.
4. Enter a `<KEY>` of your choice.
5. Enter a `<VALUE>` of your choice.
6. Select **Add entry**.
</Steps>

</TabItem> </Tabs>

### Get a value

<Tabs syncKey='CLIvsDash'><TabItem label = 'CLI'>

To access the value using Wrangler:

<Steps>
1. Run the `wrangler kv key get` subcommand in your terminal, and input your key value:

    ```sh
    # Replace [OPTIONS] with --binding or --namespace-id
    npx wrangler kv key get [OPTIONS] "<KEY>"
    ```

    A KV namespace can be specified in two ways:

    <Details header="With a `--binding`">

	```sh
	npx wrangler kv key get --binding=<BINDING_NAME> "<KEY>"
	```

    </Details>

    <Details header ="With a `--namespace-id`">

    ```sh
    npx wrangler kv key get --namespace-id=<YOUR_ID> "<KEY>"
    ```
    </Details>

</Steps>

You can add a `--preview` flag to interact with a preview namespace instead of a production namespace.

:::caution

Exactly **one** of `--binding` or `--namespace-id` is required.
:::

:::note
To view the value directly within the terminal, add `--text`
:::

Refer to the [`kv bulk` documentation](/kv/reference/kv-commands/#kv-bulk) to write a file of multiple key-value pairs to a given KV namespace.

</TabItem><TabItem label='Dashboard'>

You can view key-value pairs directly from the dashboard.
<Steps>
1. Go to your account > **Storage & Databases** > **KV**.
2. Go to the KV namespace you created (`kv_tutorial_namespace`), then select **View**.
3. Select **KV Pairs**.
</Steps>
</TabItem></Tabs>

## 5. Access your KV namespace from your Worker

<Tabs syncKey = 'CLIvsDash'><TabItem label = 'CLI'>

:::note

When using [`wrangler dev`](/workers/wrangler/commands/#dev) to develop locally, Wrangler defaults to using a local version of KV to avoid interfering with any of your live production data in KV. This means that reading keys that you have not written locally returns null.

To have `wrangler dev` connect to your Workers KV namespace running on Cloudflare's global network, call `wrangler dev --remote` instead. This uses the `preview_id` of the KV binding configuration in the Wrangler file. Refer to the [KV binding docs](/kv/concepts/kv-bindings/#use-kv-bindings-when-developing-locally) for more information.

:::

<Steps>
1. In your Worker script, add your KV binding in the `Env` interface:

	```ts
	interface Env {
		BINDING_NAME: KVNamespace;
		// ... other binding types
	}
	```

2. Use the `put()` method on `BINDING_NAME` to create a new key-value pair, or to update the value for a particular key:

	```ts
	let value = await env.BINDING_NAME.put(key, value);
	```

3. Use the KV `get()` method to fetch the data you stored in your KV database:

	```ts
	let value = await env.BINDING_NAME.get("KEY");
	```
</Steps>

Your Worker code should look like this:

```ts
export interface Env {
	BINDING_NAME: KVNamespace;
}

export default {
	async fetch(request, env, ctx): Promise<Response> {
		try {
			await env.BINDING_NAME.put("KEY", "VALUE");
			const value = await env.BINDING_NAME.get("KEY");
			if (value === null) {
				return new Response("Value not found", { status: 404 });
			}
			return new Response(value);
		} catch (err) {
			// In a production application, you could instead choose to retry your KV
			// read or fall back to a default code path.
			console.error(`KV returned error: ${err}`);
			return new Response(err, { status: 500 });
		}
	},
} satisfies ExportedHandler<Env>;
```

The code above:

1. Writes a key to `BINDING_NAME` using KV's `put()` method.
2. Reads the same key using KV's `get()` method, and returns an error if the key is null (or in case the key is not set, or does not exist).
3. Uses JavaScript's [`try...catch`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/try...catch) exception handling to catch potential errors. When writing or reading from any service, such as Workers KV or external APIs using `fetch()`, you should expect to handle exceptions explicitly.

To run your project locally, enter the following command within your project directory:

```sh
npx wrangler dev
```

When you run `wrangler dev`, Wrangler provides a URL (usually a `localhost:8787`) to review your Worker. The browser prints your value when you visit the URL provided by Wrangler.

The browser should simply return the `VALUE` corresponding to the `KEY` you have specified with the `get()` method.

</TabItem><TabItem label = 'Dashboard'>

<Steps>
1. Go to **Workers & Pages** > **Overview**.
2. Go to the `kv-tutorial` Worker you created.
3. Select **Edit Code**.
4. Clear the contents of the `workers.js` file, then paste the following code.

	```js
	export default {
		async fetch(request, env, ctx) {
			try {
				await env.BINDING_NAME.put("KEY", "VALUE");
				const value = await env.BINDING_NAME.get("KEY");
				if (value === null) {
					return new Response("Value not found", { status: 404 });
				}
				return new Response(value);
			} catch (err) {
				// In a production application, you could instead choose to retry your KV
				// read or fall back to a default code path.
				console.error(`KV returned error: ${err}`);
				return new Response(err.toString(), { status: 500 });
			}
		},
	};
	```

	The code above:

	1. Writes a key to `BINDING_NAME` using KV's `put()` method.
	2. Reads the same key using KV's `get()` method, and returns an error if the key is null (or in case the key is not set, or does not exist).
	3. Uses JavaScript's [`try...catch`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/try...catch) exception handling to catch potential errors. When writing or reading from any service, such as Workers KV or external APIs using `fetch()`, you should expect to handle exceptions explicitly.

	The browser should simply return the `VALUE` corresponding to the `KEY` you have specified with the `get()` method.
2. Select **Save**.

</Steps>

</TabItem></Tabs>

## 6. Deploy your KV

<Tabs syncKey = 'CLIvsDash'><TabItem label = 'CLI'>

<Steps>
1. Run the following command to deploy KV to Cloudflare's global network:

    ```sh
    npx wrangler deploy
    ```

2. Visit the URL for your newly created Workers KV application.

   For example, if the URL of your new Worker is `kv-tutorial.<YOUR_SUBDOMAIN>.workers.dev`, accessing `https://kv-tutorial.<YOUR_SUBDOMAIN>.workers.dev/` sends a request to your Worker that writes (and reads) from Workers KV.

</Steps>

</TabItem><TabItem label='Dashboard'>
<Steps>

1. Go to **Workers & Pages** > **Overview**.
2. Select your `kv-tutorial` Worker.
3. Select **Deployments**.
4. From the **Version History** table, select **Deploy version**.
5. From the **Deploy version** page, select **Deploy**.

	This deploys the latest version of the Worker code to production.

</Steps>
</TabItem></Tabs>

## Summary

By finishing this tutorial, you have:

1. Created a KV namespace
2. Created a Worker that writes and reads from that namespace
3. Deployed your project globally.

## Next steps

If you have any feature requests or notice any bugs, share your feedback directly with the Cloudflare team by joining the [Cloudflare Developers community on Discord](https://discord.cloudflare.com).

- Learn more about the [KV API](/kv/api/).
- Understand how to use [Environments](/kv/reference/environments/) with Workers KV.
- Read the Wrangler [`kv` command documentation](/kv/reference/kv-commands/).

---

# Glossary

URL: https://developers.cloudflare.com/kv/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's KV documentation.

<Glossary product="kv" />

---

# Cloudflare Workers KV

URL: https://developers.cloudflare.com/kv/

import {
	CardGrid,
	Description,
	Feature,
	LinkTitleCard,
	Plan,
	RelatedProduct,
	Tabs,
	TabItem,
	LinkButton,
} from "~/components";

<Description>

Create a global, low-latency, key-value data storage.

</Description>

<Plan type="workers-all" />

Workers KV is a data storage that allows you to store and retrieve data globally. With Workers KV, you can build dynamic and performant APIs and websites that support high read volumes with low latency.

For example, you can use Workers KV for:

- Caching API responses.
- Storing user configurations / preferences.
- Storing user authentication details.

Access your Workers KV namespace from Cloudflare Workers using [Workers Bindings](/workers/runtime-apis/bindings/) or from your external application using the REST API:

<Tabs>
<TabItem label="Workers Binding API">
	<Tabs>
	<TabItem label="index.ts">
```ts
export default {
	async fetch(request, env, ctx): Promise<Response> {
		// write a key-value pair
		await env.KV_BINDING.put('KEY', 'VALUE');

    	// read a key-value pair
    	const value = await env.KV_BINDING.get('KEY');

    	// list all key-value pairs
    	const allKeys = await env.KV_BINDING.list();

    	// delete a key-value pair
    	await env.KV_BINDING.delete('KEY');

    	// return a Workers response
    	return new Response(
    		JSON.stringify({
    			value: value,
    			allKeys: allKeys,
    		}),
    	);
    },

} satisfies ExportedHandler<{ KV_BINDING: KVNamespace }>;

    ```
    </TabItem>
    <TabItem label="wrangler.jsonc">

```json
{
	"$schema": "node_modules/wrangler/config-schema.json",
	"name": "WORKER-NAME",
	"main": "src/index.ts",
	"compatibility_date": "2025-02-04",
	"observability": {
		"enabled": true
	},

	"kv_namespaces": [
		{
			"binding": "KV_BINDING",
			"id": "<YOUR_BINDING_ID>"
		}
	]
}
```

    </TabItem>
    </Tabs>

See the full [Workers KV binding API reference](/kv/api/read-key-value-pairs/).

</TabItem>
<TabItem label="REST API">

    <Tabs>
    	<TabItem label="cURL">
    	```
    	curl https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/storage/kv/namespaces/$NAMESPACE_ID/values/$KEY_NAME \
    			-X PUT \
    			-H 'Content-Type: multipart/form-data' \
    			-H "X-Auth-Email: $CLOUDFLARE_EMAIL" \
    			-H "X-Auth-Key: $CLOUDFLARE_API_KEY" \
    			-d '{
    				"value": "Some Value"
    			}'

    	curl https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/storage/kv/namespaces/$NAMESPACE_ID/values/$KEY_NAME \
    			-H "X-Auth-Email: $CLOUDFLARE_EMAIL" \
    			-H "X-Auth-Key: $CLOUDFLARE_API_KEY"
    	```
    	</TabItem>
    	<TabItem label="TypeScript">
    		```ts
    		const client = new Cloudflare({
    			apiEmail: process.env['CLOUDFLARE_EMAIL'], // This is the default and can be omitted
    			apiKey: process.env['CLOUDFLARE_API_KEY'], // This is the default and can be omitted
    		});

    		const value = await client.kv.namespaces.values.update('<KV_NAMESPACE_ID>', 'KEY', {
    			account_id: '<ACCOUNT_ID>',
    			value: 'VALUE',
    		});

    		const value = await client.kv.namespaces.values.get('<KV_NAMESPACE_ID>', 'KEY', {
    			account_id: '<ACCOUNT_ID>',
    		});

    		const value = await client.kv.namespaces.values.delete('<KV_NAMESPACE_ID>', 'KEY', {
    			account_id: '<ACCOUNT_ID>',
    		});

    		// Automatically fetches more pages as needed.
    		for await (const namespace of client.kv.namespaces.list({ account_id: '<ACCOUNT_ID>' })) {
    			console.log(namespace.id);
    		}

    		```
    	</TabItem>
    </Tabs>

See the full Workers KV [REST API and SDK reference](/api/resources/kv/subresources/namespaces/methods/list/) for details on using REST API from external applications, with pre-generated SDK's for external TypeScript, Python, or Go applications.

</TabItem>
</Tabs>

<LinkButton href="/kv/get-started/">Get started</LinkButton>

---

## Features

<Feature header="Key-value storage" href="/kv/get-started/">
	Learn how Workers KV stores and retrieves data.
</Feature>

<Feature header="Wrangler" href="/workers/wrangler/install-and-update/">

The Workers command-line interface, Wrangler, allows you to [create](/workers/wrangler/commands/#init), [test](/workers/wrangler/commands/#dev), and [deploy](/workers/wrangler/commands/#publish) your Workers projects.

</Feature>

<Feature header="Bindings" href="/kv/concepts/kv-bindings/">

Bindings allow your Workers to interact with resources on the Cloudflare developer platform, including [R2](/r2/), [Durable Objects](/durable-objects/), and [D1](/d1/).

</Feature>

---

## Related products

<RelatedProduct header="R2" href="/r2/" product="r2">

Cloudflare R2 Storage allows developers to store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.

</RelatedProduct>

<RelatedProduct header="Durable Objects" href="/durable-objects/" product="durable-objects">

Cloudflare Durable Objects allows developers to access scalable compute and permanent, consistent storage.

</RelatedProduct>

<RelatedProduct header="D1" href="/d1/" product="d1">

Built on SQLite, D1 is Cloudflareâ€™s first queryable relational database. Create an entire database by importing data or defining your tables and writing your queries within a Worker or through the API.

</RelatedProduct>

---

### More resources

<CardGrid>

<LinkTitleCard title="Limits" href="/kv/platform/limits/" icon="document">
	&#x20;Learn about KV limits.
</LinkTitleCard>

<LinkTitleCard title="Pricing" href="/kv/platform/pricing/" icon="seti:shell">
	&#x20;Learn about KV pricing.
</LinkTitleCard>

<LinkTitleCard
	title="Discord"
	href="https://discord.com/channels/595317990191398933/893253103695065128"
	icon="discord"
>
	&#x20;Ask questions, show off what you are building, and discuss the platform
	with other developers.
</LinkTitleCard>

<LinkTitleCard title="Twitter" href="https://x.com/cloudflaredev" icon="x.com">
	&#x20;Learn about product announcements, new tutorials, and what is new in
	Cloudflare Developer Platform.
</LinkTitleCard>

</CardGrid>

---

# Changelog

URL: https://developers.cloudflare.com/load-balancing/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/waiting-room.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Cloudflare Load Balancing

URL: https://developers.cloudflare.com/load-balancing/

import { CardGrid, Description, Feature, GlossaryTooltip, LinkTitleCard, Plan, RelatedProduct, Render } from "~/components"

<Description>

Maximize application performance and availability
</Description>

<Plan id="traffic.load_balancing.properties.availability.summary" />

Cloudflare Load Balancing distributes traffic across your <GlossaryTooltip term="endpoint" link="/glossary/?term=endpoint">endpoints</GlossaryTooltip>, which reduces endpoint strain and latency and improves the experience for end users.

<Render file="non-contract-enablement" product="fundamentals" />

***

## Features

<Feature header="Load balancing and failover" href="/load-balancing/load-balancers/">

Distribute traffic evenly across your healthy endpoints, automatically failing over when an endpoint is unhealthy or unresponsive.


</Feature>

<Feature header="Active monitoring" href="/load-balancing/monitors/">

Monitor your endpoints at configurable intervals and across multiple data centers to look for specific status codes, response text, and timeouts.


</Feature>

<Feature header="Intelligent routing" href="/load-balancing/understand-basics/traffic-steering/">

Choose whether to distribute requests based on endpoint latency, a visitor's geographic region, or even a visitor's GPS coordinates.


</Feature>

<Feature header="Custom rules" href="/load-balancing/additional-options/load-balancing-rules/">

Customize the behavior of your load balancer based on the characteristics of individual requests.


</Feature>

<Feature header="Analytics" href="/load-balancing/reference/load-balancing-analytics/">

Review comprehensive analytics to evaluate traffic flow, assess endpoint health status, and review changes in pools and pool health over time.


</Feature>

***

## Related products

<RelatedProduct header="Standalone Health Checks" href="/health-checks/" product="health-checks">
Actively monitor whether your origin server is online by sending specific requests at regular intervals.
</RelatedProduct>

<RelatedProduct header="DNS" href="/dns/" product="dns">
Get enterprise-grade authoritative DNS service with the fastest response time, unparalleled redundancy, and advanced security with built-in DDoS mitigation and DNSSEC.
</RelatedProduct>

<RelatedProduct header="Waiting Room" href="/waiting-room/" product="waiting-room">
Route excess users to a custom-branded waiting room, helping preserve customer experience and protect origin servers from being overwhelmed with requests.
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Plans" href="https://www.cloudflare.com/plans/#overview" icon="document">
Compare available Cloudflare plans.
</LinkTitleCard>

<LinkTitleCard title="Pricing" href="https://dash.cloudflare.com/?to=/:account/:zone/traffic/load-balancing/" icon="seti:shell">
Explore pricing options for Load Balancing in the dashboard.
</LinkTitleCard>

<LinkTitleCard title="Reference Architecture" href="/reference-architecture/architectures/load-balancing/" icon="pen">
Learn more about the structure of Cloudflare Load Balancers and their various configurations.
</LinkTitleCard>

<LinkTitleCard title="Learning Paths" href="/learning-paths/" icon="open-book">
Module-based guidance on Cloudflare product workflows.
</LinkTitleCard>

</CardGrid>

---

# Logs Engine

URL: https://developers.cloudflare.com/logs/r2-log-retrieval/

import { Details } from "~/components"

Logs Engine gives you the ability to store your logs in R2 and query them directly.

:::note

Logs Engine is going to be replaced by Log Explorer. For further details, consult the [Log Explorer](/logs/log-explorer/) documentation and to request access, complete the [sign-up form](https://cloudflare.com/lp/log-explorer/).
:::

## Store logs in R2

* Set up a [Logpush to R2](/logs/get-started/enable-destinations/r2/) job.
* Create an [R2 access key](/r2/api/s3/tokens/) with at least R2 read permissions.
* Ensure that you have Logshare read permissions.
* Alternatively, create a Cloudflare API token with the following permissions:
  * Account scope
  * Logs read permissions

## Query logs

You can use the API to query and download your logs by time range or RayID.

## Authentication

The following headers are required for all API calls:

* `X-Auth-Email` - the Cloudflare account email address associated with the domain
* `X-Auth-Key` - the Cloudflare API key

Alternatively, API tokens with Logs edit permissions can also be used for authentication:

* `Authorization: Bearer <API_TOKEN>`

### Required headers

In addition to the required authentication headers mentioned, the following headers are required for the API to access logs stored in your R2 bucket.

`R2-access-key-id` (required) - [R2 Access Key Id](/r2/api/s3/tokens/)
`R2-secret-access-key` (required) - [R2 Secret Access Key](/r2/api/s3/tokens/)

## List files

List relevant R2 objects containing logs matching the provided query parameters, using the endpoint `GET /accounts/{accountId}/logs/list`.

### Query parameters

* `start` (required) string  (TimestampRFC3339) - Start time in RFC 3339 format, for example `start=2022-06-06T16:00:00Z`.

* `end` (required) string  (TimestampRFC3339) - End time in RFC 3339 format, for example `end=2022-06-06T16:00:00Z`.

* `bucket` (required) string (Bucket) - R2 bucket name, for example `bucket=cloudflare-logs`.

* `prefix` string (Prefix) - R2 bucket prefix logs are stored under, for example `prefix=http_requests/example.com/{DATE}`.

* `limit` number (Limit) - Maximum number of results to return, for example `limit=100`.

## Retrieve logs by time range

Stream logs stored in R2 that match the provided query parameters, using the endpoint `GET /accounts/{accountId}/logs/retrieve`.

### Query parameters

* `start` (required) string  (TimestampRFC3339) - Start time in RFC 3339 format, for example `start=2022-06-06T16:00:00Z`

* `end` (required) string  (TimestampRFC3339) - End time in RFC 3339 format, for example `end=2022-06-06T16:00:00Z`

* `bucket` (required) string (Bucket) - R2 bucket name, for example `bucket=cloudflare-logs`

* `prefix` string (Prefix) - R2 bucket prefix logs are stored under, for example `prefix=http_requests/example.com/{DATE}`

### Example API request

```bash
curl --globoff "https://api.cloudflare.com/client/v4/accounts/{account_id}/logs/retrieve?start=2022-06-01T16:00:00Z&end=2022-06-01T16:05:00Z&bucket=cloudflare-logs&prefix=http_requests/example.com/{DATE}" \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "R2-Access-Key-Id: R2_ACCESS_KEY_ID" \
--header "R2-Secret-Access-Key: R2_SECRET_ACCESS_KEY"
```

Results can be piped to a file using `> logs.json`.

Additionally, if you want to receive the raw GZIP bytes without them being transparently decompressed by your client, include the header `--header "Accept-Encoding: gzip"`.

## â€‹Retrieve logs by RayID

Using your logs stored in R2 - the Logpull RayID Lookup feature allows you to query an indexed time range for the presence of an RayID and return the matching result. This feature is available to users with the Logpull RayID Lookup beta subscription.

The ability to look up a RayID is a two-step process. First, a time range needs to be indexed before being able to request a record by the RayID.

Indexes will automatically expire after seven days of no usage.

### Index a time range

Before executing your query, you can specify the time range you would like to index in order to narrow down the scope of the query. In the following example, we index one minute of logs stored in the R2 bucket `"cloudflare-logs"` under the prefix `"http_requests/{DATE}"`.

### Example API request

```bash
curl https://api.cloudflare.com/client/v4/accounts/{account_id}/logs/rayids/index \
--header "Authorization: Bearer <API_TOKEN>" \
--header "R2-Access-Key-Id: <R2_ACCESS_KEY_ID>" \
--header "R2-Secret-Access-Key: <R2_SECRET_ACCESS_KEY>" \
--header "Content-Type: application/json" \
--data-raw '{
  "start": "2022-08-16T20:30:00Z",
  "end": "2022-08-16T20:31:00",
  "bucket": "cloudflare-logs",
  "prefix": "http_requests/example.com/{DATE}"
}'
```

## Lookup a RayID

After indexing a time range, perform a `GET` request with the RayID. If a matching result is found in the indexed time range, the record will be returned. Note that the parameters have moved from the request body and into the URL. The `-g` flag is required to avoid the `{DATE}` parameter from being misinterpreted by cURL.

### Example API request

```bash
curl --globoff "https://api.cloudflare.com/client/v4/accounts/{account_id}/logs/rayids/<RAY_ID>?bucket=cloudflare-logs&prefix=http_requests/example.com/{DATE}" \
--header "Authorization: Bearer <API_TOKEN>" \
--header "R2-Access-Key-Id: <R2_ACCESS_KEY_ID>" \
--header "R2-Secret-Access-Key: <R2_SECRET_ACCESS_KEY>"
```

## Troubleshooting


<Details header="I am getting an error when accessing the API">

* **Error**: Time range returned too many results. Try reducing the time range and try again.

HTTP status code `422` will be returned if the time range between the start and end parameters is too wide. Try querying a shorter time range if you are running into this limit.

* **Error**: Provided token does not have the required features enabled.

Contact your account representative to have the beta Logpull RayID Lookup subscription added to your account.

* **Error**: Time range returned too many results. Try reducing the time range and try again.

High volume zones can produce many log files in R2. Try reducing your start and end time range until you find a duration that works best for your log volume.


</Details>


<Details header="How do I know what time range to index?">

Currently, there is no process to index logs as they arrive. If you have the RayID and know the time the request was made, try indexing the next 5-10 minutes of logs after the request was completed.


</Details>


<Details header="What is the time delay between when an event happens and when I can query for it?">

Logpush delivers logs in batches as soon as possible, generally in less than one minute. After this, logs can be accessed using Logs Engine.


</Details>


<Details header="Does R2 have retention controls?">

R2 does not currently have retention controls in place. You can query back as far as when you created the Logpush job.


</Details>


<Details header="Which datasets is Logs Engine compatible with?">

The retrieval API is compatible with all the datasets we support. The full list is available on the [Log fields](/logs/reference/log-fields/) section.


</Details>

---

# Logpush

URL: https://developers.cloudflare.com/logs/about/

import { FeatureTable, LinkButton } from "~/components"

Logpush delivers logs in batches as quickly as possible, with no minimum batch size, potentially delivering files more than once per minute. This capability enables Cloudflare to provide information almost in real time, in smaller file sizes. Users can configure the batch size [using the API](/logs/get-started/api-configuration/#max-upload-parameters) for improved control in case the log destination has specific requirements.

Logpush does not offer storage or search functionality for logs; its primary aim is to send logs as quickly as they arrive.

## Availability

<FeatureTable id="analytics.logpush" />

:::note


Users without an Enterprise plan can still access [Workers Trace Events Logpush](/workers/observability/logs/logpush/) by subscribing to the [Workers Paid](/workers/platform/pricing/) plan.


:::

## Next steps

 <LinkButton variant="primary" href="/logs/get-started/">Get started</LinkButton>

---

# Changelog

URL: https://developers.cloudflare.com/logs/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/logs.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Edge Log Delivery

URL: https://developers.cloudflare.com/logs/edge-log-delivery/

import { LinkButton } from "~/components"

Edge Log Delivery allows customers to send logs directly from Cloudflareâ€™s edge to their destination of choice. You can configure the maximum interval for your log batches between 30 seconds and five minutes. However, you cannot specify a minimum interval for log batches, meaning that log files may be sent in shorter intervals than the maximum specified. Compared to Logpush, Edge Log Delivery sends logs with lower latency, more frequently, and in smaller batches.

Edge Log Delivery is only available for HTTP request logs. Refer to the [API configuration](/logs/get-started/api-configuration/#kind) page for steps on how to configure a job to use Edge Log Delivery.

 <LinkButton variant="primary" href="/logs/get-started/">Get started</LinkButton>

---

# Cloudflare Logs

URL: https://developers.cloudflare.com/logs/

import { CardGrid, Description, Feature, LinkTitleCard, RelatedProduct } from "~/components"

<Description>

Detailed logs that contain metadata generated by our products.
</Description>

These logs are helpful for debugging, identifying configuration adjustments, and creating analytics, especially when combined with logs from other sources, such as your application server. For information about the types of data Cloudflare collects, refer to [Cloudflare's Types of analytics](/analytics/types-of-analytics/).

***

## Features

<Feature header="Logpush" href="/logs/get-started/">

Push your request or event logs to your cloud service provider using Logpush, which can be configured via the Cloudflare dashboard or API.


</Feature>

<Feature header="Instant Logs" href="/logs/instant-logs/">

View HTTP request logs instantly in the Cloudflare dashboard or the CLI.


</Feature>

<Feature header="Logs Engine" href="/logs/r2-log-retrieval/">

Use Logs Engine to store your logs in R2 and query them directly.


</Feature>

***

## Related products

<RelatedProduct header="Audit Logs" href="/fundamentals/setup/account/account-security/review-audit-logs/" product="fundamentals">
Summarize the history of changes made within your Cloudflare account.
</RelatedProduct>

<RelatedProduct header="Web Analytics" href="/web-analytics/" product="analytics">
Provides privacy-first analytics without changing your DNS or using Cloudflare's proxy.
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Plans" href="https://www.cloudflare.com/products/cloudflare-logs/" icon="document">
Compare available Cloudflare plans
</LinkTitleCard>

<LinkTitleCard title="Pricing" href="https://www.cloudflare.com/plans/#overview" icon="seti:shell">
Explore pricing options for Logs
</LinkTitleCard>

</CardGrid>

---

# Instant Logs

URL: https://developers.cloudflare.com/logs/instant-logs/

import { FeatureTable } from "~/components";

Instant Logs allows Cloudflare customers to access a live stream of the traffic for their domain from the Cloudflare dashboard or from a command-line interface (CLI). Seeing data in real time allows you to investigate an attack, troubleshoot, debug or test out changes made to your network. Instant Logs is lightweight, simple to use and does not require any additional setup.

## Availability

<FeatureTable id="analytics.instant_logs" />

## Instant Logs via Cloudflare Dashboard

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select the [zone](/fundamentals/setup/accounts-and-zones/#zones) you want to use with Instant Logs.

2. Go to **Analytics** > **Instant Logs**.

3. Select **Start streaming**.

4. (optional) Select **Add filter** to narrow down the events to be shown.

Fields supported in our [HTTP requests dataset](/logs/reference/log-fields/zone/http_requests/) can be used when you add filters. Some fields with additional subscriptions required are not supported in the dashboard, you will need to use CLI instead.

Once a filter is selected and the stream has started, only log lines that match the filter criteria will appear. Filters are not applied retroactively to logs already showing in the dashboard.

## Instant Logs via CLI

### 1. Create an Instant Logs Job

Create a session by sending a `POST` request to the Instant Logs job endpoint with the following parameters:

- **Fields** - List any field available in the [HTTP requests dataset](/logs/reference/log-fields/zone/http_requests/).

- **Sample** - The sample parameter is the sample rate of the records set by the client: `"sample": 1` is 100% of records `"sample": 10` is 10% and so on.

:::note

Instant Logs has a maximum data rate supported. For high volume domains, we sample server side as indicated in the `"sampleInterval"` parameter returned in the logs.

:::

- **Filters** - Use filters to drill down into specific events. Filters consist of three parts: key, operator and value.

All supported operators can be found in the [Filters](/logs/reference/filters/) page.

Below we have three examples of filters:

```bash
# Filter when client IP country is not Canada:
"filter": "{\"where\":{\"and\":[{\"key\":\"ClientCountry\",\"operator\":\"neq\",\"value\":\"ca\"}]}}"
```

```bash
# Filter when the status code returned from Cloudflare is either 200 or 201:
"filter": "{\"where\":{\"and\":[{\"key\":\"EdgeResponseStatus\",\"operator\":\"in\",\"value\":\"200,201\"}]}}"
```

```bash
# Filter when the request path contains "/static" and the request hostname is "example.com":
"filter": "{\"where\":{\"and\":[{\"key\":\"ClientRequestPath\",\"operator\":\"contains\",\"value\":\"/static\"}, {\"where\":{\"and\":[{\"key\":\"ClientRequestHost\",\"operator\":\"eq\",\"value\":\"example.com\"}]}}"
```

Example request using cURL:

```bash
curl https://api.cloudflare.com/client/v4/zones/{zone_id}/logpush/edge/jobs \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "fields": "ClientIP,ClientRequestHost,ClientRequestMethod,ClientRequestURI,EdgeEndTimestamp,EdgeResponseBytes,EdgeResponseStatus,EdgeStartTimestamp,RayID",
  "sample": 100,
  "filter": "",
  "kind": "instant-logs"
}'
```

Response:

The response will include a new field called **destination_conf**. The value of this field is your unique WebSocket address that will receive messages from Cloudflare's global network.

```json
{
  "errors": [],
  "messages": [],
  "result": {
    "id": <JOB_ID>,
    "fields": "ClientIP,ClientRequestHost,ClientRequestMethod,ClientRequestURI,EdgeEndTimestamp,EdgeResponseBytes,EdgeResponseStatus,EdgeStartTimestamp,RayID",
    "sample": 100,
    "filter": "",
    "destination_conf": "wss://logs.cloudflare.com/instant-logs/ws/sessions/<SESSION_ID>",
    "kind": "instant-logs"
  },
  "success": true
}
```

### 2. Connect to WebSocket

Using a CLI utility like [Websocat](https://github.com/vi/websocat), you can connect to the WebSocket and start immediately receiving logs.

```bash
websocat wss://logs.cloudflare.com/instant-logs/ws/sessions/<SESSION_ID>
```

Response:

Once connected to the websocket, you will receive messages of line-delimited JSON.

### Angle Grinder

Now that you have a connection to Cloudflare's websocket and are receiving logs from Cloudflare's global network, you can start slicing and dicing the logs. A handy tool for this is [Angle Grinder](https://github.com/rcoh/angle-grinder). Angle Grinder lets you apply filtering, transformations and aggregations on stdin with first class JSON support. For example, to get the number of visitors from each country you can sum the number of events by the `ClientCountry` field.

```bash
awebsocat wss://logs.cloudflare.com/instant-logs/ws/sessions/<SESSION_ID> | agrind '* | json | sum(sampleInterval) by ClientCountry'
```

Response:

| **ClientCountry** | **\_sum** |
| ----------------- | --------- |
| pt                | `4`       |
| fr                | `3`       |
| us                | `3`       |
| om                | `2`       |
| ar                | `1`       |
| au                | `1`       |

## Datasets available

For the moment, `HTTP requests` is the only dataset supported. In the future, we will expand to other datasets.

## Export

You can download the table of logs that appears in the dashboard, in JSON format via the **Export** button.

## Limits

Instant Logs has three limits set in place:

- Only one active Instant Logs session per zone.
- Maximum session time is 60 minutes.
- If you stop listening to a socket for more than five minutes.

If either of these limits are reached, the logs stream will automatically stop.

## Connect with us

If you have any feature requests or notice any bugs, share your feedback directly with us by joining the [Cloudflare Developers community on Discord](https://discord.cloudflare.com).

---

# Log Explorer

URL: https://developers.cloudflare.com/logs/log-explorer/

import { TabItem, Tabs, Render } from "~/components";

Log Explorer enables you to store and explore your Cloudflare logs directly within the Cloudflare Dashboard or API. Giving you visibility into your logs without the need to forward them to third parties. Logs are stored on Cloudflare's global network using the R2 object storage platform and can be queried via the Dashboard or SQL API.

:::note

Log Explorer is currently in beta. To request access, complete the [sign-up form](https://cloudflare.com/lp/log-explorer/).

:::

## Supported datasets

Log Explorer is available at the account and zone level. At the zone level, datasets currently available are:

- [HTTP requests](/logs/reference/log-fields/zone/http_requests/) (`FROM http_requests`)
- [Firewall events](/logs/reference/log-fields/zone/firewall_events/) (`FROM firewall_events`)

At the account level, the datasets available are:

<Render file="log-explorer-account-datasets" product="logs" />

## Authentication

Log Explorer is available to users with the following permissions:

- **Logs Edit**: users with Logs Edit permissions can enable datasets.
- **Logs Read**: users with Logs Read permissions can run queries via the UI or API.

Note that these permissions exist at the account and zone level and you need the appropriate permission level for the datasets you wish to query.

Authentication with the API can be done via an authentication header or API token. Append your API call with either of the following additional parameters.

- **Authentication header**

  - `X-Auth-Email` - the Cloudflare account email address associated with the domain
  - `X-Auth-Key` - the Cloudflare API key

- **API token**

  - `Authorization: Bearer <API_TOKEN>` To create an appropriately scoped API token, refer to [Create API token](/fundamentals/api/get-started/create-token/) documentation. Copy and paste the token into the authorization parameter for your API call.

## Enable Log Explorer

In order for Log Explorer to begin storing logs, you need to enable the desired datasets. You can do this via the dashboard or the API.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account or domain (also known as zone).
2. Go to **Analytics & Logs** > **Log Explorer**.
3. Select **Enable a dataset** to select the datasets you want to query. You can enable more datasets later.

:::note

It may take a few minutes for the logs to become available for querying.
:::

</TabItem> <TabItem label="API">

Use the Log Explorer API to enable Log Explorer for each dataset you wish to store. It may take a few minutes after a log stream is enabled before you can view the logs.

The following curl command is an example for enabling the zone-level dataset `http_requests`, as well as the expected response when the command succeeds.

```bash
curl https://api.cloudflare.com/client/v4/zones/{zone_id}/logs/explorer/datasets \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{
  "dataset": "http_requests"
}'
```

```json
{
  "result": {
    "id": <JOB_ID>,
    "dataset": "http_requests",
    "created_at": "2023-09-25T22:12:31Z",
    "updated_at": "2023-09-25T22:12:31Z"
  },
  "success": true,
  "errors": [],
  "messages": []
}
```

If you would like to enable an account-level dataset, replace `zones/{zone_id}` with `accounts/{account_id}` in the curl command. For example:

```bash
curl https://api.cloudflare.com/client/v4/accounts/{account_id}/logs/explorer/datasets \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{
  "dataset": "access_requests"
}'
```

</TabItem> </Tabs>

## Use Log Explorer

Filtering and viewing your logs is available via the Cloudflare Dashboard or via query API.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account or domain (also known as zone).
2. Go to **Analytics & Logs** > **Log Explorer**.
3. From the dropdown, select the **Dataset** you want to use.
4. Select a **Limit**. That is the maximum number of results to return, for example, 50.
5. Select the **Time period** from which you want to query, for example, the previous 12 hours.
6. Select **Add filter** to create your query. Select a **Field**, an **Operator**, and a **Value**.
7. A query preview is displayed. Select **Use custom SQL**, if you would like to change it.
8. Select **Run query** when you are done. The results are displayed below within the **Query results** section.

:::note

You can also access the Log Explorer dashboard directly from the [Security Analytics dashboard](/waf/analytics/security-analytics/#logs). When doing so, the filters you applied in Security Analytics will automatically carry over to your query in Log Explorer.

:::

</TabItem> <TabItem label="API">

Log Explorer exposes a query endpoint that uses a familiar SQL syntax for querying your logs generated with Cloudflare's network.

For example, to find an HTTP request with a specific [Ray ID](/fundamentals/reference/cloudflare-ray-id/), you can perform the following SQL query.

```bash
curl https://api.cloudflare.com/client/v4/zones/{zone_id}/logs/explorer/query/sql \
--header "Authorization: Bearer <API_TOKEN>" \
--url-query query="SELECT clientRequestScheme, clientRequestHost, clientRequestMethod, edgeResponseStatus, clientRequestUserAgent FROM http_requests WHERE RayID = '806c30a3cec56817' LIMIT 1"
```

Which returns the following HTTP request details:

```json
{
	"result": [
		{
			"clientrequestscheme": "https",
			"clientrequesthost": "example.com",
			"clientrequestmethod": "GET",
			"clientrequestuseragent": "curl/7.88.1",
			"edgeresponsestatus": 200
		}
	],
	"success": true,
	"errors": [],
	"messages": []
}
```

For another example using an account-level dataset, to find Cloudflare Access requests with selected columns from a specific timeframe, you can perform the following SQL query.

```bash
curl https://api.cloudflare.com/client/v4/account/{account_id}/logs/explorer/query/sql \
--header "Authorization: Bearer <API_TOKEN>" \
--url-query query="SELECT CreatedAt, AppDomain, AppUUID, Action, Allowed, Country, RayID, Email, IPAddress, UserUID FROM access_requests WHERE Date >= '2025-02-06' AND Date <= '2025-02-06' AND CreatedAt >= '2025-02-06T12:28:39Z' AND CreatedAt <= '2025-02-06T12:58:39Z'"
```

Which returns the following request details:

```json
{
	"result": [
		{
			"createdat": "2025-01-14T18:17:55Z",
			"appdomain": "example.com",
			"appuuid": "a66b4ab0-ccdf-4d60-a6d0-54a59a827d92",
			"action": "login",
			"allowed": true,
			"country": "us",
			"rayid": "90fbb07c0b316957",
			"email": "user@example.com",
			"ipaddress": "1.2.3.4",
			"useruid": "52859e81-711e-4de0-8b31-283336060e79"
		}
	],
	"success": true,
	"errors": [],
	"messages": []
}
```

</TabItem> </Tabs>

## Output formats

Log Explorer output can be presented in different formats, besides JSON: JSON Lines (also known as NDJSON), CSV, and plain text. The plain text uses ASCII tables similar to psql's `aligned` output mode. Besides the convenience factor of not having to translate the format on the client side, JSON Lines, CSV, and plain text formats have the advantage of being streamed from the API. So for large result sets, you will get a response earlier.

You can choose the output format with an HTTP `Accept` header, as shown in the table below:

| Output format | Content type           | Streaming? |
| ------------- | ---------------------- | ---------- |
| JSON          | `application/json`     | No         |
| JSON Lines    | `application/x-ndjson` | Yes        |
| CSV           | `text/csv`             | Yes        |
| Plain text    | `text/plain`           | Yes        |

## Optimizing your queries

All the tables supported by Log Explorer contain a special column called `date`, which helps to narrow down the amount of data that is scanned to respond to your query, resulting in faster query response times. The value of `date` must be in the form of `YYYY-MM-DD`. For example, to query logs that occurred on October 12, 2023, add the following to your `WHERE` clause: `date = '2023-10-12'`. The column supports the standard operators of `<`, `>`, and `=`.

```bash
curl https://api.cloudflare.com/client/v4/zones/{zone_id}/logs/explorer/query/sql \
--header "Authorization: Bearer <API_TOKEN>" \
--url-query query="SELECT clientRequestMethod, clientRequestPath, clientRequestProtocol FROM http_requests WHERE date = '2023-10-12' LIMIT 500"
```

### Additional query optimization tips

- Narrow your query time frame. Focus on a smaller time window to reduce the volume of data processed. This helps avoid querying excessive amounts of data and speeds up response times.
- Omit `ORDER BY` and `LIMIT` clauses. These clauses can slow down queries, especially when dealing with large datasets. For queries that return a large number of records, reduce the time frame instead of limiting to the newest `N` records from a broader time frame.
- Select only necessary columns. For example, replace `SELECT *` with the list of specific columns you need. You can also use `SELECT RayId` as a first iteration and follow up with a query that filters by the Ray IDs to retrieve additional columns. Additionally, you can use `SELECT COUNT(*)` to probe for time frames with matching records without retrieving the full dataset.

## SQL queries supported

These are the SQL query clauses supported by Log Explorer.

### SELECT

The `SELECT` clause specifies the columns that you want to retrieve from the database tables. It can include individual column names, expressions, or even wildcard characters to select all columns.

### FROM

The `FROM` clause specifies the tables from which to retrieve data. It indicates the source of the data for the `SELECT` statement.

### WHERE

The `WHERE` clause filters the rows returned by a query based on specified conditions. It allows you to specify conditions that must be met for a row to be included in the result set.

### GROUP BY

The `GROUP BY` clause is used to group rows that have the same values into summary rows.

### HAVING

The `HAVING` clause is similar to the `WHERE` clause but is used specifically with the `GROUP BY` clause. It filters groups of rows based on specified conditions after the `GROUP BY` operation has been performed.

### ORDER BY

The `ORDER BY` clause is used to sort the result set by one or more columns in ascending or descending order.

### LIMIT

The `LIMIT` clause is used to constrain the number of rows returned by a query. It is often used in conjunction with the `ORDER BY` clause to retrieve the top N rows or to implement pagination.

:::note

Log Explorer does not support `JOINs`, `DDL`, `DML`, or `EXPLAIN` queries.

:::

## FAQs

### Which fields (or columns) are available for querying?

All fields listed in the datasets [Log Fields](/logs/reference/log-fields/) are viewable in Log Explorer. For filtering, only fields with simple values, such as those of type `bool`, `int`, `float`, or `string` are supported. Fields with key-value pairs are currently not supported. For example, you cannot use the fields `RequestHeaders` and `Cookies` from the HTTP requests dataset in a filter.

### Why does my query not complete or time out?

Log Explorer performs best when query parameters focus on narrower ranges of time. You may experience query timeouts when your query would return a large quantity of data. Consider refining your query to improve performance.

If your query times out with an HTTP status of 524 (Gateway Timeout), consider using one of the [streaming output formats](/logs/log-explorer/#output-formats), such as `application/x-ndjson`.

### Why don't I see any logs in my queries after enabling the dataset?

Log Explorer starts ingesting logs from the moment you enable the dataset. It will not display logs for events that occurred before the dataset was enabled. Make sure that new events have been generated since enabling the dataset, and check again.

### My query returned an error. How do I figure out what went wrong?

We are actively working on improving error codes. If you receive a generic error, check your SQL syntax (if you are using the custom SQL feature), make sure you have included a date and a limit, and that the field you are filtering is not a key-value pair. If the query still fails it is likely timing out. Try refining your filters.

### Where is the data stored?

The data is stored in Cloudflare R2. Each Log Explorer dataset is stored on a per-customer level, similar to Cloudflare D1, ensuring that your data is kept separate from that of other customers. In the future, this single-tenant storage model will provide you with the flexibility to create your own retention policies and decide in which regions you want to store your data.

### Does Log Explorer support Customer Metadata Boundary?

Customer metadata boundary is currently not supported for Log Explorer.

---

# Changelog

URL: https://developers.cloudflare.com/magic-cloud-networking/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/magic-cloud-networking.yaml. Update the file there for new entries to appear here.. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Cloud on-ramps

URL: https://developers.cloudflare.com/magic-cloud-networking/cloud-on-ramps/

import { Render } from "~/components"

<Render file="magic-wan-on-ramps" params={{ mwanAccount: "Have a Magic WAN account. Contact your account team to learn more." }} />

---

# Get started

URL: https://developers.cloudflare.com/magic-cloud-networking/get-started/

To get started with Magic Cloud Networking (beta) you need to give Cloudflare permission to interact with cloud providers on your behalf. You might have multiple provider accounts for the same cloud provider â€” for example, you might want Cloudflare to manage virtual private clouds (VPCs) belonging to two different AWS accounts.

Once Cloudflare has the credentials required to access your cloud environments, Magic Cloud Networking will automatically begin discovering your cloud resources â€” like routing tables and virtual private networks. Discovered resources appear in your [Cloud resource catalog](/magic-cloud-networking/manage-resources/#cloud-resource-catalog).

## Set up Amazon AWS

### 1. Create integration

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. Select **Manage Account** > **Cloud integrations**.
3. Go to **Integrations** and select **Connect integration**.
4. Select **AWS integration**.
5. Give a descriptive name to your integration. Optionally, you can also add a description for it.
6. Select **Create integration**.
7. Select **Authorize access** to start the process of connecting your Cloudflare account to Amazon AWS.

### 2. Create IAM policy

1. Create a [custom access policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create-console.html) in your AWS account, and take note of the name you entered. Then, paste the following [JSON code](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_version.html) in the JSON tab:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:AcceptTransitGatewayPeeringAttachment",
                "ec2:CreateTransitGatewayPeeringAttachment",
                "ec2:DeleteTransitGatewayPeeringAttachment",
                "ec2:DescribeRegions",
                "ec2:DescribeTransitGatewayPeeringAttachments",
                "ec2:RejectTransitGatewayPeeringAttachment",
                "ec2:GetManagedPrefixListEntries",
                "ec2:CreateManagedPrefixList",
                "ec2:ModifyManagedPrefixList",
                "ec2:DeleteManagedPrefixList",
                "ec2:CreateTransitGatewayPrefixListReference",
                "ec2:DeleteTransitGatewayPrefixListReference",
                "ec2:GetTransitGatewayPrefixListReferences",
                "ec2:ModifyTransitGatewayPrefixListReference"
            ],
            "Resource": "*"
        }
    ]
}
```

### 3. Authorize access to your AWS account

1. Create a [custom access policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create-console.html) in your AWS account, and take note of the name you entered.
2. Create an [AWS role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-custom.html) with the following settings:
   - **Trusted entity type**: Select **Custom trust policy**, and paste the custom trust policy returned by the Cloudflare dashboard.
   - **Permissions**: Add the IAM policy created in step 1, along with these AWS-managed policies:
      - `NetworkAdministrator`
      - `AmazonEC2ReadOnlyAccess`
      - `AmazonVPCReadOnlyAccess`
      - `IAMReadOnlyAccess`
   - **ARN**: Copy the ARN for your newly created user.

   :::note
	 The trust policy may take several minutes to propagate to all regions. It usually takes less than four minutes, but can sometimes take longer. You may have to retry the **Authorize** button while the propagation takes effect.
	 :::

3. Select **I authorize Cloudflare to access my AWS account.**
4. Select **Authorize**.

The first discovery of resources may not succeed in all regions, while the IAM policy is propagating. If you do not see all resources after creating your cloud integration, please try re-discovering.

## Set up Microsoft Azure

### 1. Create integration

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. Select **Manage Account** > **Cloud integrations**.
3. Go to **Integrations** and select **Connect integration**.
4. Select **Azure integration**.
5. Give a descriptive name to your integration. Optionally, you can also add a description for it.
6. Select **Create integration**.
7. Select **Authorize access** to start the process of connecting your Cloudflare account to Microsoft Azure.

### 2. Authorize access to your Azure account

1. Select **Create service principal**. You will be redirected to Microsoft's login page.
2. Enter your Azure credentials. If your account does not have administrator privileges, you may need to pass this link to an account that has administrator privileges.
3. The next screen lists Cloudflare required permissions to access your account. Select **Accept**.
4. [Add a role assignment](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal). The purpose of this step is to give the app that you registered in step 1 permission to access your Azure Subscription.
   - In step 3 of the linked document, select the **Contributor** role from the **Privileged administrator roles** tab.
   - In step 4 of the linked document, search for  `mcn-provider-integrations-bot-prod` when selecting members.
5. In **Provide account information**, enter the **Tenant ID** and **Subscription ID** you copied from step 4.
6. In **Verify account ownership**, [add the tags displayed in the Cloudflare dashboard](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources-portal).

   :::note
	 The tags may take several minutes to propagate and become readable to Cloudflare. It usually takes less than four minutes, but can sometimes take longer. You may have to retry the **Authorize** button while the propagation takes effect.
	 :::

7. Select **I authorize Cloudflare to access my AWS account.** If your account does not have administrator privileges, you may need to pass this link to an account that has administrator privileges.
8. Select **Authorize**.

The first discovery of resources may not succeed in all regions, while the IAM policy is propagating. If you do not see all resources after creating your cloud integration, please try re-discovering.

## Set up Google Cloud

### 1. Create integration

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. Select **Manage Account** > **Cloud integrations**.
3. Go to **Integrations** and select **Connect integration**.
4. Select **Google  integration**.
5. Give a descriptive name to your integration. Optionally, you can also add a description for it.
6. Select **Create integration**.
7. Select **Authorize access** to start the process of connecting your Cloudflare account to Google Cloud.

### 2. Authorize access to your Google account

1. Create a new [GCP service account](https://cloud.google.com/iam/docs/service-accounts-create) in your **Google account** > **GCP Console** > **IAM & Admin** > **Service Accounts**.
2. Grant the new service account these roles:
	- `Compute Network Admin`
	- `Compute Viewer`
3. Grant the **Service Account Token Creator** role to our bot account to allow it to impersonate this service account. Learn how to grant a specific role [in Google's documentation](https://cloud.google.com/iam/docs/manage-access-service-accounts#grant-single-role):
   - `mcn-integrations-bot-prod@mcn-gcp-01.iam.gserviceaccount.com`
4. In **Provide the new service account email**, enter the email account that you used to create the GCP service account.
5. In your newly created account, add the [values displayed on the dash](https://www.google.com/url?q=https://cloud.google.com/resource-manager/docs/creating-managing-labels%23create-labels&sa=D&source=docs&ust=1740049107852729&usg=AOvVaw2u7AYwBxhB39ojXesn7tlm).
6. Select **I authorize Cloudflare to access my GCP account.** If your account does not have administrator privileges, you may need to pass this link to an account that has administrator privileges.
7. Select **Authorize**.

You have successfully connected your cloud provider to Magic Cloud Networking. Cloud resources found by Magic Cloud Networking are available in the [Cloud resource catalog](/magic-cloud-networking/manage-resources/#cloud-resource-catalog).

The first discovery of resources may not succeed in all regions, while the IAM policy is propagating. If you do not see all resources after creating your cloud integration, please try re-discovering.

## Next steps

- [Set up Magic WAN](/magic-cloud-networking/cloud-on-ramps/) as an on-ramp to your cloud.
- [Manage resources](/magic-cloud-networking/manage-resources/) found by Magic Cloud Networking.
- [Edit](/magic-cloud-networking/manage-resources/#edit-cloud-integrations) cloud integrations.

---

# Magic Cloud Networking (beta)

URL: https://developers.cloudflare.com/magic-cloud-networking/

import { Description, Feature, Plan, RelatedProduct } from "~/components"

<Description>
Automate resource discovery, and reduce management burden when connecting to your public cloud.
</Description>

<Plan type="enterprise" />

Magic Cloud Networking (beta) simplifies the process of connecting to your public cloud infrastructure, like Amazon Web Services, Google Cloud Platform, or Azure. With Magic Cloud Networking you can automatically discover your cloud resources through Cloudflare's dashboard, and effortlessly combine your cloud networks with your office and data center networks.

Magic Cloud Networking allows you to connect, accelerate, and manage your virtual private clouds securely through Cloudflare. Grow your multi-cloud network architecture in a consistent and scalable manner, centered on Cloudflare's connectivity cloud services.

Magic Cloud Networking is currently in closed beta. If you would like to be among the first customers to try it out, [fill out this form](https://www.cloudflare.com/lp/cloud-networking/).

Learn how to [get started](/magic-cloud-networking/get-started/).

---

## Features

<Feature header="Discover your cloud resources automatically" href="/magic-cloud-networking/get-started/" cta="Use cloud resource discovery">
Discover your cloud resources like virtual private clouds (VPCs), subnets, virtual machines (VMs), route tables, and routes automatically, and easily set up your integrations.
</Feature>

<Feature header="Automatically connect a cloud network" href="/magic-cloud-networking/cloud-on-ramps/" cta="Create cloud on-ramps">
Automatically build VPN tunnels between cloud networks and Magic WAN.
</Feature>

---

## Related products

<RelatedProduct header="Cloudflare Magic WAN" href="/magic-wan/" product="magic-wan">
Secure your network from incoming Internet traffic, and improve performance at Cloudflare scale.
</RelatedProduct>

---

# Manage resources

URL: https://developers.cloudflare.com/magic-cloud-networking/manage-resources/

## Cloud resource catalog

Your cloud environment is built from individual cloud resources, like virtual private clouds (VPCs), subnets, virtual machines (VMs), route tables, and routes. Magic Cloud Networking (beta) discovers all of your cloud resources and stores their configuration and status in the Cloud resource catalog, a read-only snapshot of your cloud environment. Discovery runs regularly in the background, keeping your catalog up to date as your environment changes.

When Magic Cloud Networking creates or modifies configurations in the cloud provider - for example, to deploy a managed IPsec on-ramp for Magic WAN - the created resources will be labeled as **Managed** in the resource catalog.

To browse the resources in your catalog:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. Select **Manage Account** > **Cloud integrations**.
3. Go to **Resource catalog**.
4. Select a resource to inspect its details.

## Edit Cloud integrations

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. Select **Manage Account** > **Cloud integrations**.
3. Select the integration you want to edit > **Edit**.
4. In **Credentials**, select **I would like to update my credentials**, and make the required changes to your credentials.
5. Select **Save** when you are finished.
6. (Optional) You can also select **Delete** to delete your cloud integration.

## Download cloud resource catalog

You can download a JSON file containing metadata and configuration for all your cloud resources:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. Select **Manage Account** > **Cloud integrations**.
3. Go to **Resource catalog**.
4. Select **Download catalog**.

After your browser finishes downloading the ZIP file, expand it to access the JSON with the information about your cloud resources.

---

# Changelog

URL: https://developers.cloudflare.com/magic-firewall/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/magic-firewall.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Reference

URL: https://developers.cloudflare.com/magic-cloud-networking/reference/

Refer to this page for details about how Cloudflare orchestrates VPN connectivity to your cloud networks.

## Cloud on-ramps

### AWS

![Diagram showing how Cloudflare creates on-ramps to AWS](~/assets/images/magic-cloud-networking/reference/aws.png)

When using Magic Cloud Networking (beta) to automatically create on-ramps to your AWS account, you should be aware of the following configuration changes Cloudflare will make on your behalf:

- Cloudflare will create a new customer-managed prefix list named **Magic WAN and Cloudflare Edge** populated with your [Magic WAN Address Space](/magic-cloud-networking/cloud-on-ramps/#magic-wan-address-space) prefixes and the IPv4 address ranges for Cloudflare's global network servers (the latter prefixes are necessary if you use any Cloudflare L7 processing features). You must create rules in your Network Security Groups (NSGs) allowing traffic to/from this prefix list in order to have connectivity with Magic WAN. (The prefix list will contain around 15 to 25 entries, which each count against the rules-per-security-group quota for NSGs in your AWS account.)
- Cloudflare will create a Virtual Private Gateway and attach it to your Virtual Private Cloud (VPC). If an existing Virtual Private Gateway is already attached to the VPC, on-ramp creation will fail.
- Cloudflare will enable route propagation from the Virtual Private Gateway into all route tables in your VPC. This will result in a route for each prefix in your [Magic WAN Address Space](/magic-cloud-networking/cloud-on-ramps/#magic-wan-address-space) targeting the gateway.
- Cloudflare will add a route in Magic WAN for each IPv4 CIDR block in your VPC.

### Azure

![Diagram showing how Cloudflare creates on-ramps to AWS](~/assets/images/magic-cloud-networking/reference/azure.png)

When using Magic Cloud Networking to automatically create on-ramps to your Azure account, you should be aware of the following configuration changes Cloudflare will make on your behalf:

- Cloudflare will create a Virtual Network Gateway in your Virtual Network (VNet). Virtual Network Gateways in Azure require a subnet named `GatewaySubnet`. Cloudflare will create a `GatewaySubnet` if one does not already exist in your VNet. If there is not enough unused address space left in your VNet to create a `/27` subnet  for the`GatewaySubnet`, or if a `GatewaySubnet` exists but does not have enough address space left for a Virtual Network Gateway, on-ramp creation will fail.
- Cloudflare will enable gateway route propagation on all route tables in your VNet. This will result in a route for each prefix in your [Magic WAN Address Space](/magic-cloud-networking/cloud-on-ramps/#magic-wan-address-space) pointing to the gateway. If your VNet has other Virtual Network Gateways, their routes will also propagate to your route tables. If you delete the on-ramp, route propagation will not be disabled.
- By default, Network Security Groups in Azure contain Allow rules for outbound/inbound traffic to/from the `VirtualNetwork` service tag, which includes Virtual Network Gateway address space (and therefore your Magic WAN Address Space). If you do not want all resources in your VNet to be accessible from Magic WAN, add the appropriate Deny rules to your Network Security Groups (NSGs).
- Cloudflare will add a route in Magic WAN for each IPv4 address range in your VNet.

### GCP

![Diagram showing how Cloudflare creates on-ramps to GCP](~/assets/images/magic-cloud-networking/reference/gcp.png)

When using Magic Cloud Networking to automatically create on-ramps to your Google Cloud Platform (GCP) account, you should be aware of the following configuration changes Cloudflare will make on your behalf:

- Cloudflare will reserve a public Internet routable IP address from GCP.
- Cloudflare will create a VPN Gateway and two VPN Tunnels in the region you specify.
- Cloudflare will create routes for each prefix in your [Magic WAN Address Space](/magic-cloud-networking/cloud-on-ramps/#magic-wan-address-space) within your VPC pointing to the VPN Tunnels.
- Cloudflare will add routes in Magic WAN for all subnet CIDR prefixes in your VPC. This includes all regions within the VPC. Traffic bound for a region other than the VPN Gateway's region will be subject to GCP's [Inter-region Pricing](https://cloud.google.com/vpc/network-pricing#inter-region-data-transfer).
- Traffic sent to and from your VM instances through the VPN Tunnels is still subject to VPC firewall rules, and may [require further configuration](https://cloud.google.com/network-connectivity/docs/vpn/how-to/configuring-firewall-rules#firewall_rules).

## Supported resources

Magic Cloud Networking discovers the following resource types.

### AWS

- AWS Customer Gateway
- AWS EC2 Managed Prefix List
- AWS EC2 Transit Gateway
- AWS EC2 Transit Gateway Prefix List
- AWS EC2 Transit Gateway VPC Attachment
- AWS Egress Only Internet Gateway
- AWS Internet Gateway
- AWS Instance
- AWS Network Interface
- AWS Route Table
- AWS Route Table Association
- AWS Security Group
- AWS Subnet
- AWS VPC
- AWS VPC IPv4 CIDR Block Association
- AWS VPC Security Group Egress Rule
- AWS VPC Security Group Ingress Rule
- AWS VPN Connection
- AWS VPN Connection Route
- AWS VPN Gateway

### Azure

- Azure Application Security Group
- Azure Load Balancer
- Azure Load Balancer Backend Address Pool
- Azure Load Balancer NAT Pool
- Azure Load Balancer NAT Rule
- Azure Load Balancer Rule
- Azure Local Network Gateway
- Azure Network Interface
- Azure Network Interface Application Security Group Association
- Azure Network Interface Backend Address Pool Association
- Azure Network Interface Security Group Association
- Azure Network Security Group
- Azure Public IP
- Azure Route
- Azure Route Table
- Azure Subnet
- Azure Subnet Route Table Association
- Azure Virtual Machine
- Azure Virtual Machine Gateway Connection
- Azure Virtual Network
- Azure Virtual Network Gateway
- Azure Virtual Network Gateway Connection

### GCP

- Google Compute Address
- Google Compute Forwarding Rule
- Google Compute Global Address
- Google Compute HA VPN Gateway
- Google Compute Interconnect Attachment
- Google Compute Network
- Google Compute Network Firewall Policy
- Google Compute Network Firewall Policy Rule
- Google Compute Route
- Google Compute Router
- Google Compute Subnetwork
- Google Compute VPN Gateway
- Google Compute VPN Tunnel

---

# Overview

URL: https://developers.cloudflare.com/magic-firewall/

import { Description, Feature, GlossaryTooltip, Plan, RelatedProduct } from "~/components"

<Description>
Protect your cloud infrastructure or network offices with advanced, scalable firewall-as-a-service protection.
</Description>

<Plan type="enterprise" />

Magic Firewall is a <GlossaryTooltip term="firewall-as-a-service">firewall-as-a-service (FWaaS)</GlossaryTooltip> delivered from the Cloudflare global network to protect office networks and cloud infrastructure with advanced, scalable protection. With Magic Firewall, you can apply filter rules on a variety of criteria, such as protocol and <GlossaryTooltip term="data packet">packet</GlossaryTooltip> length, to filter unwanted traffic before it reaches your network.

Magic Firewall uses Wireshark syntax, a domain-specific language (DSL) common in the networking world and the same syntax used across our other products. With this syntax, you can easily craft powerful rules to precisely allow or deny any traffic in or out of your network.

Magic Firewall is available with the purchase of [Magic Transit](/magic-transit/) or [Magic WAN](/magic-wan/).

***

## Features

<Feature header="Intrusion Detection System (IDS)" href="/magic-firewall/how-to/enable-ids/">
Actively monitor for a wide range of known threat signatures in your traffic. 
</Feature>

***

## Related products

<RelatedProduct header="Cloudflare Magic Transit" href="/magic-transit/" product="cloudflare-one">
Secure your network from incoming Internet traffic, and improve performance at Cloudflare scale. 
</RelatedProduct>

<RelatedProduct header="Cloudflare Magic WAN" href="/magic-wan/" product="magic-wan">
Improve security and performance for your entire corporate networking, reducing cost and operation complexity. 
</RelatedProduct>

---

# Plans

URL: https://developers.cloudflare.com/magic-firewall/plans/

import { GlossaryTooltip } from "~/components"

If you are a [Magic Transit](/magic-transit/) or [Magic WAN](/magic-wan/) user, you are automatically provided with a standard list of Magic Firewall features. For additional features available for purchase, refer to the list of advanced features below.

## Standard features

* Filtering rules based on protocol, port, IP addresses, <GlossaryTooltip term="data packet">packet</GlossaryTooltip> length, and <GlossaryTooltip term="bit field matching">bit field match</GlossaryTooltip>.
* Fast propagation of rule changes in less than a minute.
* Single dashboard to manage <GlossaryTooltip term="firewall">firewall</GlossaryTooltip> and network configuration.
* Programmable API for automated deployment and management â€” compatible with infrastructure-as-code platforms like [Terraform](/terraform/).
* Traffic analytics per rule in the dashboard and using the [GraphQL API](/analytics/graphql-api/).
* Integration with [Magic WAN network-as-a-service](/magic-wan/).
* Included DDoS protection with [Magic Transit](/magic-transit/).

## Advanced features

All standard features are included with the purchase of the advanced features below:

* Customizable IP lists.
* Managed threat intelligence IP lists (Anonymizer, Botnet, Malware, Open Proxies, VPNs).
* Geoblocking based on user location by country.
* Block or allow packets based on Autonomous System Number (ASN).
* Packet captures on demand for network troubleshooting.
* [Protocol validation rules](/magic-firewall/about/protocol-validation-rules/) to inspect traffic validity and enforce a positive security model.
* [Secure Web Gateway](/cloudflare-one/policies/gateway/) filtering for outbound Internet traffic (network and HTTP policies). The Secure Web Gateway supports all TCP and UDP ports, as well as traffic sourced from RFC. Gateway will proxy BYOIP traffic to egress via the default Cloudflare IPs or your assigned [dedicated egress IPs](/cloudflare-one/policies/gateway/egress-policies/dedicated-egress-ips/).
* Intrusion Detection System (IDS).

---

# API

URL: https://developers.cloudflare.com/magic-network-monitoring/api/

Use Magic Network Monitoring's API to configure your [account](/api/resources/magic_network_monitoring/subresources/configs/methods/get/) and [rules](/api/resources/magic_network_monitoring/subresources/rules/methods/list/).

## Account configuration

Refer to [account configuration](/api/resources/magic_network_monitoring/subresources/configs/methods/get/) to:

- Create, list, update, and delete Magic Network Monitoring configurations
- List default sampling, router IPs, and rules for an account

## Rules configuration

Refer to [rules configuration](/api/resources/magic_network_monitoring/subresources/rules/methods/list/) to:

- Create, list, update, and delete rules
- Update advertisement for a rule

---

# FAQ

URL: https://developers.cloudflare.com/magic-network-monitoring/faq/

Below you will find answers to our most commonly asked questions. If you cannot find the answer you are looking for, refer to the [community page](https://community.cloudflare.com/) to explore more resources.

## I am getting an "Invalid account settings request body: account name format contains illegal characters or is not supported" error when trying to create a rule.

This probably means that your account name has unsupported characters. Make sure your account name does not have characters like, for example, `&`, `<`, `>`, `"`, `'`, ``` ` ```.

Refer to [Account name](/fundamentals/setup/account/customize-account/account-name/) to learn how to change your account name.

## Can I send NetFlow/sFlow data to Cloudflare in a secure, encrypted way?

Yes. Both enterprise and free customers can send encrypted network flow data to Cloudflare.

Enterprise customers with Magic Transit or Magic WAN are able to send encrypted network flow data via an IPsec tunnel to Cloudflare's network. You can achieve this by:

1. Configuring your [NetFlow](/magic-network-monitoring/routers/netflow-ipfix-config/) or [sFlow](/magic-network-monitoring/routers/sflow-config/) data to be sent to Cloudflare's network for parsing.
2. Directing that network flow data to be sent over [Magic Transit IPsec tunnels](/magic-transit/how-to/configure-tunnels/) or [Magic WAN IPsec tunnels](/magic-wan/configuration/manually/how-to/configure-tunnels/) to Cloudflare's network.

Cloudflare's network will then identify this traffic via the destination IP address/port, and direct the network flow traffic to Magic Network Monitoring for parsing.

Free customers can route their network flow traffic through a device that is running the WARP client. Then, network flow traffic can be forwarded from the WARP enabled device to Cloudflare's network flow endpoints. You can learn more about this in the [Encrypt network flow data tutorial](/magic-network-monitoring/tutorials/encrypt-network-flow-data).

## I have Auto-Advertisement enabled and it was triggered by an attack. Do I have to turn Magic Transit off manually?

Once Auto-Advertisement is activated for an IP prefix that is under attack, the IP prefix will continue to be advertised by Cloudflare even if the attack ends. You will then need to manually disable advertisement for that IP prefix. Refer to [Configure dynamic advertisement](/byoip/concepts/dynamic-advertisement/best-practices/#configure-dynamic-advertisement) to learn how to withdraw your prefixes, and stop using Magic Transit.

## If Auto-Advertisement is enabled, and the threshold has been triggered, will the IP prefix show as advertised in the dashboard?

Yes, the IP prefix will show as advertised under the [IP Prefixes tab](/byoip/concepts/dynamic-advertisement/best-practices/#configure-dynamic-advertisement).

## Does Auto-advertisement also work with BGP-controlled advertisements?

No. Auto-advertisement only works with API-controlled advertisement, not BGP-controlled advertisement.

## In the API, Magic Network Monitoring rules have a `bandwidth_threshold` data field. Does the value for this field refer to bytes transferred or current throughput?

The threshold for a [Magic Network Monitoring (MNM) rule](/api/resources/magic_network_monitoring/subresources/rules/methods/list/) has two values. The first value is `bandwidth_threshold`. This value is a measure of the total ingress throughput on a network at any given moment. The second value is `duration`. The `duration`value refers to the amount of time that `bandwidth_threshold` must be exceeded before an alert is sent to the customer.

For example, you create a MNM rule with the following parameters:

```txt
"bandwidth_threshold": 50000000
"duration": "1m0s"
```

With this rule, your network needs to receive a throughput greater than 50,000,000 bits per second (50 Gigabits per second or Gbps) for 60 seconds. If both of these conditions are met, then MNM will send you an alert.

## My router's public IP address is different from the IP address of my network flow `agent-ip`. I cannot change my network flow `agent-ip`, and I am not seeing my router's traffic in MNM analytics

It is recommended that you set your router's public IP address and network flow `agent-ip` to the same value. However, if you are unable to do this, you can register both your router's public IP and your network flow `agent-ip` in the Magic Network Monitoring (MNM) [router configuration](/magic-network-monitoring/get-started/). This will prevent MNM from blocking network traffic received from any unknown IP addresses, and will show your router's network flow data underneath the router's `agent-ip`.

## What is Magic Network Monitoring's data retention policy for Netflow/sFlow received from customer's routers?

Currently, all data received from a customer's router goes to our servers in the US. If you enable data sovereignty in Europe, you cannot currently use Magic Network Monitoring.

GraphQL analytics is retained for 90 days for enterprise customers. For non-enterprise customers, data retention is seven days. Cloudflare also retains data for six hours in the US, for threshold crossing detection.

---

# Changelog

URL: https://developers.cloudflare.com/magic-network-monitoring/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/magic-network-monitoring.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Get started

URL: https://developers.cloudflare.com/magic-network-monitoring/get-started/

import { GlossaryTooltip } from "~/components";

To begin using Magic Network Monitoring, complete the list of tasks below.

If you are an Enterprise customer, Cloudflare can significantly accelerate the onboarding timeline during active-attack scenarios. Enterprise customers that would like to use Magic Network Monitoring and Magic Transit On Demand together can begin by [configuring Magic Transit](/magic-transit/get-started/).

## 1. Verify NetFlow or sFlow capabilities

Verify your routers are capable of exporting <GlossaryTooltip term="NetFlow">NetFlow</GlossaryTooltip> or <GlossaryTooltip term="sFlow">sFlow</GlossaryTooltip> to an IP address on Cloudflare's network. Magic Network Monitoring supports NetFlow v5, NetFlow v9, IPFIX, and sFlow.

Refer to [Supported routers](/magic-network-monitoring/routers/supported-routers) to view a list of supported routers. The list is not exhaustive.

## 2. Register your router with Cloudflare

When you register your router with Cloudflare, your router links your NetFlow or sFlow data to your Cloudflare account.

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/login), and select your account.
2. Go to **Analytics & Logs** > **Magic Monitoring**.
3. In **Magic Network Monitoring Analytics**, select **Configure Magic Network Monitoring**.
4. Select the **Configure routers** tab.
5. Under **IP Address**, enter your router's public IP address.
6. Under **Default router sampling rate**, enter a value for the <GlossaryTooltip term="sampling">sampling</GlossaryTooltip> rate. The value should match the sampling rate of your NetFlow or sFlow configuration.
7. Select **Next**.

## 3. Configure your router

Next, configure your router to send NetFlow/SFlow data to Cloudflare. For this step, you will also need to have your router's configuration menu open to input the values shown in the Cloudflare dashboard.

Refer to the [NetFlow/IPFIX configuration](/magic-network-monitoring/routers/netflow-ipfix-config/) or the [sFlow configuration guide](/magic-network-monitoring/routers/sflow-config/) for more information.

1. From **Configure routers** in the dashboard, select either **NetFlow Configuration** or **sFlow configuration**.
2. Follow the configuration steps for the selected configuration type.
3. Enter the values shown in your router's configuration.
4. Select **Next**.

## 4. Check your router configuration

After setting up your router, confirm the configuration was successfully set up.

From the **Check routers** page on the dashboard, you can view the status of your routers. Keep in mind that router data takes five to ten minutes to be sent to Cloudflare.

Refer to **Router status description** to confirm whether data is successfully being sent.

When you are done with router configuration, select **Finish onboarding**.

:::note
This will only be visible during the onboarding process. When you are finished onboarding, this page will no longer be visible.
:::

## 5. Create rules

Create rules to analyze data for a specific set of destinations or to implement thresholds. Refer to [Rules](/magic-network-monitoring/rules/) for more information.

---

# Glossary

URL: https://developers.cloudflare.com/magic-network-monitoring/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Magic Network Monitoring documentation.

<Glossary product="magic-network-monitoring" />

---

# Overview

URL: https://developers.cloudflare.com/magic-network-monitoring/

import {
	CardGrid,
	Description,
	Feature,
	GlossaryTooltip,
	LinkTitleCard,
	Plan,
	RelatedProduct,
} from "~/components";

<Description>

Improve your network visibility and detect DDoS attacks based on traffic flows.

</Description>

<Plan type="all" />

Magic Network Monitoring provides visibility into your network traffic by analyzing network <GlossaryTooltip term="flow data">flow data</GlossaryTooltip> sent from a customer's routers. Magic Network Monitoring supports NetFlow v5, NetFlow v9, IPFIX, and sFlow.

Magic Network Monitoring is generally available to everyone with a Cloudflare account by default. You can log in to your Cloudflare dashboard, select your account, then go to **Analytics & Logs** > **Magic Monitoring** to get started.

Everyone can use the [free version](/magic-network-monitoring/magic-network-monitoring-free/) of Magic Network Monitoring in a home network, network lab, or business to get end to end visibility across their network traffic. Potential enterprise customers are encouraged to use the free version to run a proof of concept.

Enterprise customers can use Magic Network Monitoring with [Magic Transit on-demand](/magic-transit/on-demand/) to monitor their network, identify volumetric DDoS attacks, and activate Magic Transit on-demand to mitigate those attacks.

Learn how to [get started](/magic-network-monitoring/get-started/).

---

## Features

<Feature header="Rules" href="/magic-network-monitoring/rules/">

Create rules to configure the threshold for data flowing from your network.

</Feature>

<Feature header="Magic Transit integration" href="/magic-network-monitoring/magic-transit-integration/">

Magic Transit On Demand customers can use Magic Network Monitoring to enable DDoS mitigation when a DDoS attack is detected.

</Feature>

<Feature header="Notifications" href="/magic-network-monitoring/notifications/">

Set up notifications to learn about an attack.

</Feature>

---

## Related products

<RelatedProduct header="Magic Transit" href="/magic-transit/" product="magic-transit">

Mitigates L7, L4, and L3 DDoS attacks with Magic Network Monitoring with Magic Transit on-demand.

</RelatedProduct>

<RelatedProduct header="DDoS Protection" href="/ddos-protection/" product="ddos-protection">

Provides HTTP DDoS attack protection for zones onboarded to Cloudflare in addition to L3 and L4 DDoS attack protection.

</RelatedProduct>

<RelatedProduct header="Cloudflare Network Interconnect" href="/network-interconnect/" product="network-interconnect">

Connects your network infrastructure directly with Cloudflare - rather than using the public Internet - for a more reliable and secure experience.

</RelatedProduct>

## More resources

<CardGrid>

<LinkTitleCard
	title="Discord"
	href="https://discord.com/invite/cloudflaredev"
	icon="discord"
>
	Connect with the Magic Network Monitoring community on Discord to ask
	questions, and share feedback.
</LinkTitleCard>

</CardGrid>

---

# Free version

URL: https://developers.cloudflare.com/magic-network-monitoring/magic-network-monitoring-free/

The free version of Magic Network Monitoring (MNM) is generally available to everyone with a Cloudflare account by default.

All free customers are encouraged to join [Cloudflare's Discord server](https://discord.com/invite/cloudflaredev) to discuss the use cases, configuration, and troubleshooting of Magic Network Monitoring. Free customers are always welcome to provide product feedback and discuss feature requests. The product and engineering team that works on Magic Network Monitoring regularly engages with the Discord community.

You can find the channel by joining the Cloudflare Developers Discord server. Then scroll down in the side bar navigation to the **Cloudflare One** category and select **magic-network-monitoring**.

## Access the free version of MNM

The free version includes all the features of the enterprise version, but has network flow volume and configuration limitations. These limits are detailed below.

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/login), and select your account.
2. Go to **Analytics & Logs** > **Magic Monitoring**.
3. Complete the onboarding wizard to configure Magic Network Monitoring. Refer to [Get started](/magic-network-monitoring/get-started/) for more information.

## Limitations

| Limitation Type                      | Value |
| ------------------------------------ | ----- |
| Number of registered routers         | 10    |
| Number of rules                      | 25    |
| Network flows per second per account | 250   |

---

# Magic Transit integration

URL: https://developers.cloudflare.com/magic-network-monitoring/magic-transit-integration/

import { Render } from "~/components"

<Render file="magic-transit-integration" />

---

# Notifications

URL: https://developers.cloudflare.com/magic-network-monitoring/notifications/

You can receive Magic Network Monitoring notifications via email, PagerDuty, or webhooks.

For more information on receiving notifications via PagerDuty or using webhooks, refer to [Create a notification](/notifications/get-started/#create-a-notification).

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/login), and select your account.
2. Select **Notifications** > **Add**.
3. Locate **Magic Transit** > **Magic Network Monitoring: Volumetric Attack** in the list, and choose **Select** to add a notification.
4. Enter a name and description for the notification.
5. Add an email address for the person who should receive the notification.
6. Select **Create** when you are done.

---

# About

URL: https://developers.cloudflare.com/magic-transit/about/

import { GlossaryTooltip } from "~/components"

Magic Transit is a network security and performance solution that offers DDoS protection, traffic acceleration, and more for on-premise, cloud-hosted, and hybrid networks.

Magic Transit delivers its connectivity, security, and performance benefits by serving as the front door to your IP network. This means it accepts IP <GlossaryTooltip term="data packet">packets</GlossaryTooltip> destined for your network, processes them, and then outputs them to your origin infrastructure.

The Cloudflare network uses [Border Gateway Protocol (BGP)](https://www.cloudflare.com/learning/security/glossary/what-is-bgp/) to announce your company's IP address space, extending your network presence globally, and <GlossaryTooltip term="anycast" link="https://www.cloudflare.com/learning/cdn/glossary/anycast-network/">anycast</GlossaryTooltip> to ingest your traffic. Today, Cloudflare's anycast global network spans [hundreds of cities worldwide](https://www.cloudflare.com/network/).

Once [packets](https://www.cloudflare.com/learning/network-layer/what-is-a-packet/) hit Cloudflare's network, traffic is inspected for attacks, filtered, <GlossaryTooltip term="traffic steering">steered</GlossaryTooltip>, accelerated, and sent onward to your origin. Magic Transit connects to your origin infrastructure using anycast <GlossaryTooltip term="GRE tunnel">Generic Routing Encapsulation (GRE)</GlossaryTooltip> tunnels over the Internet or, with [Cloudflare Network Interconnect (CNI)](/network-interconnect/), via physical or virtual interconnect.

Magic Transit users have two options for their implementation: ingress traffic or ingress and [egress traffic](/magic-transit/reference/egress/). Users with an egress implementation will need to set up <GlossaryTooltip term="policy-based routing">policy-based routing (PBR)</GlossaryTooltip> or ensure default routing on their end forwards traffic to Cloudflare via tunnels.

```mermaid
flowchart LR
accTitle: Magic Transit
accDescr: Diagram showing how Magic Transit protects traffic on the customer's network.

A(DDoS <br> attack)
B[("Cloudflare global <br> anycast network <br> (DDoS protection + <br> network firewall)")]
C[Customer <br> network]
D((User))
E([BGP <br> announcement])

A --x B
E --- B
B-- Anycast <br> GRE tunnel ---C
B-- Cloudflare <br> Network <br> Interconnect ---C
C-- Egress via <br> Direct Server <br> Return --> D
D -- Ingress --> B

style A stroke: red,fill: red,color: white
style B stroke: orange,fill: orange,color: black
style C stroke: #ADD8E6,fill: #ADD8E6,color: black
style D stroke: blue,fill: blue,color: white
linkStyle 0 stroke-width:3px,stroke:red
linkStyle 1 stroke-width:2px,stroke:orange
linkStyle 2 stroke-width:2px,stroke:#ADD8E6
linkStyle 3 stroke-width:2px,stroke:gray
linkStyle 4 stroke-width:3px,stroke:green
```

:::note
Magic Transit is not yet supported on Cloudflare's China Network.
:::

For detailed information on Magic Transit architecture, refer to the [Reference section](/magic-transit/reference/).

---

# Alerts

URL: https://developers.cloudflare.com/magic-transit/alerts/

import { AvailableNotifications, Render } from "~/components"

You can configure alerts to receive notifications for changes in your network.

<AvailableNotifications product="Magic Transit" />

<Render file="get-started" product="notifications" />

---

# Changelog

URL: https://developers.cloudflare.com/magic-transit/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/magic-transit.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Cloudflare IPs

URL: https://developers.cloudflare.com/magic-transit/cloudflare-ips/

import { GlossaryTooltip } from "~/components"

In addition to using Magic Transit with your own IP address, you can use Magic Transit with a Cloudflare-owned IP address. This option is helpful for users who do not meet the `/24` <GlossaryTooltip term="prefix">prefix</GlossaryTooltip> length requirements or who want to protect a smaller network.

To protect your network using a Cloudflare IP address, contact your account manager. After receiving your IP address, you will need to:

- [Create a tunnel](/magic-transit/how-to/configure-tunnels/).
- [Set up static routes](/magic-transit/how-to/configure-static-routes) or [BGP peering](/magic-transit/how-to/bgp-peering/).
- [Configure health checks](/magic-transit/how-to/run-endpoint-health-checks).
- Confirm [tunnel](/magic-transit/how-to/tunnel-health-checks/) and endpoint health checks were properly configured.
- Update your infrastructure at your own pace to use the allocated Cloudflare IPs.

When you use a Cloudflare-owned IP space, you do not need a <GlossaryTooltip term="letter of agency" link="/magic-transit/get-started/#draft-letter-of-agency">Letter of Agency (LOA)</GlossaryTooltip>. You can skip this step from the Prerequisites page.

---

# DDoS protection

URL: https://developers.cloudflare.com/magic-transit/ddos/

Cloudflare DDoS protection automatically detects and mitigates Distributed Denial of Service (DDoS) attacks using its Autonomous Edge. Magic Transit customers have access to additional features, such as:

- [Advanced TCP protection](/ddos-protection/advanced-ddos-systems/overview/advanced-tcp-protection/) (disabled by default)
- [Advanced DNS protection (beta)](/ddos-protection/advanced-ddos-systems/overview/advanced-dns-protection/)

Refer to [Cloudflare DDoS documentation](/ddos-protection/) for more information.

---

## Execution order

The execution order of the different mitigation systems for Magic Transit customers is the following:

1. [DDoS managed rulesets](/ddos-protection/managed-rulesets/)
2. [Advanced TCP Protection](/ddos-protection/advanced-ddos-systems/overview/advanced-tcp-protection/)
3. [Advanced DNS Protection](/ddos-protection/advanced-ddos-systems/overview/advanced-dns-protection/)
4. [Magic Firewall](/magic-firewall/)

---

# Get started

URL: https://developers.cloudflare.com/magic-transit/get-started/

import { GlossaryTooltip, Render } from "~/components";

Before you can begin using Magic Transit, be sure to complete the onboarding steps below. Cloudflare can significantly accelerate this timeline during active-attack scenarios.

## 1. Scope your configuration

The onboarding process begins with an initial kickoff call where Cloudflare engages with your organization to confirm the scope and timeline for setting up Magic Transit.

After your call with Cloudflare, complete the prerequisites step.

## 2. Prerequisites

Before you can begin using Magic Transit, verify that you meet Cloudflare's onboarding requirements.

### Verify router compatibility

Magic Transit relies on <GlossaryTooltip term="anycast">anycast</GlossaryTooltip> tunnels to transmit <GlossaryTooltip term="data packet">packets</GlossaryTooltip> from Cloudflare's global network to your origin network.

The routers at your tunnel endpoints must meet the following requirements to ensure compatibility with Magic Transit.

- Support anycast tunneling.
- Allow configuration of at least one tunnel per Internet service provider (ISP).
- Support <GlossaryTooltip term="maximum segment size (MSS)">maximum segment size (MSS)</GlossaryTooltip> clamping.

### Draft Letter of Agency

Draft a [Letter of Agency (LOA)](/byoip/concepts/loa/) - sometimes referred to as a Letter of Authorization - that identifies the prefixes you want to advertise and gives Cloudflare permission to announce them. The LOA is required by Cloudflare's transit providers so they can accept the routes Cloudflare advertises on your behalf. See this [LOA template](/byoip/concepts/loa/) for an example.

If you are an Internet service provider (ISP) and advertising <GlossaryTooltip term="prefix">prefixes</GlossaryTooltip> on behalf of a customer, an LOA is required for the ISP and for the customer.

If you are using a [Cloudflare IP address](/magic-transit/cloudflare-ips/), you do not need to submit an LOA.

:::note[Note]
The LOA must be a PDF. Transit providers may reject the LOA if it is a JPG or PNG.
:::

### Verify IRR entries

Verify that your Internet Routing Registry (IRR) entries match your corresponding origin autonomous system numbers (ASNs) to ensure Magic Transit routes traffic to the correct autonomous systems (AS). For guidance, refer to [Verify IRR entries](/byoip/concepts/irr-entries/best-practices/#verify-an-irr-entry).

If you are using a Cloudflare IP, you do not need to verify your IRR entries.

#### Optional: RPKI check for prefix validation

You can also use the Resource Public Key Infrastructure (RPKI) as an additional option to validate your prefixes. RPKI is a [security framework method](https://blog.cloudflare.com/rpki/) that associates a route with an autonomous system. It uses cryptography to validate the information before being passed onto the routers.

If you operate a network (ISP, cloud provider, enterprise, etc.), using RPKI ensures that your IP prefixes are correctly recognized. This prevents service disruptions and protects your brand's reputation. Without RPKI, attackers could announce your IP space, misdirect your traffic, and potentially harm your business.

To check your prefixes, you can use [Cloudflare's RPKI Portal](https://rpki.cloudflare.com/?view=validator).

### Set maximum segment size

<Render
	file="prerequisites/maximum-segment-size"
	params={{ productName: "Magic Transit" }}
/>

#### MSS clamping recommendations

##### GRE tunnels as off-ramp

<Render file="mtu-mss/mss-clamping-gre" />

##### IPsec tunnels

<Render file="mtu-mss/mss-clamping-ipsec" />

:::caution[Important]
Refer to your device documentation to check if it sets IPsec MSS clamping automatically. If that is not the case and you are using IPsec inside GRE, you have to set MSS clamp manually.
:::

Refer to [Maximum transmission unit and maximum segment size](/magic-transit/reference/mtu-mss/) for more details.

#### Clear Do not fragment (DF)

If you are unable to set the MSS on your physical interfaces to a value lower than 1500 bytes, you can choose to clear the `do not fragment` bit in the IP header. When this option is enabled, Cloudflare fragments [packets](https://www.cloudflare.com/learning/network-layer/what-is-a-packet/) greater than 1500 bytes, and the packets are reassembled on your infrastructure after decapsulation. In most environments, enabling this option does not have significant impact on traffic throughput.

To enable this option for your network, contact your account team.

Refer to [Maximum transmission unit and maximum segment size](/magic-transit/reference/mtu-mss/) for more details.

### Follow router vendor guidelines

<Render file="prerequisites/router-vendor-guidelines-mss-settings-origin" />

## 3. Configure tunnels

[Configure the tunnels](/magic-transit/how-to/configure-tunnels/) on both the Cloudflare side and your router side to connect to your origin infrastructure.

## 4. Configure static routes or BGP peering

Configure [static routes](/magic-transit/how-to/configure-static-routes/) or [BGP peering](/magic-transit/how-to/bgp-peering/) to route traffic from Cloudflare's global network to your locations.

## 5. Run pre-flight checks

After setting up your tunnels and routes, Cloudflare validates tunnel connectivity, tunnel and endpoint [health checks](/magic-transit/reference/tunnel-health-checks/#tunnel-health-checks), <GlossaryTooltip term="letter of agency">Letter of Agency (LOA)</GlossaryTooltip>, Internet Routing Registry (IRR), and <GlossaryTooltip term="maximum segment size (MSS)">maximum segment size (MSS) configurations</GlossaryTooltip>. Configurations for Cloudflare global network are applied and take around one day to rollout.

## 6. Advertise prefixes

Once pre-flight checks are completed, Cloudflare will unlock your <GlossaryTooltip term="prefix">prefixes</GlossaryTooltip> for you to [advertise via the dashboard, API or BGP](/magic-transit/how-to/advertise-prefixes/) at a time of your choosing. Refer to [Dynamic advertisement best practices](/byoip/concepts/dynamic-advertisement/best-practices/) to learn more about advertising prefixes.

If you are using a Cloudflare IP, you do not need to advertise your prefixes.

:::caution[Important]
You must [put the appropriate MSS clamps](#set-maximum-segment-size) in place before [routing](https://www.cloudflare.com/learning/network-layer/what-is-routing/) changes are made. Failure to apply an MSS clamp can result in dropped packets and hard-to-debug connectivity issues.

Also, when using [Cloudflare Network Interconnect](/magic-transit/network-interconnect/) with Magic Transit you must set the following MSS clamp sizes to accommodate additional overhead:

- GRE tunnels over Classic CNI: 1476 bytes
- Direct CNI / Classic CNI with a maximum transmission unit (MTU) size of 1500 bytes handoff does not require an MSS clamp.

MSS clamps are used to backhaul data from the data center where traffic is ingested (close to the end user) to the facility with the CNI link.
:::

---

# Glossary

URL: https://developers.cloudflare.com/magic-transit/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Magic Transit documentation.

<Glossary product="magic-transit" />

---

# Overview

URL: https://developers.cloudflare.com/magic-transit/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Description>
Secure your network and improve performance at Cloudflare scale.
</Description>

<Plan type="enterprise" />

Magic Transit is a network security and performance solution that offers DDoS protection, traffic acceleration, and more for on-premise, cloud-hosted, and hybrid networks.

Learn how to [get started](/magic-transit/get-started/).

---

## Features

<Feature header="Tunnel health checks" href="/magic-transit/reference/tunnel-health-checks/" cta="Learn about health checks">
Magic Transit sends health check probes to monitor network status and the health of specific network components.
</Feature>

<Feature header="Traffic steering" href="/magic-transit/reference/traffic-steering/" cta="Learn about traffic steering">
Magic Transit steers traffic along tunnel routes based on priorities you define during the onboarding process.
</Feature>

<Feature header="Cloudflare IPs" href="/magic-transit/cloudflare-ips/">
Use Cloudflare-owned IP addresses if you want to protect a smaller network and do not meet Magic Transit's `/24` prefix length requirements.
</Feature>

<Feature header="BGP peering" href="/magic-transit/how-to/bgp-peering/">
Use BGP peering between your networks and Cloudflare to automate the process of adding or removing networks and subnets, and take advantage of failure detection and session recovery features.
</Feature>

---

## Related products

<RelatedProduct header="Magic Firewall" href="/magic-firewall/" product="magic-firewall">
Magic Firewall is a firewall-as-a-service (FWaaS) delivered from the Cloudflare global network to protect office networks and cloud infrastructure with advanced, scalable protection.
</RelatedProduct>

<RelatedProduct header="Cloudflare Network Interconnect" href="/network-interconnect/" product="network-interconnect">
Cloudflare Network Interconnect (CNI) allows you to connect your network infrastructure directly with Cloudflare - rather than using the public Internet - for a more reliable and secure experience.
</RelatedProduct>

<RelatedProduct header="DDoS Protection" href="/ddos-protection/" product="ddos-protection">
Cloudflare DDoS protection secures websites, applications, and entire networks while ensuring the performance of legitimate traffic is not compromised.
</RelatedProduct>

<RelatedProduct header="Bringing Your Own IP (BYOIP)" href="/byoip/" product="byoip">
With Bringing Your Own IPs (BYOIP), Cloudflare announces your IPs in all our locations. Use your IPs with Magic Transit, Spectrum, or CDN services.
</RelatedProduct>

---

## More resources

<CardGrid>

<LinkTitleCard title="Reference Architecture" href="/reference-architecture/architectures/magic-transit/" icon="pen">

Deep dive into the key architecture, functionalities, and network deployment options of Cloudflare Magic Transit.
</LinkTitleCard>

</CardGrid>

---

# Magic Network Monitoring

URL: https://developers.cloudflare.com/magic-transit/magic-network-monitoring/

import { Render } from "~/components"

<Render file="magic-transit-integration" product="magic-network-monitoring" />

---

# Network Interconnect (CNI)

URL: https://developers.cloudflare.com/magic-transit/network-interconnect/

import { Render } from "~/components";

<Render
  file="magic-cni"
  product="network-interconnect"
  params={{
    magic: "Magic Transit",
    productName: "Magic Transit",
    legacyHCLink: "/magic-transit/how-to/configure-tunnels/#legacy-bidirectional-health-checks"
  }}
/>

---

# Magic Transit on-demand

URL: https://developers.cloudflare.com/magic-transit/on-demand/

Customers with access to the Magic Transit on-demand option can [configure prefix advertisement](/byoip/concepts/dynamic-advertisement/best-practices/#configure-dynamic-advertisement) from the **IP Prefixes** page in their Cloudflare account home or via the [Cloudflare API](/api/resources/addressing/subresources/prefixes/subresources/advertisement_status/methods/edit/).

A common workflow is to enable prefix advertisement during an attack so that you can take advantage of Cloudflare protection and then disable advertisement once the incident is resolved. Prefixes using BGP-controlled advertisements cannot be used in conjunction with dynamic advertisement (via dashboard/API). Please specify your preferred on-demand advertisement method during the prefix onboarding.

To ensure smooth operation in general and simplify the advertisement process during an attack scenario, refer to [Dynamic advertisement: Best practices](/byoip/concepts/dynamic-advertisement/best-practices/).

:::note
Magic Transit on-demand cannot be used with Cloudflare leased IPs.
:::

---

# Migration guides

URL: https://developers.cloudflare.com/migration-guides/

import { CardGrid, Description, LinkTitleCard } from "~/components"

<Description>

Content designed to help you migrate from another provider to Cloudflare.

This section is still in work. Expect more guides soon. 
</Description>

***

## More resources

<CardGrid>

<LinkTitleCard title="Reference architectures" href="/reference-architecture/" icon="open-book">

High-level overviews of Cloudflare's network and platform. 
</LinkTitleCard>

<LinkTitleCard title="Cloudflare blog" href="https://blog.cloudflare.com/" icon="open-book">

Read articles and announcements about the latest Cloudflare products and features. 
</LinkTitleCard>

<LinkTitleCard title="Learning Paths" href="/learning-paths/" icon="open-book">

Module-based guidance on Cloudflare product workflows. 
</LinkTitleCard>

</CardGrid>

---

# Changelog

URL: https://developers.cloudflare.com/magic-wan/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/magic-wan.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Get started

URL: https://developers.cloudflare.com/magic-wan/get-started/

import { GlossaryTooltip, Render } from "~/components";

Magic WAN allows you to achieve any-to-any connectivity across branch and retail sites and data centers, with Cloudflare connectivity cloud.

## Before you begin

Magic WAN is an Enterprise-only product. [Contact Cloudflare](https://www.cloudflare.com/magic-wan/) to acquire Magic WAN. If you plan on using Magic WAN Connector to automatically onboard your locations to Cloudflare, you will need to purchase Magic WAN first.

## Set up method

Magic WAN supports an automatic setup and a manual setup. The automatic setup through Magic WAN Connector is the preferred method.

### Automatic set up

Setting up Magic WAN automatically is done through Magic WAN Connector, and is the preferred method. You can choose between the hardware version and the virtual version of the Magic WAN Connector. The virtual version can be installed on your own machines.

If you plan on using Magic WAN Connector, you can skip the prerequisites below, and refer to [Configure with Connector](/magic-wan/configuration/connector/) for more information on how to continue.

### Manual set up

Setting up Magic WAN manually is done through a combination of third-party devices in your premises and the Cloudflare dashboard. To be successful, you need to:

1. Read the [Prerequisites](#prerequisites) below.
2. Follow the steps in [Manual configuration](/magic-wan/configuration/manually/how-to/configure-tunnels/).

## Prerequisites

:::note
The list of prerequisites below is only for customers planning to connect manually to Cloudflare with a third-party device. If you plan on using Magic WAN Connector, skip this section and refer to [Configure with Connector](/magic-wan/configuration/connector/).
:::

### Use compatible tunnel endpoint routers

Magic WAN relies on <GlossaryTooltip term="GRE tunnel" link="/magic-wan/reference/tunnels/">GRE</GlossaryTooltip> and <GlossaryTooltip term="IPsec tunnel" link="/magic-wan/reference/tunnels/#ipsec-tunnels">IPsec tunnels</GlossaryTooltip> to transmit [packets](https://www.cloudflare.com/learning/network-layer/what-is-a-packet/) from Cloudflare's global network to your origin network. To ensure compatibility with Magic WAN, the routers at your tunnel endpoints must:

- Allow configuration of at least one tunnel per Internet service provider (ISP).
- Support <GlossaryTooltip term="maximum segment size (MSS)">maximum segment size (MSS)</GlossaryTooltip> clamping.
- Support the configuration parameters for IPsec mentioned in [IPsec tunnels](/magic-wan/reference/tunnels/#supported-configuration-parameters).

### Set maximum segment size

<Render
	file="prerequisites/maximum-segment-size"
	product="magic-transit"
	params={{ productName: "Magic WAN" }}
/>
#### MSS clamping recommendations

##### GRE tunnels as off-ramp

<Render file="mtu-mss/mss-clamping-gre" />

##### IPsec tunnels

<Render file="mtu-mss/mss-clamping-ipsec" />

:::caution[Important]
Refer to your device documentation to check if it sets IPsec MSS clamping automatically. If that is not the case and you are using IPsec inside GRE, you have to set MSS clamp manually.
:::

Refer to [Maximum transmission unit and maximum segment size](/magic-wan/reference/mtu-mss/) for more details.

### Follow router vendor guidelines

<Render
	file="prerequisites/router-vendor-guidelines-mss-settings-origin"
	product="magic-transit"
/>

---

# Glossary

URL: https://developers.cloudflare.com/magic-wan/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Magic WAN documentation.

<Glossary product="magic-wan" />

---

# Overview

URL: https://developers.cloudflare.com/magic-wan/

import {
	CardGrid,
	Description,
	Feature,
	GlossaryTooltip,
	LinkTitleCard,
	Plan,
	RelatedProduct,
} from "~/components";

<Description>
	Improve security and performance for your entire corporate networking,
	reducing cost and operation complexity.
</Description>

<Plan type="enterprise" />

Magic WAN provides secure, performant connectivity and [routing](https://www.cloudflare.com/learning/network-layer/what-is-routing/) for your entire corporate networking, reducing cost and operation complexity. [Magic Firewall](/magic-firewall/) integrates smoothly with Magic WAN, enabling you to enforce network firewall policies at Cloudflare's global network, across traffic from any entity within your network.

With Magic WAN, you can securely connect any traffic source - data centers, offices, devices, cloud properties - to Cloudflare's network and configure routing policies to get the bits where they need to go, all within one SaaS solution.

Magic WAN supports a variety of <GlossaryTooltip term="on-ramp">on-ramps</GlossaryTooltip> including any device that supports <GlossaryTooltip term="anycast">anycast</GlossaryTooltip> <GlossaryTooltip term="GRE tunnel">GRE</GlossaryTooltip> or <GlossaryTooltip term="IPsec tunnel">IPsec</GlossaryTooltip> tunnels. To make it easier to onboard your cloud properties, you can use [Magic Cloud Networking](/magic-wan/configuration/magic-cloud-networking/), which automates the process of creating on-ramps from your cloud networks.

Refer to [On-ramps](/magic-wan/on-ramps/) for a full list of supported on-ramps.

Learn how to [get started](/magic-wan/get-started/).

---

## Features

<Feature
	header="Connect your network automatically"
	href="/magic-wan/configuration/connector/"
	cta="Use Magic WAN Connector"
>
	Use Magic WAN Connector to automatically connect, steer, and shape any IP
	traffic.
</Feature>

<Feature
	header="Connect your network manually"
	href="/magic-wan/configuration/manually/third-party/"
	cta="Use a third-party device"
>
	Magic WAN is compatible with a host of third-party devices. If you do not have
	Magic WAN Connector, start here to learn how to set up Magic WAN manually.
</Feature>

<Feature
	header="Automatic cloud on-ramps"
	href="/magic-wan/configuration/magic-cloud-networking/"
	cta="Automate your cloud on-ramps"
>
	Automate resource discovery, and reduce management burden when connecting to
	your public cloud.
</Feature>

<Feature
	header="Zero Trust integration"
	href="/magic-wan/zero-trust/"
	cta="Integrate with other Zero Trust products"
>
	Learn how you can use Magic WAN with other Cloudflare Zero Trust products.
</Feature>

<Feature header="BGP peering" href="/magic-wan/configuration/manually/how-to/bgp-peering/">
Use BGP peering between your networks and Cloudflare to automate the process of adding or removing networks and subnets, and take advantage of failure detection and session recovery features.
</Feature>

---

## Related products

<RelatedProduct
	header="Cloudflare Zero Trust"
	href="/cloudflare-one/"
	product="cloudflare-one"
>
	Cloudflare Zero Trust replaces legacy security perimeters with our global
	edge, making the Internet faster and safer for teams around the world.
</RelatedProduct>

<RelatedProduct
	header="Magic Firewall"
	href="/magic-firewall/"
	product="magic-firewall"
>
	Magic Firewall is a firewall-as-a-service (FWaaS) delivered from the
	Cloudflare global network to protect office networks and cloud infrastructure
	with advanced, and scalable protection.
</RelatedProduct>

<RelatedProduct
	header="Magic Cloud Networking"
	href="/magic-cloud-networking/"
	product="magic-cloud-networking"
>
	Simplify and automate cloud resource discovery, and reduce your management
	burden when connecting to your public cloud.
</RelatedProduct>

<RelatedProduct
	header="Cloudflare Network Interconnect"
	href="/network-interconnect/"
	product="network-interconnect"
>
	Cloudflare Network Interconnect (CNI) allows you to connect your network
	infrastructure directly with Cloudflare - rather than using the public
	Internet - for a more reliable and secure experience.
</RelatedProduct>

<RelatedProduct
	header="Load Balancing"
	href="/load-balancing/"
	product="load-balancing"
>
	Cloudflare Load Balancing distributes traffic across your endpoints, which
	reduces endpoint strain and latency and improves the experience for end users.
</RelatedProduct>

---

## More resources

<CardGrid>

<LinkTitleCard title="Reference Architecture" href="/reference-architecture/architectures/sase/" icon="pen">

Deep dive into key architecture and functionalities aspects of Cloudflare One, and learn more about Magic WAN and its structure.

</LinkTitleCard>

</CardGrid>

---

# Load Balancing

URL: https://developers.cloudflare.com/magic-wan/load-balancing/

You can use Cloudflare Load Balancing with Magic WAN to distribute traffic across endpoints, reducing strain and improving the performance of your network. This works through Private Network Load Balancing, which supports both on-ramping and off-ramping traffic to Magic WAN tunnels.

Refer to [Private Network Load Balancing](/load-balancing/private-network/) for more information about the feature and how to set it up. You will need to [enable Load Balancing](/load-balancing/) before you can use this feature.

---

# Network Interconnect (CNI)

URL: https://developers.cloudflare.com/magic-wan/network-interconnect/

import { Render } from "~/components";

<Render
  file="magic-cni"
  product="network-interconnect"
  params={{
    magic: "Magic WAN",
    productName: "Magic WAN",
    legacyHCLink: "/magic-wan/configuration/manually/how-to/configure-tunnels/#legacy-bidirectional-health-checks"
  }}
/>

<Render file="traceroute" />

---

# On-ramps

URL: https://developers.cloudflare.com/magic-wan/on-ramps/

To on-ramp your network traffic to Magic WAN, you can use [Magic WAN Connector](/magic-wan/configuration/connector/), a lightweight software package you can install in corporate network locations to automatically connect, steer, and shape any IP traffic.

You can also use any device that supports [GRE or IPsec](/magic-wan/configuration/manually/third-party/) tunnels with the supported configuration parameters.

Additional compatible on-ramps include:

- [Cloudflare Network Interconnect (CNI)](/magic-wan/network-interconnect/): Connect your network infrastructure directly with Cloudflare - rather than using the public Internet - for a more reliable and secure experience.
- [Cloudflare Tunnel](/magic-wan/zero-trust/cloudflare-tunnel/): Magic WAN can be used together with Cloudflare Tunnel for easy access between your networks and applications.
- [WARP](/cloudflare-one/connections/connect-devices/warp/): Protect corporate devices by securely and privately sending traffic from those devices to Cloudflare's global network, where Cloudflare Gateway can apply advanced web filtering.
- [Magic Cloud Networking](/magic-wan/configuration/magic-cloud-networking/): Automatically create on-ramps from your cloud networks to Magic WAN.
- [Network on-ramp partnerships](https://www.cloudflare.com/network-onramp-partners/): Refer to our [third-party integration tutorials](/magic-wan/configuration/manually/third-party/) for guidance on configuring the most asked for third-party products.

---

# Security filters

URL: https://developers.cloudflare.com/magic-wan/security/

Magic WAN customers have [automatic access to Magic Firewall](/magic-firewall/plans/). Magic Firewall is Cloudflare's firewall-as-a-service solution that allows you to protect your infrastructure. Magic Firewall supports layers three and four of the [OSI model](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/), and enables you to allow or block traffic on a variety of packet characteristics.

Refer to [Magic Firewall](/magic-firewall/) for more information about this product.

As a Magic WAN customer, you can also use Cloudflare Gateway to set up policies to inspect network and HTTP traffic to the Internet or your private network infrastructure. Refer to [Connect to Cloudflare Gateway with Magic WAN](/magic-wan/zero-trust/cloudflare-gateway/) to learn how to filter Magic WAN traffic with Gateway policies.

---

# Get started

URL: https://developers.cloudflare.com/network-error-logging/get-started/

Network Error Logging is available to users on all plan types.

To enable Network Error Logging for Free and Pro zones:

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/).
2. Select **Network** and locate **Network Error Logging Monitoring**.
3. Select the toggle to enable Network Error Logging.

To enable this for Business and Enterprise, contact support or your account team.

---

# How to

URL: https://developers.cloudflare.com/network-error-logging/how-to/

Use NEL reports to view information such as:

* Why a request failed
* The country a request failed from
* The last mile network a request failed from
* The Cloudflare data center the request was most likely meant for

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/).
2. Select **Analytics & Logs** > **Edge Reachability**.

Click a tab under **Reachability summary** to view specific information related to your Origin ASN, Origin, IP, or data center. Hover over a location on the map to view the number of reachable requests.

Under **Reachability by data center**, click a location under Data Centers to filter reachability by a specific location.

To view the log fields available for NEL, refer to [NEL reports](/logs/reference/log-fields/zone/nel_reports/).

---

# Overview

URL: https://developers.cloudflare.com/network-error-logging/

Network Error Logging (NEL) is a browser-based reporting system that allows users to report their own failures to an external endpoint. You can use Network Error Logging to gain insight into connectivity issues on the Internet to learn when and where an incident is happening, who is impacted, and how they are being impacted.

## The last mile

The last mile is the path from a user to the first point of ingress to the resource, whether that be a network like Cloudflare or directly to the origin server. The last mile is important because it is in the critical path of the request for a resource: if the last mile has issues, users cannot connect to their resources. When Network Error Logging is enabled, you can receive alerts about issues in the last mile â€” which are typically difficult to detect â€” to learn what the problem is and how to fix it.

![The last mile diagram, showing the steps involved in delivering data to a customer](~/assets/images/network-error-logging/last-mile.png)

## How NEL affects requests

The Report-To header is present in all requests to Cloudflare zones that have NEL enabled:Â Â 

```txt
report-to: {"group":"cf-nel","max_age":31536000,"endpoints":[{"url":"`[`https://a.nel.cloudflare.com/report?lkg-colo=lhr&lkg-time=1600338181`](https://gcp.nel.cloudflare.com/report?lkg-colo=lhr&lkg-time=1600338181&lkg-ip=1.1.1.1)`"}]}
```

A sample Network Error Report payload appears as follows:

```json
{
  "age": 20,
  "type": "network-error",
  "url": "https://example.com/previous-page",
  "body": {
    "elapsed_time": 18,
    "method": "POST",
    "phase": "dns",
    "protocol": "http/1.1",
    "referrer": "https://example.com/previous-page",
    "sampling_fraction": 1,
    "server_ip": "",
    "status_code": 0,
    "type": "dns.name_not_resolved",
    "url": "https://example-host.com/"
  }
}
```

## Privacy

Cloudflare uses geolocation lookups to extract the following information from every client IP in a NEL report:

* Client ASN
* Client country
* Client metro area

Cloudflare uses internal lookups to associate the above data with a customer domain and customer account.

Cloudflare does not store any PII or user-specific data, and any IP data is only kept for the duration of the request as it is processed. After the report is processed through the NEL pipeline, all PII data is purged from the system.

The client IP address is only stored in volatile memory for the lifetime of the request to Cloudflareâ€™s NEL endpoint (order of milliseconds) and is dropped immediately after the request completes. Cloudflare does not log the client IP address anywhere in the Network Error Logging pipeline. Customers can opt out of having their end users consume the NEL headers by emailing Cloudflare support.

---

# Reference

URL: https://developers.cloudflare.com/network-error-logging/reference/

If a user is able to connect to Cloudflare and the site they connect to has NEL enabled, Cloudflare passes back two headers to the browser indicating that they should report any network failures to an endpoint specified in the headers. The browser will operate as usual, and if something happens that prevents the browser from connecting to the site, the browser will log the failure as a report and send it to the endpoint.

Network Error Logging failures can occur for different reasons which are outlined below.

## Internet Service Provider (ISP) outage

An ISP outage appears to NEL users as failures from one particular last-mile network. By examining NEL data to look at the client autonomous system number (ASN) view, you can see which networks are causing the most impact.

For customers, this scenario appears as an influx of `tcp.timed_out errors`, as well as `tcp.failed`, `h2.protocol_error` and `h3.protocol_error`.

In the event of a last-mile outage, the best course of action is to contact the provider to investigate.

## Transit Flap

Transit flaps look like momentary outages caused by transits re-establishing BGP sessions.

To customers, this will appear as `tcp.timed_out` reports from a variety of ASNs over a short period of time. This could happen for several reasons:

- Maintenance in the transit network necessitated a reset of the session.
- Maintenance or reboots in Cloudflare necessitated a reset of the BGP session.
- Packet loss in the network caused the session to flap.

Heavy packet loss in the network will likely result in a series of flaps over time. Maintenance is typically one impact period that lasts no more than two minutes.

## Infrastructure outage

Infrastructure outages occur at shared peering points, such as Internet exchanges.

These outages appear to customers as an increase in `tcp.timed_out`, `tcp.failed`, and `tcp.aborted reports`. These failures will likely appear across multiple networks for an extended period of time.

Depending on the severity of the report volume, Cloudflare may declare an incident to track remediation. Alternatively, Cloudflare may deactivate peering from these shared points until the issue is resolved.

## Cloudflare outage

Cloudflare outages consist of issues within Cloudflareâ€™s data-center fabric.

These outages appear to customers as an increase in `tcp.timed_out`, `tcp.failed`, and `tcp.aborted` reports and will likely appear across multiple networks for a short period of time.

By pivoting by data center, customers can track the impact across Cloudflare points of presence. Cloudflare-based incidents will always be tracked through a status page, which will indicate whether or not there are issues within the impacted region.

## Provider sending traffic through scrubbing center/blocking traffic

This type of outage manifests as TLS errors, such as `tls.cert.authority_invalid`, `tls.cert.name_invalid,` or others and may also present with `tcp.aborted errors`.

Customers may uncover this behavior by looking at which last-mile ASNs are displaying increased failures, as it will typically be only one.

Customers can seek remediation by contacting the provider that they believe is scrubbing their traffic.

## Certificate issues

Certificate issues are also detectable through NEL. The `TLS.version`, `cipher_mismatch`, or other errors may present across multiple ISPs in multiple Cloudflare locations.

If this is detected in NEL, the issue can be remediated by deploying new certificates or using [Cloudflareâ€™s SSL management suite](/ssl/edge-certificates/advanced-certificate-manager/) to automatically deploy new certificates.

---

# gRPC connections

URL: https://developers.cloudflare.com/network/grpc-connections/

import { FeatureTable, Render } from "~/components"

Cloudflare offers support for gRPC to protect your APIs on any [proxied gRPC endpoints](/dns/proxy-status/). The gRPC protocol helps build efficient APIs with smaller payloads for reduced bandwidth usage, decreased latency, and faster implementations.

## Availability

<FeatureTable id="network.grpc" />

Charges may occur for gRPC traffic over add-on products such as [Argo Smart Routing](/argo-smart-routing/), [WAF](/waf/), and [Bot Management](/bots/).

## Limitations

Running gRPC traffic on Cloudflare is compatible with most Cloudflare products.

However, the following products have limited capabilities with gRPC requests:

* The [Cloudflare WAF](/waf/) will only run for header inspection during the connection phase. WAF Managed Rules will not run on the content of a gRPC stream.
* <Render file="tunnel/grpc-support" product="cloudflare-one" />
* [Cloudflare Access](/cloudflare-one/policies/access/) does not support gRPC traffic sent through Cloudflareâ€™s reverse proxy. gRPC traffic will be ignored by Access if gRPC is enabled in Cloudflare. We recommend disabling gRPC for any sensitive origin servers protected by Access or enabling another means of authenticating gRPC traffic to your origin servers.

## Enable gRPC

### Requirements

* Your gRPC endpoint must listen on port 443.Â 
* Your gRPC endpoint must support TLS and HTTP/2.
* HTTP/2 must be advertised over ALPN.
* Use `application/grpc` or `application/grpc+<message type` (for example: `application/grpc+proto`) for the **Content-Type** header of gRPC requests.
* Make sure that the hostname that hosts your gRPC endpoint:
  * Is set to [proxied](/dns/proxy-status/)
  * Uses at least the [Full SSL/TLS encryption mode](/ssl/origin-configuration/ssl-modes/full/).

### Procedure

To change the **gRPC** setting in the dashboard:

1. Log in to your [Cloudflare account](https://dash.cloudflare.com) and go to a specific domain.
2. Go toÂ **Network**.
3. ForÂ **gRPC**, switch the toggle to **On**.

---

# Overview

URL: https://developers.cloudflare.com/network/

import { Description, Feature, Plan, RelatedProduct } from "~/components"

<Description>
Manage network settings for your website.
</Description>

<Plan type="all" />

***

## Features

<Feature header="IP Geolocation" href="/network/ip-geolocation/">
Include the country code of the visitor location with all requests to your website. 
</Feature>

<Feature header="IPv6 Compatibility" href="/network/ipv6-compatibility/">
Enable IPv6 support and gateway. 
</Feature>

<Feature header="WebSockets" href="/network/websockets/">
Allow WebSockets connections to your origin server. 
</Feature>

***

## Related products

<RelatedProduct header="China Network" href="/china-network/" product="china-network">
The Cloudflare China Network is a package of selected Cloudflareâ€™s performance and security products running on data centers located in mainland China and operated by Cloudflareâ€™s partner JD Cloud. 
</RelatedProduct>

<RelatedProduct header="Managed Transforms" href="/rules/transform/managed-transforms/" product="rules">
Managed Transforms allow you to perform common adjustments to HTTP request and response headers with the click of a button. 
</RelatedProduct>

---

# IP geolocation

URL: https://developers.cloudflare.com/network/ip-geolocation/

import { FeatureTable, TabItem, Tabs } from "~/components";

IP geolocation adds the [`CF-IPCountry` header](/fundamentals/reference/http-headers/#cf-ipcountry) to all requests to your origin server.

Cloudflare automatically updates its IP geolocation database using MaxMind and other data sources, typically twice a week.

## Availability

<FeatureTable id="network.ip_geolocation" />

## Add IP geolocation information

The recommended procedure to enable IP geolocation information is to [enable the **Add visitor location headers** Managed Transform](/rules/transform/managed-transforms/reference/#add-visitor-location-headers). This Managed Transform adds HTTP request headers with location information for the visitor's IP address, such as city, country, continent, longitude, and latitude.

If you only want the request header for the visitor's country, you can enable **IP Geolocation**.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To enable **IP Geolocation** in the dashboard:

1. Log in to your [Cloudflare account](https://dash.cloudflare.com) and go to a specific domain.
2. Go toÂ **Network**.
3. ForÂ **IP Geolocation**, switch the toggle to **On**.

</TabItem> <TabItem label="API">

To enable **IP Geolocation** with the API, send a [`PATCH`](/api/resources/zones/subresources/settings/methods/edit/) request with `ip_geolocation` as the setting name in the URI path, and the `value` parameter set to `"on"`.

</TabItem> </Tabs>

:::note

In order to use this data, you will need to then retrieve it from the [`CF-IPCountry` header](/fundamentals/reference/http-headers/#cf-ipcountry).

:::

---

## Report an incorrect IP location

If you find an incorrect IP location, consider the following:

- If the IP is part of the [current Cloudflare IP ranges](https://www.cloudflare.com/ips/), report to `geoip@cloudflare.com`.
- If the IP is **not** listed within the [current Cloudflare IP ranges](https://www.cloudflare.com/ips/), [report to MaxMind](https://www.maxmind.com/en/geoip-data-correction-request).

---

# IPv6 compatibility

URL: https://developers.cloudflare.com/network/ipv6-compatibility/

import { FeatureTable, TabItem, Tabs } from "~/components";

Cloudflare enables IPv6 on all domains without requiring additional configuration or hardware (as long as your host provides IPv6 support).

When both IPv4 and IPv6 connections are available, Cloudflare prefers IPv4.

## Availability

<FeatureTable id="network.ipv6" />

## Enable IPv6 compatibility

By default, IPv6 compatibility is enabled on your domain and will apply to all domains and subdomains covered by [proxied DNS records](/dns/proxy-status/).

:::note

If you have signed up for Cloudflare through a [Cloudflare hosting partner](http://www.cloudflare.com/hosting-partners) or by use [partial setup](/dns/zone-setups/partial-setup/), IPv6 compatibility does not apply to your apex domain.

:::

## Disable IPv6 compatibility

If your origin web server only understands IPv4 formatted IP addresses, non-Enterprise customers should [enable **Pseudo IPv4**](/network/pseudo-ipv4/).

Alternatively, customers with an Enterprise account can disable Cloudflare's IPv6 compatibility.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To disable **IPv6 Compatibility** in the dashboard:

1. Log in to your [Cloudflare account](https://dash.cloudflare.com) and go to a specific domain.
2. Go toÂ **Network**.
3. ForÂ **IPv6 Compatibility**, switch the toggle to **Off**.

</TabItem> <TabItem label="API">

To disable **IPv6 Compatibility** with the API, send a [`PATCH`](/api/resources/zones/subresources/settings/methods/edit/) request with `ipv6` as the setting name in the URI path, and the `value` parameter set to `"off"`.

</TabItem> </Tabs>

:::note

Even when IPv6 is disabled, domains can still receive IPv6 traffic via theÂ Tor network. To completely disable all IPv6 traffic:

- DisableÂ [**Onion Routing**](/network/onion-routing/).
- Use a [WAF custom rule](/waf/custom-rules/create-dashboard/) to block `0:0:0:0:0:0:0:0/0` using the filter `ip.src in {::/0}`.

:::

---

## Troubleshoot an IPv6 network issue

Provide the following information toÂ [Cloudflare Support](/support/contacting-cloudflare-support/)Â if you experience issues with IPv6 connectivity:

- A [traceroute](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/#perform-a-traceroute)Â that demonstrates the IPv6 connection issues,
- the [Cloudflare data center serving your request](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/#identify-the-cloudflare-data-center-serving-your-request) when the IPv6 issues occur, and
- confirmation of whether [disabling IPv6 Compatibility](#disable-ipv6-compatibility)Â resolves the issue.

---

# Onion Routing and Tor support

URL: https://developers.cloudflare.com/network/onion-routing/

import { FeatureTable, TabItem, Tabs } from "~/components";

Improve the Tor user experience by enabling Onion Routing, which enables Cloudflare to serve your websiteâ€™s content directly through the Tor network and without requiring exit nodes.

## Availability

<FeatureTable id="network.onion_routing" />

## How it works

Due to the behavior of some individuals using the Tor network (spammers, distributors of malware, attackers), the IP addresses of Tor exit nodes mayÂ earn a bad reputation, elevating their Cloudflare threat score.

Our [basic protection level](/waf/tools/security-level/) issues challenges to visitors whose IP address has a high threat score, depending on the level chosen by the Cloudflare customer.

One way to address this threat score is to create [custom WAF rules](/waf/custom-rules/). Cloudflare assigns the two-letter code `T1` for Tor.Â  There's no geographical country associated with these IPs, but this approach lets Cloudflare customers override the default Cloudflare threat score to define the experience for their Tor visitors.Â Cloudflare updates its list of Tor exit node IP addresses every hour.

The other way to improve the Tor user experience is through Onion Routing. This improves Tor browsing as follows:

- Tor users no longer access your site via exit nodes, which can sometimes be compromised, and may snoop on user traffic.
- Human Tor users and bots can be distinguished by our Onion services, such that interactive challenges are only served to malicious bot traffic.

[Tor Browser](https://tb-manual.torproject.org/about/) users receive an [alt-svc header](https://httpwg.org/specs/rfc7838.html#alt-svc) as part of the response to the first request to your website. The browser then creates a Tor Circuit to access this website using the `.onion` TLD service provided by this header.

You should note that the visible domain in the UI remains unchanged, as the host header and the SNI are preserved. However, the underlying connection changes to be routed through Tor, as the [UI denotes on the left of the address bar](https://tb-manual.torproject.org/managing-identities/#managing-identities) with a Tor Circuit. Cloudflare does not provide a certificate for the `.onion` domain provided as part of alt-svc flow, which therefore cannot be accessed via HTTPS.

## Enable Onion Routing

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To enable **Onion Routing** in the dashboard:

1. Log in to your [Cloudflare account](https://dash.cloudflare.com) and go to a specific domain.
2. Go toÂ **Network**.
3. ForÂ **Onion Routing**, switch the toggle to **On**.

</TabItem> <TabItem label="API">

To enable **Onion Routing** with the API, send a [`PATCH`](/api/resources/zones/subresources/settings/methods/edit/) request with `opportunistic_onion` as the setting name in the URI path, and the `value` parameter set to `"on"`.

</TabItem> </Tabs>

---

# Pseudo IPv4

URL: https://developers.cloudflare.com/network/pseudo-ipv4/

import { FeatureTable, Render, TabItem, Tabs } from "~/components";

Cloudflare customers can use **Pseudo IPv4** if their origin web server only understands IPv4 formatted IP addresses (meaning it would not support Cloudflare's default [IPv6 compatibility](/network/ipv6-compatibility/)).

## Availability

<FeatureTable id="network.pseudo_ipv4" />

## Background

Some older origin server analytics and fraud detection software expect IP addresses in an IPv4 format and do not support IPv6 addresses.

**Pseudo IPv4**Â uses theÂ [Class E IPv4 address space](https://tools.ietf.org/html/rfc1112#section-4)Â to provide as many unique IPv4 addresses corresponding to IPv6 addresses as possible.

- Example Class E IPv4 address: `240.16.0.1`
- Example IPv6 address: `2400:cb00:f00d:dead:beef:1111:2222:3333`

:::note

Class E IPv4 addresses are designated as experimental and are not used
for production Internet traffic.
:::

## Configure Pseudo IPv4

Cloudflare offers three options for configuring **Pseudo IPv4**:

- **Off**: Default value.
- **Add Header**: Cloudflare automatically adds the `Cf-Pseudo-IPv4` header with a Class E IPv4 address hashed from the original IPv6 address.
- **Overwrite Headers**:Â <Render file="pseudo-ipv4-warning" product="fundamentals" />

:::note

When using _Overwrite Headers_, no software changes are necessary in
your origin web server.
:::

To configure **Pseudo IPv4**:

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To change the **Pseudo IPv4** setting in the dashboard:

1. Log in to your [Cloudflare account](https://dash.cloudflare.com) and go to a specific domain.
2. Go toÂ **Network**.
3. ForÂ **Pseudo IPv4**, choose your desired setting.

</TabItem> <TabItem label="API">

To change **Pseudo IPv4** with the API, send a [`PATCH`](/api/resources/zones/subresources/settings/methods/edit/) request with `pseudo_ipv4` as the setting name in the URI path, and the `value` parameter set to your desired value: `"off"`, `"add_header"`, or `"overwrite_header"`.

</TabItem> </Tabs>

---

# Understanding the True-Client-IP Header

URL: https://developers.cloudflare.com/network/true-client-ip-header/

import { FeatureTable } from "~/components";

Enabling the True-Client-IP Header adds the [`True-Client-IP` header](/fundamentals/reference/http-headers/#true-client-ip-enterprise-plan-only) to all requests to your origin server, which includes the end user's IP address.

## Availability

<FeatureTable id="network.true_client_ip_header" />

## Add True-Client-IP Header

The recommended procedure to access client IP information is to [enable the **Add "True-Client-IP" header** Managed Transform](/rules/transform/managed-transforms/reference/#add-true-client-ip-header).

:::note
To use this data, you will need to then retrieve it from the [`True-Client-IP` header](/fundamentals/reference/http-headers/#cf-ipcountry).
:::

## Additional resources

For additional guidance on using True-Client-IP Header with Cloudflare, refer to the following resources:

- [Available Managed Transforms](/rules/transform/managed-transforms/reference/#add-true-client-ip-header)
- [Cloudflare HTTP headers](/fundamentals/reference/http-headers/#true-client-ip-enterprise-plan-only)
- [Restoring original visitor IPs](/support/troubleshooting/restoring-visitor-ips/restoring-original-visitor-ips/)

---

# Response Buffering

URL: https://developers.cloudflare.com/network/response-buffering/

import { FeatureTable, TabItem, Tabs } from "~/components";

If your domain sends many small packets, it may be faster to buffer the file and deliver the full payload all at once (instead of streaming it).

## Availability

<FeatureTable id="network.response_buffering" />

## How it works

By default, Cloudflare **streams** traffic data, meaning that each packet is sent as it becomes available. This can improve the delivery of large files. However, this streaming behavior only applies to dynamic traffic; cacheable traffic is buffered and this behavior cannot be changed.

If your domain sends many small packets, however, it might be faster to **buffer** the file. This approach waits to send the full file until all packets are ready, preventing a client browser from having to re-assemble packets.

## Enable Response Buffering

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To enable **Response Buffering** in the dashboard:

1. Log in to your [Cloudflare account](https://dash.cloudflare.com) and go to a specific domain.
2. Go toÂ **Network**.
3. ForÂ **Response Buffering**, switch the toggle to **On**.

</TabItem> <TabItem label="API">

To enable **Response Buffering** with the API, send a [`PATCH`](/api/resources/zones/subresources/settings/methods/edit/) request with `response_buffering` as the setting name in the URI path, and the `value` parameter set to `"on"`.

</TabItem> </Tabs>

---

# WebSockets

URL: https://developers.cloudflare.com/network/websockets/

import { FeatureTable, TabItem, Tabs } from "~/components";

Cloudflare supports proxied WebSocket connections without additional configuration.

## Background

WebSockets are open connections sustained between the client and the origin server. Inside a WebSockets connection, the client and the origin can pass data back and forth without having to reestablish sessions. This makes exchanging data within a WebSockets connection fast. WebSockets are often used for real-time applications such as live chat and gaming.

## Enable WebSockets

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To enable **WebSockets** connections to your origin server in the dashboard:

1. Log in to your [Cloudflare account](https://dash.cloudflare.com) and go to a specific domain.
2. Go toÂ **Network**.
3. ForÂ **WebSockets**, switch the toggle to **On**.

</TabItem> <TabItem label="API">

To enable **WebSockets** connections to your origin server with the API, send a [`PATCH`](/api/resources/zones/subresources/settings/methods/edit/) request with `websockets` as the setting name in the URI path, and the `value` parameter set to `"on"`.

</TabItem> </Tabs>

## Compatibility notes

| Product                                  | Compatible | Notes                                                                                                                                                                                                                                                            |
| ---------------------------------------- | ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [SSL](/ssl/)                             | Yes        |                                                                                                                                                                                                                                                                  |
| [WAF](/waf/)                             | Yes\*      | The initial HTTP 101 request is subject to WAF managed rules, custom rules, rate limiting rules, and other WAF features like any other WebSockets connection. However, once a connection has been established, the WAF does not perform any further inspections. |
| [Workers](/workers/examples/websockets/) | Yes        | You can also use [Durable Objects](/durable-objects/) as an endpoint for WebSocket sessions, giving you full control over messages sent to and from clients.                                                                                                     |

:::note

Cloudflare also supports [ASP.NET SignalR](http://signalr.net/), which helps negotiate which transport method to use (long polling or WebSockets).

:::

## Availability

WebSockets are supported on all Cloudflare plans.

## Requests and Bandwidth measurement

Given the nature of WebSocket connections, you may notice they differ from typical HTTP traffic in terms of requests and bandwidth usage. If you are an Enterprise customer, it is important to consider how Cloudflare measures requests and bandwidth to accurately estimate your usage.

Cloudflare measures a single WebSocket connection in the following way:

- **Requests**: Cloudflare recognizes only the initial upgrade request per WebSocket connection as an HTTP request. Even though you can send a bidirectional message stream through the established WebSocket connection, it will be counted as a single long-lived HTTP request.

- **Bandwidth**: Cloudflare measures data transfer sent from Cloudflare to the client. This typically means that messages from the WebSocket server behind Cloudflare to the WebSocket client are counted towards bandwidth usage.

Once a WebSocket connection is closed, you can view your aggregated WebSocket usage through [Traffic Analytics](/analytics/account-and-zone-analytics/zone-analytics/#traffic), the [GraphQL Analytics API](/analytics/graphql-api/), and [HTTP requests logs](/logs/reference/log-fields/zone/http_requests/).

## Technical note

When Cloudflare releases new code to its global network, we may restart servers, which terminates WebSockets connections.

### Best practices

- Implement a [keepalive](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_servers#pings_and_pongs_the_heartbeat_of_websockets).
- Review and then remove or extend timeout settings on the origin and/or on the client.

### Troubleshooting

Investigating issues with Websocket can be facilitated with client tools like [wscat](https://github.com/websockets/wscat).
Being able to reproduce an issue on a single URL with a minimalistic tool helps narrowing down the issue.

The `EdgeStartTimestamp` and `EdgeStopTimestamp` fields in [HTTP requests logs](/logs/reference/log-fields/zone/http_requests/) represent the duration of the WebSocket connection (they do not represent the initial HTTP connection).

---

# Changelog

URL: https://developers.cloudflare.com/network-interconnect/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/network-interconnect.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# About

URL: https://developers.cloudflare.com/network-interconnect/about/

Cloudflare supports a variety of options to connect your network to Cloudflare:

- Direct CNI, for Magic WAN and Magic Transit.
- Classic CNI, for Magic Transit.
- Cloud CNI, for Magic WAN and Magic Transit.
- Peering via either an Internet exchange, or a private network interconnect (PNI).

Below is a brief overview of the options to help you decide which method best fits your network.

| Network interconnect mode | Use cases | Capabilities |
| --- | --- | --- |
| [Direct CNI](/network-interconnect/express-cni/) | Use for Magic WAN or Magic Transit, interconnecting directly with on-premise locations. | - 10, 100 Gbps <br/> - Available at Direct CNI-capable Cloudflare locations <br/> - No tunnel required <br/> - Set up via dashboard in less than three minutes. |
| [Classic CNI](/network-interconnect/classic-cni/) | Use for Magic Transit, interconnecting directly or via a partner with on-premise locations. | - 10, 100 Gbps <br/> - Available at classic CNI capable Cloudflare data centers <br/> - Overlay GRE tunnel required for egress <br/> - BGP route reflector signaling <br/> - Setup facilitated by account team. |
| [Cloud CNI](/network-interconnect/cloud-cni/) | Use for Magic WAN or Magic Transit, interconnecting directly with virtual networks (VPCs) from public cloud providers. | - Speed depends on cloud provider <br/> - Available at Cloud CNI capable Cloudflare locations <br/> - Setup facilitated by account team. |
| [Peering, or private network interconnect (PNI)](/network-interconnect/pni-and-peering/) | Use for connecting your users to any Cloudflare services, via a more direct, performant and potentially cost-effective network path. | - No requirement to be a customer <br/> - Speed depends on IX or PNI <br/> - Available at PNI capable Cloudflare locations (PNI) and any Internet Exchange (IX) where Cloudflare peers today. <br/> - Setup facilitated by `peering@cloudflare.com` |

---

# Overview

URL: https://developers.cloudflare.com/network-interconnect/

import { Description, Plan, RelatedProduct } from "~/components"

<Description>
Connect your network infrastructure directly to Cloudflare
</Description>

<Plan type="enterprise" />

Cloudflare Network Interconnect (CNI) allows you to connect your network infrastructure directly with Cloudflare â€“ rather than using the public Internet â€“ for a more reliable and secure experience. With CNI, you can bring Cloudflare's full suite of network functions to your physical network edge.

***

## Related products

<RelatedProduct header="Magic Transit" href="/magic-transit/" product="magic-transit">
Magic Transit is a network security and performance solution that offers DDoS protection, traffic acceleration, and more for on-premise, cloud-hosted, and hybrid networks.
</RelatedProduct>

<RelatedProduct header="Magic WAN" href="/magic-wan/" product="magic-wan">
Improve security and performance for your entire corporate network, reducing cost and operation complexity.
</RelatedProduct>

---

# Cloud CNI

URL: https://developers.cloudflare.com/network-interconnect/cloud-cni/

Cloud CNI allows you to connect your virtual private cloud (VPC) virtual networks directly with Cloudflare â€“ for a more reliable and secure experience.

Connecting to Cloudflare directly with a Cloud CNI reduces latency, makes your network more stable by bypassing Internet performance potential bottlenecks, and will often reduce your cloud provider network egress bandwidth charges.

The use case for Cloud CNI is Magic Transit or Magic WAN. If you have publicly routable origins that are behind Magic Transit over a Cloud CNI, then all Cloudflare services that work with public origins will run over the CNI (e.g. Load Balancer, WAF, Cache etc).

## Supported cloud providers

Cloudflare supports interconnect with:

- Amazon Web Services (AWS) DirectConnect
- Google Cloud (GCP) Interconnect
- IBM Cloud Direct Link
- Oracle Cloud Infrastructure (OCI) FastConnect

Azure ExpressRoute support is coming soon.

## Cloud CNI Setup

Enterprise customers using Magic WAN or Magic Transit can get started with Cloud CNI by contacting their account team.

---

# PNI and peering setup

URL: https://developers.cloudflare.com/network-interconnect/pni-and-peering/

Cloudflare has an [open peering policy](https://www.cloudflare.com/peering-policy/). There is no requirement to be a Cloudflare customer for public peering, or a Private Network Interconnect (PNI).

You can use BGP to peer with Cloudflare at any of the Public Internet Exchanges listed on [Cloudflare's PeeringDB page](https://www.peeringdb.com/net/4224). If you have many users accessing websites protected and proxied by Cloudflare, then peering with Cloudflare may help you remove bandwidth from your Internet transit links, and increase performance by reducing latency to Cloudflare.

You may also optionally sign up for the [Cloudflare Peering Portal](https://www.cloudflare.com/partners/peering-portal/), which allows operators of public BGP Autonomous System Number (ASN) listed on PeeringDB to view where their network exchanges traffic with Cloudflare. Finally, if our networks exchange more than 1 Gbps of traffic in a single location, we can move your peering from the Internet Exchange to a Private Network Interconnect (PNI).

If you operate a public autonomous system on the BGP table, and would like to peer with Cloudflare at a Public Internet Exchange listed on [Cloudflare's PeeringDB page](https://www.peeringdb.com/asn/13335), you can request peering by emailing `peering@cloudflare.com`.

## PNI and peering setup

You can use a peering portal, such as PeeringDB, to view and maintain your database of peering locations.

Before you begin using PeeringDB, you must create an account and affiliate with Cloudflare.

### Log in to the PeeringDB portal

:::note
You must first [create PeeringDB portal account](https://www.peeringdb.com/register) before you can log in.
:::

1. [Log in](https://www.peeringdb.com/account/login/?next=/register) to your account.
2. Ensure your email address is affiliated with the ASN you want to request access for.
   Select the **OIDC PeeringDB** and follow the log in process.

If you receive a message stating your account has not been affiliated with an organization, you will need to request affiliation.

![Error message about missing organization affiliation](~/assets/images/network-interconnect/peeringdb-request-affiliation.png)

When your affiliation is approved, **Cloudflare, Inc.** appears under **Existing affiliations** on your profile.

![List of existing affiliations](~/assets/images/network-interconnect/peeringdb-affiliation-approved.png)

### Request peer sessions

Before you can request a peering session for an ASN, you must be an admin for that ASN.

1. On the **ASN** page on [PeeringDB](https://www.peeringdb.com/), select **Users**. Confirm your email address in the **Admin** group.
2. From the **Peering Portal**, locate **Peering Locations**.
3. From the **Sessions** toggle, select **Potential**.

The **Peer** button under **Peering Request** only appears active to admins.

![Admin view of peering locations list](~/assets/images/network-interconnect/peeringdb-admin-view.png)

Non-admin users will be unable to select **Peer** and hovering over the button will display a message about the need for admin access.

![Non-admin view of peering locations list](~/assets/images/network-interconnect/peeringdb-nonadmin-view.png)

---

# Changelog

URL: https://developers.cloudflare.com/notifications/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/notifications.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Notifications

URL: https://developers.cloudflare.com/notifications/

import { Plan } from "~/components"

<Plan type="all" />

Cloudflare Notifications help you stay up to date with your Cloudflare account. Manage your Notifications to define what you want to be warned about and how, be it a denial-of-service attack or an issue with your server.

The available Notification features vary according to your plan:

- Free plans can set up email-based Notifications.
- Business and higher plans can also [access PagerDuty](/notifications/get-started/configure-pagerduty/).
- Professional and higher plans can also [use webhooks](/notifications/get-started/configure-webhooks/).

The notification service only works on the [proxied](/dns/proxy-status/) domains because Cloudflare needs enough information necessary to decide if we need to trigger a notification or not.

:::note

The availability of delivery methods like PagerDuty and webhooks in Free or Professional zones depends on the highest zone plan in your Cloudflare account:

- PagerDuty is available in zones on a Free/Professional plan if your Cloudflare account has at least one zone in a Business plan (or higher).
- Webhooks are available in zones on a Free plan if your Cloudflare account has at least one zone in a Professional plan (or higher). 
:::

---

# Available Notifications

URL: https://developers.cloudflare.com/notifications/notification-available/

import { AvailableNotifications } from "~/components"

Available Notifications depend on your Cloudflare plan. Cloudflare offers a variety of Notifications for our products and services, such as [Billing](/fundamentals/subscriptions-and-billing/), [Denial of Service protection](/ddos-protection/), [Magic Transit](/magic-transit/), and [SSL/TLS](/ssl/).

Depending on your plan, you can also configure webhooks, allowing you to connect your account with external services such as Slack and Google Chat, and PagerDuty to receive Cloudflare Notifications.

## Actions available on receiving a Notification

Each Notification carries different types of information about the status of your Cloudflare account, or the type of action you can take.

Refer to information below to understand what each Notification does and what to do when receiving one.

<AvailableNotifications />

---

# Notification History

URL: https://developers.cloudflare.com/notifications/notification-history/

Notification History is a log of notifications that have been sent to your account via the Notifications service. Information contained in Notification History includes the notification itself, when the notification was sent, and who the notification was sent to.

## How to access Notification History

Currently, customers can access Notification History [via the Cloudflare API](/api/resources/alerting/subresources/history/methods/list/). Using `GET`, customers can retrieve a list of history records for notifications sent to an account. The records are displayed for the last 30 or 90 days, based on the type of plan.

```txt title="Syntax"
GET accounts/{account_id}/alerting/v3/history
```

```bash title="Example"
curl "https://api.cloudflare.com/client/v4/accounts/{account_id}/alerting/v3/history?page=1&per_page=25" \
--header "Authorization: Bearer <API_TOKEN>"
```

## Availability

Notification History is available on all plans. The amount of history clients have access to depends on the type of plan:

- **Free, Pro, and Business**: History from the past 30 days.
- **Enterprise**: History from the past 90 days.

:::note

Customers will not be able to access Notification History from before 2021-10-11. 
:::

---

# Changelog

URL: https://developers.cloudflare.com/page-shield/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/page-shield.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Get started

URL: https://developers.cloudflare.com/page-shield/get-started/

import { Render } from "~/components";

## Activate Page Shield

To enable Page Shield:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Security** > **Page Shield**.
3. Select **Enable Page Shield**.

If you do not have access to Page Shield in the Cloudflare dashboard, check if your user has one of the [necessary roles](/page-shield/reference/roles-and-permissions/).

## Review detected scripts

When you enable Page Shield, it may take a while to get the list of detected scripts in your domain.

Review the scripts displayed in the [**Monitors** dashboard](/page-shield/detection/monitor-connections-scripts/), checking them for signs of malicious activity.

Depending on your plan, you may be able to also review the connections made by scripts in your domain's pages and check them for malicious activity.

## Configure alerts

:::note
Only available to customers on a Business or Enterprise plan.
:::

<Render file="alerts-intro" />

<Render file="alerts-configure" />

## Define policies

:::note
Only available to Enterprise customers with a paid add-on.
:::

[Policies](/page-shield/policies/) define allowed resources on your websites. Create policies to implement a positive security model [^1].

1. [Create a policy](/page-shield/policies/create-dashboard/) with the _Log_ action.

2. After some time, [review the list of policy violations](/page-shield/policies/violations/) to make sure the policy is correct. Update the policy if needed.

3. Change the policy action to _Allow_ to start blocking resources not covered by the policy.

[^1]: A positive security model is one that defines what is allowed and rejects everything else. In contrast, a negative security model defines what will be rejected and accepts the rest.

---

# Cloudflare Page Shield

URL: https://developers.cloudflare.com/page-shield/

import { Description, Feature, FeatureTable, Plan } from "~/components";

<Description>

Ensures the safety and privacy of your website visitors' browsing environment.

</Description>

<Plan type="all" />

Page Shield helps manage resources loaded by your website visitors â€” including scripts, their connections, and cookies â€” and triggers alert notifications when resources change or are considered malicious.

Learn how to [get started](/page-shield/get-started/).

---

## Features

<Feature
	header="Monitors"
	href="/page-shield/detection/monitor-connections-scripts/"
	cta="Monitor resources"
>
	Displays information about loaded scripts in your domain's pages and the
	connections they make.
</Feature>

<Feature
	header="Page attribution"
	href="/page-shield/detection/monitor-connections-scripts/#view-details"
	cta="Find resource occurrences"
>
	Find in which page a resource first appeared, and view a list of the latest
	occurrences of the resource in your pages.
</Feature>

<Feature
	header="Malicious script detection"
	href="/page-shield/detection/review-malicious-scripts/"
	cta="Review malicious scripts"
>
	Detects malicious scripts in your pages using threat intelligence and machine
	learning.
</Feature>

<Feature
	header="Code change detection"
	href="/page-shield/detection/review-changed-scripts/"
	cta="Review changed scripts"
>
	Detects any changes in the scripts loaded in your pages.
</Feature>

<Feature
	header="Alerts"
	href="/page-shield/detection/configure-alerts/"
	cta="Configure alerts"
>
	Receive notifications about newly detected scripts, scripts loaded from
	unknown domains, new scripts considered malicious, or code changes in your
	existing scripts.
</Feature>

<Feature header="Policies" href="/page-shield/policies/">
	Policies define allowed resources on your websites. Use policies to enforce an
	allowlist of resources, effectively blocking resources not included in your
	policies.
</Feature>

## Availability

<FeatureTable id="security.page_shield" />

---

# Troubleshooting

URL: https://developers.cloudflare.com/page-shield/troubleshooting/

import { GlossaryTooltip } from "~/components";

## Why do I not see scripts after I activated Page Shield?

Page Shield does not collect data on every single page view. Instead, it uses a sampling approach to gather information efficiently. This means that domains with lower traffic might take longer to generate initial reports, as these domains need more page views to accumulate enough samples. To speed up the reporting process, it is recommended that you actively generate traffic to your application after [activating Page Shield](/page-shield/get-started/). This will provide Page Shield with more data to work with, leading to faster report generation.

## Why do I see scripts that I do not recognize?

Scripts often reference other scripts outside your application.

But, if you see unexpected scripts on your Script Monitor dashboard, check them for signs of malicious activity.

## Why do I see warnings in my browser's developer tools related to Content Security Policy (CSP)?

Page Shield uses a <GlossaryTooltip term="content security policy (CSP)">Content Security Policy (CSP)</GlossaryTooltip> report-only directive to gather a list of all scripts running on your application.

Some browsers display scripts being reported as warnings in the console pane of their developer tools. For example:

```txt
[Report Only] Refused to execute inline script because it violates
the following Content Security Policy directive: "script-src 'none'".

Either the 'unsafe-inline' keyword, a hash ('sha256-RFWPLDbv2BY+rCkDzsE+0fr8ylGr2R2faWMhq4lfEQc='), or a nonce ('nonce-...')
is required to enable inline execution.
```

You can safely ignore these warnings, since they are related to the reports that Page Shield requires to detect loaded scripts. For more information, refer to [How Page Shield works](/page-shield/how-it-works/).

## Why do I get policy violation reports for a domain I allowlisted?

Policy violations reported via CSP's [report-only directive](/page-shield/reference/csp-header/) do not take into consideration any redirects or redirect HTTP status codes. This is [by design](https://www.w3.org/TR/CSP3/#create-violation-for-request) for security reasons.

Some third-party services you may want to cover in your Page Shield allow policies perform redirects. An example of such a service is Google Ads, which [does not work well with CSP policies](https://support.google.com/adsense/thread/102839782?hl=en&msgid=103611259).

For example, if you add the `adservice.google.com` domain to an allow policy, you could get policy violation reports for this domain due to redirects to a different domain (not present in your allow policy). In this case, the violation report would still mention the original domain, and not the domain of the redirected destination, which can cause some confusion.

To try to solve this issue, add the domain of the redirected destination to your allow policy. You may need to add several domains to your policy due to redirects.

## Do I have access to Page Shield?

Yes, Page Shield is available on all plans. For details on the available features per plan, refer to [Availability](/page-shield/#availability).

### How do I set up Page Shield?

For help setting up Page Shield, refer to our [get started guide](/page-shield/get-started/).

---

# Get started

URL: https://developers.cloudflare.com/privacy-gateway/get-started/

Privacy Gateway implementation consists of three main parts:

1. Application Gateway Server/backend configuration (operated by you).
2. Client configuration (operated by you).
3. Connection to a Privacy Gateway Relay Server (operated by Cloudflare).

***

## Before you begin

Privacy Gateway is currently in closed beta. If you are interested, [contact us](https://www.cloudflare.com/lp/privacy-edge/).

***

## Step 1 - Configure your server

As a customer of the Privacy Gateway, you also need to add server support for OHTTP by implementing an application gateway server. The application gateway is responsible for decrypting incoming requests, forwarding the inner requests to their destination, and encrypting the corresponding response back to the client.

The [server implementation](#resources) will handle incoming requests and produce responses, and it will also advertise its public key configuration for clients to access. The public key configuration is generated securely and made available via an API. Refer to the [README](https://github.com/cloudflare/privacy-gateway-server-go#readme) for details about configuration.

Applications can also implement this functionality themselves. Details about [public key configuration](https://datatracker.ietf.org/doc/html/draft-ietf-ohai-ohttp-05#section-3), HTTP message [encryption and decryption](https://datatracker.ietf.org/doc/html/draft-ietf-ohai-ohttp-05#section-4), and [server-specific details](https://datatracker.ietf.org/doc/html/draft-ietf-ohai-ohttp-05#section-5) can be found in the OHTTP specification.

### Resources

Use the following resources for help with server configuration:

* **Go**:
  * [Sample gateway server](https://github.com/cloudflare/privacy-gateway-server-go)
  * [Gateway library](https://github.com/chris-wood/ohttp-go)
* **Rust**: [Gateway library](https://github.com/martinthomson/ohttp/tree/main/ohttp-server)
* **JavaScript / TypeScript**: [Gateway library](https://github.com/chris-wood/ohttp-js)

***

## Step 2 - Configure your client

As a customer of the Privacy Gateway, you need to set up client-side support for the gateway. Clients are responsible for encrypting requests, sending them to the Cloudflare Privacy Gateway, and then decrypting the corresponding responses.

Additionally, app developers need to [configure the client](#resources-1) to fetch or otherwise discover the gatewayâ€™s public key configuration. How this is done depends on how the gateway makes its public key configuration available. If you need help with this configuration, [contact us](https://www.cloudflare.com/lp/privacy-edge/).

### Resources

Use the following resources for help with client configuration:

* **Objective C**: [Sample application](https://github.com/cloudflare/privacy-gateway-client-demo)
* **Rust**: [Client library](https://github.com/martinthomson/ohttp/tree/main/ohttp-client)
* **JavaScript / TypeScript**: [Client library](https://github.com/chris-wood/ohttp-js)

***

## Step 3 - Review your application

After you have configured your client and server, review your application to make sure you are only sending intended data to Cloudflare and the application backend. In particular, application data should not contain anything unique to an end-user, as this would invalidate the benefits that OHTTP provides.

* Applications should scrub identifying user data from requests forwarded through the Privacy Gateway. This includes, for example, names, email addresses, phone numbers, etc.
* Applications should encourage users to disable crash reporting when using Privacy Gateway. Crash reports can contain sensitive user information and data, including email addresses.
* Where possible, application data should be encrypted on the client device with a key known only to the client. For example, iOS generally has good support for [client-side encryption (and key synchronization via the KeyChain)](https://developer.apple.com/documentation/security/certificate_key_and_trust_services/keys). Android likely has similar features available.

***

## Step 4 - Relay requests through Cloudflare

Before sending any requests, you need to first set up your account with Cloudflare. That requires [contacting us](https://www.cloudflare.com/lp/privacy-edge/) and providing the URL of your application gateway server.

Then, make sure you are forwarding requests to a mutually agreed URL with the following conventions.

```txt
https://<APPLICATION_NAME>.privacy-gateway.cloudflare.com/
```

---

# Overview

URL: https://developers.cloudflare.com/privacy-gateway/

import { Description, Feature, Plan } from "~/components"

<Description>

Implements the Oblivious HTTP IETF standard to improve client privacy.
</Description>

<Plan type="enterprise" />

[Privacy Gateway](https://blog.cloudflare.com/building-privacy-into-internet-standards-and-how-to-make-your-app-more-private-today/) is a managed service deployed on Cloudflareâ€™s global network that implements part of the [Oblivious HTTP (OHTTP) IETF](https://www.ietf.org/archive/id/draft-thomson-http-oblivious-01.html) standard. The goal of Privacy Gateway and Oblivious HTTP is to hide the client's IP address when interacting with an application backend.

OHTTP introduces a trusted third party between client and server, called a relay, whose purpose is to forward encrypted requests and responses between client and server. These messages are encrypted between client and server such that the relay learns nothing of the application data, beyond the length of the encrypted message and the server the client is interacting with.

***

## Availability

Privacy Gateway is currently in closed beta â€“ available to select privacy-oriented companies and partners. If you are interested, [contact us](https://www.cloudflare.com/lp/privacy-edge/).

***

## Features

<Feature header="Get started" href="/privacy-gateway/get-started/" cta="Get started">
Learn how to set up Privacy Gateway for your application.
</Feature>

<Feature header="Legal" href="/privacy-gateway/reference/legal/" cta="Learn more">
Learn about the different parties and data shared in Privacy Gateway.
</Feature>

<Feature header="Metrics" href="/privacy-gateway/reference/metrics/" cta="Learn more">
Learn about how to query Privacy Gateway metrics.
</Feature>

---

# FAQs

URL: https://developers.cloudflare.com/pub-sub/faq/

## What messaging systems are similar?

Messaging systems that also implement or strongly align to the "publish-subscribe" model include AWS SNS (Simple Notification Service), Google Cloud Pub/Sub, Redis' PUBLISH-SUBSCRIBE features, and RabbitMQ. If you have used one of these systems before, you will notice that Pub/Sub shares similar foundations (topics, subscriptions, fan-in/fan-out models) and is easy to migrate to.

## How is Pub/Sub priced?

Cloudflare is still exploring pricing models for Pub/Sub and will share more with developers prior to GA. Users will be given prior notice and will require beta users to explicitly opt-in.

## Does Pub/Sub show data in the Cloudflare dashboard?

Pub/Sub today does not support the Cloudflare dashboard. You can set up Pub/Sub through Wrangler by following [these steps](/pub-sub/guide/).

## Where can I speak with other like-minded developers about Pub/Sub?

Try the #pubsub-beta channel on the [Cloudflare Developers Discord](https://discord.com/invite/cloudflaredev).

## What limits does Pub/Sub have?

Refer to [Limits](/pub-sub/platform/limits) for more details on client, broker, and topic-based limits.

---

# Get started

URL: https://developers.cloudflare.com/pub-sub/guide/

import { Render } from "~/components";

:::note

Pub/Sub is currently in private beta. You can [sign up for the waitlist](https://www.cloudflare.com/cloudflare-pub-sub-lightweight-messaging-private-beta/) to register your interest.

:::

Pub/Sub is a flexible, scalable messaging service built on top of the MQTT messaging standard, allowing you to publish messages from tens of thousands of devices (or more), deploy code to filter, aggregate and transform messages using Cloudflare Workers, and/or subscribe to topics for fan-out messaging use cases.

This guide will:

- Instruct you through creating your first Pub/Sub Broker using the Cloudflare API.
- Create a `<broker>.<namespace>.cloudflarepubsub.com` endpoint ready to publish and subscribe to using any MQTT v5.0 compatible client.
- Help you send your first message to the Pub/Sub Broker.

Before you begin, you should be familiar with using the command line and running basic terminal commands.

## Prerequisite: Create a Cloudflare account

In order to use Pub/Sub, you need a [Cloudflare account](/fundamentals/setup/account/). If you already have an account, you can skip this step.

## 1. Enable Pub/Sub

During the Private Beta, your account will need to be explicitly granted access. If you have not, sign up for the waitlist, and we will contact you when you are granted access.

## 2. Install Wrangler (Cloudflare CLI)

:::note

Pub/Sub support in Wrangler requires wrangler `2.0.16` or above. If you're using an older version of Wrangler, ensure you [update the installed version](/workers/wrangler/install-and-update/#update-wrangler).

:::

Installing `wrangler`, the Workers command-line interface (CLI), allows you to [`init`](/workers/wrangler/commands/#init), [`dev`](/workers/wrangler/commands/#dev), and [`publish`](/workers/wrangler/commands/#publish) your Workers projects.

To install [`wrangler`](https://github.com/cloudflare/workers-sdk/tree/main/packages/wrangler), ensure you have [`npm` installed](https://docs.npmjs.com/getting-started), preferably using a Node version manager like [Volta](https://volta.sh/) or [nvm](https://github.com/nvm-sh/nvm). Using a version manager helps avoid permission issues and allows you to easily change Node.js versions. Then run:

<Render file="install_wrangler" product="workers" />

Validate that you have a version of `wrangler` that supports Pub/Sub:

```sh
wrangler --version
```

```sh output
2.0.16 # should show 2.0.16 or greater - e.g. 2.0.17 or 2.1.0
```

With `wrangler` installed, we can now create a Pub/Sub API token for `wrangler` to use.

## 3. Fetch your credentials

To use Wrangler with Pub/Sub, you'll need an API Token that has permissions to both read and write for Pub/Sub. The `wrangler login` flow does not issue you an API Token with valid Pub/Sub permissions.

:::note

This API token requirement will be lifted prior to Pub/Sub becoming Generally Available.

:::

1. From the [Cloudflare dashboard](https://dash.cloudflare.com), click on the profile icon and select **My Profile**.
2. Under **My Profile**, click **API Tokens**.
3. On the [**API Tokens**](https://dash.cloudflare.com/profile/api-tokens) page, click **Create Token**
4. Choose **Get Started** next to **Create Custom Token**
5. Name the token - e.g. "Pub/Sub Write Access"
6. Under the **Permissions** heading, choose **Account**, select **Pub/Sub** from the first drop-down, and **Edit** as the permission.
7. Select **Add More** below the newly created permission. Choose **User** > **Memberships** from the first dropdown and **Read** as the permission.
8. Select **Continue to Summary** at the bottom of the page, where you should see _All accounts - Pub/Sub:Edit_ as the permission.
9. Select **Create Token** and copy the token value.

In your terminal, configure a `CLOUDFLARE_API_TOKEN` environmental variable with your Pub/Sub token. When this variable is set, `wrangler` will use it to authenticate against the Cloudflare API.

```sh
export CLOUDFLARE_API_TOKEN="pasteyourtokenhere"
```

:::caution[Warning]

This token should be kept secret and not committed to source code or placed in any client-side code.

:::

With this environmental variable configured, you can now create your first Pub/Sub Broker!

## 4. Create your first namespace

A namespace represents a collection of Pub/Sub Brokers, and they can be used to separate different environments (production vs. staging), infrastructure teams, and in the future, permissions.

Before you begin, consider the following:

- **Choose your namespace carefully**. Although it can be changed later, it will be used as part of the hostname for your Brokers. You should not use secrets or other data that cannot be exposed on the Internet.
- Namespace names are global; they are globally unique.
- Namespaces must be valid DNS names per RFC 1035. In most cases, this means only a-z, 0-9, and hyphens are allowed. Names are case-insensitive.

For example, a namespace of `my-namespace` and a broker of `staging` would create a hostname of `staging.my-namespace.cloudflarepubsub.com` for clients to connect to.

With this in mind, create a new namespace. This example will use `my-namespace` as a placeholder:

```sh
wrangler pubsub namespace create my-namespace
```

```json output
{
	"id": "817170399d784d4ea8b6b90ae558c611",
	"name": "my-namespace",
	"description": "",
	"created_on": "2022-05-11T23:13:08.383232Z",
	"modified_on": "2022-05-11T23:13:08.383232Z"
}
```

If you receive an HTTP 403 (Forbidden) response, check that your credentials are correct and that you have not pasted erroneous spaces or characters.

## 5. Create a broker

A broker, in MQTT terms, is a collection of connected clients that publish messages to topics, and clients that subscribe to those topics and receive messages. The broker acts as a relay, and with Cloudflare Pub/Sub, a Cloudflare Worker can be configured to act on every message published to it.

This broker will be configured to accept `TOKEN` authentication. In MQTT terms, this is typically defined as username:password authentication. Pub/Sub uses JSON Web Tokens (JWT) that are unique to each client, and that can be revoked, to make authentication more secure.

Broker names must be:

- Chosen carefully. Although it can be changed later, the name will be used as part of the hostname for your brokers. Do not use secrets or other data that cannot be exposed on the Internet.
- Valid DNS names (per RFC 1035). In most cases, this means only `a-z`, `0-9` and hyphens are allowed. Names are case-insensitive.
- Unique per namespace.

To create a new MQTT Broker called `example-broker` in the `my-namespace` namespace from the example above:

```sh
wrangler pubsub broker create example-broker --namespace=my-namespace
```

```json output
{
	"id": "4c63fa30ee13414ba95be5b56d896fea",
	"name": "example-broker",
	"authType": "TOKEN",
	"created_on": "2022-05-11T23:19:24.356324Z",
	"modified_on": "2022-05-11T23:19:24.356324Z",
	"expiration": null,
	"endpoint": "mqtts://example-broker.namespace.cloudflarepubsub.com:8883"
}
```

In the example above, a broker is created with an endpoint of `mqtts://example-broker.my-namespace.cloudflarepubsub.com`. This means:

- Our Pub/Sub (MQTT) Broker is reachable over MQTTS (MQTT over TLS) - port 8883
- The hostname is `example-broker.my-namespace.cloudflarepubsub.com`
- [Token authentication](/pub-sub/platform/authentication-authorization/) is required to clients to connect.

## 6. Create credentials for your broker

In order to connect to a Pub/Sub Broker, you need to securely authenticate. Credentials are scoped to each broker and credentials issued for `broker-a` cannot be used to connect to `broker-b`.

Note that:

- You can generate multiple credentials at once (up to 100 per API call), which can be useful when configuring multiple clients (such as IoT devices).
- Credentials are associated with a specific Client ID and encoded as a signed JSON Web Token (JWT).
- Each token has a unique identifier (a `jti` - or `JWT ID`) that you can use to revoke a specific token.
- Tokens are prefixed with the broker name they are associate with (for example, `my-broker`) to make identifying tokens across multiple Pub/Sub brokers easier.

:::note

Ensure you do not commit your credentials to source control, such as GitHub. A valid token allows anyone to connect to your broker and publish or subscribe to messages. Treat credentials as secrets.

:::

To generate two tokens for a broker called `example-broker` with a 48 hour expiry:

```sh
wrangler pubsub broker issue example-broker --namespace=NAMESPACE_NAME --number=2 --expiration=48h
```

You should receive a success response that resembles the example below, which is a map of Client IDs and their associated tokens.

```json
{
  "01G3A5GBJE5P3GPXJZ72X4X8SA": "eyJhbGciOiJFZERTQSIsImtpZCI6IkpEUHVZSnFIT3Zxemxha2tORlE5a2ZON1dzWXM1dUhuZHBfemlSZG1PQ1UifQ.
  not-a-real-token.ZZL7PNittVwJOeMpFMn2CnVTgIz4AcaWXP9NqMQK0D_iavcRv_p2DVshg6FPe5xCdlhIzbatT6gMyjMrOA2wBg",
  "01G3A5GBJECX5DX47P9RV1C5TV": "eyJhbGciOiJFZERTQSIsImtpZCI6IkpEUHVZSnFIT3Zxemxha2tORlE5a2ZON1dzWXM1dUhuZHBfemlSZG1PQ1UifQ.also-not-a-real-token.WrhK-VTs_IzOEALB-T958OojHK5AjYBC5ZT9xiI_6ekdQrKz2kSPGnvZdUXUsTVFDf9Kce1Smh-mw1sF2rSQAQ",
}
```

Each token allows you to publish or subscribe to the associated broker.

## 7. Subscribe and publish messages to a topic

Your broker is now created and ready to accept messages from authenticated clients. Because Pub/Sub is based on the MQTT protocol, there are client libraries for most popular programming languages. Refer to the list of [recommended client libraries](/pub-sub/learning/client-libraries/).

:::note

You can view a live demo available at [demo.mqtt.dev](http://demo.mqtt.dev) that allows you to use your own Pub/Sub Broker and a valid token to subscribe to a topic and publish messages to it. The `JWT` field in the demo accepts a valid token from your Broker.
:::

The example below uses [MQTT.js](https://github.com/mqttjs/MQTT.js) with Node.js to subscribe to a topic on a broker and publish a very basic "hello world" style message. You will need to have a [supported Node.js](https://nodejs.org/en/download/current/) version installed.

```sh
# Check that Node.js is installed
which node
# Install MQTT.js
npm i mqtt --save
```

Set your environment variables.

```sh
export CLOUDFLARE_API_TOKEN="YourAPIToken"
export CLOUDFLARE_ACCOUNT_ID="YourAccountID"
export DEFAULT_NAMESPACE="TheNamespaceYouCreated"
export BROKER_NAME="TheBrokerYouCreated"
```

We can now generate an access token for Pub/Sub. We will need both the client ID and the token (a JSON Web Token) itself to authenticate from our MQTT client:

```sh
curl -s -H "Authorization: Bearer ${CLOUDFLARE_API_TOKEN}" -H "Content-Type: application/json" "https://api.cloudflare.com/client/v4/accounts/${CLOUDFLARE_ACCOUNT_ID}/pubsub/namespaces/namespace/brokers/is-it-broken/credentials?type=TOKEN&topicAcl=#" | jq '.result | to_entries | .[0]'
```

This will output a `key` representing the `clientId`, and a `value` representing our (secret) access token, resembling the following:

```json
{
	"key": "01HDQFD5Y8HWBFGFBBZPSWQ22M",
	"value": "eyJhbGciOiJFZERTQSIsImtpZCI6IjU1X29UODVqQndJbjlFYnY0V3dzanRucG9ycTBtalFlb1VvbFZRZDIxeEUifQ....NVpToBedVYGGhzHJZmpEG1aG_xPBWrE-PgG1AFYcTPEBpZ_wtN6ApeAUM0JIuJdVMkoIC9mUg4vPtXM8jLGgBw"
}
```

Copy the `value` field and set it as the `BROKER_TOKEN` environmental variable:

```sh
export BROKER_TOKEN="<VALUE>"
```

Create a file called `index.js `, making sure that:

- `brokerEndpoint` is set to the address of your Pub/Sub broker.
- `clientId` is the `key` from your newly created access token
- The `BROKER_TOKEN` environmental variable populated with your access token.

:::note

Your `BROKER_TOKEN` is sensitive, and should be kept secret to avoid unintended access to your Pub/Sub broker. Avoid committing it to source code.

:::

```js
const mqtt = require("mqtt");

const brokerEndpoint = "mqtts://my-broker.my-namespace.cloudflarepubsub.com";
const clientId = "01HDQFD5Y8HWBFGFBBZPSWQ22M"; // Replace this with your client ID

const options = {
	port: 8883,
	username: clientId, // MQTT.js requires this, but Pub/Sub does not
	clientId: clientId, // Required by Pub/Sub
	password: process.env.BROKER_TOKEN,
	protocolVersion: 5, // MQTT 5
};

const client = mqtt.connect(brokerEndpoint, options);

client.subscribe("example-topic");
client.publish(
	"example-topic",
	`message from ${client.options.clientId}: hello at ${Date.now()}`,
);
client.on("message", function (topic, message) {
	console.log(`received message on ${topic}: ${message}`);
});
```

Run the example. You should see the output written to your terminal (stdout).

```sh
node index.js
```

```sh output
> received message on example-topic: hello from 01HDQFD5Y8HWBFGFBBZPSWQ22M at 1652102228
```

Your client ID and timestamp will be different from above, but you should see a very similar message. You can also try subscribing to multiple topics and publishing to them by passing the same topic name to `client.publish`. Provided they have permission to, clients can publish to multiple topics at once or as needed.

If you do not see the message you published, or you are receiving error messages, ensure that:

- The `BROKER_TOKEN` environmental variable is not empty. Try echo `$BROKER_TOKEN` in your terminal.
- You updated the `brokerEndpoint` to match the broker you created. The **Endpoint** field of your broker will show this address and port.
- You correctly [installed MQTT.js](https://github.com/mqttjs/MQTT.js#install).

## Next Steps

What's next?

- [Connect a worker to your broker](/pub-sub/learning/integrate-workers/) to programmatically read, parse, and filter messages as they are published to a broker
- [Learn how PubSub and the MQTT protocol work](/pub-sub/learning/how-pubsub-works)
- [See example client code](/pub-sub/examples) for publishing or subscribing to a PubSub broker

---

# Overview

URL: https://developers.cloudflare.com/pub-sub/

:::note

Pub/Sub is currently in private beta. Browse the documentation to understand how Pub/Sub works and integrates with our broader Developer Platform, and [sign up for the waitlist](https://www.cloudflare.com/cloudflare-pub-sub-lightweight-messaging-private-beta/) to get access in the near future.

:::

Pub/Sub is Cloudflare's distributed MQTT messaging service. MQTT is one of the most popular messaging protocols used for consuming sensor data from thousands (or tens of thousands) of remote, distributed Internet of Things clients; publishing configuration data or remote commands to fleets of devices in the field; and even for building notification or messaging systems for online games and mobile apps.

Pub/Sub is ideal for cases where you have many (from a handful to tens of thousands of) clients sending small, sub-1MB messages â€” such as event, telemetry or transaction data â€” into a centralized system for aggregation, or where you need to push configuration updates or remote commands to remote clients at scale.

Pub/Sub:

* Scales automatically. You do not have to provision "vCPUs" or "memory", or set autoscaling parameters to handle spikes in message rates.
* Is global. Cloudflare's Pub/Sub infrastructure runs in [hundreds of cities worldwide](https://www.cloudflare.com/network/). Every edge location is part of one, globally distributed Pub/Sub system.
* Is secure by default. Clients must authenticate and connect over TLS, and clients are issued credentials that are scoped to a specific broker.
* Allows you to create multiple brokers to isolate clients or use cases, for example, staging vs. production or customers A vs. B vs. C â€” as needed. Each broker is addressable by a unique DNS hostname.
* Integrates with Cloudflare Workers to enable programmable messaging capabilities: parse, filter, aggregate, and re-publish MQTT messages directly from your serverless code.
* Supports MQTT v5.0, the most recent version of the MQTT specification, and one of the most ubiquitous messaging protocols in use today.

If you are new to the MQTT protocol, visit the [How Pub/Sub works](/pub-sub/learning/how-pubsub-works/) to better understand how MQTT differs from other messaging protocols.

---

# Demos and architectures

URL: https://developers.cloudflare.com/pages/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use Pages within your existing application and architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Pages.

<ExternalResources type="apps" products={["Pages"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use Pages:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["Pages"]} />

---

# Overview

URL: https://developers.cloudflare.com/pages/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct, Render } from "~/components"

<Description>

Create full-stack applications that are instantly deployed to the Cloudflare global network.
</Description>

<Plan type="all" />

Deploy your Pages project by connecting to [your Git provider](/pages/get-started/git-integration/), uploading prebuilt assets directly to Pages with [Direct Upload](/pages/get-started/direct-upload/) or using [C3](/pages/get-started/c3/) from the command line.

***

## Features

<Feature header="Pages Functions" href="/pages/functions/">

Use Pages Functions to deploy server-side code to enable dynamic functionality without running a dedicated server.


</Feature>

<Feature header="Rollbacks" href="/pages/configuration/rollbacks/">

Rollbacks allow you to instantly revert your project to a previous production deployment.


</Feature>

<Feature header="Redirects" href="/pages/configuration/redirects/">

Set up redirects for your Cloudflare Pages project.


</Feature>

***

## Related products

<RelatedProduct header="Workers" href="/workers/" product="workers">

Cloudflare Workers provides a serverless execution environment that allows you to create new applications or augment existing ones without configuring or maintaining infrastructure.


</RelatedProduct>

<RelatedProduct header="R2" href="/r2/" product="r2">

Cloudflare R2 Storage allows developers to store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.


</RelatedProduct>

<RelatedProduct header="D1" href="/d1/" product="d1">

D1 is Cloudflareâ€™s native serverless database. Create a database by importing data or defining your tables and writing your queries within a Worker or through the API.


</RelatedProduct>

<RelatedProduct header="Zaraz" href="/zaraz/" product="zaraz">

Offload third-party tools and services to the cloud and improve the speed and security of your website.


</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Limits" href="/pages/platform/limits/" icon="document">
Learn about limits that apply to your Pages project (500 deploys per month on the Free plan).
</LinkTitleCard>

<LinkTitleCard title="Migration guides" href="/pages/migrations/" icon="pen">
Migrate to Pages from your existing hosting provider.
</LinkTitleCard>

<LinkTitleCard title="Framework guides" href="/pages/framework-guides/" icon="open-book">
Deploy popular frameworks such as React, Hugo, and Next.js on Pages.
</LinkTitleCard>

<LinkTitleCard title="Developer Discord" href="https://discord.cloudflare.com" icon="discord">
Connect with the Workers community on Discord to ask questions, show what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="@CloudflareDev" href="https://x.com/cloudflaredev" icon="x.com">
Follow @CloudflareDev on Twitter to learn about product announcements, and what is new in Cloudflare Workers.
</LinkTitleCard>

</CardGrid>

---

# Overview

URL: https://developers.cloudflare.com/pulumi/

import { CardGrid, Description, Feature, LinkTitleCard, RelatedProduct } from "~/components"

<Description>


Create, deploy, and manage Cloudflare resources in various programming languages.


</Description>

Provision and manage Cloudflare using infrastructure as code through [Pulumi](https://www.pulumi.com/). With the [Pulumi Cloudflare package](https://www.pulumi.com/registry/packages/cloudflare/), you can build, deploy, and manage Cloudflare resources using standard programming languages (TypeScript, JavaScript, Python, .NET, Java, Go, and YAML). You can define the desired state for your infrastructure in code and leverage language features like loops, functions, classes, and package management.

***

## Features

<Feature header="Open Source" href="https://www.pulumi.com/blog/pulumi-hearts-opensource/" cta="View open source commitment">
[Pulumi](https://github.com/pulumi/pulumi) is open source and uses the Apache 2.0 license. 
</Feature>

<Feature header="Multiple languages and SDKs" href="https://www.pulumi.com/docs/languages-sdks/">
Use TypeScript, JavaScript, Python, Go, .Net, Java, or YAML to write Pulumi programs. Each language is as capable as the other and supports the entire [Pulumi Registry](https://www.pulumi.com/registry/). 
</Feature>

***

## Related products

<RelatedProduct header="Pulumi Cloud" href="https://www.pulumi.com/product/pulumi-cloud/" product="pulumi">
Pulumi Cloud fully manages infrastructure state and secrets, provides rich search capabilities, and more. 
</RelatedProduct>

<RelatedProduct header="Pulumi AI" href="https://www.pulumi.com/ai" product="pulumi">
Pulumi AI is an experimental feature that lets you use natural-language prompts to generate Pulumi infrastructure-as-code programs in any language. 
</RelatedProduct>

<RelatedProduct header="Pulumi ESC" href="https://www.pulumi.com/product/esc/" product="pulumi"> 
Pulumi ESC provides centralized management of environments, secrets, and configurations. 
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Visit the Pulumi docs" href="https://www.pulumi.com/docs" icon="open-book">

To learn more about Pulumi. 
</LinkTitleCard>

<LinkTitleCard title="Report issues" href="https://github.com/pulumi/pulumi" icon="heart">

Report Pulumi configuration issues via GitHub. 
</LinkTitleCard>

</CardGrid>

---

# Get started

URL: https://developers.cloudflare.com/pulumi/installing/

Follow the recommended steps for your operating system below. For official instructions on installing Pulumi and other install options, refer to [Install Pulumi](https://www.pulumi.com/docs/install/).

:::note

Pulumi is free, open source, and optionally pairs with the [Pulumi Cloud](https://www.pulumi.com/product/pulumi-cloud/) to make managing infrastructure secure, reliable, and hassle-free.

:::

:::caution

To avoid resource management conflicts, itâ€™s **always** recommended to manage Pulumi-controlled resources via Pulumi.

:::

## Installation

### Mac

Install via Homebrew package manager.

```sh
brew install pulumi/tap/pulumi
```

### Linux

Use the installation script.

```sh
curl -fsSL https://get.pulumi.com | sh
```

### Windows

1. Download the latest installer from the [Pulumi Repository](https://github.com/pulumi/pulumi-winget/releases/latest)
2. Double click the MSI file and complete the wizard.

## Verify installation

To verify your installation, run the following in the terminal:

```sh
pulumi version
```

:::note[Note]

For upgrades and installation alternatives, refer to [Install Pulumi](https://www.pulumi.com/docs/install/).

:::

## Next steps

Follow the [Hello World tutorial](/pulumi/tutorial/hello-world/) to write a simple Pulumi program. It takes about 10 minutes to complete.

---

# Demos and architectures

URL: https://developers.cloudflare.com/queues/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use Queues within your existing application and architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Queues.

<ExternalResources type="apps" products={["Queues"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use Queues:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["Queues"]} />

---

# Get started

URL: https://developers.cloudflare.com/queues/get-started/

import { Render, PackageManagers, WranglerConfig } from "~/components";

Cloudflare Queues is a flexible messaging queue that allows you to queue messages for asynchronous processing. By following this guide, you will create your first queue, a Worker to publish messages to that queue, and a consumer Worker to consume messages from that queue.

## Prerequisites

To use Queues, you will need:

<Render file="prereqs" product="workers" />

## 1. Create a Worker project

You will access your queue from a Worker, the producer Worker. You must create at least one producer Worker to publish messages onto your queue. If you are using [R2 Bucket Event Notifications](/r2/buckets/event-notifications/), then you do not need a producer Worker.

To create a producer Worker, run:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"producer-worker"}
/>

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "hello-world",
		type: "Hello World Worker",
		lang: "TypeScript",
	}}
/>

This will create a new directory, which will include both a `src/index.ts` Worker script, and a [`wrangler.jsonc`](/workers/wrangler/configuration/) configuration file. After you create your Worker, you will create a Queue to access.

Move into the newly created directory:

```sh
cd producer-worker
```

## 2. Create a queue

To use queues, you need to create at least one queue to publish messages to and consume messages from.

To create a queue, run:

```sh
npx wrangler queues create <MY-QUEUE-NAME>
```

Choose a name that is descriptive and relates to the types of messages you intend to use this queue for. Descriptive queue names look like: `debug-logs`, `user-clickstream-data`, or `password-reset-prod`.

Queue names must be 1 to 63 characters long. Queue names cannot contain special characters outside dashes (`-`), and must start and end with a letter or number.

You cannot change your queue name after you have set it. After you create your queue, you will set up your producer Worker to access it.

## 3. Set up your producer Worker

To expose your queue to the code inside your Worker, you need to connect your queue to your Worker by creating a binding. [Bindings](/workers/runtime-apis/bindings/) allow your Worker to access resources, such as Queues, on the Cloudflare developer platform.

To create a binding, open your newly generated `wrangler.jsonc` file and add the following:

<WranglerConfig>

```toml
[[queues.producers]]
 queue = "MY-QUEUE-NAME"
 binding = "MY_QUEUE"
```

</WranglerConfig>

Replace `MY-QUEUE-NAME` with the name of the queue you created in [step 2](/queues/get-started/#2-create-a-queue). Next, replace `MY_QUEUE` with the name you want for your `binding`. The binding must be a valid JavaScript variable name. This is the variable you will use to reference this queue in your Worker.

### Write your producer Worker

You will now configure your producer Worker to create messages to publish to your queue. Your producer Worker will:

1. Take a request it receives from the browser.
2. Transform the request to JSON format.
3. Write the request directly to your queue.

In your Worker project directory, open the `src` folder and add the following to your `index.ts` file:

```ts null {8}
export default {
  async fetch(request, env, ctx): Promise<Response> {
    let log = {
      url: request.url,
      method: request.method,
      headers: Object.fromEntries(request.headers),
    };
    await env.<MY_QUEUE>.send(log);
    return new Response('Success!');
  },
} satisfies ExportedHandler<Env>;
```

Replace `MY_QUEUE` with the name you have set for your binding from your `wrangler.jsonc` file.

Also add the queue to `Env` interface in `index.ts`.

```ts null {2}
export interface Env {
   <MY_QUEUE>: Queue<any>;
}
```

If this write fails, your Worker will return an error (raise an exception). If this write works, it will return `Success` back with a HTTP `200` status code to the browser.

In a production application, you would likely use a [`try...catch`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/try...catch) statement to catch the exception and handle it directly (for example, return a custom error or even retry).

### Publish your producer Worker

With your Wrangler file and `index.ts` file configured, you are ready to publish your producer Worker. To publish your producer Worker, run:

```sh
npx wrangler deploy
```

You should see output that resembles the below, with a `*.workers.dev` URL by default.

```
Uploaded <YOUR-WORKER-NAME> (0.76 sec)
Published <YOUR-WORKER-NAME> (0.29 sec)
  https://<YOUR-WORKER-NAME>.<YOUR-ACCOUNT>.workers.dev
```

Copy your `*.workers.dev` subdomain and paste it into a new browser tab. Refresh the page a few times to start publishing requests to your queue. Your browser should return the `Success` response after writing the request to the queue each time.

You have built a queue and a producer Worker to publish messages to the queue. You will now create a consumer Worker to consume the messages published to your queue. Without a consumer Worker, the messages will stay on the queue until they expire, which defaults to four (4) days.

## 4. Create your consumer Worker

A consumer Worker receives messages from your queue. When the consumer Worker receives your queue's messages, it can write them to another source, such as a logging console or storage objects.

In this guide, you will create a consumer Worker and use it to log and inspect the messages with [`wrangler tail`](/workers/wrangler/commands/#tail). You will create your consumer Worker in the same Worker project that you created your producer Worker.

:::note

Queues also supports [pull-based consumers](/queues/configuration/pull-consumers/), which allows any HTTP-based client to consume messages from a queue. This guide creates a push-based consumer using Cloudflare Workers.

:::

To create a consumer Worker, open your `index.ts` file and add the following `queue` handler to your existing `fetch` handler:

```ts null {11}
export default {
  async fetch(request, env, ctx): Promise<Response> {
    let log = {
      url: request.url,
      method: request.method,
      headers: Object.fromEntries(request.headers),
    };
    await env.<MY_QUEUE>.send(log);
    return new Response('Success!');
  },
  async queue(batch, env): Promise<void> {
    let messages = JSON.stringify(batch.messages);
    console.log(`consumed from our queue: ${messages}`);
  },
} satisfies ExportedHandler<Env>;
```

Replace `MY_QUEUE` with the name you have set for your binding from your `wrangler.jsonc` file.

Every time messages are published to the queue, your consumer Worker's `queue` handler (`async queue`) is called and it is passed one or more messages.

In this example, your consumer Worker transforms the queue's JSON formatted message into a string and logs that output. In a real world application, your consumer Worker can be configured to write messages to object storage (such as [R2](/r2/)), write to a database (like [D1](/d1/)), further process messages before calling an external API (such as an [email API](/workers/tutorials/)) or a data warehouse with your legacy cloud provider.

When performing asynchronous tasks from within your consumer handler, use `waitUntil()` to ensure the response of the function is handled. Other asynchronous methods are not supported within the scope of this method.

### Connect the consumer Worker to your queue

After you have configured your consumer Worker, you are ready to connect it to your queue.

Each queue can only have one consumer Worker connected to it. If you try to connect multiple consumers to the same queue, you will encounter an error when attempting to publish that Worker.

To connect your queue to your consumer Worker, open your Wrangler file and add this to the bottom:



<WranglerConfig>

```toml
[[queues.consumers]]
 queue = "<MY-QUEUE-NAME>"
 # Required: this should match the name of the queue you created in step 3.
 # If you misspell the name, you will receive an error when attempting to publish your Worker.
 max_batch_size = 10 # optional: defaults to 10
 max_batch_timeout = 5 # optional: defaults to 5 seconds
```

</WranglerConfig>

Replace `MY-QUEUE-NAME` with the queue you created in [step 2](/queues/get-started/#2-create-a-queue).

In your consumer Worker, you are using queues to auto batch messages using the `max_batch_size` option and the `max_batch_timeout` option. The consumer Worker will receive messages in batches of `10` or every `5` seconds, whichever happens first.

`max_batch_size` (defaults to 10) helps to reduce the amount of times your consumer Worker needs to be called. Instead of being called for every message, it will only be called after 10 messages have entered the queue.

`max_batch_timeout` (defaults to 5 seconds) helps to reduce wait time. If the producer Worker is not sending up to 10 messages to the queue for the consumer Worker to be called, the consumer Worker will be called every 5 seconds to receive messages that are waiting in the queue.

### Publish your consumer Worker

With your Wrangler file and `index.ts` file configured, publish your consumer Worker by running:

```sh
npx wrangler deploy
```

## 5. Read messages from your queue

After you set up consumer Worker, you can read messages from the queue.

Run `wrangler tail` to start waiting for our consumer to log the messages it receives:

```sh
npx wrangler tail
```

With `wrangler tail` running, open the Worker URL you opened in [step 3](/queues/get-started/#3-set-up-your-producer-worker).

You should receive a `Success` message in your browser window.

If you receive a `Success` message, refresh the URL a few times to generate messages and push them onto the queue.

With `wrangler tail` running, your consumer Worker will start logging the requests generated by refreshing.

If you refresh less than 10 times, it may take a few seconds for the messages to appear because batch timeout is configured for 10 seconds. After 10 seconds, messages should arrive in your terminal.

If you get errors when you refresh, check that the queue name you created in [step 2](/queues/get-started/#2-create-a-queue) and the queue you referenced in your Wrangler file is the same. You should ensure that your producer Worker is returning `Success` and is not returning an error.

By completing this guide, you have now created a queue, a producer Worker that publishes messages to that queue, and a consumer Worker that consumes those messages from it.

## Related resources

- Learn more about [Cloudflare Workers](/workers/) and the applications you can build on Cloudflare.

---

# Glossary

URL: https://developers.cloudflare.com/queues/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Queues documentation.

<Glossary product="queues" />

---

# Overview

URL: https://developers.cloudflare.com/queues/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Description>


Send and receive messages with guaranteed delivery and no charges for egress bandwidth.


</Description>

<Plan type="paid" />

Cloudflare Queues integrate with [Cloudflare Workers](/workers/) and enable you to build applications that can [guarantee delivery](/queues/reference/delivery-guarantees/), [offload work from a request](/queues/reference/how-queues-works/), [send data from Worker to Worker](/queues/configuration/configure-queues/), and [buffer or batch data](/queues/configuration/batching-retries/).

***

## Features

<Feature header="Batching, Retries and Delays" href="/queues/configuration/batching-retries/">

Cloudflare Queues allows you to batch, retry and delay messages.


</Feature>

<Feature header="Dead Letter Queues" href="/queues/configuration/dead-letter-queues/">

Redirect your messages when a delivery failure occurs.


</Feature>

<Feature header="Pull consumers" href="/queues/configuration/pull-consumers/">

Configure pull-based consumers to pull from a queue over HTTP from infrastructure outside of Cloudflare Workers.


</Feature>

***

## Related products

<RelatedProduct header="R2" href="/r2/" product="r2">

Cloudflare R2 Storage allows developers to store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.


</RelatedProduct>

<RelatedProduct header="Workers" href="/workers/" product="workers">

Cloudflare Workers allows developers to build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale.


</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Pricing" href="/queues/platform/pricing/" icon="seti:shell">
Learn about pricing.
</LinkTitleCard>

<LinkTitleCard title="Limits" href="/queues/platform/limits/" icon="document">
Learn about Queues limits.
</LinkTitleCard>

<LinkTitleCard title="Try the Demo" href="https://github.com/Electroid/queues-demo#cloudflare-queues-demo" icon="open-book">
Try Cloudflare Queues which can run on your local machine.
</LinkTitleCard>

<LinkTitleCard title="@CloudflareDev" href="https://x.com/cloudflaredev" icon="x.com">
Follow @CloudflareDev on Twitter to learn about product announcements, and what is new in Cloudflare Workers.
</LinkTitleCard>

<LinkTitleCard title="Developer Discord" href="https://discord.cloudflare.com" icon="discord">
Connect with the Workers community on Discord to ask questions, show what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="Configuration" href="/queues/configuration/configure-queues/" icon="open-book">
Learn how to configure Cloudflare Queues using Wrangler.
</LinkTitleCard>

<LinkTitleCard title="JavaScript APIs" href="/queues/configuration/javascript-apis/" icon="open-book">
Learn how to use JavaScript APIs to send and receive messages to a Cloudflare Queue.
</LinkTitleCard>

</CardGrid>

---

# Demos and architectures

URL: https://developers.cloudflare.com/r2/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use R2 within your existing application and architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for R2.

<ExternalResources type="apps" products={["R2"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use R2:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["R2"]} />

---

# Get started

URL: https://developers.cloudflare.com/r2/get-started/

import { Render } from "~/components"

Cloudflare R2 Storage allows developers to store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.

<div style="position: relative; padding-top: 56.25%;"><iframe src="https://customer-6qw1mjlclhl2mqdy.cloudflarestream.com/c247ba8eb4b61355184867bec9e5c532/iframe?poster=https%3A%2F%2Fcustomer-6qw1mjlclhl2mqdy.cloudflarestream.com%2Fc247ba8eb4b61355184867bec9e5c532%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D%26height%3D600" style="border: none; position: absolute; top: 0; left: 0; height: 100%; width: 100%;" allow="accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;" allowfullscreen="true"></iframe></div>

## 1. Install and authenticate Wrangler

:::note
Before you create your first bucket, you must purchase R2 from the Cloudflare dashboard.
:::

1. [Install Wrangler](/workers/wrangler/install-and-update/) within your project using npm and Node.js or Yarn.

<Render file="install_wrangler" product="workers" />

2. [Authenticate Wrangler](/workers/wrangler/commands/#login) to enable deployments to Cloudflare. When Wrangler automatically opens your browser to display Cloudflare's consent screen, select **Allow** to send the API Token to Wrangler.

```txt
wrangler login
```

## 2. Create a bucket

To create a new R2 bucket from the Cloudflare dashboard:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select **R2**.
2. Select **Create bucket**.
3. Enter a name for the bucket and select **Create bucket**.

## 3. Upload your first object

1. From the **R2** page in the dashboard, locate and select your bucket.
2. Select **Upload**.
3. Choose to either drag and drop your file into the upload area or **select from computer**.

You will receive a confirmation message after a successful upload.

## Bucket access options

Cloudflare provides multiple ways for developers to access their R2 buckets:

* [Workers Runtime API](/r2/api/workers/workers-api-usage/)
* [S3 API compatibility](/r2/api/s3/api/)
* [Public buckets](/r2/buckets/public-buckets/)

---

# Overview

URL: https://developers.cloudflare.com/r2/

import { CardGrid, Description, Feature, LinkButton, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Description>


Object storage for all your data.


</Description>

Cloudflare R2 Storage allows developers to store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.

You can use R2 for multiple scenarios, including but not limited to:

* Storage for cloud-native applications
* Cloud storage for web content
* Storage for podcast episodes
* Data lakes (analytics and big data)
* Cloud storage output for large batch processes, such as machine learning model artifacts or datasets

 <LinkButton variant="primary" href="/r2/get-started/">Get started</LinkButton> <LinkButton variant="secondary" href="/r2/examples/">Browse the examples</LinkButton>


***

## Features

<Feature header="Location Hints" href="/r2/reference/data-location/#location-hints">

Location Hints are optional parameters you can provide during bucket creation to indicate the primary geographical location you expect data will be accessed from.


</Feature>

<Feature header="CORS" href="/r2/buckets/cors/">

Configure CORS to interact with objects in your bucket and configure policies on your bucket.


</Feature>

<Feature header="Public buckets" href="/r2/buckets/public-buckets/">

Public buckets expose the contents of your R2 bucket directly to the Internet.


</Feature>

<Feature header="Bucket scoped tokens" href="/r2/api/s3/tokens/">

Create bucket scoped tokens for granular control over who can access your data.


</Feature>

***

## Related products

<RelatedProduct header="Workers" href="/workers/" product="workers">

A [serverless](https://www.cloudflare.com/learning/serverless/what-is-serverless/) execution environment that allows you to create entirely new applications or augment existing ones without configuring or maintaining infrastructure.
</RelatedProduct>

<RelatedProduct header="Stream" href="/stream/" product="stream">

Upload, store, encode, and deliver live and on-demand video with one API, without configuring or maintaining infrastructure.
</RelatedProduct>

<RelatedProduct header="Images" href="/images/" product="images">

A suite of products tailored to your image-processing needs.
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Pricing" href="/r2/pricing" icon="seti:shell">
&#x20;Understand pricing for free and paid tier rates.
</LinkTitleCard>

<LinkTitleCard title="Discord" href="https://discord.cloudflare.com" icon="discord">
&#x20;Ask questions, show off what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="Twitter" href="https://x.com/cloudflaredev" icon="x.com">
&#x20;Learn about product announcements, new tutorials, and what is new in Cloudflare Workers.
</LinkTitleCard>

</CardGrid>

---

# Pricing

URL: https://developers.cloudflare.com/r2/pricing/

import { InlineBadge } from "~/components";

R2 charges based on the total volume of data stored, along with two classes of operations on that data:

1. [Class A operations](#class-a-operations) which are more expensive and tend to mutate state.
2. [Class B operations](#class-b-operations) which tend to read existing state.

For the Infrequent Access storage class, [data retrieval](#data-retrieval) fees apply. There are no charges for egress bandwidth for any storage class.

All included usage is on a monthly basis.

:::note

To learn about potential cost savings from using R2, refer to the [R2 pricing calculator](https://r2-calculator.cloudflare.com/).

:::

## R2 pricing

|                                    | Standard storage         | Infrequent Access storage<InlineBadge preset="beta" /> |
| ---------------------------------- | ------------------------ | ------------------------------------------------------ |
| Storage                            | $0.015 / GB-month        | $0.01 / GB-month                                       |
| Class A Operations                 | $4.50 / million requests | $9.00 / million requests                               |
| Class B Operations                 | $0.36 / million requests | $0.90 / million requests                               |
| Data Retrieval (processing)        | None                     | $0.01 / GB                                             |
| Egress (data transfer to Internet) | Free [^1]                | Free [^1]                                              |

### Free tier

You can use the following amount of storage and operations each month for free. The free tier only applies to Standard storage.

|                                    | Free                        |
| ---------------------------------- | --------------------------- |
| Storage                            | 10 GB-month / month         |
| Class A Operations                 | 1 million requests / month  |
| Class B Operations                 | 10 million requests / month |
| Egress (data transfer to Internet) | Free [^1]                   |

### Storage usage

Storage is billed using gigabyte-month (GB-month) as the billing metric. A GB-month is calculated by averaging the _peak_ storage per day over a billing period (30 days).

For example:

- Storing 1 GB constantly for 30 days will be charged as 1 GB-month.
- Storing 3 GB constantly for 30 days will be charged as 3 GB-month.
- Storing 1 GB for 5 days, then 3 GB for the remaining 25 days will be charged as `1 GB * 5/30 month + 3 GB * 25/30 month = 2.66 GB-month`

For objects stored in Infrequent Access storage, you will be charged for the object for the minimum storage duration even if the object was deleted or moved before the duration specified.

### Class A operations

Class A Operations include `ListBuckets`, `PutBucket`, `ListObjects`, `PutObject`, `CopyObject`, `CompleteMultipartUpload`, `CreateMultipartUpload`, `LifecycleStorageTierTransition`, `ListMultipartUploads`, `UploadPart`, `UploadPartCopy`, `ListParts`, `PutBucketEncryption`, `PutBucketCors` and `PutBucketLifecycleConfiguration`.

### Class B operations

Class B Operations include `HeadBucket`, `HeadObject`, `GetObject`, `UsageSummary`, `GetBucketEncryption`, `GetBucketLocation`, `GetBucketCors` and `GetBucketLifecycleConfiguration`.

### Free operations

Free operations include `DeleteObject`, `DeleteBucket` and `AbortMultipartUpload`.

### Data retrieval

Data retrieval fees apply when you access or retrieve data from the Infrequent Access storage class. This includes any time objects are read or copied.

### Minimum storage duration

For objects stored in Infrequent Access storage, you will be charged for the object for the minimum storage duration even if the object was deleted, moved, or replaced before the specified duration.

| Storage class                                          | Minimum storage duration |
| ------------------------------------------------------ | ------------------------ |
| Standard storage                                       | None                     |
| Infrequent Access storage<InlineBadge preset="beta" /> | 30 days                  |

## Data migration pricing

### Super Slurper

Super Slurper is free to use. You are only charged for the Class A operations that Super Slurper makes to your R2 bucket. Objects with sizes < 100MiB are uploaded to R2 in a single Class A operation. Larger objects use multipart uploads to increase transfer success rates and will perform multiple Class A operations. Note that your source bucket might incur additional charges as Super Slurper copies objects over to R2.

Once migration completes, you are charged for storage & Class A/B operations as described in previous sections.

### Sippy

Sippy is free to use. You are only charged for the operations Sippy makes to your R2 bucket. If a requested object is not present in R2, Sippy will copy it over from your source bucket. Objects with sizes < 200MiB are uploaded to R2 in a single Class A operation. Larger objects use multipart uploads to increase transfer success rates, and will perform multiple Class A operations. Note that your source bucket might incur additional charges as Sippy copies objects over to R2.

As objects are migrated to R2, they are served from R2, and you are charged for storage & Class A/B operations as described in previous sections.

## Pricing calculator

To learn about potential cost savings from using R2, refer to the [R2 pricing calculator](https://r2-calculator.cloudflare.com/).

## R2 billing examples

### Data storage example 1

If a user writes 1,000 objects in R2 for 1 month with an average size of 1 GB and requests each 1,000 times per month, the estimated cost for the month would be:

|                    | Usage                                       | Free Tier    | Billable Quantity | Price      |
| ------------------ | ------------------------------------------- | ------------ | ----------------- | ---------- |
| Class B Operations | (1,000 objects) \* (1,000 reads per object) | 10 million   | 0                 | $0.00      |
| Class A Operations | (1,000 objects) \* (1 write per object)     | 1 million    | 0                 | $0.00      |
| Storage            | (1,000 objects) \* (1 GB per object)        | 10 GB-months | 990 GB-months     | $14.85     |
| **TOTAL**          |                                             |              |                   | **$14.85** |
|                    |                                             |              |                   |            |

### Data storage example 2

If a user writes 10 objects in R2 for 1 month with an average size of 1 GB and requests 1,000 times per month, the estimated cost for the month would be:

|                    | Usage                                       | Free Tier    | Billable Quantity | Price      |
| ------------------ | ------------------------------------------- | ------------ | ----------------- | ---------- |
| Class B Operations | (1,000 objects) \* (1,000 reads per object) | 10 million   | 0                 | $0.00      |
| Class A Operations | (1,000 objects) \* (1 write per object)     | 1 million    | 0                 | $0.00      |
| Storage            | (10 objects) \* (1 GB per object)           | 10 GB-months | 0                 | $0.00      |
| **TOTAL**          |                                             |              |                   | **$0.00**  |
|                    |                                             |              |                   |            |

### Asset hosting

If a user writes 100,000 files with an average size of 100 KB object and reads 10,000,000 objects per day, the estimated cost in a month would be:

|                    | Usage                                   | Free Tier    | Billable Quantity | Price       |
| ------------------ | --------------------------------------- | ------------ | ----------------- | ----------- |
| Class B Operations | (10,000,000 reads per day) \* (30 days) | 10 million   | 290,000,000       | $104.40     |
| Class A Operations | (100,000 writes)                        | 1 million    | 0                 | $0.00       |
| Storage            | (100,000 objects) \* (100KB per object) | 10 GB-months | 0 GB-months       | $0.00       |
| **TOTAL**          |                                         |              |                   | **$104.40** |
|                    |                                         |              |                   |             |

## Cloudflare billing policy

To learn more about how usage is billed, refer to [Cloudflare Billing Policy](/support/account-management-billing/billing-cloudflare-plans/cloudflare-billing-policy/).

## Frequently asked questions

### Will I be charged for unauthorized requests to my R2 bucket?

No. You are not charged for operations when the caller does not have permission to make the request (HTTP 401 `Unauthorized` response status code).

[^1]: Egressing directly from R2, including via the [Workers API](/r2/api/workers/), [S3 API](/r2/api/s3/), and [`r2.dev` domains](/r2/buckets/public-buckets/#enable-managed-public-access) does not incur data transfer (egress) charges and is free. If you connect other metered services to an R2 bucket, you may be charged by those services.

---

# Overview

URL: https://developers.cloudflare.com/randomness-beacon/

drand (pronounced "dee-rand") is a distributed randomness beacon daemon written in Golang. Servers running drand can be linked to each other to produce collective, publicly verifiable, unbiased, unpredictable random values at fixed intervals using bilinear pairings and threshold cryptography.

drand is meant to be an Internet infrastructure level service that provides randomness to applications, similar to how NTP provides timing information and Certificate Transparency Logs provide certificate issuance information.

For the most up-to-date documentation on drand, please visit [drand.love](https://drand.love).

---

# Glossary

URL: https://developers.cloudflare.com/radar/glossary/

This page provides a list of terms and concepts to help you understand Radar and the information shown.

## AI bot and crawler traffic

HTTP request activity from user agents associated with AI assistants, AI data scrapers, and AI search crawlers. This information is normalized to show trends in traffic volume, providing insights into the activity levels of AI-driven web interactions over time. User agents included in this analysis are derived from the AI-focused user agents listed in the [ai.robots.txt](https://github.com/ai-robots-txt/ai.robots.txt) repository.

## Application-level attacks

Layer 7 attack information based on mitigated requests, including the most frequent mitigation techniques as well as the trend of mitigated request volume over time. For the "Application layer attack volume" and "Mitigated traffic sources" graphs, the selected location or ASN is the source of the mitigated requests. For the "Application layer attack distribution" graph, the Origin Location graph shows where attacks targeting the selected location are coming from and the Target Location graph shows the target locations of attacks coming from the selected location. "Application layer attack distribution" insights are not available at an ASN level.

## Authentication methods

[SPF](https://developers.cloudflare.com/dns/manage-dns-records/reference/dns-record-types/#spf), [DKIM](https://developers.cloudflare.com/dns/manage-dns-records/reference/dns-record-types/#dkim), and [DMARC](https://developers.cloudflare.com/dns/manage-dns-records/reference/dns-record-types/#dmarc) are policy-driven email authentication methods and when used together, they help prevent spammers, phishers, and other unauthorized parties from sending emails on behalf of a domain they do not own. PASS is the share of processed messages that pass the associated checks. FAIL is the share of processed messages that fail the associated checks. NONE is the share of processed messages for which no associated policy could be found. Data for these metrics comes from Cloudflareâ€™s email routing service.

## Autonomous systems

The Internet is a network of networks, and autonomous systems are the networks that make up the Internet. More specifically, an autonomous system (AS) is a large network or group of networks that has a unified routing policy - the process by which a path through one or more networks is chosen.

Data packets hop from one AS to another until they reach their final destination. Every computer or device that connects to the Internet is connected to an AS. ISPs have one or more ASes, and each AS is assigned an official Autonomous System Number (ASN) for use in Border Gateway Protocol (BGP) routing. For example, Cloudflare's ASN is AS13335. Learn more in the [Cloudflare Learning Center](https://www.cloudflare.com/learning/network-layer/what-is-an-autonomous-system/).

## BGP announcements

Border Gateway Protocol (BGP) is the routing protocol for the Internet. Much like the post office processing mail, BGP picks the most efficient routes for delivering Internet traffic. A BGP announcement is a way for an AS to say to another, "When you receive traffic to this network prefix, please send it to me". That message is then processed and (possibly) forwarded to other ASes, allowing for every AS in the path to learn where to send traffic to that network prefix. Learn more in the [Cloudflare Learning Center](https://www.cloudflare.com/learning/security/glossary/what-is-bgp/).

On Cloudflare Radar, we provide time series charts for both the volume of BGP messages announced by ASes and the total size of their announced IP address space. BGP message volume shows the level of overall routing activity for a given AS, while announced IP address space indicates the size of the networks a given AS operates over time. We represent the IP address space size with the number of minimum routable network prefix sizes, which are the number of /24 prefixes for IPv4 and /48s for IPv6. Correspondingly, a /24 prefix represents 256 IP addresses while a /48 represents 2^80 IP addresses.

## BGP route leaks

[BGP route leaks](https://www.rfc-editor.org/rfc/rfc7908.html) are defined as the propagation of routing announcements beyond their intended scope.
In Cloudflare Radar, you can inspect the detected route leak events on the corresponding autonomous system number (ASN) pages. The columns in the table are defined as follows:

* `From`: The autonomous system (AS) from which the routes are learned from.
* `By`: The AS that leaked the routes, or the leaker.
* `To`: The AS that received and propagated the leaked routes.
* `Start` and `End`: The starting and ending time of a route leak event.
* `BGP Msgs.`: The number of BGP announcements that contain leaked routes.
* `Prefixes`: The number of IP prefixes a route leak event affects.
* `Origins`: The number of origin ASes a route leak event affects.
* `Vantage Points`: The number of route collectors that observed a route leak event.

Learn more about our route leak detection system design and usages  in [How we detect route leaks and our new Cloudflare Radar route leak service](https://blog.cloudflare.com/route-leak-detection-with-cloudflare-radar/) blog post.

## BGP origin hijacks

[BGP origin hijack](https://www.cloudflare.com/learning/security/glossary/bgp-hijacking/) is one type of BGP anomaly where networks falsely announce
ownership for groups of IP addresses (prefixes) that they do not own, control, or route to. A BGP origin hijack can redirect Internet traffic to the hijacker from its
legitimate destination, causing data loss with potential leak of private/confidential information.

In Cloudflare Radar, you can inspect the detected BGP origin hijack events in the "BGP Origin Hijacks" table. The columns
of the table are defined as follows:

* `ID`: Event ID, clickable and navigates to the event details page.
* `Detected Origin`: The AS that originated the prefixes at the time of detection, potentially being a BGP hijacker.
* `Expected Origin(s)`: The AS(es) that are expected to originate the corresponding prefixes based on various evidences.
* `Start Time (UTC)` and `Duration`: The detected timestamp in UTC with a human-readable time duration for how long the event lasted. Ongoing events will not have a duration value, indicated by the `--` sign.
* `BGP Messages`: The number of BGP messages that contain the detected anomaly.
* `Prefixes`: The prefixes hijacked during the event, showing only one full prefix due to table space limitation.
* `Confidence`: The level of confidence that we have on the event being a true hijacks. Values can be `High`, `Medium`, or `Low`.
* `Tags`: The relevant evidence presented as short tags, presenting key facts we compiled using additional data sources, such as RPKI validation results or network relationship.

You can also access the detection result programmatically via our [public API](/api/resources/radar/subresources/bgp/subresources/hijacks/subresources/events/methods/list/) ([CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) license).

## Certificates

Encryption is a critical part of a safe Internet. SSL/TLS is the standard security technology for establishing an encrypted link between a client and a server.

In Cloudflare Radar, you can view all certificates issued for a given domain by a trusted Certificate Authority that are listed in active certificate transparency logs.

You can review the certificates issued for your domain name to ensure that there have been no incorrect or fraudulent issuances of certificates associated with your domains. You can also sign up to receive alerts from our certificate transparency monitor in the [Cloudflare Dashboard](https://dash.cloudflare.com/).

## Connection characteristics

Share of inbound connections to Cloudflare from mail transfer agents with the given characteristics. â€œIP Versionâ€ breaks down connections made over IPv4 and IPv6. â€œEncryptionâ€ breaks down connections made over an encrypted connection using TLS, and those made over an unencrypted connection, in the clear. Data for these metrics comes from Cloudflareâ€™s email routing service.

## Connection quality

Connection quality metrics include download and upload speed, latency (round-trip time), and latency jitter (round-trip time stability), reflecting the best expected performance for specific countries or ASNs. These metrics are derived from speed tests initiated by end users on the [Cloudflare Speed Test website](https://speed.cloudflare.com/), aggregated over the previous 90 days. The underlying raw data is freely accessible for analysis through [Measurement Lab's BigQuery](https://blog.cloudflare.com/aim-database-for-internet-quality/).

In speed, latency, and jitter rankings, only countries where users run speed tests with sufficient regularity are included. Consequently, certain countries may be excluded from the rankings, even though their data can be found in other sections of Radar.

Cloudflare Speed Test measures latency multiple times over the course of the test. Measurements taken before a download or upload begins are aggregated into idle latency and jitter, while measurements taken while a download or upload is in progress are aggregated as loaded latency and jitter.

## Content categories

Cloudflare uses a variety of data sources to categorize domains. Using Cloudflare Radar, you can view the content categories associated with a given domain. Cloudflare customers using Cloudflare Gateway or [1.1.1.1 for Families](/1.1.1.1/setup/#1111-for-families) can decide to block certain categories, like "Adult Content", in addition to security threats like malware and phishing.

In some cases, a domain may be miscategorized. For example, a social media site might be categorized as "Shopping & Auctions". If you believe a domain is miscategorized, or a domain has not yet been categorized, please provide your suggested category using [this form](https://radar.cloudflare.com/domains/feedback) to bring it to our attention.

## DNS

The [Domain Name System (DNS)](https://www.cloudflare.com/learning/dns/what-is-dns/) is a network service that is most commonly used to translate human-readable domain names into numerical IP addresses that computers can use to talk to each other.
It is an essential Internet service, and is also used to look up other network-related information.

The data displayed on Radar for DNS is based on aggregated and anonymized DNS lookups to Cloudflare's [1.1.1.1](/1.1.1.1/) public resolver service.

## Domain rankings

Domain Rankings is based on our anonymized and aggregated [1.1.1.1 DNS resolver](/1.1.1.1/) data, complies with our [privacy policy](https://www.cloudflare.com/en-gb/privacypolicy/), and aims to identify the top most popular domains that reflect how people use the Internet globally. Domain Rankingsâ€™ popularity metric is best described as the estimated number of unique users that access a domain over some period of time.

Trending domains are domains which are currently experiencing an increase in popularity. Domains Trending Today are domains spiking in popularity, reflecting increased interest potentially related to a particular event or a topic. Domains Trending This Week are domains that have steadily grown in popularity, reflecting an increase of their user base over the week.

## Geographical distribution

Countries contributing traffic to this AS, and their relative contribution as percentage of the total AS traffic seen by Cloudflare.

## Internet outages

Internet connectivity can experience outages or disruptions due to a number of factors. These factors include power outages, damage to fiber optic cables, severe weather, natural disasters, or government directed shutdowns. Outages may be sub-national or national in geographic scope, or may impact one or more [ASNs](https://www.cloudflare.com/en-gb/learning/network-layer/what-is-an-autonomous-system/). Some outages may be brief, lasting just a few minutes, while others can stretch on for months â€” the duration can be related, in part, to the underlying cause. Internet outages listed in the Cloudflare Radar Outage Center are notable drops in traffic that have generally been corroborated with third party-information, which may include a social media or status page post from a telecommunications provider, a news report, or industry/community mailing lists.

An early warning signal that an Internet outage may be underway on a given network or in a given country is an anomalous drop in traffic as compared to historical traffic patterns and trends. Internet anomalies listed in the Cloudflare Radar Outage Center represent an algorithmically-observed anomalous drop in traffic for the listed entity. If a given entry is marked as verified, it means that we have manually corroborated the observed drop in traffic across multiple Cloudflare data sources and/or third-party sources such as [IODA](https://ioda.inetintel.cc.gatech.edu/), or third-party sources of information, such as those listed above. In the case of the latter, an associated Internet outage event will be opened, with the event listed in the Internet Outages table (and API).

## Internet services ranking

Internet services ranking is based on our anonymized and aggregated [1.1.1.1 DNS resolver](/1.1.1.1/) data, complies with our [privacy policy](https://www.cloudflare.com/en-gb/privacypolicy/), and aims to identify the top most popular Internet services that reflect how people use the Internet globally. A service represents one or more domains aggregated together. Ranking popularity metric is best described as the estimated number of unique users that access domains associated with a service, over some period of time.

## Internet traffic trends

Trends observed in Internet traffic originating globally or within a given location or autonomous system within the selected time range, based on aggregated data from our network.

## IP address geolocation

IP address geolocation is the term used for the process of associating an IP address with a location in the physical world. IP geolocation used on Cloudflare Radar comes from a third-party database.

Note that a number of factors may affect the accuracy of the geolocation information, including mobile network architecture, the use of VPN services, and the use of privacy-protecting proxy services.

Learn more from [MaxMind](https://www.maxmind.com/en/geoip-data-correction-request) about how to suggest a correction if you believe the location provided is incorrect.

## IPv6 adoption

The IPv4 vs. IPv6 graph shows the distribution of traffic by IP version, and is intended to highlight IPv6 adoption trends.

Note that prior to January 23, 2023, the IPv6 percentage shown in the chart was calculated as (IPv6 requests / IPv4+IPv6 requests). After that date, the IPv6 percentage is calculated as (IPv6 requests / requests for dual-stacked content).

## IQI

The Internet Quality Index estimates connection performance under average utilization, such as web browsing. It is based on end user measurements against a fixed set of Cloudflare and third-party targets, providing numbers for bandwidth, round-trip time (latency), and DNS response time, aggregated by continent, country, or ASN.

The IQI methodology requires a minimum number of measurements to generate estimates. As a result, graphs for smaller countries and ASNs may display occasional gaps, especially during nighttime. These gaps do not indicate outages. The number of measurements underlying IQI does not necessarily correlate with the volume of traffic observed by Cloudflare in a specific country or ASN.

## Mobile operating systems

The Mobile Operating Systems graph shows the distribution of mobile device requests by operating system, representing trends observed in Internet traffic originating globally or within a given location or autonomous system within the selected time range, based on aggregated data from our network. "Mobile device" includes phones and tablets only, and does not include other types of devices, such as those classified as desktops/laptops, smart TVs, or gaming consoles.

## Most observed TLDs

[Top-level domains, also known as TLDs](https://www.cloudflare.com/learning/dns/top-level-domain/), are found in the right-most portion of a hostname. As of February 2024, there are nearly 1600 Top Level Domains listed in the [IANA Root Zone Database](https://www.iana.org/domains/root/db). On Radar, we are sharing our own perspective on these TLDs, highlighting those with the largest shares of spam and malicious emails. The analysis is based on the sending domainâ€™s TLD, found in the `From:` header of an email message. Data for this metric comes from Cloudflareâ€™s cloud email security service.

## Network-level DDoS attacks

Attacks mitigated by our Level 3 and 4 Denial of Service Attack prevention systems. We show the most used attack vectors as well as the change in attack volume over the selected time range. Selected location is the location of the data center(s) where the attacks were mitigated. Target industry and vertical categories are associated with the customers being attacked.

Industry categories include business types grouped by their primary activities, such as information technology and services, retail, or telecommunications. Vertical categories are high-level groupings that incorporate related industries, such as the "Internet and Telecom" vertical, which includes industries such as "Internet" and "Telecommunications".

Network-level DDoS attacks graphs are based on traffic measured in bytes.

## Post-quantum encryption adoption

The Post-Quantum Encryption Adoption graph shows the share of HTTPS requests to Cloudflare that are encrypted with post-quantum (PQ) cryptography. Additional details about Cloudflare's support for PQ cryptography can be found at [Cloudflare Research](https://pq.cloudflareresearch.com/).

## Robots.txt

A [robots.txt](https://www.cloudflare.com/learning/bots/what-is-robots-txt/) file contains instructions for bots that tell them which webpages they can and cannot access.

The data displayed for robots.txt is based on successfully parsed robots.txt files from the top 10,000 domains. From these files, we count the occurrences of each user agent under the `Allow` and `Disallow` directives. A user agent is classified as "fully allowed" or "fully disallowed" if the directive value is `*`. Otherwise, if the user agent is only allowed or disallowed to crawl specific paths, it is classified as "partially allowed" or "partially disallowed."

Currently, we only include AI-focused user agents listed in the [ai.robots.txt](https://github.com/ai-robots-txt/ai.robots.txt) repository.

## TCP resets and timeouts

In the Transmission Control Protocol (TCP), client-initiated connection resets (via the RST flag, TCP's "panic button") are atypical, and indicate to the server that *something went wrong* requiring the connection to be closed immediately. Similarly, connection timeouts (where the server closes a connection due to an unresponsive client) should not happen in conventional data exchanges. For comparison, a typical TCP connection consists of a 3-way handshake initiated by a client with a SYN packet to the server, then a data exchange moderated with ACK and PSH flags in the data packets, and finally a graceful close initiated from either side with a FIN packet. A FIN close is considered graceful because it ensures both sides complete their data transfer before closing the connection. In contrast, a timeout or RST flag triggers a hard stop, even if data is waiting to be sent or acknowledged. See [RFC 9293](https://datatracker.ietf.org/doc/html/rfc9293) for more details on the TCP protocol.

A TCP server may see timed-out or reset connections for a variety of reasons. Some are benign, such as client applications that lose connectivity or abruptly shut down (e.g., browsers cleaning up closed tabs or port scanners). Others are more concerning, such as [DoS attacks](https://www.cloudflare.com/learning/ddos/syn-flood-ddos-attack/) or third-party interference. In some cases, a close examination of the packets in a connection can help to shed light on the reason for termination. For example, [Global, Passive Detection of Connection Tampering](https://research.cloudflare.com/publications/SundaraRaman2023/) finds that certain packet patterns can be linked to middlebox connection tampering.

On Cloudflare Radarâ€™s [Security & Attacks page](https://radar.cloudflare.com/security-and-attacks), you can view statistics on resets and timeouts from a sample of TCP connections to Cloudflareâ€™s servers, broken down by how far the connection progressed before termination. The plot lines are defined as follows:

* **Post-SYN (mid-handshake)**: Connection resets or timeouts after the server received only a single SYN packet.
* **Post-ACK (immediately post-handshake)**: Connection resets or timeouts after the server received both a SYN packet and an ACK packet, meaning the connection was successfully established.
* **Post-PSH (after first data packet)**: Connection resets or timeouts after the server received a packet with PSH flag set, following connection establishment. The PSH flag indicates that the TCP packet contains data (such as a TLS Client Hello message) ready to deliver to the application.
* **Later (after multiple data packets)**: Connection resets within the first 10 packets from the client, but after the server has received multiple data packets.
* **None**: All other connections.

Learn more about the TCP resets and timeouts dataset in our [blog post](https://blog.cloudflare.com/tcp-resets-timeouts).

## Threat categories

Attackers use multiple types of techniques when carrying out email-based attacks, including links or attachments leading to malware; identity deception, where the message appears to be coming from a trusted contact; and brand impersonation, where the message appears to be coming from a trusted brand. Categories are assigned to the various types of threats found during the analysis of a malicious email message, and a single message can have multiple categories. These categories are aggregated into â€œLinkâ€, â€œAttachmentâ€, â€œImpersonationâ€, and â€œOtherâ€ groupings. â€œLinkâ€ groups individual threat types where the attacker is trying to get the user to click on something, â€œAttachmentâ€ groups individual threat types where  the attacker has attached a file to the email message, and â€œImpersonationâ€ groups individual threat types where the attacker is impersonating a trusted brand or contact. The â€œOtherâ€ grouping includes other threat types not covered by the previous three. The percentages represent the share of malicious email messages where the given threat categories have been found. Data for this metric comes from Cloudflareâ€™s cloud email security service.

## Threat classification

Malicious email messages may be part of a phishing campaign, where recipients are tricked into sharing personal information like login details, or they are an attempt to spread malware through embedded images, links, or attachments. The percentage shown represents the share of processed messages that are classified as malicious. Data for this metric comes from Cloudflareâ€™s cloud email security service.

## Traffic type filter

* **Human Only Traffic**: Traffic that our algorithms determine as being generated by human activity.

* **Automated Only Traffic**: Traffic that our algorithms determine as being generated by bot or automated script activity.

* **All Traffic**: Use all traffic, which includes both human activity and automated activity.

## Trends

Based on the aggregated HTTP/s metadata we see, we are able to show trends about a diverse set of metrics, including the distribution of mobile device vs. desktop traffic, the percentage of traffic detected as coming from bots, and the distribution of user agents/browsers. We also provide insights into the usage of HTTPS and IPv6.

## Verified bots

Bot traffic describes any non-human traffic to a website or an app. Some bots are useful, such as search engine bots that index content for search or customer service bots that help users. Other bots may be used to perform malicious activities, such as break into user accounts or scan the web for contact information to send spam.

Verified bots, such as the ones from search engines, are usually transparent about who they are. Cloudflare manually approves well-behaved services that benefit the broader Internet and honor robots.txt.

Each entry on the Verified Bots list exists because a corresponding IP address was seen associated with a verified bot in the last 30 days. A verified bot is not necessarily good or bad.

## Visitor location

The data displayed on domain-specific geographic traffic patterns is based solely on data from our recursive DNS services. All data displayed is in accordance with our privacy policies and commitments. This data may include attack traffic and cross-origin requests.

## WHOIS

WHOIS is a standard for publishing the contact and nameserver information for all registered domains. Each registrar maintains their own WHOIS service. Anyone can query the registrar's WHOIS service to reveal the data behind a given domain.

## Workers AI

[Workers AI](/workers-ai/) allows you to run machine learning models, on the Cloudflare network, from your own code -- whether that be from Workers, Pages, or anywhere via the Cloudflare API. The data displayed for Workers AI is based on the number of Cloudflare accounts using a model during a specific time interval.

---

# Overview

URL: https://developers.cloudflare.com/radar/

import { Description, Feature, Plan } from "~/components"

<Description>

Get access to Cloudflare's data on global Internet traffic. 
</Description>

<Plan type="all" />

[Cloudflare Radar](https://radar.cloudflare.com) is a hub that showcases global Internet traffic, attacks, and technology trends and insights. It is powered by data from Cloudflareâ€™s global network, as well as aggregated and [anonymized](/1.1.1.1/privacy/public-dns-resolver/) data from Cloudflareâ€™s [1.1.1.1 public DNS resolver](/1.1.1.1/).

Using Radar's API you can access Cloudflare's data on global Internet traffic.

Radar's API is free, allowing academics, data sleuths and other web enthusiasts to investigate Internet usage across the globe.

Data available via Radar API endpoints is made available under the [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) license.

***

## Features

<Feature header="Make your first API request" href="/radar/get-started/first-request/" cta="Make your first API request">
Start learning how to use Radar's API by making your first request. 
</Feature>

<Feature header="Compare data" href="/radar/get-started/making-comparisons/" cta="Compare data">
What to know before making comparisons between locations, [autonomous systems](https://www.cloudflare.com/en-gb/learning/network-layer/what-is-an-autonomous-system/) and more. 
</Feature>

---

# Release notes

URL: https://developers.cloudflare.com/radar/release-notes/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/radar.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Find by solution

URL: https://developers.cloudflare.com/reference-architecture/by-solution/

import { CardGrid, LinkTitleCard, Render } from "~/components";

Use the list below for reference architecture documentation that relates to a solution area you are interested in.

### Cloudflare Connectivity Cloud

Content that pertains to the Cloudflare platform in general.

#### Reference architectures

- [Cloudflare security reference architecture](/reference-architecture/architectures/security/)
- [Multi-vendor Application Security and Performance Reference Architecture](/reference-architecture/architectures/multi-vendor/)
- [Protect network infrastructure with Magic Transit](/reference-architecture/architectures/magic-transit/)
- [Protect Hybrid Cloud Networks with Cloudflare Magic Transit](/reference-architecture/diagrams/network/protect-hybrid-cloud-networks-with-cloudflare-magic-transit/)

#### Reference architecture diagrams

- [Protecting ISP and telecommunications networks from DDoS attacks](/reference-architecture/diagrams/network/protecting-sp-networks-from-ddos/)

#### Design guides

- [Extend Cloudflare's Benefits to SaaS Providers' End-Customers](/reference-architecture/design-guides/extending-cloudflares-benefits-to-saas-providers-end-customers/)

### Zero Trust / SASE

Architecture documentation related to using Cloudflare for Zero Trust, SSE and SASE initiatives for protecting your applications, data, employees and the corporate network.

#### Reference architectures

- [Evolving to a SASE architecture with Cloudflare](/reference-architecture/architectures/sase/)
- [Using Cloudflare SASE with Microsoft](/reference-architecture/architectures/cloudflare-sase-with-microsoft/)

#### Reference architecture diagrams

- [Access to private apps without having to deploy client agents](/reference-architecture/diagrams/sase/sase-clientless-access-private-dns/)
- [Securing data at rest](/reference-architecture/diagrams/security/securing-data-at-rest/)
- [Securing data in transit](/reference-architecture/diagrams/security/securing-data-in-transit/)
- [Securing data in use](/reference-architecture/diagrams/security/securing-data-in-use/)
- [Extend ZTNA with external authorization and serverless computing](/reference-architecture/diagrams/sase/augment-access-with-serverless/)
- [DNS filtering solution for Internet service providers](/reference-architecture/diagrams/sase/gateway-dns-for-isp/)
- [Magic WAN Connector deployment options](/reference-architecture/diagrams/sase/magic-wan-connector-deployment/)
- [Deploy self-hosted VoIP services for hybrid users](/reference-architecture/diagrams/sase/deploying-self-hosted-voip-services-for-hybrid-users/)

#### Design guides

- [Designing ZTNA access policies for Cloudflare Access](/reference-architecture/design-guides/designing-ztna-access-policies/)
- [Building zero trust architecture into your startup](/reference-architecture/design-guides/zero-trust-for-startups/)
- [Network-focused migration from VPN concentrators to Zero Trust Network Access](/reference-architecture/design-guides/network-vpn-migration/)
- [Using a zero trust framework to secure SaaS applications](/reference-architecture/design-guides/zero-trust-for-saas/)

#### Implementation guides

- [Secure your Internet traffic and SaaS apps](/learning-paths/secure-internet-traffic/concepts/)
- [Replace your VPN](/learning-paths/replace-vpn/concepts/)
- [Deploy Zero Trust Web Access](/learning-paths/zero-trust-web-access/concepts/)
- [Secure Microsoft 365 email with Email Security](/learning-paths/secure-o365-email/concepts/)

### Networking

#### Reference architecture diagrams

- [Protect public networks with Cloudflare](/reference-architecture/diagrams/network/protect-public-networks-with-cloudflare/)
- [Bring your own IP space to Cloudflare](/reference-architecture/diagrams/network/bring-your-own-ip-space-to-cloudflare/)
- [Protect hybrid cloud networks with Cloudflare Magic Transit](/reference-architecture/diagrams/network/protect-hybrid-cloud-networks-with-cloudflare-magic-transit/)
- [Protect ISP and telecommunications networks from DDoS attacks](/reference-architecture/diagrams/network/protecting-sp-networks-from-ddos/)

### Application Performance

Content related to DNS, caching, load balancing and other Cloudflare services designed to improve application reliability and performance.

#### Reference architectures

- [Content Delivery Network](/reference-architecture/architectures/cdn/)
- [Load Balancing](/reference-architecture/architectures/load-balancing/)

### Application Security

Content related to protecting your applications from threats such as DDoS attack, SQL injection, exploiting application vulnerabilities, scraping API data and more.

#### Reference architecture diagrams

- [Bot management](/reference-architecture/diagrams/bots/bot-management/)

#### Design guides

- [Secure application delivery](/reference-architecture/design-guides/secure-application-delivery/)

#### Implementation guides

- [Use mTLS with Cloudflare protected resources](/learning-paths/mtls/concepts/)

### Developer Platform

Architecture content for our developer platform.

#### Reference architecture diagrams

##### AI

- [Automatic captioning for video uploads](/reference-architecture/diagrams/ai/ai-video-caption/)
- [Composable AI architecture](/reference-architecture/diagrams/ai/ai-composable/)
- [Content-based asset creation](/reference-architecture/diagrams/ai/ai-asset-creation/)
- [Multi-vendor AI observability and control](/reference-architecture/diagrams/ai/ai-multivendor-observability-control/)
- [Retrieval Augmented Generation (RAG)](/reference-architecture/diagrams/ai/ai-rag/)
- [Ingesting BigQuery Data into Workers AI](/reference-architecture/diagrams/ai/bigquery-workers-ai/)

##### Serverless

- [Optimizing Image Delivery with Cloudflare Image Resizing and R2](/reference-architecture/diagrams/content-delivery/optimizing-image-delivery-with-cloudflare-image-resizing-and-r2/)
- [A/B-testing using Workers](/reference-architecture/diagrams/serverless/a-b-testing-using-workers/)
- [Fullstack Applications](/reference-architecture/diagrams/serverless/fullstack-application/)
- [Serverless ETL pipelines](/reference-architecture/diagrams/serverless/serverless-etl/)
- [Serverless global APIs](/reference-architecture/diagrams/serverless/serverless-global-apis/)
- [Serverless image content management](/reference-architecture/diagrams/serverless/serverless-image-content-management/)

##### Storage

- [Egress-free object storage in multi-cloud setups](/reference-architecture/diagrams/storage/egress-free-storage-multi-cloud/)
- [On-demand Object Storage Data Migration](/reference-architecture/diagrams/storage/on-demand-object-storage-migration/)
- [Event notifications for storage](/reference-architecture/diagrams/storage/event-notifications-for-storage/)

---

# Reference Architectures

URL: https://developers.cloudflare.com/reference-architecture/

import {
	CardGrid,
	Description,
	LinkTitleCard,
	Render,
	DirectoryListing,
} from "~/components";

![Hero image](~/assets/images/reference-architecture/reference-architecture-hero.svg)

<Description>

All the documents in this section are designed to help you understand how Cloudflare and its products are designed and architected. These documents describe how you can leverage our platform to create solutions based on your business needs.

</Description>

No matter if you know Cloudflare well, or if you are just starting out. These documents help you understand how our connectivity cloud is architectured and how the services can be integrated with your own infrastructure. Read [How to use](/reference-architecture/how-to-use/) to understand how the documentation is structured, and either navigate by type from the menu or [find by solution](/reference-architecture/by-solution/) area.

<DirectoryListing />

---

## More resources

<CardGrid>

<LinkTitleCard title="Cloudflare blog" href="https://blog.cloudflare.com/" icon="open-book">

Read articles and announcements about the latest Cloudflare products and features.

</LinkTitleCard>

<LinkTitleCard title="Learning Paths" href="/learning-paths/" icon="open-book">

Module-based guidance on Cloudflare product workflows.

</LinkTitleCard>

</CardGrid>

---

# How to use

URL: https://developers.cloudflare.com/reference-architecture/how-to-use/

import { CardGrid, Description, LinkTitleCard, Render } from "~/components";

<Description>
	The reference architecture content in our documentation is designed to help you understand how Cloudflare has been designed and built,
	and how our products and services integrate with your current IT architecture.

</Description>

Below are the different types of architecture content and information is organized from high level reference architectures, to design guides with best practices and guidelines to implementation guides which provide detailed steps to deploy a specific solution.

## Reference architectures

[Reference architectures](/reference-architecture/architectures/) provide a foundational knowledge of the Cloudflare platform and products while offering a description for how they relate to your existing infrastructure and business challenges. They are high-level, conceptual documents that walk through the concepts of an area of our platform, mapping our network, products and features to the typical architecture of a customer's environment. Detailed diagrams with supporting content explain how our technology works and how it can be integrated with your own infrastructure. The goal of a reference architecture is:

- Present thought leadership for a broad technology area
- Visualize the architecture of Cloudflare and understand how it's been designed
- Explain integration points between Cloudflare and your infrastructure

## Reference architecture diagrams

A [reference architecture diagram](/reference-architecture/diagrams/) focusses on a specific solution or use case where Cloudflare can be used. One or more diagrams are the primary content with supporting introduction and summary. These can focus on sections from a reference architecture that are not fully developed. The goal of this type of document is:

- Visualize the components of a specific solution's architecture
- Provide a quick answer to a specific question around a use case

## Design guides

These [guides](/reference-architecture/design-guides/) are typically aimed at architects, developers, and IT professionals who are tasked with designing and deploying systems that leverage the company's technologies. They typically focus on a specific solution that would be a subset of the greater architecture. For example, if you have read our [SASE Reference Architecture](/reference-architecture/architectures/sase/), but are a startup, you may want to understand the details of using a [SASE approach for a small startup](/reference-architecture/design-guides/zero-trust-for-startups/). These documents are:

- Helping you think through how to design a deployment of Cloudflare as part of an overall solution.
- More prescriptive than reference architectures, sharing best practices and guidelines.
- Focused on a solution design that you are trying to achieve, such as connecting private networks to Cloudflare, or using a web application firewall to secure a public website.
- Not a replacement for product documentation and do not describe specific product configuration or commands to run.

## Implementation guides

Implementation guides provide [step-by-step instructions](/reference-architecture/implementation-guides/) and practical guidance for how to effectively deploy and configure specific solutions or services. Implementation guides are focused on a specific implementation goal. While a design guide provides the overall best practices for designing a solution, an implementation guide details the actual steps to deploy in the context of a specific job-to-be-done. These documents are:

- Focused on a specific implementation outcome, such as connecting a remote office using the Magic WAN Connector.
- Provide information about the exact commands and configuration steps to take.

---

# About

URL: https://developers.cloudflare.com/registrar/about/

Cloudflare Registrar offers several advantages over other [registrars](https://www.cloudflare.com/learning/dns/glossary/what-is-a-domain-name-registrar/), such as domain name registration renewal [without markups fees](https://www.cloudflare.com/products/registrar/). You only pay what is charged by registries and [ICANN](https://www.icann.org/). Cloudflare Registrar also offers additional security features, such as free, one-click activation for [Domain Name System Security Extensions (DNSSEC)](/registrar/get-started/enable-dnssec/), [custom domain protection](/registrar/custom-domain-protection/), and [WHOIS redaction](/registrar/account-options/whois-redaction/).

You can buy your domain from Cloudflare Registrar or from a third party. Even when you buy it from a third party, you can manage your domain with Cloudflare to benefit from all the features Cloudflare offers.

---

# Cloudflare Custom Domain Protection

URL: https://developers.cloudflare.com/registrar/custom-domain-protection/

Cloudflare offers [Custom Domain Protection](https://www.cloudflare.com/products/registrar/custom-domain-protection/) to customers with a Cloudflare Enterprise plan and high-profile domains who need the highest level of security against domain hijacking.

Custom Domain Protection offers additional safeguard features for registered domains, including:

* **Registry lock**: Cloudflare applies Registry Lock, when available, to all domains registered through Custom Domain Protection. Any changes to a domain requires Cloudflare to first unlock the domain at the registry level.
* **Out-of-band authentication**: All changes to domain ownership or nameserver information are verified and executed manually based on an authentication process defined by the customer.
* **No interface**: Custom Domain Protection does not offer an interface, to remove the possibility of domain hijack through a compromised account.

Contact your account team if you are interested in Cloudflare's Custom Domain Protection.

---

# FAQ

URL: https://developers.cloudflare.com/registrar/faq/

Below you will find answers to our most commonly asked questions. If you cannot find the answer you are looking for, refer to the [community page](https://community.cloudflare.com/) to explore more resources.

* [Domain transfers](#domain-transfers)
* [Domain registrations](#domain-registrations)
* [Billing](#billing)

## Domain transfers

### Why did my transfer fail?

Domain transfers sometimes fail. Refer to [Registrar: troubleshoot stalled domain transfers](/registrar/troubleshooting/) for more information on what might have happened and how to solve the issue.

If you cannot solve the issue, open a support ticket or contact your account team.

### â€‹â€‹Why did my domainâ€™s expiration date change after transferring it to Cloudflare?

ICANN requires that any transfer also extends the expiration date of your domain by at least one year â€” that is one year from your current expiration date, not one year from the date of transfer. For example, if you transfer a domain on October 10, 2021, but it expires on March 10, 2022, your new expiration date will be March 10, 2023.

Whenever a domain is first registered, the registrant purchases control of that domain for some number of years â€” up to 10 years. For example, a domain registered on October 8, 2020 will have an expiration date of October 8th in some year between 2021 and 2030, depending on the amount of years originally purchased.

Transferring a domain adds time to the current expiration date, unless your domain already has [10 years on the term](#if-i-registered-my-domain-for-10-years-at-another-registrar-will-i-gain-another-year-if-i-transfer-it-to-cloudflare).


### â€‹â€‹Why did my domainâ€™s expiration date change after transferring it to Cloudflare?

Cloudflare Registrar only supports transfers of domains that are active on a Cloudflare [full setup](/dns/zone-setups/full-setup/). Domains on Cloudflare use [nameservers assigned by Cloudflare](/dns/zone-setups/reference/nameserver-assignment/) to the associated account and those nameservers must remain in place for the domain to be Active.


### How can I see the status of my domain transfer?

Once you initiate a domain transfer, your previous registrar has five days to release the domain. In most cases, they will send you an email to confirm you want to transfer. If you actively acknowledge that email (through a link or the registrar's dashboard), they can process it immediately.

To see the progress of your transfer, log in to the Cloudflare dashboard and select your account. Then, select **Domain Registration** > **Transfer Domains** to see a list of domain transfers that are in progress. To accelerate the process, be sure to check with your old registrar how you can approve the transfer out.

Once successful, you will receive an email from Cloudflare and be able to manage the domain in the dashboard under **Overview** of that site.


### â€‹â€‹Why am I not allowed to transfer my domain?

ICANN prohibits domain transfers within 60 days of a change to the WHOIS data or registrar of a domain. If you modified your contact information, transferred registrars, or registered your domain in the last 60 days, Cloudflare will be unable to process your transfer immediately.

You can leave the domain **In Progress** and Cloudflare will wait until after the 60-day window passes to attempt to process the transfer.

:::note[Note]
This information does not apply to `.uk` domains.
:::

### Why am I not able to start a transfer?

If you have an [unverified email address](/fundamentals/setup/account/verify-email-address/), you might experience issues when initiating a domain transfer.

### What happens if I enter the wrong auth code?

If you enter an incorrect auth code (also referred to as authentication code or authorization code), return to the **Domain Registration** page or the **Overview** for your site. You can use the available input field to reenter your authentication code.


### â€‹â€‹If I registered my domain for 10 years at another registrar, will I gain another year if I transfer it to Cloudflare?


No. A domain cannot have more than 10 years on the term. If you registered your domain for 10 years, you will get 10 years upon transferring it to Cloudflare.




***

## Domain registrations

### My domainâ€™s registration was not extended by one year after transferring to Cloudflare

When you transfer your domain to Cloudflare, the registry will extend your registration by one year. However, in one specific circumstance your transfer could result in you keeping your original expiration date.

When a domain expires, the registration enters the auto-renew grace period. During that time, you can renew the domain at your registrar to avoid losing it. If your domain expires at your current registrar, you renew it and then transfer to Cloudflare within 45 days, the registry can restrict the addition of an extra year.

Say you have `example.com` registered and it expires on December 10, 2021. You decide to renew it during the auto-renew grace period on December 20, 2021. That renewal extends the registration to December 20, 2022. You then transfer to Cloudflare on December 30, 2021. Since that transfer is within 45 days of the expiration, the registry may not add the year to your registration. When you transfer to Cloudflare or any registrar in this circumstance, your expiration can still remain December 20th, 2022.

If a year is not added to your registration, you have effectively paid twice for the same added year. Per ICANN rules, you are entitled to request a refund at your previous registrar.


### What Happens When a Domain Expires?

In summary, here is what will happen after a domain expires:

* **Day 0**: Expiration Date.
* **Day 1 - 30**: Grace Period (domain resolves normally).
* **Day 31 - 40**: Suspension Period (domains resolves to suspension page).
* **Day 41 - 70**: Redemption Period.
* **Day 71 - 75**: Pending Delete Period.

Cloudflare currently offers a 40-day grace period for most top-level domains (TLDs).

During this period you may renew/extend the domain at any time from within the dashboard but no further auto-renew attempts will be made. For the first 30 days of the grace period, the domain will continue to resolve as normal. On the 30th day after the expiration date, the domain will be suspended and a parked suspension page will be displayed. You may still renew the domain at any time during this suspension period. On the 40th day, the domain will enter the Redemption Period and will no longer resolve to any web page.

The redemption period lasts for 30 days. During this time, it may be possible to restore and renew the domain. A restore fee may apply in addition to the renewal fee. At the end of the 30 day redemption period, the domain will be placed in pending delete status for a period of five days, after which it will be released and made available for re-registration. The domain cannot be restored or renewed during this period.

If the domain is in a state where it can be restored, the Manage Domain page in the Registrar section of dash will display a message indicating the domain is restorable. You will then will be able to initiate the restore process directly from the dashboard.


***

## Domain restoration

### Which domains are eligible to be restored?

Domains that are in the Redemption Period and have an EPP status of redemptionPeriod may be restored. For most TLDs this will include domains that are between 40 and 70 days past expiration.

Currently `.uk` domains cannot be restored using this process. We are working on an alternative process for `.uk` domains and will provide additional information at a later date.

### â€‹â€‹Is there a fee to restore a domain?

Yes, in most cases there is a restore fee.

The amount varies depending on the TLD. The restore fee is separate from the renewal fee. You will be presented with both the restore and renewal fees before confirming they wish to proceed.

### Will the domain be renewed after the restore has completed?

Yes. We will attempt to renew the domain after the restore has been completed. While not common, it is possible for the renewal transaction to fail.

In the event of a failure, we will make several retry attempts. If we are unable to process the renewal after several retries, you will be presented with a message that you should contact support for assistance.

### â€‹â€‹How long does the restore process take?

The entire process can take a few minutes to complete.

There are multiple steps to the restore process, and each step must be completed in a specific sequence. These steps are performed automatically by the system. The UI will continue to poll for an updated status and will provide feedback as each step completes.

### What happens if the domain renewal fails?

The restore and the renewal are two distinct processes that happen sequentially.

In rare cases the domain may be successfully restored but the renewal fails. We will make several attempts to renew the domain. However, should all the renewals fail the customer may attempt to manually renew the domain or contact support so we may investigate the cause of the failure.

### â€‹â€‹Can a restore be reversed or refunded?

No. Once a restore has been completed it can not be reversed. It may be possible to delete the domain again but there are no refunds.

:::note[Note]


Domain names should be released after a period of 75 days, although the exact deletion timeline is ultimately determined by the domain's registry. You should monitor the domain status to ascertain when it will become available for registration once again.


:::



***

## Domain deletions


### Why am I unable to delete my Registrar domain?

A domain can only be deleted if all the following conditions are met:

* The user initiating the action is a Super Admin or Read/Write Administrator.
* The domain is not delete locked at the registry with either `clientDeleteProhibited` or `serverDeleteProhibited`.
* The domain is not already in `pendingDelete`, `redemptionPeriod`, or in `pendingTransfer`.
* The domain has not been administratively locked by Cloudflare.  This typically occurs for legal reasons such as a UDRP filing or court order, but may also be the result of an abuse or payment investigation.
* The domain is NOT a .UK domain. .UK domains currently cannot be deleted at the registry.

If any of the above conditions are not met, the domain cannot be deleted.

### â€‹â€‹Who has permission to delete a domain registration?


Only Super Admins and Administrators with Read/Write access can initiate the deletion of a domain.  Note that only Super Admins will receive the email with the delete token.


### Will I receive a refund for my deleted domain registration?

No. Refunds will not be issued for costs incurred by a domain registration.

### How do I get the domain deletion token?

The delete token is only sent to the Super Admins of the account. If the user requesting the deletion is not a Super Admin they will need to obtain the delete token from one of the Super Admins of the account.

### â€‹â€‹How long is the domain deletion token valid for?

The delete token is valid for 30 minutes. After the 30 minutes the code will expire and the user must restart the process.

### Will the domain be deleted immediately from my account?

If the domain is within 5 days of the initial registration, the domain will be immediately released by the registry and made available for re-registration. In this scenario the domain will be immediately removed from the registrar section of the account. You may need to refresh the page to force an update of the data.

If the domain is more than 5 days old, it will enter the redemption period and will remain in account until the redemption period expires and the registry releases the domain.




***

## Billing


### How much does Cloudflare Registrar cost?

Refer to [What is Cloudflare Registrar](https://www.cloudflare.com/learning/dns/what-is-cloudflare-registrar/) for more information on pricing.


### When will I be billed?

You will be billed when you input your authorization code and initiate the transfer of your domain to Cloudflare. Currently, Cloudflare Registrar only uses the primary payment method for any associated transaction. Make sure to copy and paste the code to avoid mistakes. The transfer will not initiate if the code is incorrect.


### â€‹â€‹Is there a fee to transfer a .UK domain?

No, there is no fee to transfer a `.uk` domain. Also, an additional year is NOT added during the transfer process. However, if the domain is nearing the expiration date and is set to auto-renew, it may be automatically renewed shortly after the completion of the transfer.

---

# Overview

URL: https://developers.cloudflare.com/registrar/

import { Description, Feature, Plan, RelatedProduct } from "~/components"

<Description>

Buy and renew your domain at cost with Cloudflare Registrar.
</Description>

<Plan type="all" />

Buy and manage your domain with Cloudflare Registrar, and add an additional layer of security to your DNS records for free. Cloudflare Registrar also offers redacted WHOIS information by default and will only charge you what is paid to the registry for your domain. No markup. No surprise fees.

***

## Features

<Feature header="DNSSEC" href="/registrar/get-started/enable-dnssec/">
Cloudflare Registrar offers one-click DNSSEC activation. DNSSEC secures DNS records with cryptographic signatures, and is free to all Cloudflare customers.
</Feature>

<Feature header="Buy domains at cost" href="/registrar/get-started/register-domain/" cta="Buy domains at cost">
Buy and renew domains through Cloudflare Registrar at cost, without markup fees. You only pay what is charged by [registries and ICANN](https://www.cloudflare.com/products/registrar/).
</Feature>

***

## Related products

<RelatedProduct header="DNS" href="/dns/" product="dns">
When you use Cloudflare DNS, all DNS queries for your domain are answered by Cloudflareâ€™s global anycast network. This network delivers performance and global availability.
</RelatedProduct>

---

# Changelog

URL: https://developers.cloudflare.com/rules/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/rules.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Troubleshoot failed domain transfers

URL: https://developers.cloudflare.com/registrar/troubleshooting/

After you start the transfer process to Cloudflare Registrar, your previous registrar has five days to release the domain after a successful transfer request. If your transfer has not been completed within that time frame, something has likely gone wrong.

Most issues with a stalled transfer can be solved by checking the following details and [restarting the transfer](#restart-your-transfer).

## Registrar lock reapplied

You have reapplied the registrar lock at your current registrar since requesting the transfer. You will need to remove it again to restart the transfer process.

## Transfer rejected

Your transfer has been rejected by your previous registrar. There are several reasons for this to happen:

* You actively rejected the transfer request in the email you received from your registrar or on your registrarâ€™s interface.
* Your registrar determined the domain is not eligible for transfer.
* Some registrars allow customers to enable a setting to reject all transfer requests.
* In some instances, registrars may reject the transfer if they suspect malicious behavior.

You will need to restart the transfer and approve the request or contact your current registrar to solve this issue.

## Auth code invalid

Your auth code (also referred to as authentication code and authorization code) has since changed or been deprecated, and Cloudflare cannot complete the transfer. Confirm the code with your current registrar again. To avoid mistakes, copy and paste the auth code provided by your current registrar.

## WHOIS Guard / privacy protection

Some registrars may prohibit transfer requests if you have WHOIS privacy services enabled. You need to first disable those services at your current registrar before you can proceed with the transfer process.

## Restart your transfer

:::note

This solution does not apply to `.uk` domains.
:::

1. Log in to [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Select **Domain Registration** > **Manage Domains**.
3. Find the correct domain and select **Manage**.
4. Select **Cancel Transfer and Retry**. After you initiate the retry, you must re-enter your auth code and confirm your WHOIS information.

---

# Examples

URL: https://developers.cloudflare.com/rules/examples/

import { ListExamples, GlossaryTooltip } from "~/components";

Explore the following <GlossaryTooltip term="code example">examples</GlossaryTooltip> for Rules.

<ListExamples
	filters={["products", "goal", "operation"]}
	additionalProducts={["Cache Rules"]}
/>

---

# Cloudflare Rules

URL: https://developers.cloudflare.com/rules/

import {
	CardGrid,
	Feature,
	LinkTitleCard,
	Plan,
	RelatedProduct,
	Render,
} from "~/components";

<Plan type="all" />

<Render file="rules-definition" />

Rules features require that you [proxy the DNS records](/dns/proxy-status/) of your domain (or subdomain) through Cloudflare.

---

## Features

<Feature header="Configuration Rules" href="/rules/configuration-rules/">
	Customize Cloudflare configuration settings for matching incoming requests.
</Feature>

<Feature header="Snippets" href="/rules/snippets/">
	Customize the behavior of your website or application using short pieces of
	JavaScript code.
</Feature>

<Feature header="Transform Rules" href="/rules/transform/">
	Adjust the URI path, query string, and HTTP headers of requests and responses
	on the Cloudflare global network.
</Feature>

<Feature header="Redirects" href="/rules/url-forwarding/">
	Redirect visitors from a source URL to a target URL with a specific HTTP
	status code. Use Single Redirects or Bulk Redirects depending on your use
	case.
</Feature>

<Feature header="Origin Rules" href="/rules/origin-rules/">
	Customize where the incoming traffic will go and with which parameters.
	Override request properties such as `Host` header, destination hostname, and
	destination port.
</Feature>

<Feature header="Cloud Connector" href="/rules/cloud-connector/">
	Route matching incoming traffic from your website to a public cloud provider
	such as AWS, Google Cloud, and Azure.
</Feature>

<Feature header="Compression Rules" href="/rules/compression-rules/">
	Customize the compression applied to responses from Cloudflare's global
	network to your website visitors, based on the file extension and content
	type.
</Feature>

<Feature header="Page Rules" href="/rules/page-rules/" cta="Use Page Rules">
	Trigger certain actions when a request matches a URL pattern.
</Feature>

<Feature
	header="URL normalization"
	href="/rules/normalization/"
	cta="Configure URL normalization"
>
	Modify the URLs of incoming requests so that they conform to a consistent
	formatting standard.
</Feature>

<Feature
	header="Custom Errors"
	href="/rules/custom-errors/"
	cta="Configure Custom Errors"
>
	Define what custom content to serve for errors returned by an origin server
	or by a Cloudflare product, including Workers.
</Feature>

---

## Related products

<RelatedProduct header="Custom rules" href="/waf/custom-rules/" product="waf">
	Control incoming traffic by filtering requests to a zone. You can block or
	challenge incoming requests according to rules you define.
</RelatedProduct>

<RelatedProduct
	header="Rate limiting rules"
	href="/waf/rate-limiting-rules/"
	product="waf"
>
	Define rate limits for requests matching an expression, and the action to
	perform when those rate limits are reached.
</RelatedProduct>

<RelatedProduct
	header="Cache rules"
	href="/cache/how-to/cache-rules/"
	product="cache"
>
	Customize the cache properties of your HTTP requests.
</RelatedProduct>

<RelatedProduct header="Workers" href="/workers/" product="workers">
	Cloudflare Workers provides a serverless execution environment that allows you
	to create new applications or augment existing ones without configuring or
	maintaining infrastructure.
</RelatedProduct>

---

## More resources

<CardGrid>

<LinkTitleCard
	title="Plans"
	href="https://www.cloudflare.com/plans/#overview"
	icon="document"
>
	Compare available Cloudflare plans
</LinkTitleCard>

</CardGrid>

---

# Changelog

URL: https://developers.cloudflare.com/spectrum/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/spectrum.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Cloudflare Ruleset Engine

URL: https://developers.cloudflare.com/ruleset-engine/

import { LinkButton } from "~/components";

The Cloudflare Ruleset Engine allows you to create and deploy rules and rulesets in different Cloudflare products using the same basic syntax.

## Main features

- **Powerful syntax**: Rule expressions use a powerful Rules language similar to the wirefilter syntax that allows you to create complex rules.
- **High-performance rule evaluation**: Allows you to have many rules in different Cloudflare products with almost no impact on performance.
- **Engine powering different Cloudflare products**: Cloudflare keeps building products on top of the Ruleset Engine, which means that you can use the same API methods for configuring different products, with the same customization possibilities. Additionally, the Ruleset Engine supports the different phases of the request life cycle at Cloudflare.

## Availability

The Ruleset Engine supports different Cloudflare products. Check each product's documentation for availability details.

---

# Get started

URL: https://developers.cloudflare.com/spectrum/get-started/

import { Details, Render } from "~/components"

Spectrum is available on all paid plans. Pro and Business support selected protocols only, whereas Enterprise supports all TCP and UDP based traffic. Refer to [Configuration options](/spectrum/reference/configuration-options/) for more configuration details.

To create a Spectrum application, you can either use an IP address, a CNAME Record or a load balancer. Independently of the method you use, you can create the application through the dashboard or via [API](/api/resources/spectrum/subresources/apps/methods/list/).

Certain fields in Spectrum request and response bodies require an Enterprise plan. Refer to the [Settings by plan](/spectrum/reference/settings-by-plan/) page for more details.

## Create a Spectrum application using an IP address

To create a Spectrum application using an IP address, Cloudflare normally assigns you an arbitrary IP from Cloudflareâ€™s IP pool to your application. If you want to use your own IP addresses, you can use [BYOIP](/spectrum/about/byoip/) or you can also use a [Static IP](/spectrum/about/static-ip/). In these two last cases, you need to create your Spectrum application through the API, as these features are not available via dash. When using the API, the field `origin_direct` takes as input the IP address.


<Details header="Add your application via Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login).
2. Select **Spectrum**.
3. Select **Create an Application**. If this is your first time using Spectrum, the **Create an Application** modal appears.
4. Select your **Application Type**.
5. Under **Domain**, enter the domain that will use Spectrum.
6. Under **Edge Port**, enter the port Cloudflare should use for your application.
7. Under **Origin**, enter your application's origin IP and port.
8. If your application requires the client IP and supports [Proxy Protocol](https://www.haproxy.com/blog/haproxy/proxy-protocol/), enable **Proxy Protocols**. Proxy Protocol is a method for a proxy like Cloudflare to send the client IP to the origin application.
9. Select **Add**.


</Details>


<Details header="Add your application via API">

Below is a curl example and the associated data being posted to the API.

**API example:**

```bash
curl "https://api.cloudflare.com/client/v4/zones/{zone_id}/spectrum/apps" \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{"protocol":"tcp/22","dns":{"type":"CNAME","name":"ssh.example.com"},"origin_direct":["tcp://192.0.2.1:22"],"proxy_protocol":"off","ip_firewall":true,"tls":"full","edge_ips":{"type":"dynamic","connectivity":"all"},"traffic_type":"direct","argo_smart_routing":true}'
```

**Example data:**

```json
{
  "success": true,
  "errors": [],
  "messages": [],
  "result": {
    "id": "ea95132c15732412d22c1476fa83f27a",
    "protocol": "tcp/22",
    "dns": {
      "type": "CNAME",
      "name": "ssh.example.com"
    },
    "origin_direct": [
      "tcp://192.0.2.1:22"
    ],
    "proxy_protocol": "off",
    "ip_firewall": true,
    "tls": "full",
    "edge_ips": {
      "type": "dynamic",
      "connectivity": "all"
    },
    "traffic_type": "direct",
    "argo_smart_routing": true,
    "created_on": "2014-01-02T02:20:00Z",
    "modified_on": "2014-01-02T02:20:00Z"
  }
}
```


</Details>

## Create a Spectrum application using a CNAME record

To create a Spectrum application using a CNAME record, you will need to create a [CNAME record](https://www.cloudflare.com/learning/dns/dns-records/dns-cname-record/) on your Cloudflare hosted zone that points to your origin's hostname. This is required to resolve to your hostname origin. Refer to [Create DNS records](/dns/manage-dns-records/how-to/create-dns-records/#create-dns-records), for more information. When using a CNAME as an origin, note that Cloudflare needs to be authoritative for that zone. When using the API, the `origin_dns` field takes as input the CNAME record.


<Details header="Add your application via Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login).
2. Select **Spectrum**.
3. Select **Create an Application**. If this is your first time using Spectrum, the **Create an Application** modal appears.
4. Select your **Application Type**.
5. Under **Domain**, enter the domain that will use Spectrum.
6. Under **Edge Port**, enter the port Cloudflare should use for your application.
7. Under **Origin**, enter your `CNAME` record name.
8. Select **Add**.


</Details>


<Details header="Add your application via API">

Below is a curl example and the associated data being posted to the API.

**API example:**

```bash
curl 'https://api.cloudflare.com/client/v4/zones/{zone_id}/spectrum/apps' \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{"dns":{"type":"CNAME","name":"spectrum-cname.example.com"},"ip_firewall":false,"protocol":"tcp/22","proxy_protocol":"off","tls":"off","origin_dns": {"name": "cname-to-origin.example.com", "ttl": 1200}, "origin_port": 22}'
```

**Example data:**

```json
{
  "dns": {
    "type": "CNAME",
    "name": "spectrum-cname.example.com"
  },
  "ip_firewall": false,
  "protocol": "tcp/22",
  "proxy_protocol": "off",
  "tls": "off",
  "origin_dns": {
    "name": "cname-to-origin.example.com",
    "ttl": 1200
  },
  "origin_port": 22
}
```


</Details>

## Create a Spectrum application using a load balancer

To create a Spectrum application using a load balancer, you will need to generate a load balancer from the dashboard or via the API. Refer to the [Load Balancing documentation](/load-balancing/additional-options/spectrum/#1-configure-your-load-balancer) for more details.

:::note


To prevent issues with DNS resolution for a Spectrum application, do not use the same Spectrum hostname as a current Load Balancing hostname.


:::


<Details header="Add your application via Dashboard">

<Render file="spectrum-with-load-balancer-dash" />


</Details>


<Details header="Add your application via API">

<Render file="spectrum-with-load-balancer-api" />


</Details>

## View traffic

You can now proxy traffic through Cloudflare without additional configuration. As you run traffic through Cloudflare, you will see the last minute of traffic from **Spectrum** in the dashboard.

If you have any feedback, please [let us know](https://community.cloudflare.com/c/website-application-performance/spectrum/48).

---

# Glossary

URL: https://developers.cloudflare.com/spectrum/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Spectrum documentation.

<Glossary product="spectrum" />

---

# Cloudflare Spectrum

URL: https://developers.cloudflare.com/spectrum/

import {
	CardGrid,
	Description,
	Feature,
	GlossaryTooltip,
	LinkTitleCard,
	Plan,
	RelatedProduct,
} from "~/components";

<Description>

Spectrum provides security and acceleration for any [TCP](https://www.cloudflare.com/learning/ddos/glossary/tcp-ip/) or [UDP](https://www.cloudflare.com/learning/ddos/glossary/user-datagram-protocol-udp/) based application.

</Description>

<Plan type="paid" />

Spectrum allows you to route MQTT, email, file transfer, version control, games, and more over TCP or UDP through Cloudflare to mask the origin and protect it from [DDoS attacks](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/).

---

## Features

<Feature
	header="Enable Proxy protocol"
	href="/spectrum/how-to/enable-proxy-protocol/"
>
	Use a proxy protocol for Cloudflare to pass on the client IP to your service.
</Feature>

<Feature
	header="DDoS Protection for Spectrum"
	href="/spectrum/about/ddos-for-spectrum/"
>
	Learn more about what L3/4 DDoS Protection is included as part of the Spectrum
	service.
</Feature>

---

## Related products

<RelatedProduct
	header="DDoS Protection"
	href="/ddos-protection/"
	product="ddos-protection"
>
	Cloudflare DDoS protection secures websites, applications, and entire networks
	while ensuring the performance of legitimate traffic is not compromised.
</RelatedProduct>

<RelatedProduct header="BYOIP" href="/byoip/" product="byoip">
	With Bringing Your Own IPs (BYOIP), Cloudflare announces your IPs in all our
	locations.
</RelatedProduct>

<RelatedProduct
	header="Load Balancing"
	href="/load-balancing/"
	product="load-balancing"
>
	Cloudflare Load Balancing distributes traffic across your{" "}
	<GlossaryTooltip term="endpoint">endpoints</GlossaryTooltip>, which reduces
	endpoint strain and latency and improves the experience for end users.
</RelatedProduct>

<RelatedProduct header="DNS" href="/dns/" product="dns">
	Cloudflare's global DNS platform provides speed and resilience. DNS customers
	also benefit from free DNSSEC, and protection against route leaks and
	hijacking.
</RelatedProduct>

---

## More resources

<CardGrid>

<LinkTitleCard
	title="Plans"
	href="https://www.cloudflare.com/plans/"
	icon="document"
>
	Compare available Cloudflare plans.
</LinkTitleCard>

</CardGrid>

---

# Security reports

URL: https://developers.cloudflare.com/security-center/app-security-reports/

:::note

Currently, this feature is only available to Enterprise customers.
:::

Application security reports provide visibility into requests blocked or challenged by the Cloudflare Application Security suite of products.

These reports allow you to get insights and analyze trends for all the zones in your account on a monthly basis, covering the mitigation actions performed by all Cloudflare layer 7 (application layer) security products. Each report includes an overview section and a per-product breakdown.

Cloudflare automatically generates a report every month, usually within the first five days of the month.

To dive deeper into the mitigations performed by Cloudflare security products, use the [Security Analytics](/waf/analytics/security-analytics/) dashboard.

## Download a report

To download a monthly application security report:

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to **Security Center** > **Security Reports**.
3. For a given month and year, select **Download** to download the report for that particular month.

:::caution

Due to limitations in the current reporting solution, some customers do not have access to reports from the past few months. We are working on a new version of app security reports without the current limitations.
:::

***

## Required roles

A Cloudflare user must have one of the following [roles](/fundamentals/setup/manage-members/roles/) to download application security reports:

* Super Administrator
* Administrator

## Number of mitigated requests

As of the April 2023 report, the number of mitigated requests in each report is a sum of the following requests:

* Blocked requests
* Challenged requests that were not solved or bypassed (that is, not issued again because the visitor had previously passed a similar challenge)

---

# Protocols per plan

URL: https://developers.cloudflare.com/spectrum/protocols-per-plan/

import { FeatureTable } from "~/components"

On this table, you have information about which protocols are available per plan.

<FeatureTable id="spectrum.spectrum" />

---

# Blocked Content

URL: https://developers.cloudflare.com/security-center/blocked-content/

If your domain has content that has been blocked, Blocked Content on the dashboard gives you the ability to request the Trust and Safety team to remove a block.

To view Blocked Content on the dashboard:

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **Security Center** > **Blocked Content**.

:::note
You must have Admin, Super Admin, or Trust and Safety [role](/fundamentals/setup/manage-members/roles/) to access Blocked Content.
:::

The Security Center dashboard displays three statuses for blocked content: active, pending, or resolved blocks.

## Active blocks

An active block is a block that is in effect on blocking content.

When you select **Request Review**, the status changes to **In Review**, and the block will be reviewed by the Trust and Safety team.

## Pending blocks

A pending block represents a blocking action Cloudflare will take at the scheduled time.

You can view all your pending blocks by selecting **Pending** on the dashboard. Selecting **Request Review** cancels the pending delayed action. This means that the block will not be placed.

## Resolved blocks

Resolved blocks list your recently resolved blocks. Resolved blocks are limited to 30 days of recently resolved blocks. Resolved blocks require no action. You can only sort and/or filter the list.

---

# Brand Protection

URL: https://developers.cloudflare.com/security-center/brand-protection/

import { AvailableNotifications, Render } from "~/components"

:::note[User permission]
While the Brand Protection tool is in beta, you will need to request access by filling in the [sign-up form](http://cloudflare.com/lp/brandprotection). Only Super Admin and users with a Brand Protection role can access Brand Protection. 
:::

<Render file="brand-protection-definition" />

## Domain search

To start searching for new domains that might be trying to impersonate your brand:

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.

2. Go to **Security Center** > **Brand Protection**.

3. In **String query**, provide a name for your query. You can add multiple brand phrases on the same query, and the results will generate matches for all of those. Once you entered the string queries, select **Search matches**.

4. In the **Character distance**, select from `0-3`. The number of characters the results can differ from your domain.

   :::note

   If a brand phrase or search term has less than five characters, you can only choose a max distance of `0` (zero).
   :::

5. You can select **Save query** to monitor it in the future and perform other actions, such as delete, clone and set up alerts, according to your Paid plan limits.

6. To export all matches from a saved query, select your **Query name** > select the three dots > **Export matches**.

In the section **Monitor Strings**, you can check all the string queries that you selected to monitor. You can delete, clone, or create notifications for a string query. Refer to [Brand Protection Alerts](#brand-protection-alerts) to set up notifications.

## Logo queries

To set up a new logo query:

1. Go to **Security Center** > **Monitor Logos** and select **Add logo**.
2. Add a name for your query and upload your logo. Only the `.png`, `.jpeg`, and `.jpg` file extensions are supported.
3. Select **Save logo**.

The browser will return to the **Monitor Images** overview page, where you can access your query and configure notifications.

## Investigate a query

In this section, the dashboard displays:

- **Domain overview**.
- **WHOIS** that provides details about the date the domain was created, registrant and nameservers.
- **Domain history** that provides information on the domain category and when it was last changed. Refer to [Investigate threats](/security-center/investigate/investigate-threats/) for more details.
- **URL Reports** that provides information on any reported URL.

To investigate a string query:

1. Go to the **Monitor Strings** or **Monitor Logos** section to view all your queries.
2. Select a monitored query to inspect all the domains that matched your query.
3. Next to the domain, select **Domain** or **URL**. This will trigger a search on the [**Investigate**](/security-center/investigate/) section in a separate tab. URL scanner will also be triggered from **Brand Protection** through **Security Center** > **Investigate**. You will also have access to a report which will be generated automatically. The report will display screenshots of the matched domain, and the registrar of your domain.

## Brand Protection Alerts

<Render file="brand-protection-alerts" />

<AvailableNotifications product="Security Center" />

To set up a Brand Protection Alert:

1. Go to **Monitor Strings** and locate the query for which you would like to create notifications.

2. Select **alerts**. This should redirect you to the **Add Notification** page, where you can configure what you want to be notified about, and how.

   :::note

   You can also set up the alerts from your [Notifications](/notifications/) menu.
   :::

3. Create a notification name, add a description (optional), and select the monitored queries. You can also add a Webhook, and a notification email. You can add multiple email addresses.

4. Select **Save**.

Manage your notifications in the **All notifications** tab. You can disable, edit, delete, or test them.

## Limitations

* While this product is in beta, all Enterprise customers and Cloudforce One subscribers have access to Brand Protection. Enterprise customers are entitled to one saved query per Enterprise zone on their account.
* You may only use the Brand Protection search tools to search for domains that may be attempting to impersonate your brand or a brand that has authorized you to conduct such search on its behalf.

---

# Changelog

URL: https://developers.cloudflare.com/security-center/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/security-center.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Overview

URL: https://developers.cloudflare.com/security-center/

import { LinkButton, Render } from "~/components"

<Render file="security-center-definition" />

## Main features

<Render file="main-features" />

 <LinkButton variant="primary" href="/security-center/get-started/">Get started</LinkButton>


***

## Availability

Cloudflare Security Center is available to customers on all plans. If you have any comments, questions, or bugs to report, create a post in the [Cloudflare Community forum](https://community.cloudflare.com/c/security/security-center/65).

The frequency of security scans depends on your Cloudflare plan. Refer to [Scan frequency](/security-center/security-insights/how-it-works/#scan-frequency) for more information.

## Limitations

* Users with an [Administrator Read Only](/fundamentals/setup/manage-members/roles/#account-scoped-roles) role cannot access the Cloudflare Security Center.
* Only Cloudflare accounts with at least one Business or Enterprise zone, or accounts on the Teams Standard or Teams Enterprise plans, can manually start a new scan.

---

# Custom Indicator Feeds

URL: https://developers.cloudflare.com/security-center/indicator-feeds/

import { Render } from "~/components";

Cloudflare's threat intelligence team crowdsources attack trends and protects users automatically, such as from zero-day vulnerabilities like the [HTTP/2 Rapid Reset attack](https://blog.cloudflare.com/technical-breakdown-http2-rapid-reset-ddos-attack/). However, in some cases, Cloudflare will partner with external entities that have their own feeds which can be shared with eligible Cloudflare users.

With Custom Indicator Feeds, Cloudflare provides a threat intelligence feed based on data received from various Cyber Defense Collaboration groups. The security filtering capabilities are available to eligible public and private sector organizations.

## Publicly available feeds

Cloudflare provides some feeds to Gateway users without the need to establish a provider relationship.

| Name                                                                                                                                      | Description                                                                                                                                                                         | Availability                              |
| ----------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- |
| [Treasury Early Indicator Feed](https://www.cloudflare.com/press-releases/2024/us-department-of-treasury-pnnl-finserv-threat-intel-feed/) | Threat data for financial institutions provided by the US Department of Treasury and Pacific Northwest National Laboratory (PNNL). For more information, contact your account team. | Approved financial services organizations |
| [UK NCSC Public Threat Indicators](https://www.ncsc.gov.uk/information/pdns)                                                              | Recursive DNS service supplied by the UK National Cyber Security Centre (NCSC) to block DNS-based malware.                                                                          | All users                                 |
| Cloudforce One - Public Feed                                                                                                              | Feed of indicators.                                                                                                                                                                 | All users                                          |

## Get started

<Render file="indicator-feeds-overview" />

If your organization is interested in becoming a provider or a subscriber, contact your account team.

### Create a Custom Indicator Feed

Providers can create and manage a Custom Indicator Feed with the [Custom Indicator Feeds API endpoints](/api/resources/intel/subresources/indicator_feeds/methods/list/):

1. Contact your account team to configure your account as an indicator feed provider.
2. Create a feed with the [Create new indicator feed endpoint](/api/resources/intel/subresources/indicator_feeds/methods/create/). Make note of the `feed_id` generated for your feed. For example:

   ```bash title="Create new indicator feed"
   curl "https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/intel/indicator-feeds" \
   	--header 'Content-Type: application/json' \
   	--header 'X-Auth-Email: <EMAIL>' \
   	--header 'X-Auth-Key: <API_KEY>' \
   	--data '{
   	"description": "Custom indicator feed to detect threats",
   	"name": "threat_indicator_feed"
   }'
   ```

   ```json output {3}
   {
   	"result": {
   		"id": 10,
   		"name": "threat_indicator_feed",
   		"description": "Custom indicator feed to detect threats",
   		"created_on": "2024-09-17T21:16:09.412Z",
   		"modified_on": "2024-09-17T21:16:09.412Z"
   	},
   	"success": true,
   	"errors": [],
   	"messages": []
   }
   ```

3. Upload data to the feed with the [Update indicator feed data endpoint](/api/resources/intel/subresources/indicator_feeds/subresources/snapshots/methods/update/). Uploaded indicator data must be in a [`.stix2`](https://oasis-open.github.io/cti-documentation/stix/intro) formatted file.

   ```bash title="Update indicator feed data"
   curl --request PUT \
   	"https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/intel/indicator-feeds/<FEED_ID>/snapshot" \
   	--header 'Content-Type: multipart/form-data' \
   	--header 'X-Auth-Email: <EMAIL>' \
   	--header 'X-Auth-Key: <API_KEY>' \
   	--form 'source=@/path/to/file'
   ```

   ```json output
   {
   	"result": {
   		"file_id": 1,
   		"filename": "snapshot_file.unified",
   		"status": "unified"
   	},
   	"errors": [],
   	"messages": [],
   	"success": true
   }
   ```

   :::note
   Indicator feeds use a snapshot system. To update feeds with new data, providers must upload a file containing all previous and new indicators.
   :::

4. (Optional) Verify the status of your feed upload with the [Get indicator feed data endpoint](/api/resources/intel/subresources/indicator_feeds/methods/data/). For example:

   ```bash title="Get indicator feed data"
   curl --request GET \
   	"https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/intel/indicator-feeds/<FEED_ID>/data" \
   	--header 'Content-Type: application/json' \
   	--header 'X-Auth-Email: <EMAIL>' \
   	--header 'X-Auth-Key: <API_KEY>'
   ```

   ```json output {8}
   {
   	"result": {
   		"id": 10,
   		"name": "threat_indicator_feed",
   		"description": "Custom indicator feed to detect threats",
   		"created_on": "2023-08-01T18:00:26.65715Z",
   		"modified_on": "2023-08-01T18:00:26.65715Z",
   		"latest_upload_status": "Complete"
   	},
   	"success": true,
   	"errors": [],
   	"messages": []
   }
   ```

5. Grant access to subscribers with the [Grant permission to indicator feed endpoint](/api/resources/intel/subresources/indicator_feeds/subresources/permissions/methods/create/). You can add subscribers to the feed's allowed subscribers list using their [account IDs](/fundamentals/setup/find-account-and-zone-ids/). For example:

   ```bash title="Update indicator feed data"
   curl --request PUT \
   	"https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/intel/indicator-feeds/<FEED_ID>/snapshot" \
   	--header 'Content-Type: multipart/form-data' \
   	--header 'X-Auth-Email: <EMAIL>' \
   	--header 'X-Auth-Key: <API_KEY>' \
   	--data '{
   	"account_tag": "823f45f16fd2f7e21e1e054aga4d2859",
   	"feed_id": 10
   }'
   ```

### Use a feed in Gateway

Once an account is granted access to a feed, it will be available to match traffic as a [selector in Gateway DNS policies](/cloudflare-one/policies/gateway/dns-policies/#indicator-feeds).

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Gateway** > **Firewall policies**. Select **DNS**.
2. To create a new DNS policy, select **Add a policy**.
3. Name your policy.
4. In **Traffic**, add a condition with the **Indicator Feeds** selector. If your account has been granted access to a Custom Indicator Feed, Gateway will list the feed in **Value**. For example, you can block sites that appear in a feed:

   | Selector        | Operator | Value               | Action |
   | --------------- | -------- | ------------------- | ------ |
   | Indicator Feeds | in       | _Threat Intel Feed_ | Block  |

5. Select **Create policy**.

For more information on creating Gateway policies, refer to [DNS policies](/cloudflare-one/policies/gateway/dns-policies/).

---

# Get started

URL: https://developers.cloudflare.com/security-center/get-started/

import { Render } from "~/components"

This guide covers the steps you need to take to set up Security Center in your Cloudflare account for the first time.

## Prerequisites

* A Cloudflare account
* At least one zone onboarded to Cloudflare

## Enable Security Insights and start initial scan

<Render file="setup" />

### Start a new scan

To manually start a scan:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to Account Home > **Security Center** > **Infrastructure**.
3. Select **Scan now**.

### Scan Frequency

Once you enable Security Insights, Cloudflare performs scans at a [regular frequency](/security-center/security-insights/how-it-works/#scan-frequency), according to your Cloudflare plan.

---

# Aggregated Internet Measurement

URL: https://developers.cloudflare.com/speed/aim/

Aggregated Internet Measurement (AIM) helps you understand your Internet quality to identify scenarios that your Internet connection is good or bad for. Typically, an Internet speed test provides you with upload and download speeds, which may not always provide a holistic view of your Internet quality.

AIM uses a scoring rubric that assigns point values based on speed tests to help you understand how your Internet quality will perform for streaming, gaming, and webchat/real-time communication (RTC).

## Scoring Rubric

AIM analyzes the following metrics to generate your score:

* Latency
* Packet Loss
* Download
* Upload
* Loaded Latency
* Jitter

After the test is run and a point value is assigned to each metric, the points are translated to a network score for streaming, gaming, and webchat/RTC.  These scores will indicate how good your Internet is in each of these scenarios.

The possible network scores are:

* Bad
* Poor
* Average
* Good
* Great

## Improve your network score

You have a few options to help improve network scores.

* **Switch to a wired connection.** When possible, switch to a wired connection instead of wireless to avoid performance issues due to radio interference and signal strength.
* **Move closer to your router.** If you are unable to use a wired connection, try to move closer to your wireless router. Signal strength drops as you move away from your wireless router and a weaker signal means poorer connectivity. Keep in mind that any objects or materials between you and your wireless router can also have a negative impact on signal strength.
* **Upgrade your router.** Ensure you are using a router capable of handling smarter queueing with hardware that will not fall over under load.
* **Contact your ISP.** If youâ€™re using a wired connection or have a good connection to your wireless router and are still seeing issues, you may have issues with your Internet connection and should reach out to your ISP.

---

# Glossary

URL: https://developers.cloudflare.com/speed/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Speed documentation.

<Glossary product="speed" />

---

# Speed

URL: https://developers.cloudflare.com/speed/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Description>

Improve the performance of your website or web application.
</Description>

<Plan type="all" />

Speed allows you to assess the performance of your website and get recommendations of Cloudflare products to enhance the website performance.

***

## Features

<Feature header="Observatory" href="/speed/speed-test/">

Use Observatory to conduct tests with both synthetic and real user data to identify potential website performance enhancements.


</Feature>

<Feature header="Optimization" href="/speed/optimization/">

Get recommendations of Cloudflare products and settings to improve your websiteâ€™s performance.


</Feature>

<Feature header="Aggregated Internet Measurement" href="/speed/aim/">

Understand your Internet quality to identify scenarios that your Internet connection is good or bad for.


</Feature>

***

## Related products

<RelatedProduct header="Cache rules" href="/cache/how-to/cache-rules/" product="cache">
Customize the cache properties of your HTTP requests.
</RelatedProduct>

<RelatedProduct header="Cloudflare Web Analytics" href="/web-analytics/" product="analytics">
Understand the performance of your webpages as experienced by your site visitors.
</RelatedProduct>

<RelatedProduct header="Cloudflare Image Resizing" href="/images/transform-images/" product="images">
Transform images on Cloudflare's edge platform: resize, adjust quality, and convert images to WebP or AVIF format on demand.
</RelatedProduct>

<RelatedProduct header="Early Hints" href="/cache/advanced-configuration/early-hints/" product="cache">
Take advantage of "server think time" to asynchronously send instructions to the browser to begin loading resources while the origin server is compiling the full response.
</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Quotas" href="/speed/speed-test/run-speed-test/#quotas" icon="document">

Learn about the quota limits for the number of tests you can run per month.
</LinkTitleCard>

<LinkTitleCard title="Community Forum" href="https://community.cloudflare.com/c/website-application-performance/88" icon="open-book">

Engage with other users and explore more resources on Cloudflare support forum.
</LinkTitleCard>

</CardGrid>

---

# Troubleshooting

URL: https://developers.cloudflare.com/speed/troubleshooting/

import { TroubleshootingList } from "~/components"

The following topics are useful for troubleshooting Speed issues.

<TroubleshootingList />

---

# Changelog

URL: https://developers.cloudflare.com/ssl/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/ssl.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Concepts

URL: https://developers.cloudflare.com/ssl/concepts/

import { Render } from "~/components"

This page defines and articulates key concepts that are relevant to Cloudflare SSL/TLS and are used in this documentation. For more concepts and broader descriptions, check out the [Cloudflare Learning Center](https://www.cloudflare.com/learning/ssl/what-is-ssl/).

## SSL/TLS certificate

An SSL/TLS certificate is what enables websites and applications to establish secure connections. With SSL/TLS, a client - such as a browser - can verify the authenticity and integrity of the server it is connecting with, and use encryption to exchange information.

Since [Cloudflare's global network](https://www.cloudflare.com/network/) is at the core of several products and services that Cloudflare offers, what this implies in terms of SSL/TLS is that, instead of only one certificate, there can actually be two certificates involved in a single request: an edge certificate and an origin certificate.

### Edge certificate

The [edge certificates](/ssl/edge-certificates/) are the ones that Cloudflare presents to clients visiting your website or application. You can manage edge certificates through the [Cloudflare Dashboard](https://dash.cloudflare.com/?to=/:account/:zone/ssl-tls/edge-certificates).

```mermaid
flowchart LR
        accTitle: Edge certificate and origin certificate
        accDescr: Diagram showing how edge certificates are positioned between Cloudflare and the browser whereas origin certificates sit between Cloudflare and the origin server.
        A[Browser] <--Edge certificate--> B((Cloudflare))<--Origin certificate--> C[(Origin server)]
```

### Origin certificate

[Origin certificates](/ssl/origin-configuration/) guarantee the security and authentication on the other side of the network, between Cloudflare and the origin server of your website or application. Origin certificates are managed on your origin server.

[SSL/TLS encryption modes](/ssl/origin-configuration/ssl-modes/) control whether and how Cloudflare will use both these ceritifcates, and you can choose between different modes on the [SSL/TLS overview page](https://dash.cloudflare.com/?to=/:account/:zone/ssl-tls).

## Validity period

One common aspect of every SSL/TLS certificate is that they must have a fixed expiration date. If a certificate is expired, clients - such as your visitor's browser - will consider that a secure connection cannot be established, resulting in warnings or errors.

Different [certificate authorities (CAs)](#certificate-authority-ca) support different validity periods. Cloudflare works with them to guarantee that both [Universal](/ssl/edge-certificates/universal-ssl/) and [Advanced](/ssl/edge-certificates/advanced-certificate-manager/) edge certificates are always renewed.

## Certificate authority (CA)

A [certificate authority (CA)](/ssl/reference/certificate-authorities/) is a trusted third party that generates and gives out SSL/TLS certificates. The CA digitally signs the certificates with their own private key, allowing client devices - such as your visitor's browser - to verify that the certificate is trustworthy.

As explained in the article about [what is an ssl certificate](https://www.cloudflare.com/learning/ssl/what-is-an-ssl-certificate/), this means that, besides not being expired, an SSL/TLS certificate should be issued by a certificate authority (CA) in order to avoid warnings or errors.

## Validation level

SSL/TLS certificates vary in terms of the level to which a CA has validated them. As explained in the article about [types of certificates](https://www.cloudflare.com/learning/ssl/types-of-ssl-certificates/), SSL/TLS certificates can be DV (Domain Validated), OV (Organization Validated) or EV (Extended Validation).

<Render file="validation-level-note" /> <br />

## Origin pull

When visitors request content from your website or application, Cloudflare first attempts to [serve content from the cache](https://www.cloudflare.com/learning/cdn/what-is-caching/). If this attempt fails, Cloudflare sends a request back to your origin web server to get the content. This request between Cloudflare and your origin web server is called origin pull.

This relates to the difference between [edge certificates](#edge-certificate) and [origin certificates](#origin-certificate), and also explains why some specifications such as [cipher suites](#cipher-suites) can be set differently depending on whether they refer to the connection between Cloudflare and your visitor's browser or between Cloudflare and your origin server.

## Cipher suites

Besides the authentication and integrity aspects that valid certificates guarantee, the other important aspect of SSL/TLS certificates is encryption. Cipher suites determine the set of algorithms that can be used for encryption/decryption and that will be negotiated during an [SSL/TLS handshake](https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake/).

For the purpose of this documentation, keep in mind that [cipher suites supported at Cloudflare's network](/ssl/edge-certificates/additional-options/cipher-suites/supported-cipher-suites/) may not be the same as [cipher suites presented by Cloudflare to your origin server](/ssl/origin-configuration/cipher-suites/).

## Trust store

The list of [certificate authority (CA)](#certificate-authority-ca) and intermediate certificates that are trusted by operating systems, web browsers or other software that interacts with SSL/TLS certificates is called trust store. Cloudflare maintains its trust store on a public [GitHub repository](https://github.com/cloudflare/cfssl_trust).

While for most cases you do not have to worry about this list or how it is used when a client checks your SSL/TLS certificate, some features such as [Custom Origin Trust Store](/ssl/origin-configuration/custom-origin-trust-store/), and processes such as [bundle methodologies](/ssl/edge-certificates/custom-certificates/bundling-methodologies/), are directly related to it.

## Chain of trust

Depending on your organization requirements, or if you have to troubleshoot an issue with your certificates, for example, you might come across the terms root certificate, intermediate certificate and leaf certificate.

These terms refer to the way in which the certificate presented to a client - the leaf certificate - has to be traceable back to a trusted certificate authority (CA) certificate - the [root certificate](https://en.wikipedia.org/wiki/Root_certificate). This process is structured around a [chain of trust](https://en.wikipedia.org/wiki/Chain_of_trust).

---

# Cloudflare SSL/TLS

URL: https://developers.cloudflare.com/ssl/

import { Description, Feature, Plan, RelatedProduct } from "~/components"

<Description>

Encrypt your web traffic to prevent data theft and other tampering.
</Description>

<Plan type="all" />

Through [Universal SSL](/ssl/edge-certificates/universal-ssl/), Cloudflare is the first Internet performance and security company to offer free SSL/TLS protection.
Cloudflare SSL/TLS also provides a number of other features to meet your encryption requirements and certificate management needs. Refer to [Get started](/ssl/get-started/) for more.

***

## Features

<Feature header="Total TLS" href="/ssl/edge-certificates/additional-options/total-tls/">
Extending the protection offered by Universal SSL, Total TLS is an easy way to automatically issue certificates for all levels of subdomains that you have.
</Feature>

<Feature header="Delegated DCV" href="/ssl/edge-certificates/changing-dcv-method/methods/delegated-dcv/">
Even if you use a different provider for authoritative DNS, you can delegate domain control validation (DCV) to Cloudflare, reducing the need of manual intervention.
</Feature>

<Feature header="Custom TLS settings" href="/ssl/edge-certificates/additional-options/minimum-tls/">
Cloudflare also allows you to specify the minimum TLS version that visitors must use to connect to your website or application, and [restrict cipher suites](/ssl/edge-certificates/additional-options/cipher-suites/customize-cipher-suites/) according to your security requirements.
</Feature>

<br />

Refer to [features and availability](/ssl/reference/all-features/) for a complete list of SSL/TLS features and their availability according to different Cloudflare plans.

***

## Related products

<RelatedProduct header="Cloudflare DNS" href="/dns/" product="dns">
When you use Cloudflare DNS, all DNS queries for your domain are answered by Cloudflare's global anycast network. This network delivers performance and global availability.
</RelatedProduct>

<RelatedProduct header="Cloudflare for SaaS" href="/cloudflare-for-platforms/cloudflare-for-saas/" product="cloudflare-for-platforms">
Cloudflare for SaaS allows you to extend the security and performance benefits of Cloudflare's network to your customers via their own custom or vanity domains.
</RelatedProduct>

---

# Get started

URL: https://developers.cloudflare.com/ssl/get-started/

import { GlossaryDefinition, Render } from "~/components"

Follow the steps below to enable SSL/TLS protection for your application.


## Before you begin


* [Create an account and register an application](/fundamentals/setup/)



## Choose an edge certificate

As explained in the [concepts page](/ssl/concepts/#ssltls-certificate), edge certificates are the SSL/TLS certificates that Cloudflare presents to your visitors.

Cloudflare offers a variety of options for your application's edge certificates:

* [**Universal certificates**](/ssl/edge-certificates/universal-ssl/): <GlossaryDefinition term="Universal SSL certificate" />
* [**Advanced certificates**](/ssl/edge-certificates/advanced-certificate-manager/): <Render file="acm-definition" />
* [**Custom certificates**](/ssl/edge-certificates/custom-certificates/): <Render file="custom-certificates-definition" />
* [**Keyless certificates**](/ssl/keyless-ssl/) (Enterprise only): <Render file="keyless-ssl-definition" />

Refer to [Edge certificates](/ssl/edge-certificates/) for more information on how different certificate types can respond to common use cases.

:::note[For SaaS providers]


<Render file="ssl-for-saas-definition" product="cloudflare-for-platforms" />

For more details, refer to [Cloudflare for SaaS (managed hostnames)](/cloudflare-for-platforms/cloudflare-for-saas/security/certificate-management/).


:::


## Choose your encryption mode


Once you have chosen your edge certificate, [choose an encryption mode](/ssl/origin-configuration/ssl-modes/).

Encryption modes specify how Cloudflare encrypts connections between (a) visitors and Cloudflare, and (b) Cloudflare and your origin server. For more context about this two-part process refer to the [concepts page](/ssl/concepts/#ssltls-certificate).

Note that some encryption modes will require you to have a valid [origin certificate](/ssl/concepts/#origin-certificate), which is managed on your origin server. Each encryption mode setup page lists out this and other requirements and you can also [consider other Cloudflare options to use with your origin server](/ssl/origin-configuration/), such as [Origin CA certificates](/ssl/origin-configuration/origin-ca/).


## Enforce HTTPS connections


<Render file="enforce-https-recommendation" />


## Optional - Enable additional features


<Render file="get-started-additional-features" />

---

# Changelog

URL: https://developers.cloudflare.com/stream/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/stream.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# FAQ

URL: https://developers.cloudflare.com/stream/faq/

import { GlossaryTooltip } from "~/components"

## Stream

### What formats and quality levels are delivered through Cloudflare Stream?

Cloudflare decides on which bitrate, resolution, and codec is best for you. We deliver all videos to industry standard H264 codec. We use a few different adaptive streaming levels from 360p to 1080p to ensure smooth streaming for your audience watching on different devices and bandwidth constraints.

### Can I download original video files from Stream?

You cannot download the *exact* input file that you uploaded. However, depending on your use case, you can use the [Downloadable Videos](/stream/viewing-videos/download-videos/) feature to get encoded MP4s for use cases like offline viewing.

### Is there a limit to the amount of videos I can upload?

* By default, a video upload can be at most 30 GB.

* By default, you can have up to 120 videos queued or being encoded simultaneously. Videos in the `ready` status are playable but may still be encoding certain quality levels until the `pctComplete` reaches 100. Videos in the `error`, `ready`, or `pendingupload` state do not count toward this limit. If you need the concurrency limit raised, [contact Cloudflare support](/support/contacting-cloudflare-support/) explaining your use case and why you would like the limit raised.

:::note


The limit to the number of videos only applies to videos being uploaded to Cloudflare Stream. This limit is not related to the number of end users streaming videos.


:::

* An account cannot upload videos if the total video duration exceeds the video storage capacity purchased.

Limits apply to Direct Creator Uploads at the time of upload URL creation.

Uploads over these limits will receive a 429 (Too Many Requests) or 413 (Payload too large) HTTP status codes with more information in the response body. Please write to Cloudflare support or your customer success manager for higher limits.

### Can I embed videos on Stream even if my domain is not on Cloudflare?

Yes. Stream videos can be embedded on any domain, even domains not on Cloudflare.

### What input file formats are supported?

Users can upload video in the following file formats:

MP4, MKV, MOV, AVI, FLV, MPEG-2 TS, MPEG-2 PS, MXF, LXF, GXF, 3GP, WebM, MPG, QuickTime

### Does Stream support High Dynamic Range (HDR) video content?

When HDR videos are uploaded to Stream, they are re-encoded and delivered in SDR format, to ensure compatibility with the widest range of viewing devices.

### What frame rates (FPS) are supported?

Cloudflare Stream supports video file uploads for any FPS, however videos will be re-encoded for 70 FPS playback. If the original video file has a frame rate lower than 70 FPS, Stream will re-encode at the original frame rate.

If the frame rate is variable we will drop frames (e.g. if there are more than 1 frames within 1/30 seconds, we will drop the extra frames within that period).

### What browsers does Stream work on?

You can embed the Stream player on the following platforms:

<table-wrap>

| Browser | Version                             |
| ------- | ----------------------------------- |
| Chrome  | Supported since Chrome version 88+  |
| Firefox | Supported since Firefox version 87+ |
| Edge    | Supported since Edge 89+            |
| Safari  | Supported since Safari version 14+  |
| Opera   | Supported since Opera version 75+   |

</table-wrap>

:::note[Note]


Cloudflare Stream is not available on Chromium, as Chromium does not support H.264 videos.


:::

<table-wrap>

| Mobile Platform       | Version                                                                  |
| --------------------- | ------------------------------------------------------------------------ |
| Chrome on Android     | Supported on Chrome 90                                                   |
| UC Browser on Android | Supported on version 12.12+                                              |
| Samsung Internet      | Supported on 13+                                                         |
| Safari on iOS         | Supported on iOS 13.4+. Speed selector supported when not in fullscreen. |

</table-wrap>

### What are the recommended upload settings for video uploads?

If you are producing a brand new file for Cloudflare Stream, we recommend you use the following settings:

* MP4 containers, AAC audio codec, H264 video codec, 30 or below frames per second
* moov atom should be at the front of the file (Fast Start)
* H264 progressive scan (no interlacing)
* H264 high profile
* Closed GOP
* Content should be encoded and uploaded in the same frame rate it was recorded
* Mono or Stereo audio (Stream will mix audio tracks with more than 2 channels down to stereo)

Below are bitrate recommendations for encoding new videos for Stream:

<table-wrap>

| Resolution | Recommended bitrate |
| ---------- | ------------------- |
| 1080p      | 8 Mbps              |
| 720p       | 4.8 Mbps            |
| 480p       | 2.4 Mbps            |
| 360p       | 1 Mbps              |

</table-wrap>

### If I cancel my stream subscription, are the videos deleted?

Videos are removed if the subscription is not renewed within 30 days.

### I use Content Security Policy (CSP) on my website. What domains do I need to add to which directives?

If your website uses <GlossaryTooltip term="content security policy (CSP)" link="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy">Content Security Policy (CSP)</GlossaryTooltip> directives, depending on your configuration, you may need to add Cloudflare Stream's domains to particular directives, in order to allow videos to be viewed or uploaded by your users.

If you use the provided [Stream Player](/stream/viewing-videos/using-the-stream-player/), `videodelivery.net` and `*.cloudflarestream.com` must be included in the `frame-src` or `default-src` directive to allow the player's `<iframe>` element to load.

```http
Content-Security-Policy: frame-src 'self' videodelivery.net *.cloudflarestream.com
```

If you use your **own** Player, add `*.videodelivery.net` and `*.cloudflarestream.com` to the `media-src`, `img-src` and `connect-src` CSP directives to allow video files and thumbnail images to load.

```http
Content-Security-Policy: media-src 'self' videodelivery.net *.cloudflarestream.com; img-src 'self' *.videodelivery.net *.cloudflarestream.com; connect-src 'self' *.videodelivery.net *.cloudflarestream.com
```

If you allow users to upload their own videos directly to Cloudflare Stream, add `*.videodelivery.net` and `*.cloudflarestream.com` to the `connect-src` CSP directive.

```http
Content-Security-Policy: connect-src 'self' *.videodelivery.net *.cloudflarestream.com
```

To ensure **only** videos from **your** Cloudflare Stream account can be played on your website, replace `*` in `*.cloudflarestream.com` and `*.videodelivery.net` in the examples above with `customer-<CODE>`, replacing `<CODE>` with your unique customer code, which can be found in the [Stream Dashboard](https://dash.cloudflare.com/?to=/:account/stream). This code is unique to your Cloudflare Account.

### Why is PageSpeed Insights giving a bad score when using the Stream Player?

If your website loads in a lot of player instances, PageSpeed Insights will penalize the JavaScript load for each player instance. Our testing shows that when actually loading the page, the script itself is only downloaded once with the local browser cache retrieving the script for the other player objects on the same page. Therefore, we believe that the PageSpeed Insights score is not matching real-world behavior in this situation.

If you are using thumbnails, you can use [animated thumbnails](/stream/viewing-videos/displaying-thumbnails/#animated-gif-thumbnails) that link to the video pages.

If multiple players are on the same page, you can lazy load any players that are not visible in the initial viewport. For more information about lazy loading, refer to [Mozilla's lazy loading documentation](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe#lazy).

---

# Get started

URL: https://developers.cloudflare.com/stream/get-started/

:::note[Before you get started:]


You must first [create a Cloudflare account](/fundamentals/setup/account/create-account/) and [create an API token](/fundamentals/api/get-started/create-token/) to begin using Stream.


:::

* [Upload your first video](/stream/get-started#upload-your-first-video)
* [Start your first live stream](/stream/get-started#start-your-first-live-stream)

## Upload your first video

### Step 1: Upload an example video from a public URL

You can upload videos directly from the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/stream) or using the API.

To use the API, replace the `API_TOKEN` and `ACCOUNT_ID` values with your credentials in the example below.

```bash title="Upload a video using the API"
curl \
-X POST \
-d '{"url":"https://storage.googleapis.com/stream-example-bucket/video.mp4","meta":{"name":"My First Stream Video"}}' \
-H "Authorization: Bearer <API_TOKEN>" \
https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/stream/copy
```

### Step 2: Wait until the video is ready to stream

Because Stream must download and process the video, the video might not be available for a few seconds depending on the length of your video. You should poll the Stream API until `readyToStream` is `true`, or use [webhooks](/stream/manage-video-library/using-webhooks/) to be notified when a video is ready for streaming.

Use the video UID from the first step to poll the video:

```bash title="Request"
curl \
-H "Authorization: Bearer <API_TOKEN>" \
https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/stream/<VIDEO_UID>
```

```json title="Response" {6}
{
  "result": {
    "uid": "6b9e68b07dfee8cc2d116e4c51d6a957",
    "preview": "https://customer-f33zs165nr7gyfy4.cloudflarestream.com/6b9e68b07dfee8cc2d116e4c51d6a957/watch",
    "thumbnail": "https://customer-f33zs165nr7gyfy4.cloudflarestream.com/6b9e68b07dfee8cc2d116e4c51d6a957/thumbnails/thumbnail.jpg",
    "readyToStream": true,
    "status": {
      "state": "ready"
    },
    "meta": {
      "downloaded-from": "https://storage.googleapis.com/stream-example-bucket/video.mp4",
      "name": "My First Stream Video"
    },
    "created": "2020-10-16T20:20:17.872170843Z",
    "size": 9032701,
   //...
  },
  "success": true,
  "errors": [],
  "messages": []
}
```

### Step 3: Play the video in your website or app

Videos uploaded to Stream can be played on any device and platform, from websites to native apps. See [Play videos](/stream/viewing-videos) for details and examples of video playback across platforms.

To play video on your website with the [Stream Player](/stream/viewing-videos/using-the-stream-player/), copy the `uid` of the video from the request above, along with your unique customer code, and replace `<CODE>` and `<VIDEO_UID>` in the embed code below:

```html
<iframe
  src="https://customer-<CODE>.cloudflarestream.com/<VIDEO_UID>/iframe"
  title="Example Stream video"
  frameBorder="0"
  allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen>
</iframe>
```

The embed code above can also be found in the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/stream).

<figure data-type="stream">
  <div class="AspectRatio" style="--aspect-ratio: calc(16 / 9)">
    <iframe
      class="AspectRatio--content"
      src="https://iframe.videodelivery.net/5d5bc37ffcf54c9b82e996823bffbb81?muted=true"
      title="Example Stream video"
      frame-border="0"
      allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen"></iframe>
  </div>
</figure>

### Next steps

* [Edit your video](/stream/edit-videos/) and add captions or watermarks
* [Customize the Stream player](/stream/viewing-videos/using-the-stream-player/)

## Start your first live stream

### Step 1: Create a live input

You can create a live input via the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/stream/inputs/create) or using the API.

To use the API, replace the `API_TOKEN` and `ACCOUNT_ID` values with your credentials in the example below.

```bash title="Request"
curl -X POST \
-H "Authorization: Bearer <API_TOKEN>" \
-D '{"meta": {"name":"test stream"},"recording": { "mode": "automatic" }}' \
https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/stream/live_inputs
```

```json title="Response"
{
  "uid": "f256e6ea9341d51eea64c9454659e576",
  "rtmps": {
    "url": "rtmps://live.cloudflare.com:443/live/",
    "streamKey": "MTQ0MTcjM3MjI1NDE3ODIyNTI1MjYyMjE4NTI2ODI1NDcxMzUyMzcf256e6ea9351d51eea64c9454659e576"
  },
  "created": "2021-09-23T05:05:53.451415Z",
  "modified": "2021-09-23T05:05:53.451415Z",
  "meta": {
    "name": "test stream"
  },
  "status": null,
  "recording": {
    "mode": "automatic",
    "requireSignedURLs": false,
    "allowedOrigins": null
  }
}
```

### Step 2: Copy the RTMPS URL and key, and use them with your live streaming application.

We recommend using [Open Broadcaster Software (OBS)](https://obsproject.com/) to get started.

### Step 3: Play the live stream in your website or app

Live streams can be played on any device and platform, from websites to native apps, using the same video players as videos uploaded to Stream. See [Play videos](/stream/viewing-videos) for details and examples of video playback across platforms.

To play the live stream you just started on your website with the [Stream Player](/stream/viewing-videos/using-the-stream-player/), copy the `uid` of the live input from the request above, along with your unique customer code, and replace `<CODE>` and `<VIDEO_UID>` in the embed code below:

```html
<iframe
  src="https://customer-<CODE>.cloudflarestream.com/<VIDEO_UID>/iframe"
  title="Example Stream video"
  frameBorder="0"
  allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen>
</iframe>
```

The embed code above can also be found in the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/stream).

### Next steps

* [Secure your stream](/stream/viewing-videos/securing-your-stream/)
* [View live viewer counts](/stream/getting-analytics/live-viewer-count/)

## Accessibility considerations

To make your video content more accessible, include [captions](/stream/edit-videos/adding-captions/) and [high-quality audio recording](https://www.w3.org/WAI/media/av/av-content/).

---

# Overview

URL: https://developers.cloudflare.com/stream/

import { CardGrid, Description, Feature, LinkButton, LinkTitleCard, Render } from "~/components"

<Description>


Serverless live and on-demand video streaming


</Description>

Cloudflare Stream lets you or your end users upload, store, encode, and deliver live and on-demand video with one API, without configuring or maintaining infrastructure.

You can use Stream to build your own video features in websites and native apps, from simple playback to an entire video platform.

Cloudflare Stream runs on [Cloudflareâ€™s global cloud network](https://www.cloudflare.com/network/) in hundreds of cities worldwide.

 <LinkButton variant="primary" href="/stream/get-started/">Get started</LinkButton> <LinkButton variant="secondary" href="https://dash.cloudflare.com/?to=/:account/stream">Stream dashboard</LinkButton>


***

## Features

<Feature header="Control access to video content" href="/stream/viewing-videos/securing-your-stream/" cta="Use Signed URLs">

Restrict access to paid or authenticated content with signed URLs.


</Feature>

<Feature header="Let your users upload their own videos" href="/stream/uploading-videos/direct-creator-uploads/" cta="Direct Creator Uploads">

Let users in your app upload videos directly to Stream with a unique, one-time upload URL.


</Feature>

<Feature header="Play video on any device" href="/stream/viewing-videos/" cta="Play videos">

Play on-demand and live video on websites, in native iOS and Android apps, and dedicated streaming devices like Apple TV.


</Feature>

<Feature header="Get detailed analytics" href="/stream/getting-analytics/" cta="Explore Analytics">

Understand and analyze which videos and live streams are viewed most and break down metrics on a per-creator basis.


</Feature>

***

## More resources

<CardGrid>

<LinkTitleCard title="Discord" href="https://discord.cloudflare.com" icon="discord">
&#x20;Join the Stream developer community 
</LinkTitleCard>

</CardGrid>

---

# Pricing

URL: https://developers.cloudflare.com/stream/pricing/

Cloudflare Stream lets you broadcast, store, and deliver video using a simple, unified API and simple pricing. Stream bills on two dimensions only:

- Minutes of video stored
- Minutes of video delivered

On-demand and live video are billed the same way.

Ingress (sending your content to us) and encoding are always free. Bandwidth is already included in "video delivered" with no additional egress (traffic/bandwidth) fees.

## Minutes of video stored

Storage is a prepaid pricing dimension purchased in increments of $5 per 1,000 minutes stored, regardless of file size. You can check how much storage you have and how much you have used on the [Stream](https://dash.cloudflare.com/?to=/:account/stream) page in Dash.

Storage is consumed by:

- Original videos uploaded to your account
- Recordings of live broadcasts
- The reserved `maxDurationSeconds` for Direct Creator and TUS uploads which have not been completed. After these uploads are complete or the upload link expires, this reservation is released.

Storage is not consumed by:

- Videos in an unplayable or errored state
- Expired Direct Creator upload links
- Deleted videos
- Downloadable files generated for [MP4 Downloads](/stream/viewing-videos/download-videos/)
- Multiple quality levels that Stream generates for each uploaded original

Storage consumption is rounded up to the second of video duration; file size does not matter. Video stored in Stream does not incur additional storage fees from other storage products such as R2.

:::note
If you run out of storage, you will not be able to upload new videos or start new live streams until you purchase more storage or delete videos.

Enterprise customers _may_ continue to upload new content beyond their contracted quota without interruption.
:::

## Minutes of video delivered

Delivery is a post-paid, usage-based pricing dimension billed at $1 per 1,000 minutes delivered. You can check how much delivery you have used on the [Billable Usage](https://dash.cloudflare.com/?to=/:account/billing/billable-usage) page in Dash or the [Stream Analytics](https://dash.cloudflare.com/?to=/:account/stream/analytics) page under Stream.

Delivery is counted for the following uses:

- Playback on the web or an app using [Stream's built-in player](/stream/viewing-videos/using-the-stream-player/) or the [HLS or DASH manifests](/stream/viewing-videos/using-own-player/)
- MP4 Downloads
- Simulcasting via SRT or RTMP live outputs

Delivery is counted by HTTP requests for video segments or parts of the MP4. Therefore:

- Client-side preloading and buffering is counted as billable delivery.
- Content played from client-side/browser cache is _not_ billable, like a short looping video. Some mobile app player libraries do not cache HLS segments by default.
- MP4 Downloads are billed by percentage of the file delivered.

Minutes delivered for web playback (Stream Player, HLS, and DASH) are rounded to the _segment_ length: for uploaded content, segments are four seconds. Live broadcast and recording segments are determined by the keyframe interval or GOP size of the original broadcast.

## Example scenarios

**Two people each watch thirty minutes of a video or live broadcast. How much would it cost?**

This will result in 60 minutes of Minutes Delivered usage (or $0.06). Stream bills on minutes of video delivered, not per viewer.

**I have a really large file. Does that cost more?**

The cost to store a video is based only on its duration, not its file size. If the file is within the [30GB max file size limitation](/stream/faq/#is-there-a-limit-to-the-amount-of-videos-i-can-upload), it will be accepted. Be sure to use an [upload method](/stream/uploading-videos/) like Upload from Link or TUS that handles large files well.

**If I make a Direct Creator Upload link with a maximum duration (`maxDurationSeconds`) of 600 seconds which expires in 1 hour, how is storage consumed?**

- Ten minutes (600 seconds) will be subtracted from your available storage immediately.
- If the link is unused in one hour, those 10 minutes will be released.
- If the creator link is used to upload a five minute video, when the video is uploaded and processed, the 10 minute reservation will be released and the true five minute duration of the file will be counted.
- If the creator link is used to upload a five minute video but it fails to encode, the video will be marked as errored, the reserved storage will be released, and no storage use will be counted.

**I am broadcasting live, but no one is watching. How much does that cost?**

A live broadcast with no viewers will cost $0 for minutes delivered, but the recording of the broadcast will count toward minutes of video stored.

If someone watches the recording, that will be counted as minutes of video delivered.

If the recording is deleted, the storage use will be released.

**I want to store and deliver millions of minutes a month. Do you have volume pricing?**

Yes, contact our [Sales Team](https://www.cloudflare.com/plans/enterprise/contact/).

---

# WebRTC

URL: https://developers.cloudflare.com/stream/webrtc-beta/

import { Badge, InlineBadge } from '~/components';

Sub-second latency live streaming (using WHIP) and playback (using WHEP) to unlimited concurrent viewers.

WebRTC is ideal for when you need live video to playback in near real-time, such as:

* When the outcome of a live event is time-sensitive (live sports, financial news)
* When viewers interact with the live stream (live Q\&A, auctions, etc.)
* When you want your end users to be able to easily go live or create their own video content, from a web browser or native app

:::note

WebRTC streaming is currently in beta, and we'd love to hear what you think. Join the Cloudflare Discord server [using this invite](https://discord.com/invite/cloudflaredev/) and hop into our [Discord channel](https://discord.com/channels/595317990191398933/893253103695065128) to let us know what you're building with WebRTC! 

:::

## Step 1: Create a live input

[Use the Stream Dashboard](https://dash.cloudflare.com/?to=/:account/stream/inputs/create), or make a POST request to the [`/live_inputs` API endpoint](/api/resources/stream/subresources/live_inputs/methods/create/)

```json title="API response from a POST request to /live_inputs" {5}
{
  "uid": "1a553f11a88915d093d45eda660d2f8c",
 ...
  "webRTC": {
    "url": "https://customer-<CODE>.cloudflarestream.com/<SECRET>/webRTC/publish"
  },
  "webRTCPlayback": {
    "url": "https://customer-<CODE>.cloudflarestream.com/<INPUT_UID>/webRTC/play"
  },
...
}
```

## Step 2: Go live using WHIP

Every live input has a unique URL that one creator can be stream to. This URL should *only* be shared with the creator â€”Â anyone with this URL has the ability to stream live video to this live input.

Copy the URL from the `webRTC` key in the API response (see above), or directly from the [Cloudflare Dashboard](https://dash.cloudflare.com/?to=/:account/stream/inputs).

Paste this URL into the provided [WHIP example code](https://github.com/cloudflare/workers-sdk/blob/main/templates/stream/webrtc/src/whip.html#L13).

```javascript title="Simplified example code" {4}
// Add a <video> element to the HTML page this code runs in:
// <video id="input-video" autoplay muted></video>

import WHIPClient from "./WHIPClient.js"; // an example WHIP client, see https://github.com/cloudflare/workers-sdk/blob/main/templates/stream/webrtc/src/WHIPClient.ts

const url = "<WEBRTC_URL_FROM_YOUR_LIVE_INPUT>"; // add the webRTC URL from your live input here
const videoElement = document.getElementById("input-video");
const client = new WHIPClient(url, videoElement);
```

Once the creator grants permission to their camera and microphone, live video and audio will automatically start being streamed to Cloudflare, using WebRTC.

You can also use this URL with any client that supports the [WebRTC-HTTP ingestion protocol (WHIP)](https://www.ietf.org/id/draft-ietf-wish-whip-06.html). See [supported WHIP clients](#supported-whip-and-whep-clients) for a list of clients we have tested and confirmed compatibility with Cloudflare Stream.

## Step 3: Play live video using WHEP

Copy the URL from the `webRTCPlayback` key in the API response (see above), or directly from the [Cloudflare Dashboard](https://dash.cloudflare.com/?to=/:account/stream/inputs). There are no limits on the number of concurrent viewers.

Paste this URL into the provided [WHEP example code](https://github.com/cloudflare/workers-sdk/blob/main/templates/stream/webrtc/src/whep.html#L13).

```javascript title="Simplified example code" {4}
// Add a <video> element to the HTML page this code runs in:
// <video id="output-video" autoplay muted></video>

import WHEPClient from "./WHEPClient.js"; // an example WHEP client, see https://github.com/cloudflare/workers-sdk/blob/main/templates/stream/webrtc/src/WHEPClient.ts

const url = "<WEBRTC_URL_FROM_YOUR_LIVE_INPUT>"; // add the webRTCPlayback URL from your live input here
const videoElement = document.getElementById("output-video");
const client = new WHEPClient(url, videoElement);
```

As long as the creator is actively streaming, viewers should see their broadcast in their browser, with less than 1 second of latency.

You can also use this URL with any client that supports the [WebRTC-HTTP egress protocol (WHEP)](https://www.ietf.org/archive/id/draft-murillo-whep-01.html). See [supported WHEP clients](#supported-whip-and-whep-clients) for a list of clients we have tested and confirmed compatibility with Cloudflare Stream.

## Using WebRTC in native apps

If you are building a native app, the example code above can run within a [WkWebView (iOS)](https://developer.apple.com/documentation/webkit/wkwebview), [WebView (Android)](https://developer.android.com/reference/android/webkit/WebView) or using [react-native-webrtc](https://github.com/react-native-webrtc/react-native-webrtc/blob/master/Documentation/BasicUsage.md). If you need to use WebRTC without a webview, you can use Google's Java and Objective-C native [implementations of WebRTC APIs](https://webrtc.googlesource.com/src/+/refs/heads/main/sdk).

## Debugging WebRTC

* **Chrome**: Navigate to `chrome://webrtc-internals` to view detailed logs and graphs.
* **Firefox**: Navigate to `about:webrtc` to view information about WebRTC sessions, similar to Chrome.
* **Safari**: To enable WebRTC logs, from the inspector, open the settings tab (cogwheel icon), and set WebRTC logging to "Verbose" in the dropdown menu.

## Supported WHIP and WHEP clients

Beyond the [example WHIP client](https://github.com/cloudflare/workers-sdk/blob/main/templates/stream/webrtc/src/WHIPClient.ts) and [example WHEP client](https://github.com/cloudflare/workers-sdk/blob/main/templates/stream/webrtc/src/WHEPClient.ts) used in the examples above, we have tested and confirmed that the following clients are compatible with Cloudflare Stream:

### WHIP

* [OBS (Open Broadcaster Software)](https://obsproject.com)
* [@eyevinn/whip-web-client](https://www.npmjs.com/package/@eyevinn/whip-web-client) (TypeScript)
* [whip-go](https://github.com/ggarber/whip-go) (Go)
* [gst-plugins-rs](https://gitlab.freedesktop.org/gstreamer/gst-plugins-rs) (Gstreamer plugins, written in Rust)
* [Larix Broadcaster](https://softvelum.com/larix/) (free apps for iOS and Android with WebRTC based on Pion, SDK available)

### WHEP

* [@eyevinn/webrtc-player](https://www.npmjs.com/package/@eyevinn/webrtc-player) (TypeScript)
* [@eyevinn/wrtc-egress](https://www.npmjs.com/package/@eyevinn/wrtc-egress) (TypeScript)
* [gst-plugins-rs](https://gitlab.freedesktop.org/gstreamer/gst-plugins-rs) (Gstreamer plugins, written in Rust)

As more WHIP and WHEP clients are published, we are committed to supporting them and being fully compliant with the both protocols.

## Supported codecs

* [VP9](https://developers.google.com/media/vp9) (recommended for highest quality)
* [VP8](https://en.wikipedia.org/wiki/VP8)
* [h264](https://en.wikipedia.org/wiki/Advanced_Video_Coding) (Constrained Baseline Profile Level 3.1, referred to as `42e01f` in the SDP offer's `profile-level-id` parameter.)

## Conformance with WHIP and WHEP specifications

Cloudflare Stream fully supports all aspects of the [WHIP](https://www.ietf.org/id/draft-ietf-wish-whip-06.html) and [WHEP](https://www.ietf.org/archive/id/draft-murillo-whep-01.html) specifications, including:

* [Trickle ICE](https://datatracker.ietf.org/doc/rfc8838/)
* [Server and client offer modes](https://www.ietf.org/archive/id/draft-murillo-whep-01.html#section-3) for WHEP

You can find the specific version of WHIP and WHEP being used in the `protocol-version` header in WHIP and WHEP API responses. The value of this header references the IETF draft slug for each protocol. Currently, Stream uses `draft-ietf-wish-whip-06` (expected to be the final WHIP draft revision) and `draft-murillo-whep-01` (the most current WHEP draft).

## Limitations while in beta

* [Recording](/stream/stream-live/watch-live-stream/#live-stream-recording-playback) is not yet supported (coming soon)
* [Simulcasting](/stream/stream-live/simulcasting) (restreaming) is not yet supported (coming soon)
* [Live viewer counts](/stream/getting-analytics/live-viewer-count/) are not yet supported (coming soon)
* [Analytics](/stream/getting-analytics/fetching-bulk-analytics/) are not yet supported (coming soon)
* WHIP and WHEP must be used together â€”Â we do not yet support streaming using RTMP/SRT and playing using WHEP, or streaming using WHIP and playing using HLS or DASH. (coming soon)
* Once generally available, WebRTC streaming will be priced just like the rest of Cloudflare Stream, based on minutes stored and minutes of video delivered.

---

# Style Guide

URL: https://developers.cloudflare.com/style-guide/

Use this guide when writing any content for product, including the dashboard and documentation.

Understanding Cloudflare style is the first step in being able to write, review, and edit documentation. Adhering to Cloudflare style ensures consistency across the company's documentation and promotes the following benefits:

* A professional and reliable product image
* A seamless customer experience across Cloudflare products
* Minimized customer confusion
* Simplified translation process

Visit the [Cloudflare Docs](https://github.com/cloudflare/cloudflare-docs) repository to contribute to developer documentation.

---

# Contacting Cloudflare Support

URL: https://developers.cloudflare.com/support/contacting-cloudflare-support/

## Guidelines for contacting Cloudflare support

Cloudflare Support _cannot_ perform the following actions:

- Make configuration or account changes on a customerâ€™s behalf
- Provide sensitive account info over the phone
- Troubleshoot or debug customer's code and its logic
- Troubleshoot or answer questions about domains not associated with the Cloudflare account email address used to contact support

:::caution

**Do not share** any sensitive information, such as passwords, credit
card numbers, private keys, or API keys with Cloudflare.
:::

Before notifying Cloudflare of an issue with your site, refer to theÂ [Cloudflare Status Page](https://www.cloudflarestatus.com/). If reporting issues with your site, ensure to provide adequate details in the support case _(refer to [Getting help with an issue](#getting-help-with-an-issue) for more information)_.

---

## Methods of contacting Cloudflare support

As a Cloudflare customer, you can contact Cloudflare for support via the community portal or by opening a support case, live chat, or phone. Support options can vary depending on your plan.

|                                                                                                                                                                                                                      | Enterprise | Business | Pro         | Free        |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | -------- | ----------- | ----------- |
| [Community](https://community.cloudflare.com/)<br/>Join the community to ask basic troubleshooting questions and to view the latest resources (such as tips for resolving common issues and configuration guidance). | Yes        | Yes      | Recommended | Recommended |
| [Discord](https://discord.cloudflare.com/)<br/>Join the Discord community to ask basic troubleshooting questions (mainly focused on the Developer Platform).                                                         | Yes        | Yes      | Recommended | Recommended |
| [Support case](#getting-help-with-an-issue)<br/>Use to troubleshoot specific issues or errors. Response times depend on your plan.                                                                                   | Yes        | Yes      | Yes         | No\*        |
| **Chat**<br/>Use to troubleshoot specific issues or errors. Response times depend on your plan.                                                                                                                      | Yes        | Yes      | No          | No          |
| **Emergency Phone** <br/>Use phone support to reach out during emergencies such as site outages or DDoS attacks.                                                                                                     | Yes        | No       | No          | No          |

---

:::note

Customers on Free plans are encouraged to utilize our Cloudflare Community and will only receive standard case support for billing, account, and registrar issues.

:::

## Verifying your identity

_(For Enterprise Emergency Phone Support)_

For account security, you must verify your identity and account ownership in the Cloudflare dashboard before discussing account settings and sensitive details with Cloudflare Support. There are two verification options:

- a single-use token that automatically refreshes every thirty (30) seconds, or
- an [authenticator app token](/fundamentals/setup/account/account-security/2fa/#configure-totp-mobile-application-authentication-for-two-factor-cloudflare-login) on your mobile device.

### Authenticating your account

1\. Log into your Cloudflare account.

2\. In the upper right corner of the Cloudflare dashboard, select **Support** > **Contact Cloudflare**.

![](~/assets/images/support/contact-cloudflare-menu.png)

3\. On the right-hand panel there is a section for _Emergency Phone Support Hotline_.

![](~/assets/images/support/Screen_Shot_2022-09-28_at_11.41.37.png)

4\. To authenticate using a single-use token, click **Get a single-use token**. A pop-up window will appear with your Unique Customer Id and One Time Passcode. The code automatically refreshes every 30 seconds.

![](~/assets/images/support/Emergency_Phone_Support.png)

5\. To authenticate using an authenticator app, click **Configure authenticator app** and follow the [configuration instructions](/fundamentals/setup/account/account-security/2fa/#configure-totp-mobile-application-authentication-for-two-factor-cloudflare-login) on the following screen. After configuration, the token code will appear in your mobile authentication application.

6\. When calling the emergency phone line, you can authenticate automatically by entering your ID and Code when requested.

---

## Getting help with an issue

To submit a support case, follow these steps:

1. Browse to [https://dash.cloudflare.com/?to=/:account/support](https://dash.cloudflare.com/?to=/:account/support).
2. Enter theÂ **email**Â andÂ **password**Â for your Cloudflare account. Your email needs to be [verified](/fundamentals/setup/account/account-security/login-and-account-issues/) in order to submit a Support case.
3. Select the appropriate account requiring assistance.
4. Verify that you are now in the Support Portal.
5. Choose the category and subcategories that best define your issue.
6. Choose the affected domains (if applicable).
7. Enter a detailed summary of the issue youâ€™re experiencing.
8. Review the right-hand panel of the Portal for recommended resources and for troubleshooting guides resulting from diagnostics run against your domain.
9. If the provided resources did not solve your issue, click Add more details to submit a case\*\*
10. Complete the case submission fields as completely as possible with the following information. _(**Please note** that missing information will increase the time it takes to resolve your issue and our team may not be able to investigate without enough information. Please review [Gathering information for troubleshooting sites](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/) and make sure you include all needed information.)_
    \- A detailed description of the issue with the following information:
    - Timestamp (UTC)
    - ZoneName/ZoneID
    - Problem frequency
    - Steps to reproduce the issue, with actual results vs expected results
      \- Any necessary information for a technical investigation
    - A description of the actual results vs expected results
    - Steps to reproduce the issue, with example URLsÂ 
    - Exact error messages
    - HAR files
    - Screenshots
    - Relevant logs from the origin web server
    - Output from [test tools](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/) such as MTR, traceroute, dig/nslookup, and cURL
      \- Priority level, impact to service / production
      \- Any collaborators whom you wish to be cc'd on the case
11. ClickÂ **Submit Case**

\*\* Available to certain plan types only. Refer to chart above for details.

### Accepted file formats in cases and chats

You can only upload the following file types in a case or a chat:

_Image_

- png, jpg, gif, ico, tiff

_Video_

- mp4, avi, webm

_Text_

- har, txt, csv, eml, css, html, json, tf

_Packet Capture_

- pcap, pcapng, cap

Cloudflare Support only accepts the WARP diagnostics as compressed files.
Please do not upload ZIP or RAR files when sharing HAR files, and please do not share compressed documents like DOCX, XLSX or PPTX.

The maximum file size is **20 MB**.
If you need to share a larger file, please provide a link to the file using Google Drive or a similar sharing platform of your choice.

When sending Cloudflare Support packet captures, please do the following: 

1. Filter for relevant traffic. Use a display filter and then save [export specified packets](https://www.wireshark.org/docs/wsug_html_chunked/ChIOExportSection.html) to reduce the file size.
2. Include the name of the device and interface/tunnel in the file name.
3. Describe what each packet capture shows.

### View open support cases

1. Log in to your Cloudflare account.
2. SelectÂ `My Activities - All Tickets`Â fromÂ **Support**Â dropdown.
3. Select your account if needed.
4. You will be redirected to the portal where you can see your own cases and cases you are CC'd on.

### Add participants to open support tickets

1. Log in to your Cloudflare account.
2. SelectÂ `My Activities - All Tickets`Â fromÂ **Support**Â dropdown.
3. Select your account if needed.
4. Select the case you want to add participants to.
5. Enter their email in the `Case Participants` box in the top right and click **Add**.

---

## Live chat support

You can also use the **live chat** (Business and Enterprise plans, only) to ask specific questionsÂ that donâ€™t require a lot of technical investigation:

1. Browse to [https://dash.cloudflare.com/?to=/:account/support](https://dash.cloudflare.com/?to=/:account/support).
2. Enter theÂ emailÂ andÂ passwordÂ for your Cloudflare account. Your email needs to be verified in order to chat live with us.
3. Select the appropriate account requiring assistance.
4. Click the **Help** icon in the bottom right corner of the screen
5. The **Live Chat** screen will appear.Â Use the search field to check for answers to your questions:
6. If the results didn't answer your question, press the **Live chat** button and start typing your message to chat with a Technical Support Engineer.

---

## Service Level Agreements and Objectives

### How we prioritize your issue

Cloudflare support responds to every case received in the following priority order:

- Premium Enterprise
- Standard Enterprise
- Business
- Pro
- Free

Cloudflare Support strives to respond to our customers as quickly as possible. Urgent issues (site down, under attack) are prioritized for the quickest response possible. Please explicitly specify the priority level and impact to your production service when reaching out to Cloudflare support.

Below are definitions of the priority levels Cloudflare assigns to cases and the associated Service Level Agreement (SLA) or Service Level Objective (SLO). Whenever possible, responses are provided quicker than the noted SLAs.

### Priority definitions

- P1 - Service is significantly impaired and unavailable in multiple user locations.
  - _Example_: site outage issues or an ongoing attack.
- P2 - Repeated inability to use the Service from a single location or localized region.
  - _Example_: Localized site outage. These issues may be with a single website or even a single server.
- P3 - Issues that may impact service performance or user experience but do not prevent service usage; account-related queries.
  - _Example_: slow performance issues, information requests, or usage questions.
- P4 - General questions related to Cloudflare's products and services

### Premium SLA

- P1 - initial response in 1 hour
- P2 - initial response in 2 hoursÂ 
- P3 - initial response in 24 hoursÂ 
- P4 - initial response in 24 hours

### EnterpriseÂ SLA

- P1 - initial response in 2 hours
- P2 - initial response in 4 hours
- P3 - initial response in 48 hours
- P4 - initial response in 48 hours

### SLOs for other plans

- PAYGOÂ and Free customers - No SLAs are offered, but customers are responded to in the order in which their request is received. For a quicker answer, we highly recommend searching or posting on ourÂ [Community forums](https://community.cloudflare.com/).

## Supported languages

For Enterprise support, Cloudflare provides support in English, but makes a best effort to offer help in the following languages:

- Chinese
- English
- French
- German
- Japanese
- Portuguese
- Spanish

## Supported regions

Cloudflare offers worldwide support, which covers:

- Asia-Pacific
- Europe, Middle East, and Africa
- North and South America

---

# Support

URL: https://developers.cloudflare.com/support/

import { DirectoryListing } from "~/components"

Below you will find links to the relevant sections for support-focused material.

<DirectoryListing />

---

# Changelog

URL: https://developers.cloudflare.com/tenant/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/tenant.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Get started

URL: https://developers.cloudflare.com/tenant/get-started/

import { Render, TabItem, Tabs } from "~/components";

Having access to Cloudflareâ€™s provisioning capabilities allows you to more easily create and manage Cloudflare accounts. The following steps will get you started on making API calls to provision accounts, users, and services.

## Before you begin

### Channel and Alliance partner account setup

Before using the Tenant API, you need to [create an account](/fundamentals/setup/account/create-account/), [verify your email address](/fundamentals/setup/account/verify-email-address/), and [add your billing information](/fundamentals/subscriptions-and-billing/create-billing-profile/).

After you sign your partner agreement with Cloudflare, Cloudflare will add [certain entitlements](/tenant/structure/) to your account that allow you to provision and manage custom accounts. If you have signed your partner agreement and your account has not yet been enabled, contact `partners@cloudflare.com`.

### API access

You also need to [retrieve your API key](/fundamentals/api/get-started/keys/#view-your-global-api-key) to authenticate your requests to the Tenant API.

For more details on using the Cloudflare API, refer to our [API overview](/fundamentals/api/).

## Step 1 - Create an account

<Render file="account-preamble" />

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

<Render file="create-account-dash" />

</TabItem> <TabItem label="API">

<Render file="create-account-api" />

</TabItem> </Tabs>

## Step 2 - Grant user access

Now that you have created an account, you need to either give your customer direct access to Cloudflare or build an interface for them to interact with.

The first method gives customers control over all aspects of Cloudflare, while the latter allows you to integrate your customer's Cloudflare experience into a dashboard that you control and that they may already be familiar with.

### Option 1 - Direct access to Cloudflare

When you grant user access to an account, Cloudflare will send an invitation to the user so they can get access to the account. If they do not already have a Cloudflare user, Cloudflare will take them through the process of creating one. Once created, they will be given access to the account and any zones already created.

#### Using the dashboard

If you want to give customers access to their individual accounts, it is the same as if you were [inviting a teammate](/fundamentals/setup/manage-members/manage/#add-account-members) to help manage your account.

#### Using the API

You can also grant access to the Cloudflare dashboard by using the API.

```bash title="Request"
curl 'https://api.cloudflare.com/client/v4/accounts/<CUSTOMER_ACCOUNT_ID>/members' \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "email": "<CUSTOMER_EMAIL>",
  "roles": ["<USER_ROLE>"]
}'
```

In most cases, you will want to create new users with a role of `Administrator` which always has the ID `05784afa30c1afe1440e79d9351c7430`.

If your customer is on an Enterprise plan, they have access to a broader set of user roles. To get a full list of available roles, send a [`GET`](/api/resources/accounts/subresources/roles/methods/list/) request to the API.

### Option 2 - Access via an interface

If you want greater control over how customers use Cloudflare or if you want your customers to use an existing dashboard of yours that they already know, use the Cloudflare API to build this experience.

This means that you will be making API calls to Cloudflare on behalf of your customers. To avoid getting [rate limited](/fundamentals/api/reference/limits/) by our API, Cloudflare recommend that you create accounts and users for each of your customers. Changes made by customer `A` should go through user `A` and changes made by customer `B` should go through user `B`.

:::note

This capability is not enabled by default. If you need this functionality, contact [Cloudflare Support](/support/contacting-cloudflare-support/).

:::

To grant access via an interface, you need to create a service user, as no one will log in to the dashboard with them. If you are planning to use this method, Cloudflare will enable you to see the API key in order to make API calls as this user.

```bash title="Request"
curl "https://api.cloudflare.com/client/v4/users" \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "email": "<ID@example.com>"
}'
```

```json title="Response"
{
	"result": {
		"id": "60758bd48392a06215ae817bc35084b6",
		"email": "<ID@example.com>",
		"first_name": null,
		"last_name": null,
		"username": "17bd2796b374cec14976ac3bced85c05",
		"telephone": null,
		"country": null,
		"created_on": "2019-02-21T23:20:28.645256Z",
		"modified_on": "2019-02-21T23:20:28.645256Z",
		"two_factor_authentication": {
			"enabled": false,
			"locked": false
		},
		"api_key": "xxx"
	},
	"success": true,
	"errors": [],
	"messages": []
}
```

## Step 3 - Create a zone

Now that you have a customer account and customer users (or service users), you need to create a zone.

To do this, send a [`POST`](/api/resources/zones/methods/create/) request to the `/zones` endpoint (including the customer account ID you received in [Step 1](#step-1---create-an-account)).

```bash title="Request"
curl "https://api.cloudflare.com/client/v4/zones" \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "name": "example.com",
  "account": {
    "id": "<CUSTOMER_ACCOUNT_ID>"
  }
}'
```

## Step 4 - Create a zone plan subscription

Now that you have a zone provisioned for the customer, you can add the appropriate zone plan based on your reseller agreement.

<Render file="create-zone-subscription" />

## Step 5 - Create other subscriptions

Depending on your agreement, you may be allowed to resell other add-on services. These are provisioned as account-level subscriptions.

<Render file="create-account-subscription" />

## Step 6 - Configure zone and services

Once you have added the necessary subscriptions, you or your customer can move on to configuring various services and fine-tuning account and zone settings.

Configuration can be done by anyone with access to the account (as well as the correct user permissions). This process does not differ from configuring any other Cloudflare account. For additional guidance, refer to our [Product docs](/).

---

# Glossary

URL: https://developers.cloudflare.com/tenant/glossary/

import { Render } from "~/components"

The following terms are used throughout the Tenant API docs. For more details on how these concepts interact with each other, refer to [Tenant structure](/tenant/structure/).

## Tenant

<Render file="tenant-definition" />

## Tenant admin

<Render file="tenant-admin-definition" />

## Account

<Render file="account-definition" />

## User

<Render file="user-definition" />

## Resource

<Render file="resource-definition" />

---

# Overview

URL: https://developers.cloudflare.com/tenant/

The Cloudflare Tenant API is a provisioning mechanism to help Channel and Alliance partners set up and manage Cloudflare accounts and services for their customers.

These APIs are built into our Client v4 API library to provide a streamlined onboarding and setup experience.

## Available resources

As you dive into the Tenant API, take advantage of the following resources:

* [Tenant structure](/tenant/structure/) explains some basic ideas behind the Tenant API and how it works.
* [Get started](/tenant/get-started/) provides the quickest path to get your customer accounts up and running.
* [How to](/tenant/how-to/) offers step-by-step instructions for specific tasks.
* [Reference topics](/tenant/reference/) contains detailed information about the Tenant API, including a list of available zone and account-level subscriptions.

---

# Tenant structure

URL: https://developers.cloudflare.com/tenant/structure/

import { Render } from "~/components"

Cloudflare helps Channel and Alliance partners manage their and their customers' accounts through a Tenant structure.

![Partner accounts contain a tenant, which is a container for customer accounts and zones. For more details, keep reading.](~/assets/images/tenant/tenant-diagram.png)

## Tenants and Tenant admins

<Render file="tenant-definition" />

<Render file="tenant-admin-definition" />

## Accounts, users, and resources

This Tenant structure gives your account streamlined administrative access to customer:

* Accounts[^1]
* Users[^2]
* Resources[^3]

At the same time, this structure keeps your customers' data and settings separate from each other.

[^1]: <Render file="account-definition" />

[^2]: <Render file="user-definition" />

[^3]: <Render file="resource-definition" />

---

# Overview

URL: https://developers.cloudflare.com/time-services/

import { DirectoryListing } from "~/components"

Learn more about Cloudflareâ€™s suite of time services.

<DirectoryListing />

---

# Network Time Security

URL: https://developers.cloudflare.com/time-services/nts/

[Network Time Security](https://datatracker.ietf.org/doc/html/rfc8915) (NTS) provides cryptographic security for the client-server mode of the Network Time Protocol (NTP). This allows users to obtain time in an authenticated manner.

## Background

The NTS protocol is divided into two phases:

1. **NTS Key Exchange**: Establishes the necessary key material between the NTP client and the server, using a [Transport Layer Security (TLS) handshake](https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake/) (the same public key infrastructure as the web). Once the keys are exchanged, the TLS channel is closed and the protocol enters the second phase.
2. **NTS Extension Fields for NTPv4**: Authenticates NTP time synchronization packets using previously established key material. For more information, refer to [RFC 8915](https://tools.ietf.org/html/rfc8915).

## Next steps

NTS is gaining support in many NTP implementations, including [Chrony](https://chrony-project.org/documentation.html), [NTPsec](https://www.ntpsec.org/), and [ntpd-rs](https://github.com/pendulum-project/ntpd-rs). Read the relevant documentation for guidance on setting them up to point to our time service, `time.cloudflare.com`. Also see [Netnod's documentation](https://www.netnod.se/netnod-time/how-to-use-nts) for configuring NTS clients.

---

# Terms of use

URL: https://developers.cloudflare.com/time-services/tos/

By using Cloudflare's suite of time services, you agree to [Cloudflare Website and Online Services Terms of Use](https://www.cloudflare.com/website-terms/).

---

# Overview

URL: https://developers.cloudflare.com/terraform/

Configure Cloudflare using HashiCorp's â€œInfrastructure as Codeâ€ tool, Terraform. With [Cloudflareâ€™s Terraform provider](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs), you can manage the Cloudflare global network using the same familiar tools you use to automate the rest of your infrastructure. Define and store configuration in source code repositories like GitHub, track and version changes over time, and roll back when needed â€” all without needing to use the Cloudflare APIs.

Report Terraform configuration issues via [GitHub](https://github.com/cloudflare/terraform-provider-cloudflare/issues/new/choose).

---

# Get started

URL: https://developers.cloudflare.com/terraform/installing/

Terraform ships as a single binary file. The examples below include installation information for popular operating systems.

For official instructions on installing Terraform, refer to [Install Terraform](https://developer.hashicorp.com/terraform/tutorials/certification-associate-tutorials/install-cli).

:::caution

Terraform maintains your configuration state, which can be broken when you make configuration changes through both Terraform and either the Cloudflare Dashboard or API.

To avoid this state, make sure you manage Terraform resources only in Terraform. For more details, refer to our [best practices](/terraform/advanced-topics/best-practices/).

:::

## Mac

The easiest way to install Terraform on macOS is with Homebrew.

```sh
brew tap hashicorp/tap
brew install hashicorp/tap/terraform
```

## Linux

You can install the `terraform` binary via your distribution's package manager. For example:

```sh
sudo apt install terraform
```

Alternatively, you can fetch a specific version directly and place the binary in your `PATH`:

```sh
wget -q https://releases.hashicorp.com/terraform/1.4.5/terraform_1.4.5_linux_amd64.zip

unzip terraform_1.4.5_linux_amd64.zip
```

```sh output
Archive:  terraform_1.4.5_linux_amd64.zip
  inflating: terraform
```

```sh
sudo mv terraform /usr/local/bin/terraform

terraform version
```

```sh output
Terraform v1.4.5
```

## Windows

1. Download the 32 or 64-bit executable from the [Download Terraform](https://developer.hashicorp.com/terraform/downloads) page.
2. Unzip and place `terraform.exe` somewhere in your path.

## Other

For additional installers, refer to the [Download Terraform](https://developer.hashicorp.com/terraform/downloads) page.

---

# Community resources

URL: https://developers.cloudflare.com/turnstile/community-resources/

Community resources for our customers to help them integrate Turnstile.

:::caution

These resources are made by the **community** and not maintained directly by Cloudflare.

As such, Cloudflare is not liable for any damages arising from using them.
:::

:::note

Did we miss your library? [Contribute to our list][1].

[1]: https://github.com/cloudflare/cloudflare-docs/blob/production/CONTRIBUTING.md#pull-request-guidelines

:::

## Client-side rendering libraries

Libraries that only support the client-side rendering of Turnstile:

- React
  - [react-turnstile](https://www.npmjs.com/package/react-turnstile)
  - [@marsidev/react-turnstile](https://www.npmjs.com/package/@marsidev/react-turnstile)

:::note

Cloudflare recommends [@marsidev/react-turnstile](https://www.npmjs.com/package/@marsidev/react-turnstile) when rendering Turnstile. We have deployed an implementation of the library and can confirm that it is safe to use and works as expected.
:::

- Vue
  - [cfturnstile-vue3](https://www.npmjs.com/package/cfturnstile-vue3)
  - [vue-turnstile](https://www.npmjs.com/package/vue-turnstile)
- [Angular](https://www.npmjs.com/package/ngx-turnstile)
- [Svelte](https://www.npmjs.com/package/svelte-turnstile)

## Server-side validation libraries

Libraries that only support the server-side validation of Turnstile:

- [fastify-cloudflare-turnstile](https://www.npmjs.com/package/fastify-cloudflare-turnstile)

## Full-stack libraries

Libraries that both support the both client-side rendering and server-side validation of Turnstile:

- [Nuxt](https://www.npmjs.com/package/@nuxtjs/turnstile)
- [Laravel](https://github.com/romanzipp/Laravel-Turnstile)
- [Phoenix](https://github.com/jsonmaur/phoenix-turnstile)

## Integrations

Turnstile integrations for popular content management systems:

- [Craft CMS](https://plugins.craftcms.com/turnstile)
- [Google Forms](https://github.com/ModMalwareInvestigation/turnstile-for-forms)
- [SilverStripe](https://github.com/webbuilders-group/silverstripe-turnstile)
- [Statamic](https://statamic.com/addons/aryeh-raber/captcha)
- [WordPress](https://wordpress.org/plugins/simple-cloudflare-turnstile)

## Other

Other resources related to integrating Turnstile:

### TypeScript definitions

- [turnstile-types](https://www.npmjs.com/package/turnstile-types)
- [@types/cloudflare-turnstile](https://www.npmjs.com/package/@types/cloudflare-turnstile)

### Additional support

- [Cloudflare Community](https://community.cloudflare.com/c/website-application-performance/turnstile/83)
- [Cloudflare Developers Discord server](https://discord.com/channels/595317990191398933/1025131875397812224)

---

# Demos

URL: https://developers.cloudflare.com/turnstile/demos/

import { ExternalResources, GlossaryTooltip } from "~/components"

Learn how you can use Turnstile within your existing application.

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Turnstile.

<ExternalResources type="apps" products={["Turnstile"]} />

---

# FAQ

URL: https://developers.cloudflare.com/turnstile/frequently-asked-questions/

## What features are available for Free or Enterprise users?

Refer to [Availability](/turnstile/#availability) for more information on features available per tier.

---

## What languages does Turnstile support?

Refer to the [list of supported languages](/turnstile/reference/supported-languages/) for more information.

---

## Does Turnstile conform to WCAG 2.1 Level AA accessibility standard?

Yes, Turnstile is WCAG 2.1 Level AA compliant.

---

## Where can I get additional support for Turnstile?

Refer to the [Cloudflare Community](https://community.cloudflare.com/c/website-application-performance/turnstile/83) or join the [Cloudflare Developers Discord server](https://discord.com/channels/595317990191398933/1025131875397812224).

---

# Changelog

URL: https://developers.cloudflare.com/turnstile/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/turnstile.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Glossary

URL: https://developers.cloudflare.com/turnstile/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Turnstile documentation.

<Glossary product="turnstile" />

---

# Overview

URL: https://developers.cloudflare.com/turnstile/

import {
	Description,
	Feature,
	FeatureTable,
	LinkButton,
	Plan,
	RelatedProduct,
	Render,
} from "~/components";

<Description>

Cloudflareâ€™s smart CAPTCHA alternative.

</Description>

Turnstile can be embedded into any website without sending traffic through Cloudflare and works without showing visitors a CAPTCHA.

![Turnstile Overview](~/assets/images/turnstile/turnstile-overview.png)

<Render file="challenge-behavior" />

Rather than try to unilaterally deprecate and replace CAPTCHA with a single alternative, we built a platform to test many alternatives and rotate new challenges in and out as they become more or less effective.

With Turnstile, we adapt the actual challenge outcome to the individual visitor or browser. First, we run a series of small non-interactive JavaScript challenges gathering more signals about the visitor/browser environment. Those challenges include, proof-of-work, proof-of-space, probing for web APIs, and various other challenges for detecting browser-quirks and human behavior. As a result, we can fine-tune the difficulty of the challenge to the specific request and avoid ever showing a visual puzzle to a user.

Turnstile also includes machine learning models that detect common features of end visitors who were able to pass a challenge before. The computational hardness of those initial challenges may vary by visitor, but is targeted to run fast.

Turnstile [widget types](/turnstile/concepts/widget/) include:

- A non-interactive challenge.
- A non-intrusive interactive challenge (such as checking a box), if the visitor is a suspected bot.
- An invisible challenge to the browser.

---

## Accessibility

Turnstile is WCAG 2.1 AA compliant.

---

## Availability

<FeatureTable id="security.turnstile" />

Refer to [Cloudflare Turnstile's product page](https://www.cloudflare.com/products/turnstile/) for more information on Turnstile's plans.

---

## Get started

{" "}
<LinkButton variant="primary" href="/turnstile/get-started/">
	Get started
</LinkButton> <LinkButton variant="secondary" href="/turnstile/migration/">
	Migration guides
</LinkButton> <LinkButton
	variant="secondary"
	href="https://dash.cloudflare.com/?to=/:account/turnstile"
>
	Dashboard
</LinkButton>

---

## Features

<Feature header="Turnstile Analytics" href="/turnstile/turnstile-analytics/">
	Assess the number of challenges issued, evaluate the challenge solve rate, and
	view the metrics of issued challenges.
</Feature>

<Feature
	header="Pre-Clearance"
	href="/turnstile/concepts/pre-clearance-support/"
>
	Integrate Cloudflare challenges on single-page applications (SPAs) by allowing
	Turnstile to issue a Pre-Clearance cookie.
</Feature>

---

## Related products

<RelatedProduct header="Bots" href="/bots/" product="bots">
	Cloudflare bot solutions identify and mitigate automated traffic to protect
	your domain from bad bots.
</RelatedProduct>

<RelatedProduct header="WAF" href="/waf/" product="waf">
	Get automatic protection from vulnerabilities and the flexibility to create
	custom rules.
</RelatedProduct>

---

# Build AI Applications

URL: https://developers.cloudflare.com/use-cases/ai/

import { ExternalResources, ProductsByTag, ResourcesBySelector, Stream } from "~/components"

Build and deploy ambitious AI applications to Cloudflare's global network.

<Stream id="3c46281a9b2b84ee6776a53f87580c45" title="Choosing the right text generation model" />

## Reference architectures

Diagrams, design patterns, and detailed best practices to help you generate solutions with Cloudflare products.

<ResourcesBySelector tags={["AI"]} types={["reference-architecture","design-guide","reference-architecture-diagram"]} />

## Demo apps

<ExternalResources tags={["AI"]} type="apps" />

## Tutorials

Step-by-step guides to help you build and learn.

<ResourcesBySelector tags={["AI"]} types={["tutorial"]} />

## Customer spotlights

Explore case studies on [AI companies building on Cloudflare](https://workers.cloudflare.com/built-with/collections/ai-workers/).

## Code examples

Examples ready to copy and paste.

<ResourcesBySelector tags={["AI"]} types={["example"]} />

:::note


Cloudflare also offers detailed code examples for various [AI models](/workers-ai/models/) and [Model providers](/ai-gateway/providers/).


:::

## Related products

<ProductsByTag tags={["AI"]} />

---

# Use cases

URL: https://developers.cloudflare.com/use-cases/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Architectures

URL: https://developers.cloudflare.com/vectorize/demos/

import { GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use Vectorize within your existing architecture.

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use Vectorize:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["Vectorize"]} />

---

# Overview

URL: https://developers.cloudflare.com/vectorize/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct, Render } from "~/components"

<Description>

Build full-stack AI applications with Vectorize, Cloudflare's powerful vector database.


</Description>

Vectorize is a globally distributed vector database that enables you to build full-stack, AI-powered applications with [Cloudflare Workers](/workers/). Vectorize makes querying embeddings â€” representations of values or objects like text, images, audio that are designed to be consumed by machine learning models and semantic search algorithms â€” faster, easier and more affordable.

<Render file="vectorize-ga" />


For example, by storing the embeddings (vectors) generated by a machine learning model, including those built-in to [Workers AI](/workers-ai/) or by bringing your own from platforms like [OpenAI](#), you can build applications with powerful search, similarity, recommendation, classification and/or anomaly detection capabilities based on your own data.

The vectors returned can reference images stored in Cloudflare R2, documents in KV, and/or user profiles stored in D1 â€” enabling you to go from vector search result to concrete object all within the Workers platform, and without standing up additional infrastructure.

***

## Features

<Feature header="Vector database" href="/vectorize/get-started/intro/" cta="Create your Vector database">

Learn how to create your first Vectorize database, upload vector embeddings, and query those embeddings from [Cloudflare Workers](/workers/).


</Feature>

<Feature header="Vector embeddings using Workers AI" href="/vectorize/get-started/embeddings/" cta="Create vector embeddings using Workers AI">

Learn how to use Vectorize to generate vector embeddings using Workers AI.


</Feature>

***

## Related products

<RelatedProduct header="Workers AI" href="/workers-ai/" product="workers-ai">

Run machine learning models, powered by serverless GPUs, on Cloudflareâ€™s global network.


</RelatedProduct>

<RelatedProduct header="R2 Storage" href="/r2/" product="r2">

Store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.


</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Limits" href="/vectorize/platform/limits/" icon="document">
Learn about Vectorize limits and how to work within them.
</LinkTitleCard>

<LinkTitleCard title="Use cases" href="/use-cases/ai/" icon="document">
Learn how you can build and deploy ambitious AI applications to Cloudflare's global network.
</LinkTitleCard>

<LinkTitleCard title="Storage options" href="/workers/platform/storage-options/" icon="document">
Learn more about the storage and database options you can build on with Workers.
</LinkTitleCard>

<LinkTitleCard title="Developer Discord" href="https://discord.cloudflare.com" icon="discord">
Connect with the Workers community on Discord to ask questions, join the `#vectorize` channel to show what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="@CloudflareDev" href="https://x.com/cloudflaredev" icon="x.com">
Follow @CloudflareDev on Twitter to learn about product announcements, and what is new in Cloudflare Developer Platform.
</LinkTitleCard>

</CardGrid>

---

# About

URL: https://developers.cloudflare.com/version-management/about/

import { Render } from "~/components"

Version Management works through a combination of **environments** and **versions**.

```mermaid
stateDiagram-v2
    V2: Version 2
    V22: Version 2 <br/>(applied manually)
    V23: Version 2 <br/>(promoted from Development)
    V24: Version 2 <br/>(promoted from Staging)
    Revert: Production (rollback)
    V1: Version 1 <br/>(rolled back due to issues)
    V2 --> Development
    Development --> Staging
    Staging --> Production
    Production --> Revert
    state Development {
        V22
    }
    note right of Development
            At each level, test then promote the version.
        end note
    state Staging {
        V23
    }
    state Production {
        V24
    }
    state Revert {
        V1
    }
    note right of Revert
            Once promoted into an environment, a version can be rolled back.
        end note
```

## Environments

<Render file="environment-definition" /> <br/>

After you [enable](/version-management/how-to/enable/) version management, you will have the ability to create default environments:

<Render file="environment-defaults" />

When you [create](/version-management/how-to/versions/#create-version) a new version, that version will be available to apply to your **Development** environment (or whatever environment has the lowest rank). Once you test a version in your **Development** environment, you would promote that version to the **Staging** environment and - with no issues - then promote it to **Production**.

To send traffic to specific environments, send requests to that environment that match the pattern specified in its [traffic filters](/version-management/reference/traffic-filters/).

## Versions

<Render file="version-definition" /> <br/>

<Render file="enable-default-creation" />

When your version is ready, you would then test and promote it through various environments until it reaches **Production** (or whatever your final environment is).

You can create a new version at any time by choosing to [**Clone**](/version-management/how-to/versions/#create-version) an existing version, which automatically copies over configurations from an existing version.

Version configurations are applied to zone traffic when you [promote a version](/version-management/how-to/environments/#promote-a-version) to a new environment and then send traffic to that environment that matches the pattern specified in its [traffic filters](/version-management/reference/traffic-filters/).

---

# Changelog

URL: https://developers.cloudflare.com/version-management/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/version-management.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Get started

URL: https://developers.cloudflare.com/version-management/get-started/

import { Render } from "~/components"

Follow this tutorial to start testing and deploying zone configuration changes with Version Management.



## Enable versioning

<Render file="enable-versioning" />


## (Optional) Create additional environments

<Render file="enable-default-creation" />

These environments each serve a specific purpose and are accessed differently: <Render file="environment-defaults" />

<Render file="create-environment-situation" /> <br/>

For more details, refer to [Create environment](/version-management/how-to/environments/#create-environment).

## Update configurations

<Render file="edit-version" />

## Test version

<Render file="test-version" />

## Promote version


Next, [promote](/version-management/how-to/environments/#change-environment-version) your version through your different environments.

<Render file="promote-version" />

After promoting to each environment, test the new version in your new environment.

## Repeat

For new changes to your zone, [create a new version](/version-management/how-to/versions/#create-version) and repeat this process.

---

# Overview

URL: https://developers.cloudflare.com/version-management/

import { Description, FeatureTable, Plan, Render } from "~/components"

<Description>

Safely test, deploy, and roll back changes to your zone configurations using Version Management. 
</Description>

<Plan type="enterprise" />

## Benefits

By using Version Management, you can:

* Create independent versions to make changes with no risk of impacting live traffic.
* Safely deploy changes to staging environments ahead of deploy to production.
* Quickly roll back deployed changes when issues occur.

## Availability

<FeatureTable id="account.version_management" />

For access, [enable](/version-management/how-to/enable/) Zone Versioning in the Cloudflare dashboard.

## Limitations

<Render file="product-limitations" />

## Requirements

To use Version Management, the following must all be true:

* Your zone is on an Enterprise plan.
* Your zone is in an [active](/dns/zone-setups/reference/domain-status/) state.
* Your zone uses [WAF managed rules](/waf/managed-rules/).
* Your zone has migrated to use [Custom Rules](/waf/reference/migration-guides/firewall-rules-to-custom-rules/) instead of Firewall Rules (deprecated).
* Your account uses the [new WAF](https://blog.cloudflare.com/new-cloudflare-waf/) (if not, contact your account team).
* Your user account must have a Super Administrator or Administrator [role](/fundamentals/setup/manage-members/roles/).
* Your user account must have an API Key provisioned (if not, [view your API Key](/fundamentals/api/get-started/keys/#view-your-global-api-key)).
* Your user account must have API Access enabled. Refer to [control API Access](/fundamentals/api/how-to/control-api-access/) for more information.
* You must use the dashboard to manage versioning.

---

# Concepts

URL: https://developers.cloudflare.com/waf/concepts/

import { GlossaryTooltip, Render } from "~/components";

<Render file="waf-intro" />

:::note[What is a Web Application Firewall?]

A Web Application Firewall or WAF creates a shield between a web app and the Internet. This shield can help mitigate many common attacks. For a more thorough definition, refer to [Web Application Firewall explained](https://www.cloudflare.com/learning/ddos/glossary/web-application-firewall-waf/) in the Learning Center.
:::

## Rules and rulesets

A [rule](/ruleset-engine/about/rules/) defines a filter and an action to perform on the incoming requests that match the filter.

A [ruleset](/ruleset-engine/about/rulesets/) is an ordered set of rules that you can apply to traffic on the Cloudflare global network.

## Main components

The Cloudflare WAF includes:

- [Managed Rules](/waf/managed-rules/) (for example, the [Cloudflare Managed Ruleset](/waf/managed-rules/reference/cloudflare-managed-ruleset/)), which are signature-based rules created by Cloudflare that provide immediate protection against known attacks.
- [Traffic detections](/waf/detections/) (for example, bot score and attack score) that enrich requests with metadata.
- User-defined rules for your specific needs, including [custom rules](/waf/custom-rules/) and <GlossaryTooltip term="rate limiting" link="/waf/rate-limiting-rules/">rate limiting rules</GlossaryTooltip>.

## Detection versus mitigation

The two main roles of the Cloudflare WAF are the following:

- **Detection**: Run incoming requests through one or more [traffic detections](/waf/detections/) to find malicious or potentially malicious activity. The scores from enabled detections are available in the [Security Analytics](/waf/analytics/security-analytics/) dashboard, where you can analyze your security posture and determine the most appropriate mitigation rules.

- **Mitigation**: Blocks, challenges, or throttles requests through different mitigation features such as [custom rules](/waf/custom-rules/), [Managed Rules](/waf/managed-rules/), and [rate limiting rules](/waf/rate-limiting-rules/). Rules that mitigate traffic can include scores from traffic scans in their expressions to better address possibly malicious requests.

:::caution[Warning]

Enabling traffic detections will not apply any mitigation measures to incoming traffic; detections only provide signals that you can use to define your attack mitigation strategy.
:::

### Available traffic detections

The WAF currently provides the following detections for finding security threats in incoming requests:

- [**Bot score**](/bots/concepts/bot-score/): Scores traffic on a scale from 1 (likely to be a bot) to 99 (likely to be human).
- [**Attack score**](/waf/detections/attack-score/): Checks for known attack variations and malicious payloads. Scores traffic on a scale from 1 (likely to be malicious) to 99 (unlikely to be malicious).
- [**Malicious uploads**](/waf/detections/malicious-uploads/): Scans content objects, such as uploaded files, for malicious signatures like malware.

To enable traffic detections in the Cloudflare dashboard, go to your domain > **Security** > **Settings**.

:::note
Currently, you cannot manage the [bot score](/bots/concepts/bot-score/) and [attack score](/waf/detections/attack-score/) detections from the **Security** > **Settings** page. Refer to the documentation of each feature for availability details.
:::

---

## Rule execution order

Cloudflare evaluates different types of rules when processing incoming requests. The rule execution order is the following:

1. [IP Access Rules](/waf/tools/ip-access-rules/)
2. [Firewall rules](/firewall/cf-firewall-rules/) (deprecated)
3. [Custom rulesets](/waf/account/custom-rulesets/)
4. [Custom rules](/waf/custom-rules/)
5. [Rate limiting rules](/waf/rate-limiting-rules/)
6. [WAF Managed Rules](/waf/managed-rules/)
7. [Cloudflare Rate Limiting](/waf/reference/legacy/old-rate-limiting/) (previous version, deprecated)

Rules are evaluated in order. If there is a match for a rule with a [terminating action](/ruleset-engine/rules-language/actions/), the rule evaluation will stop and the action will be executed immediately. Rules with a non-terminating action (such as _Log_) will not prevent subsequent rules from being evaluated and executed. For more information on how rules are evaluated, refer to [Rule evaluation](/ruleset-engine/about/rules/#rule-evaluation) in the Ruleset Engine documentation.

For more information on the phases where each WAF feature will execute, refer to [WAF phases](/waf/reference/phases/).

---

# Get started

URL: https://developers.cloudflare.com/waf/get-started/

import { Details, GlossaryTooltip } from "~/components";

The Cloudflare Web Application Firewall (Cloudflare WAF) checks incoming web and API requests and filters undesired traffic based on sets of rules called rulesets.

This page will guide you through the recommended initial steps for configuring the WAF to get immediate protection against the most common attacks.

Refer to [Concepts](/waf/concepts/) for more information on WAF concepts, main components, and roles.

:::note
This guide focuses on configuring WAF for individual domains, known as <GlossaryTooltip term="zone">zones</GlossaryTooltip>. The WAF configuration is also available at the account level for Enterprise customers with a paid add-on.
:::

## Before you begin

- Make sure that you have [set up a Cloudflare account](/fundamentals/setup/account/) and [added your domain](/fundamentals/setup/manage-domains/add-site/) to Cloudflare.

- Users on the Free plan have access to the Cloudflare Free Managed Ruleset, a subset of the Cloudflare Managed Ruleset. The Free Managed Ruleset is deployed by default on Free plans and is not specifically covered in this guide.<br/>If you are on a Free plan, you may skip to [5. Review traffic in security dashboards](#5-review-traffic-in-security-dashboards).

## 1. Deploy the Cloudflare Managed Ruleset

The [Cloudflare Managed Ruleset](/waf/managed-rules/reference/cloudflare-managed-ruleset/) protects against Common Vulnerabilities and Exposures (CVEs) and known attack vectors. This ruleset is designed to identify common attacks using signatures, while generating low false positives. Rule changes are published on a weekly basis in the [WAF changelog](/waf/change-log/). Cloudflare may also add rules at any time during emergency releases for high profile zero-day protection.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com), and select your account and domain.
2. Go to **Security** > **WAF** and select the **Managed rules** tab.
3. Under **Managed Rulesets**, select **Deploy** next to the Cloudflare Managed Ruleset.

<Details header="Default settings and ruleset customization">

By default, the Cloudflare Managed Ruleset enables only a subset of rules and it is designed to strike a balance between protection and false positives. You can review and enable additional rules based on your application technology stack.

In particular situations, enabling the managed ruleset can cause some false positives. False positives are legitimate requests inadvertently mitigated by the WAF. For information on addressing false positives, refer to [Handle false positives](/waf/managed-rules/handle-false-positives/).

If you are testing the WAF against pentesting tools, it is recommended that you enable all rules by using the following ruleset configuration:

- **Ruleset action**: _Block_
- **Ruleset status**: _Enabled_ (enables all rules in the ruleset)

For more information on configuring the Cloudflare Managed Ruleset in the dashboard, refer to [Configure field values for all the rules](/waf/managed-rules/deploy-zone-dashboard/#configure-field-values-for-all-the-rules).

</Details>

## 2. Create custom rule based on WAF attack score

:::note
WAF attack score is only available to Business customers (limited access to a single field) and Enterprise customers (full access).
:::

[WAF attack score](/waf/detections/attack-score/) is a machine-learning layer that complements Cloudflare's managed rulesets, providing additional protection against [SQL injection](https://www.cloudflare.com/learning/security/threats/sql-injection/) (SQLi), [cross-site scripting](https://www.cloudflare.com/learning/security/threats/cross-site-scripting/) (XSS), and many [remote code execution](https://www.cloudflare.com/learning/security/what-is-remote-code-execution/) (RCE) attacks. It helps identify rule bypasses and potentially new, undiscovered attacks.

If you are an Enterprise customer, do the following:

1. Reach out to your account team to get access to WAF attack score.

2. [Create a custom rule](/waf/custom-rules/create-dashboard/) using the <GlossaryTooltip term="attack score">Attack Score</GlossaryTooltip> field:

   1. Go to your domain > **Security** > **WAF** and select the **Custom rules** tab.
   2. Create a rule with the following configuration:

      - **If incoming requests match**:

        | Field        | Operator  | Value |
        | ------------ | --------- | ----- |
        | Attack Score | less than | `20`  |

      - **Choose action**: Block

If you are on a Business plan, create a custom rule as mentioned above but use the [WAF Attack Score Class](/waf/detections/attack-score/#available-scores) field instead. For example, you could use the following rule expression: `WAF Attack Score Class equals Attack`.

## 3. Create custom rule based on bot score

:::note
Bot score is only available to Enterprise customers with [Bot Management](/bots/get-started/bm-subscription/). Customers on Pro and Business plans may enable [Super Bot Fight mode](/bots/get-started/pro/) instead.
:::

Customers with access to [Bot Management](/bots/get-started/bm-subscription/) can block automated traffic (for example, from [bots scraping online content](https://www.cloudflare.com/learning/bots/what-is-content-scraping/)) using a custom rule with bot score, preventing this traffic from hitting your application.

1. Go to your domain > **Security** > **WAF** and select the **Custom rules** tab.

2. [Create a custom rule](/waf/custom-rules/create-dashboard/) using the <GlossaryTooltip term="bot score">Bot Score</GlossaryTooltip> and <GlossaryTooltip term="verified bot">Verified Bot</GlossaryTooltip> fields:

   - **If incoming requests match**:

     | Field        | Operator  | Value | Logic |
     | ------------ | --------- | ----- | ----- |
     | Bot Score    | less than | `20`  | And   |
     | Verified Bot | equals    | Off   |       |

   - **Choose action**: Managed Challenge

For a more comprehensive example of a baseline protection against malicious bots, refer to [Challenge bad bots](/waf/custom-rules/use-cases/challenge-bad-bots/#general-protection).

For more information about the bot-related fields you can use in expressions, refer to [Bot Management variables](/bots/reference/bot-management-variables/).

Once you have deployed the Cloudflare Managed Ruleset and rules based on attack score and bot score you will have achieved substantial protection, limiting the chance of false positives.

## 4. Optional - Deploy the Cloudflare OWASP Core Ruleset

After configuring the Cloudflare Managed Ruleset and attack score, you can also deploy the [Cloudflare OWASP Core Ruleset](/waf/managed-rules/reference/owasp-core-ruleset/). This managed ruleset is Cloudflare's implementation of the OWASP ModSecurity Core Rule Set. Its attack coverage significantly overlaps with Cloudflare Managed Ruleset by detecting common attack vectors such as SQLi and XSS.

:::caution
The Cloudflare OWASP Core Ruleset is prone to false positives and offers only marginal benefits when added on top of Cloudflare Managed Ruleset and WAF attack score. If you decide to deploy this managed ruleset, you will need to monitor and adjust its settings based on your traffic to prevent false positives.
:::

1. Go to your domain > **Security** > **WAF** and select the **Managed rules** tab.
2. Under **Managed Rulesets**, select **Deploy** next to the Cloudflare OWASP Core Ruleset.<br/>
   This will deploy the ruleset with the default configuration: paranoia level = _PL1_ and score threshold = _Medium - 40 and higher_.

<Details header="Ruleset configuration">

Unlike the signature-based Cloudflare Managed Ruleset, the Cloudflare OWASP Core Ruleset is score-based. You select a certain paranoia level (levels vary from _PL1_ to _PL4_, where _PL1_ is the lowest level), which enables an increasing larger group of rules. You also select a score threshold, which decides when to perform the configured action. Low paranoia with a high score threshold usually leads to fewer false positives. For an example of how the OWASP Core Ruleset is evaluated, refer to [OWASP evaluation example](/waf/managed-rules/reference/owasp-core-ruleset/example/).

Follow one of these strategies to configure the ruleset according to your needs:

- Start from a strict configuration (paranoia level = _PL4_, score threshold = _Low - 60 and higher_). Reduce the score threshold and paranoia level until you achieve a good false positives/true positives rate for your incoming traffic.
- Alternatively, start from a more permissive configuration (paranoia level = _PL1_, score threshold = _High - 25 and higher_) and increase both parameters to adjust your protection, trying to keep a low number of false positives.

For more information on configuring the Cloudflare OWASP Core Ruleset in the dashboard, refer to [Configure field values for all the rules](/waf/managed-rules/deploy-zone-dashboard/#configure-field-values-for-all-the-rules).

</Details>

## 5. Review traffic in security dashboards

:::note
Users on the Free plan only have access to Security Events.
:::

After setting up your WAF configuration, review how incoming traffic is being affected by your current settings using the following dashboards:

- Use [Security Analytics](/waf/analytics/security-analytics/) to explore all traffic, including traffic not affected by WAF mitigation measures. All data provided by [traffic detections](/waf/concepts/#available-traffic-detections) is available in this dashboard.
- Use [Security Events](/waf/analytics/security-events/) to get more information about requests that are being mitigated by Cloudflare security products.

Enterprise customers can also obtain data about HTTP requests and security events using [Cloudflare Logs](/logs/).

## 6. Optional - Next steps

After configuring the WAF based on the information in the previous sections, you should have a strong base protection against possible threats to your applications.

You can explore the following recommendations to get additional protection for specific use cases.

### Allowlist certain IP addresses

Create a custom rule to [allow traffic from IP addresses in allowlist only](/waf/custom-rules/use-cases/allow-traffic-from-ips-in-allowlist/).

### Block specific countries

Create a custom rule to [block traffic from specific countries](/waf/custom-rules/use-cases/block-traffic-from-specific-countries/).

### Define rate limits

Create a rate limiting rule to [apply rate limiting on a login endpoint](/waf/rate-limiting-rules/use-cases/#example-1).

### Prevent credential stuffing attacks

Use [leaked credential checks](/waf/managed-rules/check-for-exposed-credentials/) to prevent <GlossaryTooltip term="credential stuffing" link="https://www.cloudflare.com/learning/bots/what-is-credential-stuffing/">credential stuffing</GlossaryTooltip> attacks on your applications.

### Prevent users from uploading malware into your applications

:::note
Available to Enterprise customers with a paid add-on.
:::

[Use WAF content scanning](/waf/detections/malicious-uploads/get-started/) to scan content being uploaded to your application, searching for malicious content.

### Get additional security for your APIs

:::note
Available to Enterprise customers.
:::

The Cloudflare WAF protects your APIs from new and known application attacks and exploits such as SQL injection attacks. API-specific security products extend those protections to the unique risks in APIs such as API discovery and authentication management.

For more information on Cloudflare's API security features, refer to [Cloudflare API Shield](/api-shield/).

---

# Glossary

URL: https://developers.cloudflare.com/waf/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's WAF documentation.

<Glossary product="waf" />

---

# Cloudflare Web Application Firewall

URL: https://developers.cloudflare.com/waf/

import {
	Description,
	Feature,
	Plan,
	RelatedProduct,
	Render,
} from "~/components";

<Description>

Get automatic protection from vulnerabilities and the flexibility to create custom rules.

</Description>

<Plan type="all" />

<Render file="waf-intro" />

Learn how to [get started](/waf/get-started/).

---

## Features

<Feature header="Custom rules" href="/waf/custom-rules/">
	Create your own custom rules to protect your website and your APIs from
	malicious incoming traffic. Use advanced features like [WAF attack
	score](/waf/detections/attack-score/) and [malicious uploads
	detection](/waf/detections/malicious-uploads/) in your custom rules.
</Feature>

<Feature header="Rate limiting rules" href="/waf/rate-limiting-rules/">
	Define rate limits for incoming requests matching an expression, and the
	action to take when those rate limits are reached.
</Feature>

<Feature header="Managed rules" href="/waf/managed-rules/">
	Enable the pre-configured managed rulesets to get immediate protection. These
	rulesets are [regularly updated](/waf/change-log/), offering advanced zero-day
	vulnerability protections, and you can adjust their behavior.
</Feature>

<Feature header="Account-level configuration" href="/waf/account/">
	<Plan type="ent-add-on" />
	Create and deploy rulesets to multiple Enterprise zones.
</Feature>

<Feature
	header="Security Events"
	href="/waf/analytics/security-events/"
	cta="Explore Security Events"
>
	Review mitigated requests (rule matches) using an intuitive interface. Tailor
	your security configurations based on sampled logs.
</Feature>

<Feature
	header="Security Analytics"
	href="/waf/analytics/security-analytics/"
	cta="Explore Security Analytics"
>
	Displays information about all incoming HTTP requests, including those not
	affected by security measures.
</Feature>

---

## Related products

<RelatedProduct
	header="DDoS Protection"
	href="/ddos-protection/"
	product="ddos-protection"
>
	Cloudflare DDoS protection secures websites, applications, and entire networks
	while ensuring the performance of legitimate traffic is not compromised.
</RelatedProduct>

<RelatedProduct header="Page Shield" href="/page-shield/" product="page-shield">
	Page Shield is a comprehensive client-side security solution to ensure the
	safety of your website visitors' browser environment.
</RelatedProduct>

<RelatedProduct header="Bots" href="/bots/" product="bots">
	Cloudflare bot solutions identify and mitigate automated traffic to protect
	your domain from bad bots.
</RelatedProduct>

---

# Overview

URL: https://developers.cloudflare.com/warp-client/

import {
	Description,
	Feature,
	Plan,
	RelatedProduct,
	Render,
} from "~/components";

<Description>

Connect to the Internet faster and in a more secure way.

</Description>

<Plan type="all" />

The Cloudflare WARP client allows individuals to have a faster, more secure, and more private experience online. The WARP client sits between your device and the Internet, and has several connection modes to better suit different needs.

<Render file="consumer-warp-download" />

## Features

<Feature header="WARP modes" href="/warp-client/warp-modes/">
	The WARP client has several modes to better suit your connection needs.
</Feature>

<Feature header="OS clients" href="/warp-client/get-started/">
	WARP is available to several operating systems, including iOS and Android.
</Feature>

---

## Related products

<RelatedProduct header="1.1.1.1" href="/1.1.1.1/" product="1.1.1.1">
	1.1.1.1 is Cloudflareâ€™s public DNS resolver. It offers a fast and private way
	to browse the Internet.
</RelatedProduct>

<RelatedProduct
	header="WARP for Zero Trust"
	href="/cloudflare-one/connections/connect-devices/warp/"
	product="cloudflare-one"
>
	The enterprise version of WARP allows organizations to apply security policies
	to corporate devices.
</RelatedProduct>

---

# FAQ

URL: https://developers.cloudflare.com/warp-client/known-issues-and-faq/

Below you will find answers to our most commonly asked questions regarding the WARP client. If you cannot find the answer you are looking for, refer to the [community page](https://community.cloudflare.com/) to explore more resources.

## Why am I not connecting to a closer Cloudflare data center?

As our [Network Map](https://www.cloudflare.com/network/) shows, we have locations all over the globe. However, in the Advanced Connection stats of our application, you may notice that the server you are connecting to is not necessarily the one physically closest to your location. This can be due to a number of reasons:

* We work hard to prevent it, but sometimes your nearest server might be having problems. [Check the system status](https://www.cloudflarestatus.com/?_ga=2.155811579.1117044671.1600983837-1079355427.1599074097) for more information.
* Your Internet provider may choose to route traffic along an alternate path for reasons such as cost savings, reliability, or other infrastructure concerns.
* Not all Cloudflare locations are WARP enabled. We are constantly evaluating performance and how users are connecting, bringing more servers online with WARP all the time.

## Does WARP reveal my IP address to websites I visit?

No. 1.1.1.1 + WARP replaces your original IP address with a Cloudflare IP that consistently and accurately represents your approximate location. This happens regardless of whether the site is on the Cloudflare network or not.

Refer to our [blog post](https://blog.cloudflare.com/geoexit-improving-warp-user-experience-larger-network/) for more information on this topic.


## Why has my throughput dropped while using WARP?

Cloudflare WARP is in part powered by [1.1.1.1](/1.1.1.1/), the world's fastest DNS resolver. When visiting sites or going to a new location on the Internet, you should see fast DNS lookups. WARP, however, is built to trade some throughput for enhanced privacy, by encrypting all traffic both to and from your device. While this is not noticeable at most mobile speeds, on desktop systems in countries where high-speed broadband is available, you may notice a drop. We think the tradeoff is worth it and continue to work on improving performance all over the system.

## "What about the performance of the WARP app?

Cloudflare WARP and the 1.1.1.1 with WARP applications go through performance testing that includes battery, network and CPU on a regular basis. In addition, both applications are used by millions of users worldwide that help us stay on top of issues across a wide variety of devices, networks, sites and applications.


## What is the version of .NET Framework required for the Windows client?


The WARP client for Windows requires .NET Framework version 4.7.2 or later to be installed on your computer.




***

## Known issues

* Applications or sites that rely on location information to enforce content licensing agreements (for example, certain games, video streaming, music streaming, or radio streaming) may not function properly. We are working on a product update that will allow these clients to work, by not sending their traffic through WARP.

* Refer to [Known Limitations](/cloudflare-one/connections/connect-devices/warp/troubleshooting/known-limitations/)
  for information on devices, software, and configurations that are incompatible with Cloudflare WARP.

---

# Privacy

URL: https://developers.cloudflare.com/warp-client/privacy/

The WARP Client application uses a VPN profile and/or service that enables us to intercept and secure your DNS queries and to transmit data from your device through the Cloudflare network, depending on the services you have enabled. We only collect limited DNS query and traffic data (excluding payload) that is sent to our network when you have the app enabled on your device. All information is handled in accordance with our [Privacy Policy](https://www.cloudflare.com/application/privacypolicy/).

## What does Cloudflare do with your data?

When not part of a Cloudflare for Teams organization, Cloudflare stores the absolute minimum amount of data we believe is required to provide the service. We will not sell, rent, share or otherwise disclose your personal information to anyone except as necessary to provide our services or as otherwise described in our Privacy Policy without first providing you with notice and the opportunity to consent. We do not use the data to identify who you are or what you are doing on the Internet beyond the exceptions below. We do not store any data except as set forth in our Privacy Policy. Such data may include: your app installation ID, the amount of data transferred through Cloudflare's network, and your average speed when you are using the WARP Client application.

## How Cloudflare Uses User Data:

### Registration ID

Cloudflare uses a random identifier generated when you install the app to give you referral bonuses for referring the app to others.

### Data Transferred

Cloudflare tracks the amount of data your WARP installation has transferred to keep track of your WARP+ usage. When you refer friends to WARP, this quota is increased.

### Average Speed

Cloudflare uses this data to understand how much faster the WARP Client application is making your Internet connection. Knowing this information helps us improve the application in your region and on your mobile carrier or Internet provider.

### Aggregate Usage

Cloudflare tracks the aggregate amount of traffic by website and by region. Knowing this information helps Cloudflare plan better when we should build future data centers.

---

# WARP modes

URL: https://developers.cloudflare.com/warp-client/warp-modes/

The WARP client has several modes to better suit different connection needs.

## 1.1.1.1

1.1.1.1 is Cloudflareâ€™s public DNS resolver. It offers a fast and private way to browse the Internet. It also offers a DNS encryption service through DNS over HTTPS (DoH) or DNS over TLS (DoT) for increased security and privacy.

Refer to [1.1.1.1 resolver](/1.1.1.1/encryption/) to learn more about DNS encryption.

## 1.1.1.1 with WARP

The WARP application uses [BoringTun](https://blog.cloudflare.com/boringtun-userspace-wireguard-rust/) to encrypt and secure the traffic from your device and send it directly to Cloudflare's edge network. This ensures Internet traffic between your device and the Internet is secure and private, while also preventing third parties from accessing your traffic. If the site you are visiting is already a Cloudflare customer, the content is immediately sent to your device. If not, Cloudflare uses its global network of data centers to devise the shortest path to the site.

For more information, refer to our blog post [Introducing WARP: Fixing Mobile Internet Performance and Security](https://blog.cloudflare.com/1111-warp-better-vpn/).

:::caution[Warning]


WARP does not provide anonymity, and it is not designed to prevent servers you communicate with from identifying you. WARP also does not allow you to pretend to be accessing the Internet from a different country.


:::

## WARP via Local Proxy

Currently, this mode is available on desktop clients only. When WARP is configured as a local proxy, only the applications that you configure to use the proxy (HTTPS or SOCKS5) will have their traffic sent through WARP. This allows you to pick and choose which traffic is encrypted â€” for example, your web browser or a specific application. Everything else will not be encrypted and will be sent over a regular Internet connection.

Because this feature restricts WARP to just applications configured to use the local proxy, leaving all other traffic over the Internet unencrypted by default, we have hidden it in the **Advanced** menu. To turn it on:

1. Navigate to **Preferences** > **Advanced** and select **Configure Proxy**.
2. On the window that opens, check the box and configure the port you want to listen on.

This will enable the **WARP via Local Proxy** option in the **WARP Settings** menu.

If you enable [FIPS compliance](/cloudflare-one/policies/gateway/http-policies/tls-decryption/#fips-compliance) for TLS decryption, you must [disable QUIC](/cloudflare-one/policies/gateway/http-policies/http3/#force-http2-traffic) in your users' browsers. Otherwise, HTTP/3 traffic will bypass inspection by the WARP client.

## WARP+ Unlimited

While WARP can take advantage of the many Cloudflare data centers around the world to give you a more private and robust connection, WARP+ Unlimited subscribers get access to a larger network. More cities to connect to means you are likely to be closer to a Cloudflare data center, which can reduce latency and improve your browsing speed.

WARP+ Unlimited is a paid, monthly subscription that can be purchased via the Apple App Store and Google Play Store.

---

# About

URL: https://developers.cloudflare.com/waiting-room/about/

import { LinkButton, Render } from "~/components"

Waiting Room queues visitors when your traffic approaches a previously defined threshold that might otherwise bring an application down.

![Waiting Room process flow showing how a request is managed by Cloudflare and placed in a waiting room before reaching the origin website](~/assets/images/waiting-room/waiting-room-process-flow.png)

## User flow

Once you have [created and activated a waiting room](/waiting-room/get-started/) for a specific application page:

- If a page is not experiencing heavy traffic, a visitor accesses the page directly.
- If page traffic approaches a [user-defined threshold](/waiting-room/reference/configuration-settings/#session-duration), a visitor enters a virtual waiting room until it is their turn to access the page:

  - Each user receives a [cookie](/waiting-room/reference/waiting-room-cookie/) to manage the dynamic outflow of requests from the waiting room to the origin website in [First In First Out (FIFO)](/waiting-room/reference/queueing-methods/#first-in-first-out-fifo) order.
  - While in the waiting room, the user's browser automatically refreshes every 20 seconds to give them updated information about their estimated wait time.
  - When a user exits the waiting room and reaches your application, they can leave and re-enter without waiting for the length of time specified by the [session duration](/waiting-room/reference/configuration-settings/#session-duration).
  - Because waiting rooms support dynamic inflow and [outflow](/waiting-room/reference/configuration-settings/#session-duration), new spots appear more quickly and estimated wait times are lower and more accurate.

## Architecture

Waiting Room is built on [Workers](/workers/) that runs across a global network of Cloudflare data centers.

When a request comes to a host or path covered by a Waiting Room, that request goes to a Waiting Room Worker in the closest geographic data center. The Worker then needs to make a decision: whether to send users to the queue or the website.

That decision itself depends on two factors: [admin-defined thresholds](/waiting-room/reference/configuration-settings/) and the Waiting Room state.

For admin-defined thresholds, the two measures that matter are `total active users` and `new users per minute`:

- `total active users` is a target threshold for how many simultaneous users you want to allow on the pages covered by your waiting room.

- `new users per minute` defines the target threshold for the maximum rate of user influx to your website per minute.

A sharp spike in either of these values might result in queuing. Another configuration that affects how we calculate `the total active users` is `session duration`. A user is considered active for `session duration` minutes since the request is made to any page covered by a waiting room.

The other factor is the Waiting Room state, which is maintained at the local data center level but then also changes continuously based on the traffic around the world. Each data center works with its own Waiting Room state. This state is a snapshot of the traffic pattern for the website around the world available at that point in time. The advantage of using this approach - making decisions at the Worker level - is that we can make decisions without any significant latency added to the request. The algorithm for Waiting Room dynamically allocates a certain number of slots available to each Worker based on the Waiting Room state. Queueing starts when the slots run out within the Worker. The lack of additional latency added enables the customers to turn on the waiting room all the time without worrying about extra latency to their users.

The Waiting Room state is updated with global information every few seconds. We have a pipeline set up in Cloudflare [Durable Objects](/durable-objects/) that ensures changes in traffic get propagated around the world. This architecture ensures that we do not introduce additional latency, as well as that we are making decisions with as near-time accuracy as possible.

For even more details about the architecture and why we made these decisions, refer to our [deep-dive technical blog](https://blog.cloudflare.com/how-waiting-room-queues).

---

# Changelog

URL: https://developers.cloudflare.com/waiting-room/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/waiting-room.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Get started

URL: https://developers.cloudflare.com/waiting-room/get-started/

import { Render } from "~/components"

***

## Before you begin

Before you start this tutorial, make sure you have:

* Reviewed the [About](/waiting-room/about/) Waiting Room page.).
* Reviewed your [rate limiting rules](/waf/rate-limiting-rules/) to make sure they allow at least one request every 20 seconds (required for automatic page refreshes).

***

## Step 1 â€” Plan out your waiting room

Before you create your waiting room, think about how you want it to appear and operate.

### Location

Which page will you cover with a waiting room? You can only have one waiting room per page, so you need to identify the high-traffic areas of your website.

Specify the URL for your page by setting the `hostname` and `path` in your [configuration settings](/waiting-room/reference/configuration-settings/).

Advanced Waiting Room customers can also [specify multiple hostname and path combinations](/waiting-room/how-to/place-waiting-room/) for the same zone.

### Access method

You can direct visitors to your high-traffic page:

* Directly (via URL)
* Indirectly (via [a redirect](/rules/url-forwarding/bulk-redirects/))

### Queue activation

When you [activate your waiting room](#step-3--activate-your-waiting-room), choose whether:

* [**All visitors**](#queue-all-visitors) to be queued, in preparation for a product release or other time-based event.
* Only [**some visitors**](#queue-some-visitors) to be queued, as traffic reaches the thresholds defined in `Total active users` and `New users per minute`.

## Step 2 â€” Create your waiting room

Create your waiting room by:

* Using the [dashboard](/waiting-room/how-to/create-waiting-room/).
* Using the [API](/waiting-room/how-to/create-waiting-room/).

### Appearance (optional)

Some customers can [customize the design](/waiting-room/how-to/customize-waiting-room/) of their waiting room by editing the page's HTML and CSS.

If you have this ability, think about how you want the page to appear.

### Prepare your waiting room for mobile application traffic

If you need to manage traffic in a non-browser environment such as a mobile app or web app, use a [JSON-friendly waiting room](/waiting-room/how-to/json-response/) that can be consumed via your API endpoints. Note that if you have a mobile app or web app that depends on resources that would be protected by a waiting room, you will need to update those clients to handle Waiting Room appropriately.

## Step 3 â€” Activate your waiting room

Depending on your [queue activation](#queue-activation), you may deploy your waiting room differently.

### Queue some visitors

<Render file="queue-some" />

### Queue all visitors

<Render file="queue-all" />

## Step 4 â€” Next steps

After you have created and deployed your first waiting room, you might also want to:

* [Test your waiting room](/waiting-room/additional-options/test-waiting-room/) before it goes live.
* [Monitor your traffic](/waiting-room/how-to/monitor-waiting-room/) in real time.
* [Troubleshoot](/waiting-room/troubleshooting/) potential issues.

---

# Glossary

URL: https://developers.cloudflare.com/waiting-room/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Waiting Room documentation.

<Glossary product="waiting-room" />

---

# Cloudflare Waiting Room

URL: https://developers.cloudflare.com/waiting-room/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct, Render } from "~/components"

<Description>

A virtual waiting room to manage peak traffic.
</Description>

<Plan type="business" />

Cloudflare Waiting Room allows you to route excess users of your website to a customized waiting room, helping preserve customer experience and protect origin servers from being overwhelmed with requests.

***

## Benefits

Waiting Room protects your origin server by preventing surges in legitimate traffic that may overload your origin.

Waiting Room also benefits your visitors by:

- Keeping your application online and preventing them from reaching error pages.
- Showing estimated wait times that are continuously updated.
- Opening up new spots more quickly by tracking dynamic inflow and [outflow](/waiting-room/reference/configuration-settings/#session-duration).
- Remembering each visitor's status to prevent someone from losing their place in line or having to re-queue if they leave your site.
- Appearing in your own [branding and style](/waiting-room/how-to/customize-waiting-room/), which enhances trust and lets you provide additional information as needed.

---

## Features

<Feature header="Scheduled Event" href="/waiting-room/additional-options/create-events/">
Customize the behavior of a waiting room for a specific period of time.
</Feature>

<Feature header="Waiting Room Rules" href="/waiting-room/additional-options/waiting-room-rules/">
Create rules to indicate specific traffic or areas of your site or application that you do not want a waiting room to apply to.
</Feature>

<Feature header="Waiting Room Analytics" href="/waiting-room/waiting-room-analytics/">
Get insights into the traffic going through your waiting room.
</Feature>

<Feature header="Additional hostname and path coverage" href="/waiting-room/how-to/place-waiting-room/">
Apply a single waiting room to multiple hostnames and paths within the same zone.
</Feature>

***

## Related products

<RelatedProduct header="Cloudflare for SaaS" href="/cloudflare-for-platforms/cloudflare-for-saas/" product="cloudflare-for-platforms">
Cloudflare for SaaS allows you to extend the security and performance benefits of Cloudflareâ€™s network to your customers via their own custom or vanity domains.
</RelatedProduct>

<RelatedProduct header="Rules" href="/rules/" product="rules">
Cloudflare Rules allows you to make adjustments to requests and responses, configure Cloudflare settings, and trigger specific actions for matching requests.
</RelatedProduct>

<RelatedProduct header="SSL/TLS" href="/ssl/" product="ssl">
Cloudflare SSL/TLS encrypts your web traffic to prevent data theft and other tampering.
</RelatedProduct>

***

## Availability

The following customers have access to Cloudflare Waiting Room:

- Those qualified under [Project Fair Shot](https://www.cloudflare.com/fair-shot/)
- Customers on a Business or Enterprise plan

Access to certain features depends on a customer's [plan type](/waiting-room/plans/).

<Render file= "non-contract-enablement" product="fundamentals" />

***

## Prerequisites

- [Cloudflareâ€™s CDN](/cache/) is required to use the Waiting Room feature.
- Configure a [proxied DNS record](/dns/manage-dns-records/how-to/create-dns-records/) or a [proxied load balancer](/load-balancing/understand-basics/proxy-modes/) for the waiting roomâ€™s hostname. A DNS record is not auto-configured after a waiting room is created.
- Visitors must enable cookies. Refer to [Waiting Room cookies](/waiting-room/reference/waiting-room-cookie/) for information on how cookies are used in Cloudflare Waiting Room.

***

## More resources

<CardGrid>

<LinkTitleCard title="Pricing" href="https://www.cloudflare.com/plans/" icon="seti:shell">
Explore pricing options for Waiting Room.
</LinkTitleCard>

</CardGrid>

---

# Plans

URL: https://developers.cloudflare.com/waiting-room/plans/

import { FeatureTable, Render } from "~/components"

The features available for a waiting room depend on your plan type. You can only have **one plan** per zone.

<FeatureTable id="traffic.waiting_room" />

<Render file="non-contract-enablement" product="fundamentals" />

## How do I get started?

To get started with Waiting Room, review our [setup guide](/waiting-room/get-started/).

---

# FAQ

URL: https://developers.cloudflare.com/waiting-room/troubleshooting/

Below you will find answers to our most commonly asked questions about the Waiting Room.

* [Configuration](#configuration)
* [Features and products](#features-and-products)
* [User behavior](#user-behavior)
* [Monitor your waiting room](#monitor-your-waiting-room)

***

## Configuration

### Can I display my waiting room page in another language?

Yes. For more details, refer to [Customize a waiting room](/waiting-room/how-to/customize-waiting-room/).


### Why does my waiting room look different than how I designed it?

If you have [customized your waiting room template](/waiting-room/how-to/customize-waiting-room):

1. Preview your template before deploying it to production.
2. If you encounter any issues, check for proper syntax and a closing backslash (/).

:::note[Note]


Only Enterprise customers can customize the appearance of their waiting room.


:::

### What can I update when my waiting room is actively queueing?

You can update a [waiting room's template](/waiting-room/how-to/customize-waiting-room) and those changes will be visible to users in near-real time. We recommend these updates as a way to engage with users and provide updated information or expectations.

You can also update the [configuration settings](/waiting-room/reference/configuration-settings) of a waiting room, but only make these changes when necessary. These changes may impact the estimated wait time shown to end users and cause unnecessary confusion.




## Features and products

### Which features are included in my Waiting Room plan?

To check which features are available to different plan types, refer to [Plans](/waiting-room/plans/).



### How does Waiting Room interact with other Cloudflare products?

Some Cloudflare products run before a waiting room acts on traffic:

* DDoS Mitigation
* Web Application Firewall (WAF)
* Bot Management
* Page Rules

Other Cloudflare products run after a waiting room acts on traffic:

* Workers




## User behavior


### What happens if a user refreshes their tab when in a waiting room?

A manual tab refresh has no effect on a user's position in your waiting room.

However, if they close their tab and then try to access the application again during active queueing, they will lose their spot and have to go to the back of the queue.




## Monitor your waiting room


### Why do I observe a few users being queued in the dashboard?


Some users might be queued before your waiting room reaches is limit due to architectural designs. For more details on the behavior and how to fix it, refer to [â€‹â€‹Queueing activation](/waiting-room/how-to/monitor-waiting-room#queueing-activation).



### Why are some users not being queued in my waiting room?


If you notice users not being queued to your waiting room, make sure the path you defined exactly matches the path of your website.

The path is case-sensitive, so if you have a waiting room set up for `/Black-Friday-Sale` and users go to `/black-friday-sale`, they will bypass your waiting room.

For more details, refer to [Best practices](/waiting-room/reference/best-practices).


### Why are users being blocked from entering my waiting room?

If you have Rate Limiting, check your [rate limiting rules](/waf/rate-limiting-rules/).

The Waiting Room queue page refreshes every 20 seconds by populating the refresh header. If you have a rule set to block requests from a specific IP within 20 seconds, the user in the waiting room will be blocked. Make sure your rules allow at least one request every 20 seconds.

Your user also might not have [cookies](/waiting-room/reference/waiting-room-cookie) enabled. If they do not enable cookies and your waiting room is actively queueing traffic, they will not reach your endpoint until the queueing stops.


### Why is the estimated wait time increasing for some users?

Estimated wait times may increase if the rate of users leaving your site decreases. The estimated wait time is updated upon each page refresh based on the most recently available information about the rate of slots opening up on your site and the number of users ahead of the user in line. To make this increase less likely, you could limit the amount of time users are allowed to spend on your site by disabling session renewal. Be aware that if you change your traffic settings, estimated wait times will change as well.

---

# Waiting Room Analytics

URL: https://developers.cloudflare.com/waiting-room/waiting-room-analytics/

import { Details } from "~/components"

Waiting Room Analytics gives you historical insights into the traffic going through your waiting room compared to your waiting room settings. Data is stored for the past 30 days.

Using Waiting Room Analytics, you can:

* Evaluate peak traffic flow through your waiting room and onto your site.
* Determine how long users spent in the waiting room.
* Use analytics to help calibrate your waiting room settings.

## â€‹Dashboard Analytics

To access your waiting roomâ€™s analytics in the dashboard:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Traffic** > **Waiting Room**.
3. Expand the waiting room you would like to review metrics for, to display a preview of your waiting room analytics. The preview gives you insights into peak traffic through your waiting room over the last 24 hours including: Maximum active users, Maximum queued users and Typical time in queue for queued users.
4. Select **View More** under the Waiting Room Analytics section to get more historical analytics for your waiting room.
5. The time range for all of the metrics displayed defaults to the last 24 hours. To change the time range, select from the drop down. You can select any time range from the last 30 days that is a minimum of 30 minutes.

## Event Analytics

If your waiting room has a completed scheduled event, you can quickly access the eventâ€™s analytics by expanding the row for the waiting room you are interested in and selecting the event time. The link opens the analytics view for that waiting room, including information from the pre-queueing period to the end of the event.

To save this event information, you can either select **Download data** or **Print report**. If you delete the event, the time period link will no longer appear in your dashboard. If you edit the timing of the event, the time period link will update as well.

If you do not get a link to your eventâ€™s analytics, one of the following may have happened:

* Your event has not happened yet.
* Your event started more than 30 days ago.

## Metrics

These are metrics available in the Analytics dashboard and how they are calculated.

### Time in queue

Time in queue summary values give you an insight into the user experience by indicating how long queued users spent waiting to enter your application. It displays the time waited for the typical user, as well as for those who waited the longest, for the time period you have selected. These values are an indicator of the impact your waiting room settings combined with the traffic to your waiting room had on wait times.

If wait times are higher than you would like, and you feel comfortable doing so, you could consider taking any or all of the following actions:

* Increase `total_active_users` configured value.
* Increase `new_users_per_minute` configured value.
* Decrease `session_duration`.
* Disable session renewal.

:::note


Note that wait times are only calculated for users who went from the waiting room to your origin.


:::

### Time on origin

Time on origin summary values estimate how long users spent on the pages covered by your waiting room before leaving. For the time period selected, you will have access to the estimated time spent on origin for the typical user, as well as the time on origin for those who spend the most time on your site. Keep in mind that if your session renewal is disabled and there is no active queueing, users are issued a new waiting room session every `session_duration` minutes.  Therefore, these users may be staying for multiple sessions.  The time on origin for these users restarts each time a session expires.

The following are some takeaways you could have depending on the time on origin values.

You may want to increase session duration, giving users more time to make subrequests, and/or enable session renewal if:

* You have session renewal disabled.
* You have frequent, active queueing with long wait times.
* The typical time on origin is around 70% of your configured session duration.

These may be indicators that users need more time to complete their desired tasks on your site.

You may want to decrease session duration and/or disable session renewal if:

* Your top 5% time on origin is less than 70% of your configured session duration.
* You are seeing high queue times and do not want to increase traffic limits.

These may be indicators that users do not need as much time on your site and are taking up spots on your origin.

### Active users vs. queued users

The Active users chart is a time series chart that displays the maximum active users on any URLs covered by your waiting room as well as maximum queued users. These values are shown compared to your configured active user target threshold.

A new user is a novel request made to any URLs covered by the waiting room. Waiting Room counts the request as new if no waiting room cookie is tied to the request. Once the request is made, a waiting room cookie is issued. If there is an active queue, the user will be considered a queued user. Once that user makes it through the queue and onto the site, they are now an active user and remain active as long as they keep making HTTP requests to waiting room URLs at least once every `session_duration` minutes.

To identify and hone in on peak traffic, select a longer time period, such as 30 days. Then, drag your cursor to the left and right of any time period you would like to check with more granularity to zoom in. You can zoom in until each bar represents a one minute interval. All other metrics on the page will update automatically to reflect the data behind the time period selected.

To check for more details about a particular moment in time, hover over a bar on the graph. This displays a tooltip which will indicate the following for the time period that bar represents:

* Maximum active users reached
* Maximum queued users reached
* Configured active user target values

Queueing may occur below your configured limits, and active users may sometimes exceed your configured limits. Refer to the [Queuing activation](/waiting-room/how-to/monitor-waiting-room/#queueing-activation) section for more information.

### New users per minute

The New users per minute chart shows how many new users per minute passed through the waiting room to your origin compared to your configured New users per minute target threshold. Like the Active users chart, you can zoom in by highlighting to the left and right of the time period you are interested in, which will update the other chart as well as summary values. As you zoom out, each data point is averaged. Therefore, as you zoom in, values may fluctuate.

### Turnstile Widget Traffic

The Turnstile widget traffic chart shows the number of challenges issued per minute and the distribution of traffic seen with these challenges. Traffic is categorized into three main categories: 

- Likely Human - This represents the number of challenges that were successfully solved. 
- Likely Bots - This represents the number of unsolved challenges.
- Bots - This represents the number of failed challenges.

If your waiting room has the infinite queue option enabled, you will see a line on the graph representing the number of refresh requests from bots in the infinite queue.

## â€‹â€‹GraphQL Analytics

You can query your Waiting Room analytics data via GraphQL API. Waiting Room analytics provides near real-time visibility into your Waiting Room, allowing you to visualize the traffic to your application and how it is managed respecting the configured limits.

Here are some query examples to get started:


<Details header="Fetch values for total active users and new users per minute over a certain period.">

This is a simple query to fetch metrics values. You can filter the data with the zone tag and query the  `waitingRoomAnalyticsAdaptive` dataset. In this example, we have applied this query only on two metrics, but you can explore the schema and fetch the raw values from the GraphQL dataset without applying any aggregation methods.

```bash title="Request"
{
  viewer {
    zones(filter: {zoneTag: "example-zone"}) {
      waitingRoomAnalyticsAdaptive(limit: 3, filter: {datetime_gt: "2023-03-05T19:14:30Z", datetime_lt: "2023-03-07T19:13:00Z", waitingRoomId: "example-waiting-room-id"}) { 
        totalActiveUsers
          newUsersPerMinutes
      }
    }
  }
```

```json title="Response"
{
  "data": {
    "viewer": {
      "zones": [
        {
          "waitingRoomAnalyticsAdaptive": [
            {
              "newUsersPerMinute": 77,
              "totalActiveUsers": 1023
            },
            {
              "newUsersPerMinute": 113,
              "totalActiveUsers": 1009
            },
            {
              "newUsersPerMinute": 99,
              "totalActiveUsers": 927
            }
          ]
        }
      ]
    }
  },
  "errors": null
}
```


</Details>


<Details header="Find the average of total active users and new users per minute over a certain period, and aggregate this data over a period of 15 minutes.">

This query calculates the average of total active users and new users per minute. The time dimension in the query is 15 minutes, therefore the data is aggregated over 15 minutes for the selected time period.

```bash title="Request"
{
  viewer {
    zones(filter: {zoneTag: "example-zone"}) {
      waitingRoomAnalyticsAdaptiveGroups(limit: 10, filter: {datetime_geq: "2023-03-15T04:00:00Z", datetime_leq: "2023-03-15T04:45:00Z", waitingRoomId: "example-waiting-room-id"}, orderBy: [datetimeFifteenMinutes_ASC]) {
        avg {
          totalActiveUsers
          newUsersPerMinute
        }
        dimensions {
          datetimeFifteenMinutes
        }
      }
```

```json title="Response"
{
  "data": {
    "viewer": {
      "zones": [
        {
          "waitingRoomAnalyticsAdaptiveGroups": [
            {
              "avg": {
                "newUsersPerMinute": 119,
                "totalActiveUsers": 1180
              },
              "dimensions": {
                "datetimeFifteenMinutes": "2023-03-15T04:00:00Z"
              }
            },
            {
              "avg": {
                "newUsersPerMinute": 146,
                "totalActiveUsers": 961
              },
              "dimensions": {
                "datetimeFifteenMinutes": "2023-03-15T04:15:00Z"
              }
            },
            {
              "avg": {
                "newUsersPerMinute": 144,
                "totalActiveUsers": 1015
              },
              "dimensions": {
                "datetimeFifteenMinutes": "2023-03-15T04:30:00Z"
              }
            }
          ]
        }
      ]
    }
  },
  "errors": null
}
```


</Details>


<Details header="Find the weighted averages of time on origin (50th percentile) and total time waited (90th percentile) for a certain period and aggregate this data over one hour.">

This query calculates the weighted averages of the metrics for a certain period of time aggregated hourly.

```bash title="Request"
{
  viewer {
    zones(filter: {zoneTag: "example-zone"}) {
      waitingRoomAnalyticsAdaptiveGroups(limit: 10, filter: {datetime_geq: "2023-03-15T04:00:00Z", datetime_leq: "2023-03-15T04:45:00Z", waitingRoomId: "example-waiting-room-id"}, orderBy: [datetimeHour_ASC]) {
        avgWeighted {
          timeOnOriginP50
          totalTimeWaitedP90
        }
        dimensions {
          datetimeHour
        }
      }
```

```json title="Response"
{
  "data": {
    "viewer": {
      "zones": [
        {
          "waitingRoomAnalyticsAdaptiveGroups": [
            {
              "avgWeighted": {
                "timeOnOriginP50": 99.19,
                "totalTimeWaitedP90": 1625.63
              },
              "dimensions": {
                "datetimeHour": "2023-03-15T04:00:00Z"
              }
            }
          ]
        }
      ]
    }
  },
  "errors": null
}
```


</Details>

## Why is there no data for my waiting room?

If you are not seeing any historical data for your waiting room, one or more of the following may be true:

* Your waiting room was not receiving any traffic for the time period you are inspecting.
* Your waiting room was not enabled for the time period you are inspecting.

---

# About

URL: https://developers.cloudflare.com/web-analytics/about/

import { LinkButton, Render } from "~/components"

<Render file="web-analytics-definition" />

Web Analytics supports Adaptive Bit Rate (ABR). Cloudflare's servers will select the best resolution for each chart or table depending on the size of the data, the date range, your network connection, and other factors. For more information, refer to [Explaining Cloudflare's ABR Analytics](https://blog.cloudflare.com/explaining-cloudflares-abr-analytics/).

The data displayed in Web Analytics is real user monitoring (RUM). For more information, refer to [Real User Monitoring](https://en.wikipedia.org/wiki/Real_user_monitoring).

Cloudflare Web Analytics does not collect or use your visitorsâ€™ personal data.

 <LinkButton variant="primary" href="http://dash.cloudflare.com/sign-up/web-analytics">Get started</LinkButton> <LinkButton variant="secondary" href="/web-analytics/data-metrics/">Learn more</LinkButton>

---

# Changelog

URL: https://developers.cloudflare.com/web-analytics/changelog/

import { ProductReleaseNotes } from "~/components";

Cloudflare occasionally updates the `beacon.min.js` file to improve Web Analytics functionality. The table below includes a log of what changed in the `beacon.min.js` file and when.

{/* <!-- Actual content lives in /src/content/release-notes/web-analytics.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# FAQs

URL: https://developers.cloudflare.com/web-analytics/faq/

import { Render } from "~/components"

Below you will find answers to our most commonly asked questions. If you cannot find the answer you are looking for, refer to the [community page](https://community.cloudflare.com/) to explore more resources.

* [Errors](#errors)
* [Setup](#setup)
* [Functionality](#functionality)

## Errors

### When I add the beacon to my website and load the webpage, I see an error that includes `is not allowed by Access-Control-Allow-Origin` (CORS). What is happening?


This error usually occurs when the hostname of the site loading the analytics does not match the name of the analytics site configured in the dashboard. Double-check that they are identical.

Cloudflare matches hostnames based on a postfix. For example, if you set up analytics for `example.com`, we will allow analytics from `www.example.com`, `blog.staging.example.com`, and `fooexample.com`. However, we will not allow analytics from `example.com.br`.

You may also see this error if the site does not send a `Referer` or `Origin` header. The `Referer` header is required (do not try to use the `Referrer-policy` header instead). We have a change in-flight now that only the `Origin` header will be required â€“ we believe there is no way to disable that in the browser.

### The analytics beacon is blocked by ad-blockers (including adblockplus, Brave, DuckDuckGo extension, etc). Why is that?


Cloudflare is aware that the analytics beacon is blocked by these services.

While Cloudflare Web Analytics uses a JavaScript beacon, Cloudflareâ€™s edge analytics cannot be blocked because we can measure every request that is received. Edge analytics are available to any customer who proxies traffic through Cloudflare. Currently, users on Pro, Business, and Enterprise plans get advanced web analytics powered by our edge logs.

### Why am I not seeing all the metrics for single-page application (SPA) or multiple-page application (MPA)?


Every route change that occurs in the single-page app will send the measurement of the route before the route is changed to the beacon endpoint. The measurement for the last route change will be sent whenever the user leaves the tab or closes the browser window. That will trigger `visibilityState` to a hidden state. Whenever that happens, Beacon JS sends the payload using the [Navigator.sendBeacon method](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/sendBeacon) that should not be cancelled even when the browser window is closed. However, due to compatibility, old browsers would fallback to using AJAX (`XmlHttpRequest`), which can be cancelled when the browser window is closed, so the last payload that gets sent to the beacon endpoint can be lost. Also, due to various network conditions, there can be data loss when the payload is sent to the beacon endpoint.

### For the same site, why would I see more data reported with an automatic setup?


Unless you are using Rules to control which pages to be measured, using [automatic setup](/web-analytics/get-started/#sites-proxied-through-cloudflare) will inject the JS snippet on all pages (sub-domains) under the zone.

If you used a [manual setup](/web-analytics/get-started/#sites-not-proxied-through-cloudflare) instead, only those pages that render the JS snippet will be reported.

:::note


Since only one JS snippet can be rendered and used per page, you cannot have multiple snippets on the same page.


:::


### My website is proxied through Cloudflare, but Web Analytic's automatic setup is not working.

If you have a `Cache-Control` header set to `public, no-transform`, Cloudflare proxy will not be able to modify the original payload of the website. Therefore, the Beacon script will not be automatically injected to your site, and Web Analytics will not work. Refer to [Origin cache control](/cache/concepts/cache-control/) for more information.




***

## Setup



### I am proxying my site through Cloudflare. Should I manually add the JS beacon?

You can, but you do not have to. Cloudflare Web Analytics is designed primarily for customers who do not use Cloudflare's proxy to measure their web traffic.

Existing Cloudflare customers can access analytics collected from our edge on the **Analytics** tab of the dashboard. You can also enable Web Analytics to measure performance using JavaScript.

Using a domain proxied through Cloudflare with [automatic setup](/web-analytics/get-started/#sites-proxied-through-cloudflare) will report stats back to your own domain's `/cdn-cgi/rum` endpoint. If you have installed JS snippet yourself (a [manual setup](/web-analytics/get-started/#sites-not-proxied-through-cloudflare)), it will report back to `cloudflareinsights.com/cdn-cgi/rum` endpoint.



### Can I add Web Analytics to my site using a tag manager like Google Tag Manager (GTM)?

Yes. Instead of embedding the script using a tag manager as shown here:

```html
<script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "$SITE_TOKEN"}'></script>
```

Add the following script:

```html
<script defer src='https://static.cloudflareinsights.com/beacon.min.js?token=$SITE_TOKEN'></script>
```



### Can I use the same JS Snippet for a different domain?


No. However, if the apex domain (also known as "root domain" or "naked domain") is the same, you can use the same site tag. For example, if you have provided us a hostname `example.com` when registering a site, you can use the JS snippet from that site for `abc.example.com` and `def.example.com` since they use the same apex domain. When payload gets sent to the beacon endpoint, we validate the hostname with postfix matching, so if your domain shares the same apex domain, that would work.



### Can I use automatic setup with a DNS-only domain (CNAME setup)?

No, you can only use the [automatic setup](/web-analytics/get-started/#sites-proxied-through-cloudflare) with JS snippet injection if traffic to your domain is proxied through Cloudflare (orange-clouded).

If you have a DNS-only domain, you will have to do a [manual setup](/web-analytics/get-started/#sites-not-proxied-through-cloudflare) instead.


### What prevents the JS Snippet from being added to a page?


<Render file="web-analytics-troubleshooting" />




***

## Functionality

### Can I see server-side analytics by URL?


Web Analytics only displays client-side analytics. All Cloudflare customers who proxy their traffic also get analytics based on traffic at their edge.

Currently, users on Pro, Business, and Enterprise plans get advanced HTTP traffic analytics, which is the only way to see features like a breakdown of traffic by URL based on server-side analytics.


### Can I use Web Analytics with AMP?


Not yet, but we plan to support AMP soon.


### What is the period of time I can access data in Web Analytics?

Currently, you can access data for the previous six months.



### Does Cloudflare Web Analytics support UTM parameters?

Not yet. UTM parameters are special query string parameters that can help track where traffic is coming from.
Currently, Cloudflare Web Analytics do not log query strings to avoid collecting potentially sensitive data, but we may add support for this in the future.


### Does Web Analytics support custom events?

Not yet, but we may add support for this in the future.

### Can I track more than one website with Web Analytics?

Yes. Right now there is a soft limit of ten sites per account, but that can be adjusted by contacting Cloudflare support.

### When does the beacon send metrics to the `/cdn-cgi/rum/` endpoint?

For traditional websites, not Single Page Applications (SPAs), the Web Analytics beacon reports to the `/cdn-cgi/rum/` endpoint when the page has finished loading (load event) and when the user leaves the page. For Single Page Applications, additional metrics are sent for every route change to capture the page load event.

---

# Overview

URL: https://developers.cloudflare.com/web-analytics/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Description>

Get vital web analytics for your website without compromising user privacy. 
</Description>

<Plan type="all" />

Cloudflare Web Analytics helps you understand the performance of your web pages as experienced by your site visitors.

***

## Features

<Feature header="Filters" href="/web-analytics/configuration-options/filters/">

Use filters to refine the data displayed in Web Analytics.


</Feature>

<Feature header="Rules" href="/web-analytics/configuration-options/rules">

Use rules to configure tracking of Web Analytics for specific websites or paths.


</Feature>

<Feature header="Dimensions" href="/web-analytics/data-metrics/dimensions">

Use dimensions to categorize and organize various metrics or data types effectively.


</Feature>

***

## Related products

<RelatedProduct header="Analytics" href="/analytics/" product="analytics">
Cloudflare visualizes the metadata collected by our products in the Cloudflare dashboard. 
</RelatedProduct>

<RelatedProduct header="Speed" href="/speed/" product="speed">
Speed allows you to assess the performance of your website and get recommendations of Cloudflare products to enhance the website performance.


</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Resource hub" href="https://www.cloudflare.com/application-services/products/analytics/" icon="document">
Refer to our latest resources to learn more about security, performance and reliability.
</LinkTitleCard>

<LinkTitleCard title="Cloudflare blog" href="https://blog.cloudflare.com/privacy-first-web-analytics/" icon="open-book">
Read articles about the latest updates about Web Analytics. 
</LinkTitleCard>

</CardGrid>

---

# Limits

URL: https://developers.cloudflare.com/web-analytics/limits/

Cloudflare limits the number of sites for which you can track web analytics, as well as the number of rules allowed for each plan type. Refer to the following tables for more information.

## Site limits

Cloudflare limits the number of sites for which you can track web analytics when they are not proxied by Cloudflare.

| Site type                      | Limit    |
| ------------------------------ | -------- |
| Not proxied through Cloudflare | 10       |
| Proxied through Cloudflare     | No limit |

## Rules limits

Cloudflare limits the number of Web Analytics rules you can have by plan type. For plans with a limit of zero, Web Analytics injects the JS snippet on all subdomains.

Rules are only available for sites proxied through Cloudflare.

| Plan type  | Rules limit |
| ---------- | ----------- |
| Free       | 0           |
| Pro        | 5           |
| Business   | 20          |
| Enterprise | 100         |

---

# About

URL: https://developers.cloudflare.com/web3/about/

When you [create a gateway](/web3/how-to/manage-gateways/#create-a-gateway), Cloudflare automatically creates and adds specific [DNS records](/web3/reference/gateway-dns-records/) to your Cloudflare account.

When the hostname associated with your gateway receives requests, its DNS records route these requests to a Cloudflare Workers script.

![Cloudflare's Web3 gateways provide HTTP-accessible interfaces to the IPFS and Ethereum networks. For more details, continue reading.](~/assets/images/web3/web3-gateway-flow-diagram.png)

## Read operations

If the API call to the Worker is a read operation and the requested content is cached, the Workers script will respond with the requested information via HTTP to the client.

If the requested content is not cached, it will first be requested via API call to Cloudflare IPFS or Ethereum nodes, cached at the edge for future requests, and returned via HTTP response to the client.

## Write operations

*Only available for gateways to EVM-based chains, such as [Ethereum](/web3/how-to/use-ethereum-gateway).*

If the API call to the gateway is a write operation, the gateway will make an API call to one of the Cloudflare nodes, and the transaction is placed in the local mempool and propagated to peers.

A transaction ID is returned to the gateway, which is then returned to the client via HTTP response. Validators take transactions from the mempool and place them into a block to execute. The new block to add to the blockchain is validated, consensus is reached, and the block is added to the blockchain and propagated to the rest of the network.

---

# Get started

URL: https://developers.cloudflare.com/web3/get-started/

import { Details, Render } from "~/components"

Use this tutorial to start using Cloudflare's Web3 Gateways to the IPFS and Ethereum networks.

## Before you begin

Before you start, make sure the you have [set up an account](/fundamentals/setup/account/) and [added your website](/fundamentals/setup/manage-domains/add-site/) to Cloudflare.

## Step 1 - Subscribe to a gateway

To get access to Web3 gateways for your account, you need to first [subscribe to a gateway](/web3/how-to/enable-gateways/).

## Step 2 - Create a gateway

After purchasing a gateway subscription, create a gateway.


<Details header="Create via dashboard">

<Render file="create-gateway-dashboard" />


</Details>


<Details header="Create via API">

<Render file="create-gateway-api" />


</Details>

<Render file="post-creation-steps" />

## Step 3 - Customize Cloudflare settings

<Render file="cloudflare-settings" />

## Step 4 - Restrict gateway access (optional)

If you are using your gateway for backend services, you may want to use Cloudflare Zero Trust to [restrict gateway access](/web3/how-to/restrict-gateway-access/).

## Step 5 - Set up usage notifications

<Render file="ubb-recommendation" product="fundamentals" />

## Step 6 - Use the gateway

Once you have created a gateway and updated your Cloudflare settings, you can start using your [IPFS](/web3/how-to/use-ipfs-gateway/) or [Ethereum](/web3/how-to/use-ethereum-gateway/).

---

# Overview

URL: https://developers.cloudflare.com/web3/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, Render } from "~/components"

<Description>

Develop Web3 applications without having to worry about running infrastructure
</Description>

<Plan type="add-on" />

Web3, or the distributed web, is a set of technologies that enables hosting of content and web applications in a serverless manner by leveraging distributed systems and consensus protocols.

<Render file="non-contract-enablement" product="fundamentals" />

***

## Features

<Feature header="IPFS Gateway" href="/web3/ipfs-gateway/"> <Plan id="web3.ipfs.properties.availability.summary" />
Provides a read-only, HTTP-accessible interface to the [Interplanetary File System (IPFS)](/web3/ipfs-gateway/concepts/ipfs/).


</Feature>

<Feature header="Ethereum Gateway" href="/web3/ethereum-gateway/"> <Plan id="web3.ethereum.properties.availability.summary" />

Gives you read and write access to the [Ethereum network](/web3/ethereum-gateway/concepts/ethereum/) without installing any software on your computer.


</Feature>

***

## Benefits

Cloudflare's Web3 gateways provide HTTP-accessible interfaces to Web3 networks, providing:

* **Ease of access**: Access content from Web3 networks without installing or running any special software.
* **Security**: Get the protection benefits of Cloudflare's global anycast network for [enhanced security](https://blog.cloudflare.com/cloudflare-thwarts-17-2m-rps-ddos-attack-the-largest-ever-reported/).
* **Reduced maintenance**: Cloudflare â€” and not your developers â€” maintains and monitors security, reliability, and performance.
* **Reliability**: Cloudflare's global anycast network provides a high level of [reliability and availability](https://www.cloudflare.com/network/).
* **Performance**: With Cloudflare's edge network of data centers in [hundreds of cities worldwide](https://www.cloudflare.com/network/), content can be cached and served from data centers close to your end users.

***

## More resources

<CardGrid>

<LinkTitleCard title="Plans" href="https://www.cloudflare.com/plans/#overview" icon="document">
Compare available Cloudflare plans
</LinkTitleCard>

<LinkTitleCard title="Pricing" href="https://dash.cloudflare.com/?to=/:account/:zone/web3/" icon="seti:shell">
Explore pricing options for Web3 Gateways in the dashboard
</LinkTitleCard>

</CardGrid>

---

# Overview

URL: https://developers.cloudflare.com/workflows/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct } from "~/components"

<Description>

Build durable multi-step applications on Cloudflare Workers with Workflows.

</Description>

<Plan type="workers-all" />

Workflows is a durable execution engine built on Cloudflare Workers. Workflows allow you to build multi-step applications that can automatically retry, persist state and run for minutes, hours, days, or weeks. Workflows introduces a programming model that makes it easier to build reliable, long-running tasks, observe as they progress, and programatically trigger instances based on events across your services.

Refer to the [get started guide](/workflows/get-started/guide/) to start building with Workflows.

***

## Features

<Feature header="Deploy your first Workflow" href="/workflows/get-started/guide/" cta="Deploy your first Workflow">

Define your first Workflow, understand how to compose multi-steps, and deploy to production.

</Feature>

<Feature header="Rules of Workflows" href="/workflows/build/rules-of-workflows/" cta="Best practices">

Understand best practices when writing and building applications using Workflows.

</Feature>

<Feature header="Trigger Workflows" href="/workflows/build/trigger-workflows/" cta="Trigger Workflows from Workers">

Learn how to trigger Workflows from your Workers applications, via the REST API, and the command-line.

</Feature>

***

## Related products

<RelatedProduct header="Workers" href="/workers/" product="workers">

Build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale.


</RelatedProduct>

<RelatedProduct header="Pages" href="/pages/" product="pages">

Deploy dynamic front-end applications in record time.


</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Pricing" href="/workflows/reference/pricing/" icon="seti:shell">
Learn more about how Workflows is priced.
</LinkTitleCard>

<LinkTitleCard title="Limits" href="/workflows/reference/limits/" icon="document">
Learn more about Workflow limits, and how to work within them.
</LinkTitleCard>

<LinkTitleCard title="Storage options" href="/workers/platform/storage-options/" icon="document">
Learn more about the storage and database options you can build on with Workers.
</LinkTitleCard>

<LinkTitleCard title="Developer Discord" href="https://discord.cloudflare.com" icon="discord">
Connect with the Workers community on Discord to ask questions, show what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="@CloudflareDev" href="https://x.com/cloudflaredev" icon="x.com">
Follow @CloudflareDev on Twitter to learn about product announcements, and what is new in Cloudflare Developer Platform.
</LinkTitleCard>

</CardGrid>

---

# Changelog

URL: https://developers.cloudflare.com/workers-ai/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/workers-ai.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Demos and architectures

URL: https://developers.cloudflare.com/workers-ai/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Workers AI can be used to build dynamic and performant services. The following demo applications and reference architectures showcase how to use Workers AI optimally within your architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Workers AI.

<ExternalResources type="apps" products={["Workers AI"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use Workers AI:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["Workers AI"]} />

---

# Glossary

URL: https://developers.cloudflare.com/workers-ai/glossary/

import { Glossary } from "~/components";

Review the definitions for terms used across Cloudflare's Workers AI documentation.

<Glossary product="workers-ai" />

---

# Overview

URL: https://developers.cloudflare.com/workers-ai/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct, Render, LinkButton, Flex } from "~/components"

<Description>

Run machine learning models, powered by serverless GPUs, on Cloudflare's global network.
</Description>

<Plan type="workers-all" />

Workers AI allows you to run AI models in a serverless way, without having to worry about scaling, maintaining, or paying for unused infrastructure. You can invoke models running on GPUs on Cloudflare's network from your own code â€” from [Workers](/workers/), [Pages](/pages/), or anywhere via [the Cloudflare API](/api/resources/ai/methods/run/).

Workers AI gives you access to:
- **50+ [open-source models](/workers-ai/models/)**, available as a part of our model catalog
- Serverless, **pay-for-what-you-use** [pricing model](/workers-ai/platform/pricing/)
- All as part of a **fully-featured developer platform**, including [AI Gateway](/ai-gateway/), [Vectorize](/vectorize/), [Workers](/workers/) and more...

<div>
  <LinkButton href="/workers-ai/get-started">Get started</LinkButton>
  <LinkButton target="_blank" variant="secondary" icon="external" href="https://youtu.be/cK_leoJsBWY?si=4u6BIy_uBOZf9Ve8">Watch a Workers AI demo</LinkButton>
</div>


<Render file="custom_requirements" />

<Render file="file_issues" />

***

## Features

<Feature header="Models" href="/workers-ai/models/" cta="Browse models">

Workers AI comes with a curated set of popular open-source models that enable you to do tasks such as image classification, text generation, object detection and more.


</Feature>

***

## Related products

<RelatedProduct header="AI Gateway" href="/ai-gateway/" product="ai-gateway">

Observe and control your AI applications with caching, rate limiting, request retries, model fallback, and more.


</RelatedProduct>

<RelatedProduct header="Vectorize" href="/vectorize/" product="vectorize">

Build full-stack AI applications with Vectorize, Cloudflareâ€™s vector database. Adding Vectorize enables you to perform tasks such as semantic search, recommendations, anomaly detection or can be used to provide context and memory to an LLM.


</RelatedProduct>

<RelatedProduct header="Workers" href="/workers/" product="workers">

Build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale.


</RelatedProduct>

<RelatedProduct header="Pages" href="/pages/" product="pages">

Create full-stack applications that are instantly deployed to the Cloudflare global network.


</RelatedProduct>

<RelatedProduct header="R2" href="/r2/" product="r2">

Store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.


</RelatedProduct>

<RelatedProduct header="D1" href="/d1/" product="d1">

Create new serverless SQL databases to query from your Workers and Pages projects.


</RelatedProduct>

<RelatedProduct header="Durable Objects" href="/durable-objects/" product="durable-objects">

A globally distributed coordination API with strongly consistent storage.


</RelatedProduct>

<RelatedProduct header="KV" href="/kv/" product="kv">

Create a global, low-latency, key-value data storage.


</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Get started" href="/workers-ai/get-started/workers-wrangler/" icon="open-book">
Build and deploy your first Workers AI application.
</LinkTitleCard>

<LinkTitleCard title="Plans" href="/workers-ai/platform/pricing/" icon="seti:shell">
Learn about Free and Paid plans.
</LinkTitleCard>

<LinkTitleCard title="Limits" href="/workers-ai/platform/limits/" icon="document">
Learn about Workers AI limits.
</LinkTitleCard>

<LinkTitleCard title="Use cases" href="/use-cases/ai/" icon="document">
Learn how you can build and deploy ambitious AI applications to Cloudflare's global network.
</LinkTitleCard>

<LinkTitleCard title="Storage options" href="/workers/platform/storage-options/" icon="open-book">
Learn which storage option is best for your project.
</LinkTitleCard>

<LinkTitleCard title="Developer Discord" href="https://discord.cloudflare.com" icon="discord">
Connect with the Workers community on Discord to ask questions, share what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="@CloudflareDev" href="https://x.com/cloudflaredev" icon="x.com">
Follow @CloudflareDev on Twitter to learn about product announcements, and what is new in Cloudflare Workers.
</LinkTitleCard>

</CardGrid>

---

# Privacy

URL: https://developers.cloudflare.com/workers-ai/privacy/

Cloudflare processes certain customer data in order to provide the Workers AI service, subject to our [Privacy Policy](https://www.cloudflare.com/privacypolicy/) and [Self-Serve Subscription Agreement](https://www.cloudflare.com/terms/) or [Enterprise Subscription Agreement](https://www.cloudflare.com/enterpriseterms/) (as applicable).

Cloudflare neither creates nor trains the AI models made available on Workers AI. The models constitute Third-Party Services and may be subject to open source or other license terms that apply between you and the model provider. Be sure to review the license terms applicable to each model (if any).

Your inputs (e.g., text prompts, image submissions, audio files, etc.), outputs (e.g., generated text/images, translations, etc.), embeddings, and training data constitute Customer Content.

For Workers AI:

* You own, and are responsible for, all of your Customer Content.
* Cloudflare does not make your Customer Content available to any other Cloudflare customer.
* Cloudflare does not use your Customer Content to (1) train any AI models made available on Workers AI or (2) improve any Cloudflare or third-party services, and would not do so unless we received your explicit consent.
* Your Customer Content for Workers AI may be stored by Cloudflare if you specifically use a storage service (e.g., R2, KV, DO, Vectorize, etc.) in conjunction with Workers AI.

---

# Errors

URL: https://developers.cloudflare.com/workers-ai/workers-ai-errors/

Below is a list of Workers AI errors.

| **Name**                              | **Internal Code** | **HTTP Code** | **Description**                                                                                                                                      |
| ------------------------------------- | ----------------- | ------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| No such model                         | `5007`            | `400`         | No such model `${model}` or task                                                                                                                     |
| Invalid data                          | `5004`            | `400`         | Invalid data type for base64 input: `${type}`                                                                                                        |
| Finetune missing required files       | `3039`            | `400`         | Finetune is missing required files `(model.safetensors and config.json) `                                                                            |
| Incomplete request                    | `3003`            | `400`         | Request is missing headers or body: `{what}`                                                                                                         |
| Account not allowed for private model | `5018`            | `403`         | The account is not allowed to access this model                                                                                                      |
| Model agreement                       | `5016`            | `403`         | User has not agreed to Llama3.2 model terms                                                                                                          |
| Account blocked                       | `3023`            | `403`         | Service unavailable for account                                                                                                                      |
| Account not allowed for private model | `3041`            | `403`         | The account is not allowed to access this model                                                                                                      |
| Deprecated SDK version                | `5019`            | `405`         | Request trying to use deprecated SDK version                                                                                                         |
| LoRa unsupported                      | `5005`            | `405`         | The model `${this.model}` does not support LoRa inference                                                                                            |
| Invalid model ID                      | `3042`            | `404`         | The model name is invalid                                                                                                                            |
| Request too large                     | `3006`            | `413`         | Request is too large                                                                                                                                 |
| Timeout                               | `3007`            | `408`         | Request timeout                                                                                                                                      |
| Aborted                               | `3008`            | `408`         | Request was aborted                                                                                                                                  |
| Account limited                       | `3036`            | `429`         | You have used up your daily free allocation of 10,000 neurons. Please upgrade to Cloudflare's Workers Paid plan if you would like to continue usage. |
| Out of capacity                       | `3040`            | `429`         | No more data centers to forward the request to                                                                                                       |

---

# Glossary

URL: https://developers.cloudflare.com/workers/glossary/

import { Glossary } from "~/components";

Review the definitions for terms used across Cloudflare's Workers documentation.

<Glossary product="workers" />

---

# Demos and architectures

URL: https://developers.cloudflare.com/workers/demos/

import { ExternalResources, GlossaryTooltip, ResourcesBySelector } from "~/components"

Learn how you can use Workers within your existing application and architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Workers.

<ExternalResources type="apps" products={["Workers"]} />

## Reference architectures

Explore the following <GlossaryTooltip term="reference architecture">reference architectures</GlossaryTooltip> that use Workers:

<ResourcesBySelector types={["reference-architecture","design-guide","reference-architecture-diagram"]} products={["Workers"]} />

---

# Overview

URL: https://developers.cloudflare.com/workers/

import {
	CardGrid,
	Description,
	Feature,
	LinkButton,
	LinkTitleCard,
	Plan,
	RelatedProduct,
	Render,
} from "~/components";

<Description>

Build serverless applications and deploy instantly across the globe for exceptional performance, reliability, and scale.

</Description>

<Plan type="all" />

Cloudflare Workers provides a [serverless](https://www.cloudflare.com/learning/serverless/what-is-serverless/) execution environment that allows you to create new applications or augment existing ones without configuring or maintaining infrastructure.

Cloudflare Workers runs on [Cloudflareâ€™s global network](https://www.cloudflare.com/network/) in hundreds of cities worldwide, offering both [Free and Paid plans](/workers/platform/pricing/).

<LinkButton variant="primary" href="/workers/get-started/guide/">
	Get started
</LinkButton> <LinkButton
	variant="secondary"
	href="https://dash.cloudflare.com/?to=/:account/workers-and-pages/create"
>
	Workers dashboard
</LinkButton>

---

## Features

<Feature header="Wrangler" href="/workers/wrangler/install-and-update/">

The Workers command-line interface, Wrangler, allows you to [create](/workers/wrangler/commands/#init), [test](/workers/wrangler/commands/#dev), and [deploy](/workers/wrangler/commands/#deploy) your Workers projects.

</Feature>

<Feature header="Bindings" href="/workers/runtime-apis/bindings/">

Bindings allow your Workers to interact with resources on the Cloudflare developer platform, including [R2](/r2/), [KV](/kv/concepts/how-kv-works/), [Durable Objects](/durable-objects/), and [D1](/d1/).

</Feature>

<Feature header="Playground" href="/workers/playground/" cta="Use the Playground">

The Playground is a sandbox which gives you an instant way to preview and test a Worker directly in the browser against any site. No setup required.

</Feature>

---

## Related products

<RelatedProduct header="Workers AI" href="/workers-ai/" product="workers-ai">

Run machine learning models, powered by serverless GPUs, on Cloudflareâ€™s global network.

</RelatedProduct>

<RelatedProduct header="R2" href="/r2/" product="r2">

Store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services.

</RelatedProduct>

<RelatedProduct header="D1" href="/d1/" product="d1">

Create new serverless SQL databases to query from your Workers and Pages projects.

</RelatedProduct>

<RelatedProduct header="Durable Objects" href="/durable-objects/" product="durable-objects">

A globally distributed coordination API with strongly consistent storage.

</RelatedProduct>

<RelatedProduct header="KV" href="/kv/" product="kv">

Create a global, low-latency, key-value data storage.

</RelatedProduct>

<RelatedProduct header="Queues" href="/queues/" product="queues">

Send and receive messages with guaranteed delivery and no charges for egress bandwidth.

</RelatedProduct>

<RelatedProduct header="Hyperdrive" href="/hyperdrive/" product="hyperdrive">

Turn your existing regional database into a globally distributed database.

</RelatedProduct>

<RelatedProduct header="Vectorize" href="/vectorize/" product="vectorize">

Build full-stack AI applications with Vectorize, Cloudflareâ€™s vector database.

</RelatedProduct>

<RelatedProduct header="Zaraz" href="/zaraz/" product="zaraz">

Offload third-party tools and services to the cloud and improve the speed and security of your website.

</RelatedProduct>

---

## More resources

<CardGrid>

<LinkTitleCard
	title="Learning Path"
	href="/learning-paths/workers/concepts/"
	icon="pen"
>
	New to Workers? Get started with the Workers Learning Path.
</LinkTitleCard>

<LinkTitleCard
	title="Plans"
	href="/workers/platform/pricing/"
	icon="seti:shell"
>
	Learn about Free and Paid plans.
</LinkTitleCard>

<LinkTitleCard title="Limits" href="/workers/platform/limits/" icon="document">
	Learn about plan limits (Free plans get 100,000 requests per day).
</LinkTitleCard>

<LinkTitleCard
	title="Storage options"
	href="/workers/platform/storage-options/"
	icon="open-book"
>
	Learn which storage option is best for your project.
</LinkTitleCard>

<LinkTitleCard
	title="Developer Discord"
	href="https://discord.cloudflare.com"
	icon="discord"
>
	Connect with the Workers community on Discord to ask questions, share what you
	are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard
	title="@CloudflareDev"
	href="https://x.com/cloudflaredev"
	icon="x.com"
>
	Follow @CloudflareDev on Twitter to learn about product announcements, and
	what is new in Cloudflare Workers.
</LinkTitleCard>

</CardGrid>

---

# Local development

URL: https://developers.cloudflare.com/workers/local-development/

Cloudflare Workers and most connected resources can be fully developed and tested locally - providing confidence that the applications you build locally will work the same way in production. This allows you to be more efficient and effective by providing a faster feedback loop and removing the need to [test against remote resources](#develop-using-remote-resources-and-bindings). Local development runs against the same production runtime used by Cloudflare Workers, [workerd](https://github.com/cloudflare/workerd).

In addition to testing Workers locally with [`wrangler dev`](/workers/wrangler/commands/#dev), the use of Miniflare allows you to test other Developer Platform products locally, such as [R2](/r2/), [KV](/kv/), [D1](/d1/), and [Durable Objects](/durable-objects/).

## Start a local development server

:::note

This guide assumes you are using [Wrangler v3.0](https://blog.cloudflare.com/wrangler3/) or later.

Users new to Wrangler CLI and Cloudflare Workers should visit the [Wrangler Install/Update guide](/workers/wrangler/install-and-update) to install `wrangler`.

:::

Wrangler provides a [`dev`](/workers/wrangler/commands/#dev) command that starts a local server for developing your Worker. Make sure you have `npm` installed and run the following in the folder containing your Worker application:

```sh
npx wrangler dev
```

`wrangler dev` will run the Worker directly on your local machine. `wrangler dev` uses a combination of `workerd` and [Miniflare](https://github.com/cloudflare/workers-sdk/tree/main/packages/miniflare), a simulator that allows you to test your Worker against additional resources like KV, Durable Objects, WebSockets, and more.

### Supported resource bindings in different environments

| Product                             | Local Dev Supported | Remote Dev Supported |
| ----------------------------------- | ------------------- | -------------------- |
| AI                                  | âœ…[^1]              | âœ…                   |
| Assets                              | âœ…                  | âœ…                   |
| Analytics Engine                    | âœ…                  | âœ…                   |
| Browser Rendering                   | âŒ                  | âœ…                   |
| D1                                  | âœ…                  | âœ…                   |
| Durable Objects                     | âœ…                  | âœ…                   |
| Email Bindings                      | âŒ                  | âœ…                   |
| Hyperdrive                          | âœ…                  | âœ…                   |
| Images                              | âœ…                  | âœ…                   |
| KV                                  | âœ…                  | âœ…                   |
| mTLS                                | âŒ                  | âœ…                   |
| Queues                              | âœ…                  | âŒ                   |
| R2                                  | âœ…                  | âœ…                   |
| Rate Limiting                       | âœ…                  | âœ…                   |
| Service Bindings (multiple workers) | âœ…                  | âœ…                   |
| Vectorize                           | âœ…[^2]              | âœ…                   |
| Workflows                           | âœ…                  | âŒ                   |

With any bindings that are not supported locally, you will need to use the [`--remote` command](#develop-using-remote-resources-and-bindings) in wrangler, such as `wrangler dev --remote`.

[^1]: Using Workers AI always accesses your Cloudflare account in order to run AI models and will incur usage charges even in local development.

[^2]: Using Vectorize always accesses your Cloudflare account to run queries, and will incur usage charges even in local development.

## Work with local data

When running `wrangler dev`, resources such as KV, Durable Objects, D1, and R2 will be stored and persisted locally and not affect the production resources.

### Use bindings in Wrangler configuration files

[Wrangler](/workers/wrangler/) will automatically create local versions of bindings found in the [Wrangler configuration file](/workers/wrangler/configuration/). These local resources will not have data in them initially, so you will need to add data manually via Wrangler commands and the [`--local` flag](#use---local-flag).

When you run `wrangler dev` Wrangler stores local resources in a `.wrangler/state` folder, which is automatically created.

If you prefer to specify a directory, you can use the [`--persist-to`](/workers/wrangler/commands/#dev) flag with `wrangler dev` like this:

```sh
npx wrangler dev --persist-to <DIRECTORY>
```

Using this will write all local storage and cache to the specified directory instead of `.wrangler`.

:::note

This local persistence folder should be added to your `.gitignore` file.

:::

### Use `--local` flag

The following [Wrangler commands](/workers/wrangler/commands/) have a `--local` flag which allows you to create, update, and delete local data during development:

| Command                                              |
| ---------------------------------------------------- |
| [`d1 execute`](/workers/wrangler/commands/#execute)  |
| [`kv key`](/workers/wrangler/commands/#kv-key)       |
| [`kv bulk`](/workers/wrangler/commands/#kv-bulk)     |
| [`r2 object`](/workers/wrangler/commands/#r2-object) |

If using `--persist-to` to specify a custom folder with `wrangler dev` you should also add `--persist-to` with the same directory name along with the `--local` flag when running the commands above. For example, to put a custom KV key into a local namespace via the CLI you would run:

```sh
npx wrangler kv key put test 12345 --binding MY_KV_NAMESPACE --local --persist-to worker-local
```

Running `wrangler kv key put` will create a new key `test` with a value of `12345` on the local namespace specified via the binding `MY_KV_NAMESPACE` in the [Wrangler configuration file](/workers/wrangler/configuration/). This example command sets the local persistence directory to `worker-local` using `--persist-to`, to ensure that the data is created in the correct location. If `--persist-to` was not set, it would create the data in the `.wrangler` folder.

### Clear Wrangler's local storage

If you need to clear local storage entirely, delete the `.wrangler/state` folder. You can also be more fine-grained and delete specific resource folders within `.wrangler/state`.

Any deleted folders will be created automatically the next time you run `wrangler dev`.

## Local-only environment variables

When running `wrangler dev`, variables in the [Wrangler configuration file](/workers/wrangler/configuration/) are automatically overridden by values defined in a `.dev.vars` file located in the root directory of your worker. This is useful for providing values you do not want to check in to source control.

```shell
API_HOST = "localhost:4000"
API_ACCOUNT_ID = "local_example_user"
```

## Develop using remote resources and bindings

There may be times you want to develop against remote resources and bindings. To run `wrangler dev` in remote mode, add the `--remote` flag, which will run both your code and resources remotely:

```sh
npx wrangler dev --remote
```

For some products like KV and R2, remote resources used for `wrangler dev --remote` must be specified with preview ID/names in the [Wrangler configuration file](/workers/wrangler/configuration/) such as `preview_id` for KV or `preview_bucket name` for R2. Resources used for remote mode (preview) can be different from resources used for production to prevent changing production data during development. To use production data in `wrangler dev --remote`, set the preview ID/name of the resource to the ID/name of your production resource.

## Customize `wrangler dev`

You can customize how `wrangler dev` works to fit your needs. Refer to [the `wrangler dev` documentation](/workers/wrangler/commands/#dev) for available configuration options.

:::caution

There is a bug associated with how outgoing requests are handled when using `wrangler dev --remote`. For more information, read the [Known issues section](/workers/platform/known-issues/#wrangler-dev).

:::

## Related resources

- [D1 local development](/d1/best-practices/local-development/) - The official D1 guide to local development and testing.
- [DevTools](/workers/observability/dev-tools) - Guides to using DevTools to debug your Worker locally.

---

# Playground

URL: https://developers.cloudflare.com/workers/playground/

import { LinkButton } from "~/components";

:::note[Browser support]

The Cloudflare Workers Playground is currently only supported in Firefox and Chrome desktop browsers. In Safari, it will show a `PreviewRequestFailed` error message.

:::

The quickest way to experiment with Cloudflare Workers is in the [Playground](https://workers.cloudflare.com/playground). It does not require any setup or authentication. The Playground is a sandbox which gives you an instant way to preview and test a Worker directly in the browser.

The Playground uses the same editor as the authenticated experience. The Playground provides the ability to [share](#share) the code you write as well as [deploy](#deploy) it instantly to Cloudflare's global network. This way, you can try new things out and deploy them when you are ready.

<LinkButton href="https://workers.cloudflare.com/playground" icon="external">
	Launch the Playground
</LinkButton>

## Hello Cloudflare Workers

When you arrive in the Playground, you will see this default code:

```js
import welcome from "welcome.html";

/**
 * @typedef {Object} Env
 */

export default {
	/**
	 * @param {Request} request
	 * @param {Env} env
	 * @param {ExecutionContext} ctx
	 * @returns {Response}
	 */
	fetch(request, env, ctx) {
		console.log("Hello Cloudflare Workers!");

		return new Response(welcome, {
			headers: {
				"content-type": "text/html",
			},
		});
	},
};
```

This is an example of a multi-module Worker that is receiving a [request](/workers/runtime-apis/request/), logging a message to the console, and then returning a [response](/workers/runtime-apis/response/) body containing the content from `welcome.html`.

Refer to the [Fetch handler documentation](/workers/runtime-apis/handlers/fetch/) to learn more.

## Use the Playground

As you edit the default code, the Worker will auto-update such that the preview on the right shows your Worker running just as it would in a browser. If your Worker uses URL paths, you can enter those in the input field on the right to navigate to them. The Playground provides type-checking via JSDoc comments and [`workers-types`](https://www.npmjs.com/package/@cloudflare/workers-types). The Playground also provides pretty error pages in the event of application errors.

To test a raw HTTP request (for example, to test a `POST` request), go to the **HTTP** tab and select **Send**. You can add and edit headers via this panel, as well as edit the body of a request.

## DevTools

For debugging Workers inside the Playground, use the developer tools at the bottom of the Playground's preview panel to view `console.logs`, network requests, memory and CPU usage. The developer tools for the Workers Playground work similarly to the developer tools in Chrome or Firefox, and are the same developer tools users have access to in the [Wrangler CLI](/workers/wrangler/install-and-update/) and the authenticated dashboard.

### Network tab

**Network** shows the outgoing requests from your Worker â€” that is, any calls to `fetch` inside your Worker code.

### Console Logs

The console displays the output of any calls to `console.log` that were called for the current preview run as well as any other preview runs in that session.

### Sources

**Sources** displays the sources that make up your Worker. Note that KV, text, and secret bindings are only accessible when authenticated with an account. This means you must be logged in to the dashboard, or use [`wrangler dev`](/workers/wrangler/commands/#dev) with your account credentials.

## Share

To share what you have created, select **Copy Link** in the top right of the screen. This will copy a unique URL to your clipboard that you can share with anyone. These links do not expire, so you can bookmark your creation and share it at any time. Users that open a shared link will see the Playground with the shared code and preview.

## Deploy

You can deploy a Worker from the Playground. If you are already logged in, you can review the Worker before deploying. Otherwise, you will be taken through the first-time user onboarding flow before you can review and deploy.

Once deployed, your Worker will get its own unique URL and be available almost instantly on Cloudflare's global network. From here, you can add [Custom Domains](/workers/configuration/routing/custom-domains/), [storage resources](/workers/platform/storage-options/), and more.

---

# Changelog

URL: https://developers.cloudflare.com/zaraz/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/zaraz.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Embeds

URL: https://developers.cloudflare.com/zaraz/embeds/

Embeds are tools for incorporating external content, like social media posts, directly onto webpages, enhancing user engagement without compromising site performance and security.

Cloudflare Zaraz introduces server-side rendering for embeds, avoiding third-party JavaScript to improve security, privacy, and page speed. This method processes content on the server side, removing the need for direct communication between the user's browser and third-party servers.

To add an Embed to Your Website:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login), and select your account and domain.
2. Go to **Zaraz** > **Tools Configuration**.
3. Click "add new tool" and activate the desired tools on your Cloudflare Zaraz dashboard.
4. Add a placeholder in your HTML, specifying the necessary attributes. For a generic embed, the snippet looks like this:

```html
<componentName-embedName attribute="value"></componentName-embedName>
```

Replace `componentName`, `embedName` and `attribute="value"` with the specific Managed Component requirements. Zaraz automatically detects placeholders and replaces them with the content in a secure and efficient way.

## Examples

### X (Twitter) embed

```html
<twitter-post tweet-id="12345"></twitter-post>
```

Replace `tweet-id` with the actual tweet ID for the content you wish to embed.

### Instagram embed

```html
<instagram-post post-url="https://www.instagram.com/p/ABC/" captions="true"></instagram-post>
```

Replace `post-url` with the actual URL for the content you wish to embed. To include posts captions set captions attribute to `true`.

---

# FAQ

URL: https://developers.cloudflare.com/zaraz/faq/

import { GlossaryTooltip } from "~/components";

Below you will find answers to our most commonly asked questions. If you cannot find the answer you are looking for, refer to the [community page](https://community.cloudflare.com/) or [Discord channel](https://discord.cloudflare.com) to explore additional resources.

- [General](#general)
- [Tools](#tools)
- [Consent](#consent)

If you're looking for information regarding Zaraz Pricing, see the [Zaraz Pricing](/zaraz/pricing-info/) page.

---

## General

### Setting up Zaraz

#### Why is Zaraz not working?

If you are experiencing issues with Zaraz, there could be multiple reasons behind it. First, it's important to verify that the Zaraz script is loading properly on your website.

To check if the script is loading correctly, follow these steps:

1. Open your website in a web browser.
2. Open your browser's Developer Tools.
3. In the Console, type `zaraz`.
4. If you see an error message saying `zaraz is not defined`, it means that Zaraz failed to load.

If Zaraz is not loading, please verify the following:

- The domain running Zaraz [is proxied by Cloudflare](/dns/proxy-status/).
- Auto Injection is enabled in your [Zaraz Settings](/zaraz/reference/settings/#auto-inject-script).
- Your website's HTML is valid and includes `<head>` and `</head>` tags.
- You have at least [one enabled tool](/zaraz/get-started/) configured in Zaraz.

#### The browser extension I'm using cannot find the tool I have added. Why?

Zaraz is loading tools server-side, which means code running in the browser will not be able to see it. Running tools server-side is better for your website performance and privacy, but it also means you cannot use normal browser extensions to debug your Zaraz tools.

#### I'm seeing some data discrepancies. Is there a way to check what data reaches Zaraz?

Yes. You can use the metrics in [Zaraz Monitoring](/zaraz/monitoring/) and [Debug Mode](/zaraz/web-api/debug-mode/) to help you find where in the workflow the problem occurred.

#### Can I use Zaraz with Rocket Loader?

We recommend disabling [Rocket Loader](/speed/optimization/content/rocket-loader/) when using Zaraz. While Zaraz can be used together with Rocket Loader, there's usually no need to use both. Rocket Loader can sometimes delay data from reaching Zaraz, causing issues.

#### Is Zaraz compatible with Content Security Policies (CSP)?

Yes. To learn more about how Zaraz works to be compatible with <GlossaryTooltip term="content security policy (CSP)">CSP</GlossaryTooltip> configurations, refer to the [Cloudflare Zaraz supports CSP](https://blog.cloudflare.com/cloudflare-zaraz-supports-csp/) blog post.

#### Does Cloudflare process my HTML, removing existing scripts and then injecting Zaraz?

Cloudflare Zaraz does not remove other third-party scripts from the page. Zaraz [can be auto-injected or not](/zaraz/reference/settings/#auto-inject-script), depending on your configuration, but if you have existing scripts that you intend to load with Zaraz, you should remove them.

#### Does Zaraz work with Cloudflare Page Shield?

Yes. Refer to [Page Shield](/page-shield/) for more information related to this product.

#### Is there a way to prevent Zaraz from loading on specific pages, like under `/wp-admin`?

To prevent Zaraz from loading on specific pages, refer to [Load Zaraz selectively](/zaraz/advanced/load-selectively/).

#### How can I remove my Zaraz configuration?

Resetting your Zaraz configuration will erase all of your configuration settings, including any tools, triggers, and variables you've set up. This action will disable Zaraz immediately. If you want to start over with a clean slate, you can always reset your configuration.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Zaraz** > **Settings** > **Advanced**.
3. Click "Reset" and follow the instructions.

### Zaraz Web API

#### Why would the `zaraz.ecommerce()` method returns an undefined error?

E-commerce tracking needs to be enabled in [the Zaraz Settings page](/zaraz/reference/settings/#e-commerce-tracking) before you can start using the E-commerce Web API.

#### How would I trigger pageviews manually on a Single Page Application (SPA)?

Zaraz comes with built-in [Single Page Application (SPA) support](/zaraz/reference/settings/#single-page-application-support) that automatically sends pageview events when navigating through the pages of your SPA. However, if you have advanced use cases, you might want to build your own system to trigger pageviews. In such cases, you can use the internal SPA pageview event by calling `zaraz.spaPageview()`.

---

## Tools

### Google Analytics

#### After moving from Google Analytics 4 to Zaraz, I can no longer see demographics data. Why?

You probably have enabled **Hide Originating IP Address** in the [Settings option](/zaraz/custom-actions/edit-tools-and-actions/) for Google Analytics 4. This tells Zaraz to not send the IP address to Google. To have access to demographics data and anonymize your visitor's IP, you should use [**Anonymize Originating IP Address**](#i-see-two-ways-of-anonymizing-ip-address-information-on-the-third-party-tool-google-analytics-one-in-privacy-and-one-in-additional-fields-which-is-the-correct-one) instead.

#### I see two ways of anonymizing IP address information on the third-party tool Google Analytics: one in Privacy, and one in Additional fields. Which is the correct one?

There is not a correct option, as the two options available in Google Analytics (GA) do different things.

The "Hide Originating IP Address" option in [Tool Settings](/zaraz/custom-actions/edit-tools-and-actions/) prevents Zaraz from sending the IP address from a visitor to Google. This means that GA treats Zaraz's Worker's IP address as the visitor's IP address. This is often close in terms of location, but it might not be.

With the **Anonymize Originating IP Address** available in the [Add field](/zaraz/custom-actions/additional-fields/) option, Cloudflare sends the visitor's IP address to Google as is, and passes the 'aip' parameter to GA. This asks GA to anonymize the data.

#### If I set up Event Reporting (enhanced measurements) for Google Analytics, why does Zaraz only report Page View, Session Start, and First Visit?

This is not a bug. Zaraz does not offer all the automatic events the normal GA4 JavaScript snippets offer out of the box. You will need to build [triggers](/zaraz/custom-actions/create-trigger/) and [actions](/zaraz/custom-actions/) to capture those events. Refer to [Get started](/zaraz/get-started/) to learn more about how Zaraz works.

#### Can I set up custom dimensions for Google Analytics with Zaraz?

Yes. Refer to [Additional fields](/zaraz/custom-actions/additional-fields/) to learn how to send additional data to tools.

#### How do I attach a User Property to my events?

In your Google Analytics 4 action, select **Add field** > **Add custom field...** and enter a field name that starts with `up.` â€” for example, `up.name`. This will make Zaraz send the field as a User Property and not as an Event Property.

#### How can I enable Google Consent Mode signals?

Zaraz has built-in support for Google Consent Mode v2. Learn more on how to use it in [Google Consent Mode page](/zaraz/advanced/google-consent-mode/).

### Facebook Pixel

#### If I set up Facebook Pixel on my Zaraz account, why am I not seeing data coming through?

It can take between 15 minutes to several hours for data to appear on Facebook's interface, due the way Facebook Pixel works. You can also use [debug mode](/zaraz/web-api/debug-mode/) to confirm that data is being properly sent from your Zaraz account.

### Google Ads

#### What is the expected format for Conversion ID and Conversion Label

Conversion ID and Conversion Label are usually provided by Google Ads as a "gtag script". Here's an example for a $1 USD conversion:

```js
gtag("event", "conversion", {
	send_to: "AW-123456789/AbC-D_efG-h12_34-567",
	value: 1.0,
	currency: "USD",
});
```

The Conversion ID is the first part of `send_to` parameter, without the `AW-`. In the above example it would be `123456789`. The Conversion Label is the second part of the `send_to` parameter, therefore `AbC-D_efG-h12_34-567` in the above example. When setting up your Google Ads conversions through Zaraz, take the information from the original scripts you were asked to implement.

### Custom HTML

#### Can I use Google Tag Manager together with Zaraz?

You can load Google Tag Manager using Zaraz, but it is not recommended. Tools configured inside Google Tag Manager cannot be optimized by Zaraz, and cannot be restricted by the Zaraz privacy controls. In addition, Google Tag Manager could slow down your website because it requires additional JavaScript, and its rules are evaluated client-side. If you are currently using Google Tag Manager, we recommend replacing it with Zaraz by configuring your tags directly as Zaraz tools.

#### Why should I prefer a native tool integration instead of an HTML snippet?

Adding a tool to your website via a native Zaraz integration is always better than using an HTML snippet. HTML snippets usually depends on additional client-side requests, and require client-side code execution, which can slow down your website. They are often a security risk, as they can be hacked. Moreover, it can be very difficult to control their affect on the privacy of your visitors. Tools included in the Zaraz library are not suffering from these issues - they are fast, executed at the edge, and be controlled and restricted because they are sandboxed.

#### How can I set my Custom HTML to be injected just once in my Single Page App (SPA) website?

If you have enabled "Single Page Application support" in Zaraz Settings, your Custom HTML code may be unnecessarily injected every time a new SPA page is loaded. This can result in duplicates. To avoid this, go to your Custom HTML action and select the "Add Field" option. Then, add the "Ignore SPA" field and enable the toggle switch. Doing so will prevent your code from firing on every SPA pageview and ensure that it is injected only once.

### Other tools

#### What if I want to use a tool that is not supported by Zaraz?

The Zaraz engineering team is adding support to new tools all the time. You can also refer to the [community space](https://community.cloudflare.com/c/developers/integrationrequest/68) to ask for new integrations.

#### I cannot get a tool to load when the website is loaded. Do I have to add code to my website?

If you proxy your domain through Cloudflare, you do not need to add any code to your website. By default, Zaraz includes an automated `Pageview` trigger. Some tools, like Google Analytics, automatically add a `Pageview` action that uses this trigger. With other tools, you will need to add it manually. Refer to [Get started](/zaraz/get-started/) for more information.

#### I am a vendor. How can I integrate my tool with Zaraz?

The Zaraz team is working with third-party vendors to build their own Zaraz integrations using the Zaraz SDK. To request a new tool integration, or to collaborate on our SDK, contact us at [zaraz@cloudflare.com](mailto:zaraz@cloudflare.com).

---

## Consent

### How do I show the consent modal again to all users?

In such a case, you can change the cookie name in the _Consent cookie name_ field in the Zaraz Consent configuration. This will cause the consent modal to reappear for all users. Make sure to use a cookie name that has not been used for Zaraz on your site.

---

# Get started

URL: https://developers.cloudflare.com/zaraz/get-started/

Before being able to use Zaraz, it is recommended that you proxy your website through Cloudflare. Refer to [Set up Cloudflare](/fundamentals/setup/) for more information. If you do not want to proxy your website through Cloudflare, refer to [Use Zaraz on domains not proxied by Cloudflare](/zaraz/advanced/domains-not-proxied/).

## Add a third-party tool to your website

You can add new third-party tools and load them into your website through the Cloudflare dashboard.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login), and select your account and website.
2. Select **Zaraz** from the side menu.
3. If you have already added a tool before, select **Tools Configuration** > **Third-party tools** and click on **Add new tool**.
4. Choose a tool from the tools catalog. Select **Continue** to confirm your selection.
5. In **Set up**, configure the settings for your new tool. The information you need to enter will depend on the tool you choose. If you want to use any dynamic properties or variables, select the `+` sign in the drop-down menu next to the relevant field.
6. In **Actions** setup the actions for your new tool. You should be able to select Pageviews, Events or E-Commerce [^1].
7. Select **Save**. 

[^1]: Some tools do not supported Automatic Actions, see the section about [Custom Actions](/zaraz/custom-actions) if the tool you are adding doesn't present Automatic Actions.

## Events, triggers and actions

Zaraz relies on events, triggers and actions to determine when to load the tools you need in your website, and what action they need to perform. The way you configure Zaraz and where you start largely depend on the tool you wish to use. When using a tool that supports Automatic Actions, this process is largely done for you. If the tool you are adding doesn't support Automatic Actions, read more about configuring [Custom Actions](/zaraz/custom-actions).

When using Automatic Actions, the available actions are as follows:

- **Pageviews** - for tracking every pageview on your website
- **Events** - For tracking calls using the [`zaraz.track` Web API](/zaraz/web-api/track)
- **E-commerce** - For tracking calls to [`zaraz.ecommerce` Web API](/zaraz/web-api/ecommerce)

## Web API

If you need to programmatically start actions in your tools, Cloudflare Zaraz provides a unified Web API to send events to Zaraz, and from there, to third-party tools. This Web API includes the `zaraz.track()`, `zaraz.set()` and `zaraz.ecommerce()` methods.

[The Track method](/zaraz/web-api/track/) allows you to track custom events and actions on your website that might happen in real time. [The Set method](/zaraz/web-api/set/) is an easy shortcut to define a variable once and have it sent with every future Track call. [E-commerce](/zaraz/web-api/ecommerce/) is a unified method for sending e-commerce related data to multiple tools without needing to configure triggers and events. Refer to [Web API](/zaraz/web-api/) for more information.

## Troubleshooting

If you suspect that something is not working the way it should, or if you want to verify the operation of tools on your website, read more about [Debug Mode](/zaraz/web-api/debug-mode/) and [Zaraz Monitoring](/zaraz/monitoring/). Also, check the [FAQ](/zaraz/faq/) page to see if your question was already answered there.

## Platform plugins

Users and companies have developed plugins that make using Zaraz easier on specific platforms. We recommend checking out these plugins if you are using one of these platforms.

### WooCommerce

- [Beetle Tracking](https://beetle-tracking.com/) - Integrate Zaraz with your WordPress WooCommerce website to track e-commerce events with zero configuration. Beetle Tracking also supports consent management and other advanced features.

---

# HTTP Events API

URL: https://developers.cloudflare.com/zaraz/http-events-api/

The Zaraz HTTP Events API allows you to send information to Zaraz from places that cannot run the [Web API](/zaraz/web-api/), such as your server or your mobile app. It is useful for tracking events that are happening outside the browser, like successful transactions, sign-ups and more. The API also allows sending multiple events in batches.

## Configure the API endpoint

The API is disabled unless you configure an endpoint for it. The endpoint determines under what URL the API will be accessible. For example, if you set the endpoint to be `/zaraz/api`, and your domain is `example.com`, requests to the API will go to `https://example.com/zaraz/api`.

To enable the API endpoint:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Zaraz** > **Settings**.
3. Under **Endpoints** > **HTTP Events API**, set your desired path. Remember the path is relative to your domain, and it must start with a `/`.

:::caution[Important]
To avoid getting the API used by unwanted actors, Cloudflare recommends choosing a unique path.
:::

## Send events

The endpoint you have configured for the API will receive `POST` requests with a JSON payload. Below, there is an example payload:

```json
{
  "events": [
    {
      "client": {
        "__zarazTrack": "transaction successful",
        "value": "200"
      }
    }
  ]
}
```

The payload must contain an `events` array. Each Event Object in this array corresponds to one event you want Zaraz to process. The above example is similar to calling `zaraz.track('transaction successful', { value: "200" })` using the Web API.

The Event Object holds the `client` object, in which you can pass information about the event itself. Every key you include in the Event Object will be available as a *Track Property* in the Zaraz dashboard.

There are two reserved keys:

* `__zarazTrack`: The value of this key will be available as *Event Name*. This is what you will usually build your triggers around. In the above example, setting this to `transaction successful` is the same as [using the Web API](/zaraz/web-api/track/) and calling `zaraz.track("transaction successful")`.
* `__zarazEcommerce`: This key needs to be set to `true` if you want Zaraz to process the event as an e-commerce event.

### The `system` key

In addition to the `client` key, you can use the `system` key to include information about the device from which the event originated. For example, you can submit the `User-Agent` string, the cookies and the screen resolution. Zaraz will use this information when connecting to different third-party tools. Since some tools depend on certain fields, it is often useful to include all the information you can.

The same payload from before will resemble the following example, when we add the `system` information:

```json
{
  "events": [
    {
      "client": {
        "__zarazTrack": "transaction successful",
        "value": "200"
      },
      "system": {
        "page": {
          "url": "https://example.com",
          "title": "My website"
        },
        "device": {
          "language": "en-US",
          "ip": "192.168.0.1"
        }
      }
    }
  ]
}
```

For all available system keys, refer to the table below:

| Property                   | Type  | Description                                                                                |
| -------------------------- | ------------------------------ | ------------------------------------------------------------------------------------------ |
| `system.cookies`           | Object                         | A key-value object holding cookies from the device associated with the event.              |
| `system.device.ip`         | String                         | The IP address of the device associated with the event.                                    |
| `system.device.resolution` | String                         | The screen resolution of the device associated with the event, in a `WIDTHxHEIGHT` format. |
| `system.device.viewport`   | String                         | The viewport of the device associated with the event, in a `WIDTHxHEIGHT` format.          |
| `system.device.language`   | String                         | The language code used by the device associated with the event.                            |
| `system.device.user-agent` | String                         | The `User-Agent` string of the device associated with the event.                           |
| `system.page.title`        | String                         | The title of the page associated with the event.                                           |
| `system.page.url`          | String                         | The URL of the page associated with the event.                                             |
| `system.page.referrer`     | String                         | The URL of the referrer page in the time the event took place.                             |
| `system.page.encoding`     | String                         | The encoding of the page associated with the event.                                        |

:::note

It is currently not possible to override location related properties, such as City, Country, and Continent. 
:::

## Process API responses

For each Event Object in your payload, Zaraz will respond with a Result Object. The Result Objects order matches the order of your Event Objects.

Depending on what tools you are loading using Zaraz, the body of the response coming from the API might include information you will want to process. This is because some tools do not have a complete server-side implementation and still depend on cookies, client-side JavaScript or similar mechanisms. Each Result Object can include the following information:

| Result key | Description |
| --- | --- |
| `fetch` | Fetch requests that tools want to send from the user browser. |
| `execute` | JavaScript code that tools want to execute in the user browser. |
| `return` | Information that tools return. |
| `cookies` | Cookies that tools want to set for the user. |

You do not have to process the information above, but some tools might depend on this to work properly. You can start using the HTTP Events API without processing the information in the table above, and adjust accordingly later.

---

# Overview

URL: https://developers.cloudflare.com/zaraz/

import {
	CardGrid,
	Description,
	Feature,
	LinkTitleCard,
	Plan,
	Render,
} from "~/components";

<Description>

Offload third-party tools and services to the cloud and improve the speed and security of your website.

</Description>

<Plan id="zaraz.zaraz.properties.availability.summary" />

<Render file="zaraz-definition" />

---

## Features

<Feature header="Third-party tools" href="/zaraz/get-started/">
	You can add many third-party tools to Zaraz, and offload them from your
	website.
</Feature>

<Feature
	header="Custom Managed Components"
	href="/zaraz/advanced/load-custom-managed-component/"
>
	You can add Custom Managed Components to Zaraz and run them as a tool.
</Feature>

<Feature header="Web API" href="/zaraz/web-api/">
Zaraz provides a client-side web API that you can use anywhere inside the `<body>` tag of a page.
</Feature>

<Feature header="Consent management" href="/zaraz/consent-management/">
	Zaraz provides a Consent Management platform to help you address and manage
	required consents.
</Feature>

---

## More resources

<CardGrid>

<LinkTitleCard title="Discord Channel" href="https://discord.cloudflare.com" icon="discord">

If you have any comments, questions, or bugs to report, contact the Zaraz team on their Discord channel.

</LinkTitleCard>

<LinkTitleCard title="Community Forum" href="https://community.cloudflare.com/c/developers/zaraz/67" icon="open-book">

Engage with other users and the Zaraz team on Cloudflare support forum.

</LinkTitleCard>

</CardGrid>

---

# Pricing

URL: https://developers.cloudflare.com/zaraz/pricing-info/

Zaraz is available to all Cloudflare users, across all tiers. Each month, every Cloudflare account gets 1,000,000 free Zaraz Events. For additional usage, the Zaraz Paid plan costs $5 per month for each additional 1,000,000 Zaraz Events.

All Zaraz features and tools are always available on all accounts. Learn more about our pricing in [the following pricing announcement](https://blog.cloudflare.com/zaraz-announces-new-pricing)

## The Zaraz Event unit

One Zaraz Event is an event youâ€™re sending to Zaraz, whether thatâ€™s a page view, a `zaraz.track` event, or similar. You can easily see the total number of Zaraz Events youâ€™re currently using under the [Monitoring section](/zaraz/monitoring/) in the Cloudflare Zaraz Dashboard.

## Enabling Zaraz Paid

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login), and select your account and domain.
2. Go to **Zaraz** > **Plans**.
3. Click the **Enable Zaraz usage billing** button and follow the instructions.

## Using Zaraz Free

If you don't enable Zaraz Paid, you'll receive email notifications when you reach 50%, 80%, and 90% of your free allocation. Zaraz will be disabled until the next billing cycle if you exceed 1,000,000 events without enabling Zaraz Paid.

---

# DNS over TLS

URL: https://developers.cloudflare.com/1.1.1.1/encryption/dns-over-tls/

By default, DNS is sent over a plaintext connection. DNS over TLS (DoT) is one way to send DNS queries over an encrypted connection. Cloudflare supports DNS over TLS on standard port 853 and is compliant with [RFC 7858](https://tools.ietf.org/html/rfc7858). With DoT, the encryption happens at the transport layer, where it adds TLS encryption on top of a TCP connection.

## How it works

Cloudflare supports DNS over TLS (DoT) on `1.1.1.1`, `1.0.0.1`, and the corresponding IPv6 addresses (`2606:4700:4700::1111` and `2606:4700:4700::1001`) on port `853`. If your DoT client does not support IP addresses, Cloudflare's DoT endpoint can also be reached by hostname on `one.one.one.one`. A stub resolver (the DNS client on a device that talks to the DNS resolver) connects to the resolver over a TLS connection:

1. Before the connection, the DNS stub resolver has stored a base64 encoded SHA256 hash of the TLS certificate from 1.1.1.1 (called SPKI).
2. DNS stub resolver establishes a TCP connection with `1.1.1.1:853`.
3. DNS stub resolver initiates a TLS handshake.
4. In the TLS handshake, 1.1.1.1 presents its TLS certificate.
5. Once the TLS connection is established, the DNS stub resolver can send DNS over an encrypted connection, preventing eavesdropping and tampering.
6. All DNS queries sent over the TLS connection must comply with specifications of [sending DNS over TCP](https://tools.ietf.org/html/rfc1035#section-4.2.2).

## Example

```sh
kdig -d @1.1.1.1 +tls-ca +tls-host=one.one.one.one example.com
```

```sh output

;; DEBUG: Querying for owner(example.com.), class(1), type(1), server(1.1.1.1), port(853), protocol(TCP)
;; DEBUG: TLS, imported 138 system certificates
;; DEBUG: TLS, received certificate hierarchy:
;; DEBUG:  #1, C=US,ST=California,L=San Francisco,O=Cloudflare\, Inc.,CN=cloudflare-dns.com
;; DEBUG:      SHA-256 PIN: GP8Knf7qBae+aIfythytMbYnL+yowaWVeD6MoLHkVRg=
;; DEBUG:  #2, C=US,O=DigiCert Inc,CN=DigiCert TLS Hybrid ECC SHA384 2020 CA1
;; DEBUG:      SHA-256 PIN: e0IRz5Tio3GA1Xs4fUVWmH1xHDiH2dMbVtCBSkOIdqM=
;; DEBUG: TLS, skipping certificate PIN check
;; DEBUG: TLS, The certificate is trusted.
;; TLS session (TLS1.3)-(ECDHE-X25519)-(ECDSA-SECP256R1-SHA256)-(AES-256-GCM)
;; ->>HEADER<<- opcode: QUERY; status: NOERROR; id: 3395
;; Flags: qr rd ra; QUERY: 1; ANSWER: 1; AUTHORITY: 0; ADDITIONAL: 1

;; EDNS PSEUDOSECTION:
;; Version: 0; flags: ; UDP size: 1232 B; ext-rcode: NOERROR
;; PADDING: 408 B

;; QUESTION SECTION:
;; example.com.        		IN	A

;; ANSWER SECTION:
example.com.        	75897	IN	A	93.184.216.34

;; Received 468 B
;; Time 2023-06-23 18:05:42 PDT
;; From 1.1.1.1@853(TCP) in 12.1 ms
```

## Supported TLS versions

Cloudflare's DNS over TLS supports TLS 1.3 and TLS 1.2.

---

# Encryption

URL: https://developers.cloudflare.com/1.1.1.1/encryption/

Traditionally, DNS queries and replies are performed over plaintext. They are sent over the Internet without any kind of encryption or protection, even when you are accessing a secured website. This has a great impact on security and privacy, as these queries might be subject to surveillance, spoofing and tracking by malicious actors, advertisers, ISPs, and others.

To prevent untrustworthy entities from interpreting and manipulating your queries, 1.1.1.1 supports different standards to encrypt plaintext DNS traffic and improve DNS privacy:

- [DNS over TLS (DoT)](/1.1.1.1/encryption/dns-over-tls/)
- [DNS over HTTPS (DoH)](/1.1.1.1/encryption/dns-over-https/)
- [Oblivious DNS over HTTPS (ODoH)](/1.1.1.1/encryption/oblivious-dns-over-https/)

You can also [configure your browser](/1.1.1.1/encryption/dns-over-https/encrypted-dns-browsers/) to secure your DNS queries.

If you need to secure connections in your smartphone, refer to 1.1.1.1 [iOS](/1.1.1.1/setup/ios/) or [Android](/1.1.1.1/setup/android/) apps.

---

# DNSKEY

URL: https://developers.cloudflare.com/1.1.1.1/encryption/dnskey/

[DNSSEC is a protocol](https://www.cloudflare.com/learning/dns/dns-records/dnskey-ds-records/) that adds a layer of security to the domain name system (DNS). DNSSEC does this by providing authentication through public signing keys using two DNS records: DNSKEY and DS. They can be used to verify DNSSEC signatures in [RRSIG records](https://www.cloudflare.com/dns/dnssec/how-dnssec-works/).

1.1.1.1 supports the following signature algorithms:

* RSA/SHA-1
* RSA/SHA-256
* RSA/SHA-512
* RSASHA1-NSEC3-SHA1
* ECDSA Curve P-256 with SHA-256 (ECDSAP256SHA256)
* ECDSA Curve P-384 with SHA-384 (ECDSAP384SHA384)
* ED25519

---

# Oblivious DNS over HTTPS

URL: https://developers.cloudflare.com/1.1.1.1/encryption/oblivious-dns-over-https/

As announced on [our blog](https://blog.cloudflare.com/oblivious-dns/), since late 2020, Cloudflare 1.1.1.1 supports Oblivious DNS over HTTPS (ODoH).

:::caution
ODoH is defined in [RFC 9230](https://www.rfc-editor.org/rfc/rfc9230.html). This RFC is experimental and is not endorsed by the IETF.
:::

## How ODoH works

ODoH improves privacy by separating the contents of an HTTP request (and response) from its requester IP address. To achieve this, a proxy and a target are introduced between the client and the upstream DNS resolver:

- The proxy has no visibility into the DNS messages, with no ability to identify, read, or modify either the query being sent by the client or the answer being returned by the target.

- The target only has access to the encrypted query and the proxy's IP address, while not having visibility over the client's IP address.

- Only the intended target can read the content of the query and produce a response, which is also encrypted.

This means that, as long as the proxy and the target do not collude, no single entity can have access to both the DNS messages and the client IP address at the same time. Also, clients are in complete control of proxy and target selection.

Additionally, clients encrypt their query for the target using Hybrid Public Key Encryption (HPKE). A target's public key is obtained via DNS, where it is bundled into an HTTPS resource record and protected by DNSSEC.

## Cloudflare and third-party products

Cloudflare 1.1.1.1 supports ODoH by acting as a target that can be reached at `odoh.cloudflare-dns.com`.

To make ODoH queries you can use open source clients such as [dnscrypt-proxy](https://github.com/DNSCrypt/dnscrypt-proxy).

Also, [iCloud Private Relay](https://support.apple.com/102602) is based on ODoH and uses [Cloudflare as one of their partners](https://blog.cloudflare.com/icloud-private-relay/).

## Related resources

- [HPKE: Standardizing public-key encryption](https://blog.cloudflare.com/hybrid-public-key-encryption/) blog post
- [Privacy Gateway](/privacy-gateway/)

---

# Extended DNS error codes

URL: https://developers.cloudflare.com/1.1.1.1/infrastructure/extended-dns-error-codes/

[Extended DNS Error Codes](https://www.rfc-editor.org/rfc/rfc8914.html) is a method to return additional information about the cause of DNS errors. As there are many reasons why a DNS query might fail, it became necessary to provide additional information on the exact cause of an error.

1.1.1.1 supports Extended DNS Error Codes. Below is a list of error codes 1.1.1.1 returns, what they mean, and steps you may want to take to resolve the issue.





<table>
    <thead>
        <tr>
            <th style="width:5%">Code number</th>
            <th style="width:20%">Code name</th>
            <th style="width:35%">Example output</th>
            <th style="width:40%">Next steps</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1</td>
            <td><p>Unsupported DNSKEY Algorithm</p></td>
            <td><code>EDE: 1 (Unsupported DNSKEY Algorithm): (failed to verify example.com. A: unsupported key size, DNSKEY example.com., id = 12345)</code></td>
            <td>The domain did not pass DNSSEC validation. Check which <a href="/1.1.1.1/encryption/dnskey/">signature key algorithm</a> your website uses and confirm it is supported by 1.1.1.1.</td>
        </tr>
        <tr>
            <td>2</td>
            <td>Unsupported DS Digest Type</td>
            <td><code>EDE: 2 (Unsupported DS Digest Type): (no supported DS digest type for example.com.)</code></td>
            <td>The domain did not pass DNSSEC validation due to an unsupported digest type on the DS record. If none of the provided DS records are supported, the domain will fail to resolve. Make sure to <a href="/dns/dnssec/">add a supported DS record</a> with your registrar.</td>
        </tr>
        <tr>
            <td>3</td>
            <td>Stale Answer</td>
            <td><code>EDE: 3 (Stale Answer)</code></td>
            <td>This is a silent error. It notifies that the DNS resolver could only return stale data. If the issue persists reach out on the 1.1.1.1 <a href="https://community.cloudflare.com/c/reliability/dns-1111/47">community forum</a>.</td>
        </tr>
        <tr>
            <td>6</td>
            <td>DNSSEC Bogus</td>
            <td>
                <code>EDE: 6 (DNSSEC Bogus): (proof of non-existence of example.com. A)</code>
                <br/><br/>
                <code>EDE: 6 (DNSSEC Bogus): (found duplicate CNAME records for example.com. (1 duplicate RRs))</code>
            </td>
            <td>This domain did not pass DNSSEC validation. The signatures for the target record, or the proof of non-existence of the target records, are invalid. Check your <a href="/dns/">DNS configuration</a>.</td>
        </tr>
        <tr>
            <td>7</td>
            <td>Signature Expired</td>
            <td><code>EDE: 7 (Signature Expired): (for DNSKEY example.com., id = 12345: RRSIG example.com., expiration = 123456)</code></td>
            <td>This domain did not pass DNSSEC validation due to an expired signature. Make sure your zone is signed with valid <a href="/dns/dnssec/troubleshooting/">DNSSEC signatures</a>.</td>
        </tr>
        <tr>
            <td>8</td>
            <td>Signature Not Yet Valid</td>
            <td><code>EDE: 8 (Signature Not Yet Valid): (for DNSKEY example.com., id = 12345: RRSIG example.com., inception = 12345)</code></td>
            <td>This domain did not pass DNSSEC validation. Make sure your zone is signed with valid <a href="/dns/dnssec/troubleshooting/">DNSSEC signatures</a>.</td>
        </tr>
        <tr>
            <td>9</td>
            <td>DNSKEY Missing</td>
            <td><code>EDE: 9 (DNSKEY Missing): (no SEP matching the DS found for example.com.)</code></td>
            <td>This domain did not pass DNSSEC validation. It does not have a SEP DNSKEY that matches the set of DS records at the registry. Make sure to either sign the zone using keys that match the current DS set, or <a href="/dns/dnssec/">add the missing DS records</a> with your registrar.</td>
        </tr>
        <tr>
            <td>10</td>
            <td>RRSIGs Missing</td>
            <td><code>EDE: 10 (RRSIGs Missing): (for DNSKEY example.com., id = 12345)</code></td>
            <td>1.1.1.1 was unable to retrieve Resource Record Signatures (RRSigs) to verify the authenticity of the records. Check your <a href="/dns/">DNS configuration</a> and the response code. If the response code is not <code>SERVFAIL</code>, this error indicates that there is a non-operational key issue somewhere along the path, but the resolver found at least one successful path for validation. Examples of non-operational key issues include but are not limited to key rollover in-progress, stand-by key, and attacker stripping signatures made by a certain key.</td>
        </tr>
        <tr>
            <td>11</td>
            <td>No Zone Key Bit Set</td>
            <td><code>EDE: 11 (No Zone Key Bit Set): (for DNSKEY example.com., id = 12345)</code></td>
            <td>This domain did not pass DNSSEC validation. The zone's SEP DNSKEY must <a href="https://datatracker.ietf.org/doc/html/rfc4035#section-5.3.1">set a Zone Key flag</a>. Check your <a href="/dns/dnssec/">DNSSEC configuration</a> or DNSSEC's <a href="/dns/dnssec/troubleshooting/">troubleshooting guide</a>.</td>
        </tr>
        <tr>
            <td>12</td>
            <td>NSEC Missing</td>
            <td><code>EDE: 12 (NSEC Missing): failed to verify an insecure referral proof for example.com</code></td>
            <td>This domain did not pass DNSSEC validation. The upstream nameserver did not include a valid proof of non-existence for the target name. Make sure the zone is <a href="/dns/dnssec/troubleshooting/">signed with DNSSEC</a> and has valid <a href="https://www.cloudflare.com/dns/dnssec/dnssec-complexities-and-considerations/">NSEC/NSEC3 records</a>.</td>
        </tr>
        <tr>
            <td>13</td>
            <td>Cached Error</td>
            <td><code>EDE: 13 (Cached Error)</code></td>
            <td>1.1.1.1 returned a cached error. If this issue persists, reach out to the <a href="https://community.cloudflare.com/c/reliability/dns-1111/47">community forum</a>.</td>
        </tr>
        <tr>
            <td>22</td>
            <td>No Reachable Authority</td>
            <td><code>EDE: 22 (No Reachable Authority): (at delegation example.com.)</code></td>
            <td>1.1.1.1 could not reach some or all of the authoritative nameservers (or they potentially refused to resolve). This can occur if the authoritative nameservers are overloaded or temporarily unavailable. If this issue persists, reach out to the <a href="https://community.cloudflare.com/c/reliability/dns-1111/47">community forum</a>.</td>
        </tr>
        <tr>
            <td>23</td>
            <td>Network Error</td>
            <td><code>EDE: 23 (Network Error): (1.1.1.1:53 rcode=SERVFAIL for example.com. A)</code></td>
            <td>1.1.1.1 could not determine a network path to the upstream nameservers, or the nameserver did not respond. If this issue persists, reach out to the <a href="https://community.cloudflare.com/c/reliability/dns-1111/47">community forum</a>.</td>
        </tr>
        <tr>
            <td>30</td>
            <td>Invalid Query Type</td>
            <td><code>EDE: 30 (Invalid Query Type): Invalid Query Type</code></td>
            <td>The record type in the request cannot give a valid answer. If this is returned for standard query types, such as A or AAAA records, please reach out to the <a href="https://community.cloudflare.com/c/reliability/dns-1111/47">community forum</a>.</td>
        </tr>
    </tbody>
</table>

---

# Infrastructure

URL: https://developers.cloudflare.com/1.1.1.1/infrastructure/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Support for IPv6-only networks

URL: https://developers.cloudflare.com/1.1.1.1/infrastructure/ipv6-networks/

While network infrastructure is shifting towards IPv6-only networks, providers still need to support IPv4 addresses. Dual-stack networks are networks in which all nodes have both IPv4 and IPv6 connectivity capabilities, and can therefore understand both IPv4 and IPv6 packets.

1.1.1.1 supports DNS64, a mechanism that synthesizes AAAA records from A records when no AAAA records exist. DNS64 allows configuring a DNS resolver to synthesize IPv6 addresses from IPv4 answers.

:::note


You should only enable DNS64 if you are managing or using an IPv6-only network. While the resolver can synthesize IPv6 addresses, it cannot synthesize their record signatures for domains using DNSSEC, so a DNS client that is able to revalidate signatures would reject these extra records without signatures.

A good tradeoff is to use a secure protocol such as DNS over TLS, or DNS over HTTPS between the client and the resolver to prevent tampering.


:::

## Configure DNS64

DNS64 is specifically for networks that already have NAT64 support. If you are a network operator who has NAT64, you can test our DNS64 support by updating it to the following IP addresses:

```txt
2606:4700:4700::64
2606:4700:4700::6400
```

Some devices use separate fields for all eight parts of IPv6 addresses and cannot accept the `::` IPv6 abbreviation syntax. For such fields enter:

```txt
2606:4700:4700:0:0:0:0:64
2606:4700:4700:0:0:0:0:6400
```

## Test DNS64

After your configuration, visit an IPv4 only address to check if you can reach it. For example, you can visit [https://ipv4.google.com](https://ipv4.google.com).

Visit [http://test-ipv6.com/](http://test-ipv6.com/) to test if it can detect your IPv6 address. If you receive a `10/10`, your IPv6 is configured correctly.

---

# Network operators

URL: https://developers.cloudflare.com/1.1.1.1/infrastructure/network-operators/

Network operators, including Internet Service Providers (ISPs), device manufacturers, public Wi-Fi networks, municipal broadband providers, and security scanning services can use [1.1.1.1](/1.1.1.1/setup/) in place of operating their own recursive DNS infrastructure.

Cloudflare also partners with ISPs and network equipment providers to make [1.1.1.1 for Families](/1.1.1.1/setup/#1111-for-families) available within their offerings. Refer to our [blog post](https://blog.cloudflare.com/safer-resolver/) for details.

Using 1.1.1.1 can improve performance for end-users due to Cloudflare's extensive [global network](https://www.cloudflare.com/network/), as well as provide higher overall cache hit rates due to our regional caches.

The 1.1.1.1 resolver was designed with a privacy-first approach. Refer to our [data and privacy policies](/1.1.1.1/privacy/public-dns-resolver/) for what is logged and retained by 1.1.1.1.

## Configuring 1.1.1.1

There are multiple ways to use 1.1.1.1 as an operator:

* Including a [DNS over HTTPS](/1.1.1.1/encryption/dns-over-https/) or [DNS over TLS](/1.1.1.1/encryption/dns-over-tls/) proxy on end-user routers or devices (best for privacy).
* Pushing 1.1.1.1 to devices via DHCP/PPP within an operator network (recommended; most practical).
* Having a DNS proxy on a edge router make requests to 1.1.1.1 on behalf of all connected devices.

Where possible, we recommend using encrypted transports (DNS over HTTPS or TLS) for queries, as this provides the highest degree of privacy for users over last-mile networks.

## Available Endpoints

:::note


[Cloudflare Zero Trust](https://www.cloudflare.com/products/zero-trust/) supports customizable [DNS policies](/cloudflare-one/policies/gateway/dns-policies/), analytics, additional built-in filtering categories, and custom rate limiting capabilities.

If you require additional controls over our public 1.1.1.1 resolver, [contact us](https://www.cloudflare.com/products/zero-trust/).


:::

The publicly available endpoints for 1.1.1.1 are detailed in the following table:



| Resolver                                 | IPv4 address               | IPv6 <br /> address                                  | DNS over <br /> HTTPS endpoint                  | DNS over <br /> TLS endpoint  |
| ---------------------------------------- | -------------------------- | ---------------------------------------------------- | ----------------------------------------------- | ----------------------------- |
| 1.1.1.1 <br />(unfiltered)               | `1.1.1.1` <br /> `1.0.0.1` | `2606:4700:4700::1111` <br /> `2606:4700:4700::1001` | `https://cloudflare-dns.com/dns-query`          | `cloudflare-dns.com`          |
| Families <br />(Malware)                 | `1.1.1.2` <br /> `1.0.0.2` | `2606:4700:4700::1112` <br /> `2606:4700:4700::1002` | `https://security.cloudflare-dns.com/dns-query` | `security.cloudflare-dns.com` |
| Families <br />(Adult Content + Malware) | `1.1.1.3` <br /> `1.0.0.3` | `2606:4700:4700::1113` <br /> `2606:4700:4700::1003` | `https://family.cloudflare-dns.com/dns-query`   | `family.cloudflare-dns.com`   |



You may wish to provide end users with options to change from the default 1.1.1.1 resolver to one of the [1.1.1.1 for Families](/1.1.1.1/setup/#1111-for-families) endpoints.

## Rate Limiting

Operators using 1.1.1.1 for typical Internet-facing applications and/or users should not encounter any rate limiting for their users. In some rare cases, security scanning use-cases or proxied traffic may be rate limited to protect our infrastructure as well as upstream DNS infrastructure from potential abuse.

Best practices include:

* Avoiding tunneling or proxying all queries from a single IP address at high rates. Distributing queries across multiple public IPs will improve this without impacting cache hit rates (caches are regional).
* A high rate of "uncacheable" responses (such as `SERVFAIL`) against the same domain may be rate limited to protect upstream, authoritative nameservers. Many authoritative nameservers enforce their own rate limits, and we strive to avoid overloading third party infrastructure where possible.

## Help

If you are a network operator and still have outstanding questions, contact `resolver@cloudflare.com` with your use case, so it can be discussed further. Make sure to visit [1.1.1.1/help](https://one.one.one.one/help) from within your network and share the resulting report when contacting Cloudflare.

---

# SLA and technical support

URL: https://developers.cloudflare.com/1.1.1.1/infrastructure/sla-and-support/

As you use 1.1.1.1 in your infrastructure or service, note that dedicated technical support is limited.

You are subject to the [Cloudflare Website and Online Services Terms of Use](https://www.cloudflare.com/website-terms/) and no service level agreements (SLAs) are provided.

If you need SLAs and dedicated support, consider using [Cloudflare Gateway](/cloudflare-one/policies/gateway/) instead.

Gateway includes other advanced options such as domain categories, customized filtering, and scheduling capabilities. For example, if you are a device manufacturer or network operator, you can use a multi-tenant environment to allow your customers to configure their own individual filters.

---

# Cloudflare Resolver for Firefox

URL: https://developers.cloudflare.com/1.1.1.1/privacy/cloudflare-resolver-firefox/

## Frequently asked questions about the Cloudflare resolver for Firefox

### What is the Cloudflare resolver for Firefox?

Every time you type a web address, such as [www.mozilla.org](http://www.mozilla.org) or [www.firefox.com](http://www.firefox.com), into a web browser, the web browser sends a query to a DNS resolver. If DNS is like the card catalog of the Internet, then a DNS resolver is like a helpful librarian that knows how to use the information from that catalog to track down the exact location of a website. Whenever a resolver receives your query it looks up the IP address associated with the web address that you entered and relays that information to your web browser. â€œDNS resolutionâ€ as this process is referred to, is a crucial component of your Internet experience because without it your web browser would be unable to communicate with the servers that host your favorite websites, since communication requires knowing the IP addresses of those websites.

For most Internet users, the DNS resolver that they use is either the one that comes with the operating system running on their machines or the one that is set by their network provider. In some cases, these resolvers leave a lot to be desired because of their susceptibility to unwanted spying and other security threats.

To counter such threats, Mozilla has partnered with Cloudflare to provide direct DNS resolution from within the Firefox browser using the Cloudflare Resolver for Firefox. What this means is that whenever you select or type a web address in the Firefox browser your DNS lookup request will be sent over a secure channel to the Cloudflare Resolver for Firefox rather than to an unknown DNS resolver, significantly decreasing the odds of any unwanted spying or man in the middle attacks.

### What information does the Cloudflare resolver for Firefox collect?

Any data Cloudflare handles as a result of its resolver for Firefox is as a data processor acting pursuant to Firefoxâ€™s data processing instructions. Therefore, the data Cloudflare collects and processes pursuant to its agreement with Firefox is not covered by the [Cloudflare Privacy Policy](https://www.cloudflare.com/privacypolicy/). As part of its agreement with Firefox, Cloudflare has agreed to collect only a limited amount of data about the DNS requests that are sent to the Cloudflare Resolver for Firefox via the Firefox browser. Cloudflare will collect only the following information from Firefox users:

* date
* dateTime
* srcAsNum
* srcIPVersion
* dstIPVersion
* dstIPv6
* dstIPv4
* dstPort
* protocol
* queryName
* queryType
* queryClass
* queryRd
* queryDo
* querySize
* queryEdns
* ednsVersion
* ednsPayload
* ednsNsid
* responseType
* responseCode
* responseSize
* responseCount
* responseTimeMs
* responseCached
* responseMinTTL
* answerData type
* answerData
* validationState
* coloID (unique Cloudflare data center ID)
* metalId (unique Cloudflare data center ID)

All of the above information will be stored briefly as part of Cloudflareâ€™s temporary logs, and then permanently deleted within 24 hours of Cloudflareâ€™s receipt of such information. In addition to the above information, Cloudflare will also collect and store the following information as part of its permanent logs.

* Total number of requests processed by each Cloudflare co-location facility.
* Aggregate list of all domain names requested.
* Samples of domain names queried along with the times of such queries.

Information stored in Cloudflareâ€™s permanent logs will be anonymized and may be held indefinitely by Cloudflare for its own internal research and development purposes.

### What is the Cloudflare promise?

Cloudflare understands how important your data is to you, which is why we promise to use the information that we collect from the Cloudflare Resolver for Firefox solely to improve the performance of Cloudflare Resolver for Firefox and to assist us in debugging efforts if an issue arises. In addition to limiting our collection and use of your data, Cloudflare also promises:

* Cloudflare will not retain or sell or transfer to any third party (except as may be required by law) any personal information, IP addresses or other user identifiers from the DNS queries sent from the Firefox browser to the Cloudflare Resolver for Firefox;

* Cloudflare will not combine the data that it collects from such queries, with any other Cloudflare or third party data in any way that can be used to identify individual end users;

* Cloudflare will not sell, license, sublicense, or grant any rights to your data to any other person or entity without Mozillaâ€™s explicit written permission.

### What about government requests for content blocking?

Cloudflare does not block or filter content through the Cloudflare Resolver for Firefox. As part of its agreement with Mozilla, Cloudflare is providing only direct DNS resolution. If Cloudflare were to receive written requests from law enforcement and government agencies to block access to domains or content through the Cloudflare resolver for Firefox, Cloudflare would, in consultation with Mozilla, exhaust our legal remedies before complying with such a request. We also commit to documenting any government request to block access in our semi-annual transparency report, unless legally prohibited from doing so.

---

# 1.1.1.1 Public DNS Resolver

URL: https://developers.cloudflare.com/1.1.1.1/privacy/public-dns-resolver/

*Last updated March 27, 2024*

## Cloudflareâ€™s commitment to privacy: 1.1.1.1 Public DNS Resolver

The 1.1.1.1 public DNS resolver is governed by our [Privacy Policy](https://www.cloudflare.com/privacypolicy/). This document provides additional details on our collection, use, and disclosure of the information collected from the 1.1.1.1 public DNS resolver.

***

Nearly everything on the Internet starts with a DNS request. DNS is the Internetâ€™s directory. Select a link, open an app, send an email, and the first thing your phone or computer does is ask its directory: where can I find this?

Unfortunately, by default, DNS is usually slow and insecure. Your ISP, and anyone else listening in on the Internet, can see every site you visit and every app you use â€” even if their content is encrypted. Creepily, some DNS providers sell data about your Internet activity or use it to target you with ads.

Given the current state of affairs, Cloudflare created a DNS resolver with your privacy and security in mind. Cloudflare, in partnership with APNIC, runs the 1.1.1.1 public resolver, a recursive DNS service that values user privacy and security. DNS requests sent to our public resolver can be sent over a secure channel, significantly decreasing the odds of any unwanted spying or man in the middle attacks.

The 1.1.1.1 public DNS resolver was designed for privacy first, and Cloudflare commits to the following:

1. Cloudflare will not sell or share Public Resolver usersâ€™ personal data with third parties or use personal data from the Public Resolver to target any user with advertisements.
2. Cloudflare will only retain or use what is being asked, not information that will identify who is asking it. Except for randomly sampled network packets captured from at most 0.05% of all traffic sent to Cloudflareâ€™s network infrastructure, Cloudflare will not retain the source IP from DNS queries to the Public Resolver in non-volatile storage. These randomly sampled packets are solely used for network troubleshooting and DoS mitigation purposes.
3. A Public Resolver userâ€™s IP address (referred to as the client or source IP address) will not be stored in non-volatile storage. Cloudflare will anonymize source IP addresses via IP truncation methods (last octet for IPv4 and last 80 bits for IPv6). Cloudflare will delete the truncated IP address within 25 hours.
4. Cloudflare will retain only the limited transaction and debug log data (â€œPublic Resolver Logsâ€) set forth below, for the legitimate operation of our Public Resolver and research purposes, and Cloudflare will delete the Public Resolver Logs within 25 hours.
5. Cloudflare will not share the Public Resolver Logs with any third parties except for APNIC pursuant to a Research Cooperative Agreement. APNIC will only have limited access to query the anonymized data in the Public Resolver Logs and conduct research related to the operation of the DNS system.

Cloudflare has taken technical steps to ensure that we cannot retain our userâ€™s information.

We have also retained one of the top four accounting firms to audit our practices and publish a public report confirming we are doing what we said we would. The report is available in the [Certifications and compliance resources](https://www.cloudflare.com/trust-hub/compliance-resources/) page.

## Limited data sharing with APNIC

Cloudflare has partnered with [APNIC Labs](https://labs.apnic.net/?p=1127), the regional Internet registry for the Asia-Pacific region to make the 1.1.1.1 IP address the home of the Cloudflare Public DNS Resolver. As part of its mission to ensure a global, open and secure Internet, APNIC conducts research about the functioning and governance of the Internet, which it makes available on its website, located at [www.apnic.net](http://www.apnic.net).

Cloudflare has agreed to provide APNIC with access to some of the anonymized data that Cloudflare collects through the Cloudflare Public DNS Resolver. Specifically, APNIC will be permitted to access query names, query types, resolver location and other metadata via a Cloudflare API that will allow APNIC to study topics like the volume of DDoS attacks launched on the Internet and adoption of IPv6.

APNIC Labs will use such data for non-profit operational research. As part of Cloudflareâ€™s commitment to privacy, Cloudflare will not provide APNIC with any access to the IP address associated with a client.

Aside from APNIC, Cloudflare will not share the Public Resolver Logs with any third party.

## Data in public resolver logs

The Public Resolver Logs we store consist entirely of the following fields:

* answerData type
* answerData
* coloID (unique Cloudflare data center ID)
* date
* dateTime
* dstIPVersion
* dstIPv6
* dstIPv4
* dstPort
* ede
* ednsVersion
* ednsPayload
* ednsNsid
* feature.uid
* feature.value
* metalId (unique Cloudflare data center ID)
* ns ip
* ns name
* protocol
* queryName
* queryType
* queryClass
* queryRd
* queryDo
* querySize
* queryEdns
* queryCd
* responseType
* responseCode
* responseSize
* responseCount
* responseTimeMs
* responseCached
* responseMinTTL
* reused
* srcAsNum
* srcCountry
* srcIPVersion
* validationState

Additionally, recursive resolvers perform outgoing queries to various authoritative nameservers in the DNS hierarchy that are logged in subrequest fields. These logs are used for the operation and debugging of our public DNS resolver service.

The following subrequest data is included in the Public Resolver Logs:

* subrequest.ipv6 (authoritative nameserver)
* subrequest.ipv4 (authoritative nameserver)
* subrequest.protocol
* subrequest.durationMs
* subrequest.queryName
* subrequest.queryType
* subrequest.responseCode
* subrequest.responseCount
* subrequest.recordType
* subrequest.recordData
* subrequest.error

Except for the limited sampled data from the Public Resolver Logs (which do not include truncated IP addresses) used to generate the aggregated data described below, all of the Public Resolver Logs are deleted within 25 hours of Cloudflareâ€™s receipt of such information.

Cloudflare may make the following aggregations:

* Total number of queries with different protocol settings (for example, tcp/udp/dnssec) by Cloudflare data centers.
* Response code/time quantiles with different protocol settings by Cloudflare data centers.
* Total Number of Requests Processed by Cloudflare data centers.
* Aggregate List of All Domain Names Requested and aggregate number of requests and timestamp of first time requested by region.
* Number of unique clients, queries over IPv4, queries over IPv6, queries with the RD bit set, queries asking for DNSSEC, number of bogus, valid, and invalid DNSSEC answers, queries by type, number of answers with each response code, response time quantiles (e.g. 50 percentile), response TTL, and number of cached answers per minute, per day, per protocol (HTTPS/UDP/TCP/TLS), per region, per Cloudflare data center, and per Autonomous System Number.
* Number of queries, number of queries with EDNS, number of bytes and time in answers quantiles (e.g. 50 percentile) by day, month, Cloudflare data center, and by IPv4 vs IPv6.
* Number of queries, response codes and response code quantiles (e.g. 50 percentile) by day, region, name and type.

Cloudflare may store the data described above indefinitely in order to power Cloudflare Radar and assist Cloudflare in improving Cloudflare services, such as, enhancing the overall performance of the Cloudflare Resolver and identifying security threats.

## What about requests for content blocking?

Cloudflare does not block or filter any content through the 1.1.1.1 Public DNS Resolver, which is designed for direct, fast DNS resolution, not for blocking or filtering content. Cloudflare does block and filter malware and adult content through 1.1.1.1 for Families, which is designed to help individuals protect their home networks.

In general, Cloudflare views government or civil requests to block content at the DNS level as ineffective, inefficient, and overboard. Because such a block would apply globally to all users of the resolver, regardless of where they are located, it would affect end users outside of the blocking governmentâ€™s jurisdiction. A government request to block content through a globally available public recursive resolver like the 1.1.1.1 Public DNS Resolver and 1.1.1.1 for Families should therefore be evaluated as a request to block content globally.

Given the broad extraterritorial effect, if Cloudflare were to receive written requests from law enforcement and government agencies to block access to domains or content through the 1.1.1.1 Public DNS Resolver or to block access to domains or content through 1.1.1.1 for Families that is outside the scope of the filtering in that product, Cloudflare would pursue its legal remedies before complying with such a request. We also commit to documenting any government request to block access in our semi-annual transparency report, unless legally prohibited from doing so.

---

# Privacy

URL: https://developers.cloudflare.com/1.1.1.1/privacy/

* [1.1.1.1 Public DNS Resolver](/1.1.1.1/privacy/public-dns-resolver/): This document provides details on our collection, use, and disclosure of the information processed by the 1.1.1.1 public DNS resolver. The 1.1.1.1 public DNS resolver service is governed by our [Privacy Policy](https://www.cloudflare.com/privacypolicy/).

* [Resolver for Firefox](/1.1.1.1/privacy/cloudflare-resolver-firefox/): This document outlines our collection, use, and disclosure of the information processed by the Cloudflare Resolver for Firefox. Any data Cloudflare processes in connection with the Cloudflare Resolver for Firefox is as a data processor acting pursuant to Mozillaâ€™s data processing instructions. Cloudflare Resolver for Firefox is not covered by our main Privacy Policy and is separate from the 1.1.1.1 public DNS resolver.

* [1.1.1.1 Application](https://www.cloudflare.com/application/privacypolicy/): This policy applies to our collection, use, and disclosure of the information from Cloudflareâ€™s consumer-facing 1.1.1.1 Applications, such as our 1.1.1.1 Application for iOS and Android.

---

# DNS in Google Sheets

URL: https://developers.cloudflare.com/1.1.1.1/other-ways-to-use-1.1.1.1/dns-in-google-sheets/

import { Details } from "~/components"

## Create a function

1.1.1.1 works directly inside Google Sheets. To get started, create a [Google Function](https://developers.google.com/apps-script/guides/sheets/functions) with the following code:

```js
function NSLookup(type, domain, useCache = false, minCacheTTL = 30) {

  if (typeof type == 'undefined') {
    throw new Error('Missing parameter 1 dns type');
  }

  if (typeof domain == 'undefined') {
    throw new Error('Missing parameter 2 domain name');
  }

  if (typeof useCache != "boolean") {
    throw new Error('Only boolean values allowed in 3 use cache');
  }

  if (typeof minCacheTTL != "number") {
    throw new Error('Only numeric values allowed in 4 min cache ttl');
  }

  type = type.toUpperCase();
  domain = domain.toLowerCase();

  let cache = null;
  if (useCache) {
    // Cache key and hash
    cacheKey = domain + "@" + type;
    cacheHash = Utilities.base64Encode(cacheKey);
    cacheBinKey = "nslookup-result-" + cacheHash;

    cache = CacheService.getScriptCache();
    const cachedResult = cache.get(cacheBinKey);
    if (cachedResult != null) {
      return cachedResult;
    }
  }

  const url = 'https://cloudflare-dns.com/dns-query?name=' + encodeURIComponent(domain) + '&type=' + encodeURIComponent(type);
  const options = {
    muteHttpExceptions: true,
    headers: {
      accept: "application/dns-json"
    }
  };

  const result = UrlFetchApp.fetch(url, options);
  const rc = result.getResponseCode();
  const resultText = result.getContentText();

  if (rc !== 200) {
    throw new Error(rc);
  }

  const errors = [
    { name: "NoError", description: "No Error"}, // 0
    { name: "FormErr", description: "Format Error"}, // 1
    { name: "ServFail", description: "Server Failure"}, // 2
    { name: "NXDomain", description: "Non-Existent Domain"}, // 3
    { name: "NotImp", description: "Not Implemented"}, // 4
    { name: "Refused", description: "Query Refused"}, // 5
    { name: "YXDomain", description: "Name Exists when it should not"}, // 6
    { name: "YXRRSet", description: "RR Set Exists when it should not"}, // 7
    { name: "NXRRSet", description: "RR Set that should exist does not"}, // 8
    { name: "NotAuth", description: "Not Authorized"} // 9
  ];

  const response = JSON.parse(resultText);

  if (response.Status !== 0) {
    return errors[response.Status].name;
  }

  const outputData = [];
  let cacheTTL = 0;

  for (const i in response.Answer) {
    outputData.push(response.Answer[i].data);
    const ttl = response.Answer[i].TTL;
    cacheTTL = Math.min(cacheTTL || ttl, ttl);
  }

  const outputString = outputData.join(',');

  if (useCache) {
    cache.put(cacheBinKey, outputString, Math.max(cacheTTL, minCacheTTL));
  }

  return outputString;
}
```

## Using 1.1.1.1

When you feed the function `NSLookup` a record type and a domain, you will get a DNS record value in the cell you called `NSLookup`.

To limit the number of DNS lookups and speed up the results (especially in larger Google Sheets), you can cache the returned DNS record value. Both the cache usage and the cache TTL can be controlled in arguments 3 and 4, respectively.

<Details header="Supported DNS record types">

* `A`
* `AAAA`
* `CAA`
* `CNAME`
* `DS`
* `DNSKEY`
* `MX`
* `NS`
* `NSEC`
* `NSEC3`
* `RRSIG`
* `SOA`
* `TXT`


</Details>

For example, typing:

```txt
NSLookup(B1, B2)
```

Or - depending on your regional settings - you may have to use this formula:

```txt
NSLookup(B1; B2)
```

<div class="medium-img">

![Google sheets function](~/assets/images/1.1.1.1/google-sheet-function.png)

</div>

<br/>

Returns

```txt
198.41.214.162, 198.41.215.162
```

<div class="medium-img">

![Google sheets function](~/assets/images/1.1.1.1/google-sheet-result.png)

</div>

---

# DNS over Discord

URL: https://developers.cloudflare.com/1.1.1.1/other-ways-to-use-1.1.1.1/dns-over-discord/

import { Details } from "~/components"

1.1.1.1 works from a Discord server, thanks to the 1.1.1.1 bot. [Invite the bot to your Discord server](https://cfl.re/3nM6VfQ) to start using DNS over Discord. Or, [add the bot to your Discord account](https://dns-over-discord.v4.wtf/invite/user) to use it anywhere in Discord.

## Perform DNS lookups

Once the bot is in your server, type `/dig` to start performing DNS lookups. This will provide a native interface within Discord that allows you to specify the domain to lookup, an optional DNS record type and an optional flag for a short result.

If only a domain is given for the command, the bot will default to looking for `A` DNS records, and will return the full format result, not the short form.

Example:

```txt
/dig domain: cloudflare.com
```

### Supported record types

Discord has a limit of 25 options in slash commands, so DNS over Discord offers the 25 most common DNS record types to choose from.


<Details header="Supported DNS record types">

* `A`
* `AAAA`
* `CAA`
* `CDNSKEY`
* `CDS`
* `CERT`
* `CNAME`
* `DNSKEY`
* `DS`
* `HINFO`
* `HTTPS`
* `LOC`
* `MX`
* `NAPTR`
* `NS`
* `PTR`
* `SMIMEA`
* `SOA`
* `SPF`
* `SRV`
* `SSHFP`
* `SVCB`
* `TLSA`
* `TXT`
* `URI`


</Details>

To query other DNS record types, or multiple record types at once, use the `/multi-dig` command.

### Short form response

DNS over Discord has an optional flag in the `/dig` command that allows the user to request a response in the short form.

When you request a response in the short form, the name and TTL columns will be excluded. The command only returns the data column without formatting, similar to the equivalent `dig` command-line interface response.

Example:

```txt
/dig domain: cloudflare.com type: AAAA records short: True
```

### Disable DNSSEC checking

You can disable DNSSEC checking in the `dig` command by passing `cdflag` as true. This will return the DNS records even if the DNSSEC validation fails.

Example:

```txt
/dig domain: cloudflare.com type: AAAA records cdflag: True
```

### Refreshing existing results

You can refresh the DNS lookup results by clicking the Refresh button. Clicking it will trigger the bot to re-request the DNS query in the message, and update the results in the message. Any user can click this button.

The refresh button is available on all responses to the `/dig` command, including those that resulted in an error, such as an unknown domain or no records found.

### Changing DNS provider

By default, the DNS over Discord bot uses Cloudflare's 1.1.1.1 DNS service. You can run the DNS lookup with alternate DNS providers by selecting the dropdown below the result. This shows you a list of available providers. Selecting a new provider updates the results in the message. Any user can change the DNS provider.

## `multi-dig` command

If you want to look up multiple DNS record types at once, use the `/multi-dig` command. This allows you to specify any supported DNS record type, and multiple types separated by a space.

Example:

```txt
/multi-dig domain: cloudflare.com types: A AAAA
```

### Supported record types

When providing DNS record types for the `/multi-dig` command, Discord will not prompt you with options. You have to provide a space-separated list of valid DNS record types to lookup, as any invalid options will be silently dropped. `A` records will be used as the default if no valid types are given.


<Details header="DNS record types supported and considered valid by the bot">

Use a `*` (asterisk) in place of a record type to get DNS results for all supported types.

* `A`
* `AAAA`
* `AFSDB`
* `APL`
* `CAA`
* `CDNSKEY`
* `CDS`
* `CERT`
* `CNAME`
* `CSYNC`
* `DHCID`
* `DLV`
* `DNAME`
* `DNSKEY`
* `DS`
* `EUI48`
* `EUI64`
* `HINFO`
* `HIP`
* `HTTPS`
* `IPSECKEY`
* `KEY`
* `KX`
* `LOC`
* `MX`
* `NAPTR`
* `NS`
* `NSEC`
* `NSEC3`
* `NSEC3PARAM`
* `OPENPGPKEY`
* `PTR`
* `RP`
* `SMIMEA`
* `SOA`
* `SPF`
* `SRV`
* `SSHFP`
* `SVCB`
* `TA`
* `TKEY`
* `TLSA`
* `TXT`
* `URI`
* `ZONEMD`


</Details>

### Short form response

Like the main `/dig` command, the `/multi-dig` command also supports the optional short flag after the types have been specified in the slash command.

Example:

```txt
/multi-dig domain: cloudflare.com types: CDS CDNSKEY short: True
```

### Disable DNSSEC checking

As with the `dig` command, you can disable DNSSEC checking by passing `cdflag` as true. This will return the DNS records even if the DNSSEC validation fails.

Example:

```txt
/multi-dig domain: cloudflare.com type: AAAA records cdflag: True
```

### Refreshing existing results

The `/multi-dig` command also provides a refresh button below each set of DNS results requested (or after each block of 10 DNS record types, if you requested more than 10).

As with the `/dig` command, any user can press the refresh button to refresh the displayed DNS results, including for DNS queries that had previously failed.

### Changing DNS provider

Like the `/dig` command, you can change the DNS provider when using the `/multi-dig` command. The menu appears after each set of DNS results (or after each block of results if more than 10 record types are requested).

This menu can be used be any user to change the DNS provider used for the lookup.

## `whois` command

The `/whois` command allows you to perform a RDAP/WHOIS lookup right in Discord for a given domain, IP or ASN.

Examples:

```txt
/whois query: cloudflare.com
/whois query: 104.16.132.229
/whois query: 2606:4700::6810:84e5
/whois query: 13335
```

## Other commands

The bot also has a set of helper commands available to get more information about the bot and quick links.

### `help` command

The `/help` command provides in-Discord documentation about all the commands available in the 1.1.1.1 DNS over Discord bot.

Example:

```txt
/help
```

### `privacy` command

The `/privacy` command displays the Privacy Policy notice for using the 1.1.1.1 DNS over Discord bot. You can also [refer to the Privacy Policy page](https://dns-over-discord.v4.wtf/privacy) to access it.

Example:

```txt
/privacy
```

### `terms` command

The `/terms` command displays the Terms of Service notice for using the 1.1.1.1 DNS over Discord bot. You can also [refer to the Terms of Service page](https://dns-over-discord.v4.wtf/terms) to access it.

Example:

```txt
/terms
```

### `github` command

The DNS over Discord bot is open-source, and the `/github` command provides a quick link to access the GitHub repository. The GitHub repository can be accessed at [https://github.com/MattIPv4/DNS-over-Discord/](https://github.com/MattIPv4/DNS-over-Discord/).

Example:

```txt
/github
```

### `invite` command

The `/invite` command provides the user with a quick link to invite the 1.1.1.1 DNS over Discord bot to another Discord server, or to add it to a Discord account.
The bot can be invited at any time with [https://cfl.re/3nM6VfQ](https://cfl.re/3nM6VfQ).
The bot can also be added to accounts with [https://dns-over-discord.v4.wtf/invite/user](https://dns-over-discord.v4.wtf/invite/user).

```txt
/invite
```

***

## Development

The DNS over Discord bot is deployed on [Cloudflare Workers](https://workers.cloudflare.com/).

You can find the source code for the bot on GitHub, as well as information on getting started with contributing to the project, at [https://github.com/MattIPv4/DNS-over-Discord/](https://github.com/MattIPv4/DNS-over-Discord/).

---

# Other ways to use 1.1.1.1

URL: https://developers.cloudflare.com/1.1.1.1/other-ways-to-use-1.1.1.1/

import { DirectoryListing } from "~/components"

There are many other ways to use 1.1.1.1 beyond the traditional set up in operating systems and routers.

<DirectoryListing />

---

# DNS over Tor

URL: https://developers.cloudflare.com/1.1.1.1/other-ways-to-use-1.1.1.1/dns-over-tor/

:::caution

The hidden resolver is still an experimental service and should not be used in production or for other critical uses.

:::

If you do not want to disclose your IP address to the resolver, you can use our Tor onion service. Resolving DNS queries through the Tor network guarantees a significantly higher level of anonymity than making the requests directly. Not only does doing so prevent the resolver from ever seeing your IP address, but it also prevents your ISP from knowing that you attempted to resolve a domain name.

Read more about this service in [this blog post](https://blog.cloudflare.com/welcome-hidden-resolver/).

## Setting up a Tor client

The important difference between using all other modes of DNS and this one is that packet routing no longer uses IP addresses, and therefore all connections must be routed through a Tor client.

Before you start, head to the [Tor Project website](https://www.torproject.org/download/download.html.en) to download and install a Tor client. If you use the Tor Browser, it will automatically start a [SOCKS proxy](https://en.wikipedia.org/wiki/SOCKS) at `127.0.0.1:9150`.

If you use Tor from the command line, create the following configuration file:

```txt
SOCKSPort 9150
```

Then you can run tor with:

```sh
tor -f tor.conf
```

Also, if you use the Tor Browser, you can head to the resolver's address to see the usual 1.1.1.1 page:

```txt
https://dns4torpnlfs2ifuz2s2yf3fc7rdmsbhm6rw75euj35pac6ap25zgqad.onion/
```

:::note[Note]

The HTTPS certificate indicator should say "Cloudflare, Inc. (US)."

:::

If you ever forget 1.1.1.1's address, use cURL to retrieve it:

```sh
curl -sI https://tor.cloudflare-dns.com | grep -i alt-svc
```

```sh output
alt-svc: h2="dns4torpnlfs2ifuz2s2yf3fc7rdmsbhm6rw75euj35pac6ap25zgqad.onion:443"; ma=315360000; persist=1
```

## Setting up a local DNS proxy using socat

Of course, not all DNS clients support connecting to the Tor client, so the easiest way to connect any DNS-speaking software to the hidden resolver is by forwarding ports locally, for instance [using `socat`](http://www.dest-unreach.org/socat/).

### DNS over TCP, TLS, and HTTPS

The hidden resolver is set up to listen on TCP ports 53 and 853 for DNS over TCP and TLS. After setting up a Tor proxy, run the following `socat` command as a privileged user, replacing the port number appropriately:

```sh
PORT=853; socat TCP4-LISTEN:${PORT},reuseaddr,fork SOCKS4A:127.0.0.1:dns4torpnlfs2ifuz2s2yf3fc7rdmsbhm6rw75euj35pac6ap25zgqad.onion:${PORT},socksport=9150
```

From here, you can follow the regular guide for [setting up 1.1.1.1](/1.1.1.1/setup/), except you should always use `127.0.0.1` instead of `1.1.1.1`. If you need to access the proxy from another device, simply replace `127.0.0.1` in `socat` commands with your local IP address.

### DNS over HTTPS

[As explained in the blog post](https://blog.cloudflare.com/welcome-hidden-resolver/), our favorite way of using the hidden resolver is using DNS over HTTPS (DoH). To set it up:

1. Download `cloudflared` by following the guide for [connecting to 1.1.1.1 using DNS over HTTPS clients](/1.1.1.1/encryption/dns-over-https/dns-over-https-client/).

2. Start a Tor SOCKS proxy and use `socat` to forward port TCP:443 to localhost:

```sh
socat TCP4-LISTEN:443,reuseaddr,fork SOCKS4A:127.0.0.1:dns4torpnlfs2ifuz2s2yf3fc7rdmsbhm6rw75euj35pac6ap25zgqad.onion:443,socksport=9150
```

3. Instruct your machine to treat the `.onion` address as localhost:

```bash
cat << EOF >> /etc/hosts
127.0.0.1 dns4torpnlfs2ifuz2s2yf3fc7rdmsbhm6rw75euj35pac6ap25zgqad.onion
EOF
```

4. Finally, start a local DNS over UDP daemon:

```sh
cloudflared proxy-dns --upstream "https://dns4torpnlfs2ifuz2s2yf3fc7rdmsbhm6rw75euj35pac6ap25zgqad.onion/dns-query"
```

```sh output
INFO[0000] Adding DNS upstream                           url="https://dns4torpnlfs2ifuz2s2yf3fc7rdmsbhm6rw75euj35pac6ap25zgqad.onion/dns-query"
INFO[0000] Starting DNS over HTTPS proxy server          addr="dns://localhost:53"
INFO[0000] Starting metrics server                       addr="127.0.0.1:35659"
```

---

# Android

URL: https://developers.cloudflare.com/1.1.1.1/setup/android/

import { Details, Render } from "~/components"

[1.1.1.1: Faster Internet](https://play.google.com/store/apps/details?id=com.cloudflare.onedotonedotonedotone) is the preferred method of setting up 1.1.1.1 DNS resolver and 1.1.1.1 for Families. It allows you to automatically configure your phone to use 1.1.1.1 on any network you connect to.

The app also allows you to enable encryption for DNS queries or enable [WARP mode](/warp-client/), which keeps all your HTTP traffic private and secure, including your DNS queries to 1.1.1.1.

You can select between the options available in the app settings. By default, 1.1.1.1: Faster Internet is configured to WARP mode.
## Set up 1.1.1.1: Faster Internet

1. Download [1.1.1.1: Faster Internet from Google Play](https://play.google.com/store/apps/details?id=com.cloudflare.onedotonedotonedotone) for free.
2. Launch 1.1.1.1: Faster Internet and accept the Terms of Service.
3. Toggle the **WARP** button to **Connected**.
4. Install the VPN profile that allows your phone to connect securely to 1.1.1.1.

Your connection to the Internet and your DNS queries are now protected.

### Enable 1.1.1.1 for Families

1. Open 1.1.1.1: Faster Internet.
2. Tap the **menu button**.
3. Select **Advanced** > **Connection options**.
4. In **DNS settings** > **1.1.1.1 for Families**, select the option you want to use.

## Configure 1.1.1.1 manually

### Android 11 or later

Android 11 or later versions support both DNS over TLS (DoT) and DNS over HTTPS (DoH).

1. Go to **Settings** > **Network & internet**.
2. Select **Advanced** > **Private DNS**.
3. Select the **Private DNS provider hostname** option.
4. Depending on what you want to configure, use one of the following DNS hostnames or [IP addresses](/1.1.1.1/ip-addresses/) and select **Save**.


  <Details header="Use 1.1.1.1 resolver">

  * `one.one.one.one`

  Or the corresponding IP address if your device requires it:

    * **IPv4**: `1.1.1.1` or `1.0.0.1`
    * **IPv6**: `2606:4700:4700::1111` or `2606:4700:4700::1001`

  </Details>

  <Details header="Block malware with 1.1.1.1 for Families">

  * `security.cloudflare-dns.com`

  Or the corresponding IP address if your device requires it:

    * **IPv4**: `1.1.1.2` or `1.0.0.2`
    * **IPv6**: `2606:4700:4700::1112` or `2606:4700:4700::1002`

  </Details>

  <Details header="Block malware and adult content with 1.1.1.1 for Families">

  * `family.cloudflare-dns.com`

  Or the corresponding IP address if your device requires it:

    * **IPv4**: `1.1.1.3` or `1.0.0.3`
    * **IPv6**: `2606:4700:4700::1113` or `2606:4700:4700::1003`

  </Details>

### Android 9 or 10

Android 9 and Android 10 support DNS over TLS to secure your queries through encryption. In Android, this option is called Private DNS. It prevents your queries from being tracked, modified or surveilled by third-parties. Unlike previous versions of Android, this method also ensures 1.1.1.1 does not need to be configured for each new Wi-Fi network your smartphone joins.

1. Go to **Settings** > **Network & internet**.
2. Select **Advanced** > **Private DNS**.
3. Select the **Private DNS provider hostname** option.
4. Enter `one.one.one.one` and select **Save**. Or consider the following options if you want to use 1.1.1.1 for Families.


  <Details header="Block malware with 1.1.1.1 for Families">

  * `security.cloudflare-dns.com`

  Or the corresponding IP address if your device requires it:

    * **IPv4**: `1.1.1.2` or `1.0.0.2`
    * **IPv6**: `2606:4700:4700::1112` or `2606:4700:4700::1002`

  </Details>

  <Details header="Block malware and adult content with 1.1.1.1 for Families">

  * `family.cloudflare-dns.com`

  Or the corresponding IP address if your device requires it:

    * **IPv4**: `1.1.1.3` or `1.0.0.3`
    * **IPv6**: `2606:4700:4700::1113` or `2606:4700:4700::1003`

  </Details>

### Previous Android versions

Before making changes, take note of any DNS addresses you might have and save them in a safe place in case you need to use them later.

1. Open **Settings** > **Wi-Fi**.
2. Press down and hold the name of the network you are currently connected to.
3. Select **Modify Network**.
4. Select the checkbox **Show Advanced Options**.
5. Change the IP Settings to **Static**.
6. <Render file="all-ipv4" />
7. <Render file="all-ipv6" />
8. Select **Save**. You may need to disconnect from the Wi-Fi and reconnect for the changes to take place.

<Render file="captive-portals" />

---

# Azure

URL: https://developers.cloudflare.com/1.1.1.1/setup/azure/

import { Render } from "~/components"

1. Log in to your Azure portal.
2. From the Azure portal side menu, select **Virtual Networks**.
3. Navigate to the virtual network associated with your virtual machine (VM).
4. Select **DNS Servers** > **Custom**, and add two entries: <Render file="ipv4" />
5. Select **Save**.

---

# Gaming consoles

URL: https://developers.cloudflare.com/1.1.1.1/setup/gaming-consoles/

import { Render } from "~/components"

## PS4

1. Go to **Settings** > **Network** > **Set Up Internet Connection**.
2. Select **Wi-Fi** or **LAN** depending on your Internet connection.
3. Select **Custom**.
4. Set **IP Address Settings** to **Automatic**.
5. Change **DHCP Host Name** to **Do Not Specify**.
6. Set **DNS Settings** to **Manual**.
7. Change **Primary DNS** and **Secondary DNS** to: <Render file="ipv4" />
8. If you are able to add more DNS servers, you can add the IPv6 addresses as well: <Render file="ipv6" />
9. Set **MTU Settings** to **Automatic**.
10. Set **Proxy Server** to **Do Not Use**.

## Xbox One

1. Open the Network screen by pressing the Xbox button on your controller.
2. Go to **Settings** > **Network** > **Network Settings**.
3. Next, go to **Advanced Settings** > **DNS Settings**.
4. Select **Manual**.
5. Set **Primary DNS** and **Secondary DNS** to: <Render file="ipv4" />
6. If you have the option to add more DNS servers, you can add the IPv6 addresses as well: <Render file="ipv6" />
7. When you are done, you will be shown a confirmation screen. Press **B** to save.

## Nintendo

The following instructions work on New Nintendo 3DS, New Nintendo 3DS XL, New Nintendo 2DS XL, Nintendo 3DS, Nintendo 3DS XL, and Nintendo 2DS.

1. Go to the home menu and choose **System Settings** (the wrench icon).
2. Select **Internet Settings** > **Connection Settings**.
3. Select your Internet connection and then select **Change Settings**.
4. Select **Change DNS**.
5. Set **Auto-Obtain DNS** to **No**.
6. Select **Detailed Setup**.
7. Set **Primary DNS** and **Secondary DNS** to: <Render file="ipv4" />
8. If you are able to add more DNS servers, you can add the IPv6 addresses as well: <Render file="ipv6" />
9. Select **Save** > **OK**.

## Nintendo Switch

1. Press the home button and select **System Settings**.
2. Scroll down and select **Internet** > **Internet Settings**.
3. Select your Internet connection and then select **Change Settings**.
4. Select **DNS Settings** > **Manual**.
5. Set **Primary DNS** and **Secondary DNS** to: <Render file="ipv4" />
6. Select **Save** > **OK**.

---

# Google Cloud

URL: https://developers.cloudflare.com/1.1.1.1/setup/google-cloud/

import { Render } from "~/components"

Google Cloud supports configuring [outbound server policy](https://cloud.google.com/dns/docs/server-policies-overview#dns-server-policy-out) within Cloud DNS. Policies are applied per Virtual Private Cloud (VPC) network, and will affect all resources within that VPC network, including any existing virtual machines.

:::note


If you are using [Cloudflare Zero Trust](/cloudflare-one/), you can choose assigned [locations](/cloudflare-one/connections/connect-devices/agentless/dns/locations/) to apply custom [DNS policies](/cloudflare-one/policies/gateway/dns-policies/) via Gateway.


:::

To configure 1.1.1.1 for your Google Cloud VPC network(s):

1. Open the [Google Cloud Console](https://console.cloud.google.com).
2. Navigate to **Network Services** > **Cloud DNS** and select [**DNS Server Policies**](https://console.cloud.google.com/net-services/dns/policies).
3. Select **Create Policy**.
4. Provide a name for your Policy (such as `cloudflare-1-1-1-1`) and select associated VPC network or networks.
5. Under **Alternate DNS servers**, select **Add Item** and type: <Render file="ipv4" />
6. Select **Create**.

DNS requests within the configured VPC network(s) will now use 1.1.1.1.

---

# Set up

URL: https://developers.cloudflare.com/1.1.1.1/setup/

import { Details, DirectoryListing, Render } from "~/components"

By default, the [DNS server](https://www.cloudflare.com/learning/dns/what-is-dns/) your devices use is provided by your Internet service provider (ISP). Some [ISPs and network equipment providers](/1.1.1.1/infrastructure/network-operators/) partner with Cloudflare to add safer browsing to their offerings.

If your providers are not currently using Cloudflare, you can change the DNS settings on your device or router as detailed in the following instructions.


<Details header="Device or router specific guides">

<DirectoryListing />


</Details>

You can also set up [1.1.1.1 for Families](#1111-for-families) for an added layer of protection on your home network against malware and adult content. 1.1.1.1 for Families leverages Cloudflare's global network to ensure that it is fast and secure around the world, and includes the same [strong privacy guarantees](/1.1.1.1/privacy/public-dns-resolver/) that Cloudflare committed to when launching 1.1.1.1.

***

## 1.1.1.1 for Families

1.1.1.1 for Families categorizes destinations on the Internet based on the potential threat they pose regarding malware, phishing, or other types of security risks.

1.1.1.1 for Families has two default options:


<Details header="Block malware">

Use the following DNS resolvers to block malicious content:

* `1.1.1.2`
* `1.0.0.2`
* `2606:4700:4700::1112`
* `2606:4700:4700::1002`


</Details>


<Details header="Block malware and adult content">

Use the following DNS resolvers to block malware and adult content:

* `1.1.1.3`
* `1.0.0.3`
* `2606:4700:4700::1113`
* `2606:4700:4700::1003`


</Details>

Cloudflare returns `0.0.0.0` if the [fully qualified domain name (FQDN)](https://en.wikipedia.org/wiki/Fully_qualified_domain_name) or IP in a DNS query is classified as malicious.

:::note[Domain miscategorization]


If you are using 1.1.1.1 for Families and see a domain that you believe is miscategorized, [fill in this form](https://radar.cloudflare.com/categorization-feedback/) to bring it to our attention. Your submission will remain anonymous.

We review these submissions to improve Cloudflareâ€™s categorization.


:::

### Test 1.1.1.1 for Families

After configuring 1.1.1.1 for Families, you can test if it is working as intended with the following URLs:

* [https://malware.testcategory.com/](https://malware.testcategory.com/): Use this to test if 1.1.1.1 for Families is blocking known malware addresses correctly.
* [https://nudity.testcategory.com/](https://nudity.testcategory.com/): Use this to test if 1.1.1.1 for Families is blocking known adult content and malware addresses correctly.

### DNS over HTTPS (DoH)

If you have a DoH-compliant client, such as a compatible router, you can set up 1.1.1.1 for Families to encrypt your DNS queries over HTTPS. This prevents spoofing and tracking by malicious actors, advertisers, ISPs, and others. For more information on DoH, refer to the [Learning Center article on DNS encryption](https://www.cloudflare.com/learning/dns/dns-over-tls/).

To configure an encrypted DoH connection to 1.1.1.1 for Families, type one of the following URLs into the appropriate field of your DoH-compliant client:


<Details header="Block malware">

```txt
https://security.cloudflare-dns.com/dns-query
```


</Details>


<Details header="Block malware and adult content">

```txt
https://family.cloudflare-dns.com/dns-query
```


</Details>

### DNS over TLS (DoT)

1.1.1.1 for Families also supports DoT if you have a compliant client, such as a compatible DoT router. DoT allows you to encrypt your DNS queries, protecting you from spoofing, malicious actors, and others. You can learn more about DoT in the [Learning Center article on DNS encryption](https://www.cloudflare.com/learning/dns/dns-over-tls/).

To configure an encrypted DoT connection to 1.1.1.1 for Families, type one of the following URLs into the appropriate field of your DoT-compliant client:


<Details header="Block malware">

```txt
security.cloudflare-dns.com
```


</Details>


<Details header="Block malware and adult content">

```txt
family.cloudflare-dns.com
```


</Details>

---

# iOS

URL: https://developers.cloudflare.com/1.1.1.1/setup/ios/

import { Render } from "~/components"

[1.1.1.1: Faster Internet](https://apps.apple.com/us/app/1-1-1-1-faster-internet/id1423538627) is the preferred method of setting up 1.1.1.1 DNS resolver and 1.1.1.1 for Families in iOS devices. It allows you to automatically configure your phone to use 1.1.1.1 on any network you connect to, and solves iOS inability of using an alternative DNS resolver in cellular connections.

The app also allows you to enable encryption for DNS queries or enable [WARP mode](/warp-client/), which keeps all your HTTP traffic private and secure, including your DNS queries to 1.1.1.1.

You can select between the options available in the app's settings. By default, 1.1.1.1: Faster Internet is configured to WARP mode.

## Set up 1.1.1.1: Faster Internet

1. Download [1.1.1.1: Faster Internet from the App Store](https://apps.apple.com/us/app/1-1-1-1-faster-internet/id1423538627) for free.
2. Launch 1.1.1.1: Faster Internet and accept the Terms of Service.
3. Install the VPN profile that allows your phone to connect securely to 1.1.1.1.
4. Toggle the **WARP** button to **Connected**.

### Enable 1.1.1.1 for Families

1. Open 1.1.1.1: Faster Internet.
2. Tap the **menu button**.
3. Select **Advanced** > **Connection options**.
4. In **DNS settings** > **1.1.1.1 for Families**, select the option you want to use.

## Configure 1.1.1.1 manually

:::note


If you configure 1.1.1.1 manually, you will have to do it for every Wi-Fi network your device connects to. This method does not work for cellular connections.


:::

Take note of any DNS addresses you might have set up, and save them in a safe place in case you need to use them later.

1. Go to **Settings** > **Wi-Fi**.
2. Select the **'i'** icon next to the Wi-Fi network you are connected to.
3. Scroll down and select **Configure DNS**.
4. Change the configuration from **Automatic** to **Manual**.
5. Select **Add Server**.
6. <Render file="all-ipv4" />
7. <Render file="all-ipv6" />
8. Select **Save**.

<Render file="captive-portals" />

---

# Linux

URL: https://developers.cloudflare.com/1.1.1.1/setup/linux/

import { Render } from "~/components";

Before you begin, take note of any DNS addresses you might have set up, and save them in a safe place in case you need to use them later.

Consider the sections below to set up 1.1.1.1 using either the [command line interface (CLI)](#use-command-line-interface-cli) or a [graphical user interface (GUI)](#use-graphical-user-interface-gui) of your preference.

## Use command line interface (CLI)

Choose whether you want to use 1.1.1.1 or 1.1.1.1 For Families, and replace `1.1.1.1` with the corresponding [IPv4 or IPv6 address](/1.1.1.1/ip-addresses/) accordingly.

### `resolv.conf`

Usually, `/etc/resolv.conf` is where you can configure the resolver IPs that your system is using.

In that case, you can use the following one-line command to specify `1.1.1.1` as your DNS resolver and `1.0.0.1` as backup:

```sh
echo -e "nameserver 1.1.1.1\nnameserver 1.0.0.1" | sudo tee /etc/resolv.conf
```

:::caution

Note that other systems, such as dynamic host configuration protocol (DHCP), may automatically write to `/etc/resolv.conf` and change that configuration. In those cases, consider changing your network settings or DHCP to use `1.1.1.1`.
:::

Alternatively, you can use an editor (`nano` or `vim`, for example) to manually edit the file.

### `systemd-resolved`

If you use `systemd-resolved` utility and the resolver IPs configuration is in `/etc/systemd/resolved.conf`, consider the steps below:

1. Run the following command, replacing `<EDITOR>` with your preferred editor.

```sh
sudo <EDITOR> /etc/systemd/resolved.conf
```

2. In the editor, add or edit the following lines:

```txt
[Resolve]
DNS=1.1.1.1
```
To use DNS over TLS, add `#one.one.one.one` and set `DNSOverTLS` to `yes`, as in the following example:

```txt
[Resolve]
DNS=1.1.1.1#one.one.one.one
DNSOverTLS=yes
```

## Use graphical user interface (GUI)

### GNOME

1. Go to **Show Applications** > **Settings** > **Network**.
2. Select the adapter you want to configure â€” like your Ethernet adapter or Wi-Fi card â€” and select the **Settings** button.
3. On the **IPv4** tab > **DNS** section, disable the **Automatic** toggle.
4. <Render file="all-ipv4" />
5. Go to **IPv6**.
6. <Render file="all-ipv6" />
7. Select **Apply**.

### KDE Plasma

1. Go to **System Settings** > **Wi-Fi & Internet** > **Wi-Fi & Networking**. (or **Connections**, if on Plasma 5)
2. Select the connection you want to configure - like your current connected network.
3. On the **IPv4** tab, select the **Method** drop-down menu > _Automatic (Only addresses)_.
4. Select the text box next to **DNS servers**.
5. <Render file="all-ipv4" />
6. On the **IPv6** tab, select the **Method** drop-down menu > _Automatic (Only addresses)_.
7. Select the text box next to **DNS servers**.
8. <Render file="all-ipv6" />
9. Select **Apply**.

<Render file="captive-portals" />

---

# Router

URL: https://developers.cloudflare.com/1.1.1.1/setup/router/

import { Render } from "~/components"

1. Go to the **IP address** used to access your router's admin console in your browser.
   * Linksys and Asus routers typically use `http://192.168.1.1` or `http://router.asus.com` (for ASUS).
   * Netgear routers typically use `http://192.168.1.1` or `http://routerlogin.net`.
   * D-Link routers typically use `http://192.168.0.1`.
   * Ubiquiti routers typically use `http://unifi.ubnt.com`.

2. Enter the router credentials. For consumer routers, the default credentials for the admin console are often found under or behind the device.

3. In the admin console, find the place where **DNS settings** are set. This may be contained within categories such as **WAN** and **IPv6** (Asus Routers) or **Internet** (Netgear Routers). Consult your router's documentation for details.

4. Take note of any DNS addresses that are currently set and save them in a safe place in case you need to use them later.

5. <Render file="all-ipv4" />

6. <Render file="all-ipv6" />

7. Save the updated settings.

## Using DNS-Over-TLS on OpenWrt

It is possible to encrypt DNS traffic out from your router using DNS-over-TLS if it is running OpenWrt. For more details, see our blog post on the topic: [Adding DNS-Over-TLS support to OpenWrt (LEDE) with Unbound](https://blog.cloudflare.com/dns-over-tls-for-openwrt/).

## FRITZ!Box

Starting with [FRITZ!OS 7.20](https://en.avm.de/press/press-releases/2020/07/fritzos-720-more-performance-convenience-security/), DNS over TLS is supported, see [Configuring different DNS servers in the FRITZ!Box](https://en.avm.de/service/knowledge-base/dok/FRITZ-Box-7590/165_Configuring-different-DNS-servers-in-the-FRITZ-Box/).

---

# Windows

URL: https://developers.cloudflare.com/1.1.1.1/setup/windows/

import { Render } from "~/components"

## Windows 10

Take note of any DNS addresses you might have set up, and save them in a safe place in case you need to use them later.

1. Select the **Start menu** > **Settings**.
2. On **Network and Internet**, select **Change Adapter Options**.
3. Right-click on the Ethernet or Wi-Fi network you are connected to and select **Properties**.
4. Choose **Internet Protocol Version 4**.
5. Select **Properties** > **Use the following DNS server addresses**.
6. <Render file="all-ipv4" />
7. Select **OK**.
8. Go to **Internet Protocol Version 6**.
9. Select **Properties** > **Use the following DNS server addresses**.
10. <Render file="all-ipv6" />
11. Select **OK**.

## Windows 11

Take note of any DNS addresses you might have set up, and save them in a safe place in case you need to use them later.

1. Select the **Start menu** > **Settings**.
2. On **Network and Internet**, choose the adapter you want to configure - like your Ethernet adapter or Wi-Fi card.
3. Scroll to **DNS server assignment** and select **Edit**.
4. Select the **Automatic (DHCP)** drop-down menu > **Manual**.
5. Select the **IPv4** toggle to turn it on.
6. <Render file="all-ipv4" />
7. Select the **IPv6** toggle.
8. <Render file="all-ipv6" />
9. Select **Save**.

<Render file="captive-portals" />

<Render file="encrypted" />

---

# macOS

URL: https://developers.cloudflare.com/1.1.1.1/setup/macos/

import { Render } from "~/components"

Take note of any DNS addresses you might have set up, and save them in a safe place in case you need to use them later.

1. Go to **System Settings**. You can find it by pressing `CMD + Space` on your keyboard and typing `System Settings`.
2. Go to **Network**.
3. Select a network service.
4. Select **Details**.
5. Go to **DNS**.
6. Under **DNS Servers**, select **Add**.
7. <Render file="all-ipv4" />
8. <Render file="all-ipv6" />
9. Select **OK**.

<Render file="captive-portals" />

<Render file="encrypted" />

---

# Access and CNI

URL: https://developers.cloudflare.com/aegis/configuration-options/access-cni/

You can use Aegis combined with [Cloudflare Network Interconnect (CNI)](/network-interconnect/) to secure your applications with [Cloudflare Access](/cloudflare-one/policies/access/) without installing software or customizing code on your server.

While Access allows you to enforce policies at the hostname level, other solutions are usually necessary to protect against origin IP bypass â€” when an attacker knows your origin server IP and uses it to directly interact with the target application.

With Aegis IPs, you only allow a small number of IPs (that are not publicly listed) through your [network firewall](/aegis/configuration-options/network-firewall/). And with Cloudflare Network Interconnect, you can use a completely private path between Cloudflare and your application server, without exposure to the public Internet.

Aegis IPs are included within [BGP advertisement over CNI](/network-interconnect/classic-cni/set-up/configure-bgp-bfd/).

For details and background, refer to the [Cloudflare blog](https://blog.cloudflare.com/access-aegis-cni).

---

# Data Localization Suite

URL: https://developers.cloudflare.com/aegis/configuration-options/data-localization/

[Data Localization Suite (DLS)](/data-localization/) is an enterprise add-on that enables you to choose the location where Cloudflare encrypts, decrypts, and stores data.

To ensure egress will happen from DLS-specified locations, make sure you have Aegis IPs provisioned in those locations. Refer to [IPs allocation](/aegis/about/ips-allocation/) for details.

---

# Configuration options

URL: https://developers.cloudflare.com/aegis/configuration-options/

import { DirectoryListing } from "~/components"

Use Aegis in combination with different Cloudflare products.

<DirectoryListing />

---

# Load Balancing

URL: https://developers.cloudflare.com/aegis/configuration-options/load-balancing/

[Cloudflare Load Balancing](/load-balancing/) allows you to intelligently distribute traffic across your origins by issuing regular monitors (that assess origin health) and following the traffic steering policies you define.

By default, the Load Balancing monitors will use public Cloudflare IP addresses.

To avoid inconsistencies between what the Load Balancing monitors report and what you observe in service traffic with Aegis, make sure to turn on the **Simulate Zone** option in the [monitor settings](/load-balancing/monitors/create-monitor/#create-a-monitor).

---

# Network firewall

URL: https://developers.cloudflare.com/aegis/configuration-options/network-firewall/

One of the main benefits of using Cloudflare Aegis is being able to update your network firewall rules to be more restrictive. Instead of allowing all [Cloudflare IP ranges](https://www.cloudflare.com/ips/), once you onboard to Aegis, you can update your firewall rules to only allow Aegis IPs.

This means moving from allowlisting millions of IPs that are publicly listed to only a few IPs that are much harder for attackers to discover, and not possible for them to use as the Source IP for requests.

---

# Spectrum

URL: https://developers.cloudflare.com/aegis/configuration-options/spectrum/

[Spectrum](/spectrum/) allows you to route email, file transfer, games, and more over TCP or UDP through Cloudflare. This means you can mask your origin and protect it from DDoS attacks.

While you can use [BYOIP](/byoip/) or static IPs to control which IPs are used for ingress with Spectrum, Aegis allows you to have a more strict list of [egress IPs](/aegis/about/) as well.

```mermaid
flowchart LR
        accTitle: Cloudflare as a reverse proxy
        accDescr: Diagram showing Cloudflare's network between clients and the origin server.
        A[Client] --ingress--> B((Cloudflare))--egress--> C[(Origin server)]
```

Aegis with Spectrum supports both TCP and UDP application types. HTTP/HTTPS types are also supported, although through a different configuration.

If you are interested in any of these solutions, contact your account team.

---

# Workers

URL: https://developers.cloudflare.com/aegis/configuration-options/workers/

[Workers](/workers/) provides a serverless execution environment for you to create applications leveraging Cloudflare's global network.

When paired with Aegis, whenever Cloudflare's developer platform needs to access services on your origin, it will use the dedicated IP addresses.

:::note
For Workers subrequests â€” meaning requests from one Worker to another â€” it is expected that different IPs are used. However, requests to external origins made by a Worker invoked via a subrequest will use the dedicated IP addresses.
:::

---

# How Aegis works

URL: https://developers.cloudflare.com/aegis/about/

When you use Cloudflare [as a reverse proxy](/fundamentals/concepts/how-cloudflare-works/#how-cloudflare-works-as-a-reverse-proxy), [Cloudflare's global network](https://www.cloudflare.com/network/) sits between client requests and your origin servers.

```mermaid
flowchart LR
        accTitle: Cloudflare as a reverse proxy
        accDescr: Diagram showing Cloudflare's network between clients and the origin server.
        A[Client] <--> B((Cloudflare))<--> C[(Origin server)]
```

Zooming in to what happens as a request routes through Cloudflare, you can consider two parts of the process: ingress and egress.

```mermaid
flowchart LR
        accTitle: Cloudflare as a reverse proxy
        accDescr: Diagram showing Cloudflare's network between clients and the origin server.
        A[Client] --ingress--> B((Cloudflare))--egress--> C[(Origin server)]
```

Ingress refers to the data center where the client request lands on, based on Internet routing. From there on, the request will be processed according to your Cloudflare configurations and, if needed, a connection to the origin will be initiated via an egress data center.

Traditionally, Cloudflare maintains a very large pool of egress IPs that are used by all Cloudflare customers and are [publicly documented](https://www.cloudflare.com/ips/). With Aegis, Cloudflare provides dedicated egress IP addresses that are reserved for you.

:::note
Each dedicated egress pool can consist of either IPs from a [BYOIP prefix](/byoip/) or Cloudflare-leased IPs. A single dedicated egress pool cannot contain both BYOIPs and leased IPs.
:::

## Benefits

With dedicated egress IPs, you can:

* Lock down your [network firewall](/aegis/configuration-options/network-firewall/) to only allow traffic from the Aegis IPs.
* Use [Cloudflare Access](/aegis/configuration-options/access-cni/) to secure your applications without installing software or customizing code on your server.
* Ensure only authorized [Workers](/aegis/configuration-options/workers/) can access your origin services.

Refer to the [introductory blog post](https://blog.cloudflare.com/cloudflare-aegis/) for details and example use cases.

## Scope

You can assign Aegis IPs to single or multiple Cloudflare zones, and across different Cloudflare accounts.

Aegis IPs are included within [BGP advertisement over CNI](/network-interconnect/classic-cni/set-up/configure-bgp-bfd/).

---

# Connection forwarding

URL: https://developers.cloudflare.com/aegis/about/connection-forwarding/

Since IPv6 address ranges are deployed globally, no forwarding is needed.

For IPv4 traffic, based on [IPs allocation](/aegis/about/ips-allocation/), not all egress data centers will have access to an applicable Aegis IP.

Aegis does not forward to another location in response to traffic spikes. Instead, each IPv4 can be split across up to four locations, where some of these locations may have multiple data centers. IP capacity in each data center can also be adjusted in accordance with the amount of traffic that reaches each location.

After a request reaches Cloudflare on an ingress data center, and the cache service sends a request for the egress router to connect to your origin, the following scenarios are possible.

## Traffic can egress from the same server

If the server running the egress router has access to an applicable Aegis IP, traffic egresses from that server.

```mermaid
flowchart LR
        accTitle: Cloudflare Aegis and connection forwarding
        accDescr: Diagram showing IPv4 connection forwarding for Cloudflare Aegis - Same data center.
        A[Client]
        subgraph Data center A
        X[(Cache service)] --> B[(Egress router <br/> <small>*has Aegis IP</small>)]
        end
        C[(Origin server)]

        A --ingress--> X
        B --egress--> C
```

## Connection forwarding is needed

If the server does not have access to an applicable Aegis IP, the following options are checked and the first that is possible will take place:

* Another server in the same data center has access to an applicable Aegis IP and the connection is forwarded to that server.

```mermaid
flowchart LR
        accTitle: Cloudflare Aegis and connection forwarding
        accDescr: Diagram showing IPv4 connection forwarding for Cloudflare Aegis - Same data center.
        A[Client]
        subgraph Data center A
        X[(Cache service)] --> B[(Egress router <br/> <small>*no Aegis IP</small>)]
        B --> Y[(Egress server <br/> <small>*has Aegis IP</small>)]
        end
        C[(Origin server)]

        A --ingress--> X
        Y --egress--> C
```

* Another data center in the same location has access to an applicable Aegis IP and the connection is forwarded to that data center.

```mermaid
flowchart LR
        accTitle: Cloudflare Aegis and connection forwarding
        accDescr: Diagram showing IPv4 connection forwarding for Cloudflare Aegis - Different data center.
        A[Client]
        subgraph Location 1
        subgraph Data center A
        X[(Cache service)] --> B[(Egress router <br/> <small>*no Aegis IP</small>)]
        end
        subgraph Data center B
        B --> Y[(Egress server <br/> <small>*has Aegis IP</small>)]
        end
        end
        C[(Origin server)]


        A --ingress--> X
        Y --egress--> C
```

* Another data center in a different location has access to an applicable Aegis IP. The closest location is selected and connection is forwarded to that location.

```mermaid
flowchart LR
        accTitle: Cloudflare Aegis and connection forwarding
        accDescr: Diagram showing IPv4 connection forwarding for Cloudflare Aegis - Different location.
        A[Client]
        subgraph Location 1
          subgraph Data center A
          X[(Cache service)] --> B[(Egress router <br/> <small>*no Aegis IP</small>)]
          end
        end
        subgraph Location 2
          subgraph Data center C
            B --> Y[(Egress server <br/> <small>*has Aegis IP</small>)]
          end
        end
        C[(Origin server)]


        A --ingress--> X
        Y --egress--> C
```

---

# IPs allocation

URL: https://developers.cloudflare.com/aegis/about/ips-allocation/

Cloudflare Aegis supports both IPv4 and IPv6 addresses.

IPv6 address ranges are deployed globally, meaning your Aegis IPv6 addresses can be used for connections from Cloudflare to your origin servers across all Cloudflare data centers.

:::note[China exception]
Aegis is currently **not** available in the [Cloudflare China Network](/china-network/), operated by Cloudflare's partner JD Cloud.
:::

For IPv4 addresses, you should work with your account team to choose the locations where each IP should be deployed. Ideally, your Aegis IPv4 addresses should be placed near your origin servers and adjusted to the amount of traffic expected for each region.

Refer to [connection forwarding](/aegis/about/connection-forwarding/) to understand how requests are processed when reaching different Cloudflare data centers.

## Connections to your origin

Each Aegis IP can support 40,000 concurrent connections per origin IP. For example, if you have one Aegis IP and two origins (A and B), this single Aegis IP can support 40,000 concurrent connections to origin A, while simultaneously supporting 40,000 concurrent connections to origin B.

## Regional services

If you are using [Regional Services](/data-localization/regional-services/), you should take this into consideration when allocating Aegis IPv4. Traffic will egress from the specified locations as long as you have Aegis IPs provisioned in those locations.

---

# API Reference

URL: https://developers.cloudflare.com/agents/api-reference/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Configuration

URL: https://developers.cloudflare.com/agents/api-reference/configuration/

import { MetaInfo, Render, Type, WranglerConfig } from "~/components";

An Agent is configured like any other Cloudflare Workers project, and uses [a wrangler configuration](/workers/wrangler/configuration/) file to define where your code is and what services (bindings) it will use.

The typical file structure for an Agent project created from `npm create cloudflare@latest agents-starter -- --template cloudflare/agents-starter` follows:

```sh
.
|-- package-lock.json
|-- package.json
|-- public
|   `-- index.html
|-- src
|   `-- index.ts // your Agent definition
|-- test
|   |-- index.spec.ts // your tests
|   `-- tsconfig.json
|-- tsconfig.json
|-- vitest.config.mts
|-- worker-configuration.d.ts
`-- wrangler.jsonc // your Workers & Agent configuration
```

Below is a minimal `wrangler.jsonc` file that defines the configuration for an Agent, including the entry point, `durable_object` namespace, and code `migrations`:

<WranglerConfig>

```jsonc
{
	"$schema": "node_modules/wrangler/config-schema.json",
	"name": "agents-example",
	"main": "src/index.ts",
	"compatibility_date": "2025-02-23",
	"compatibility_flags": ["nodejs_compat"],
	"durable_objects": {
		"bindings": [
			{
				// Required:
				"name": "MyAgent", // How your Agent is called from your Worker
				"class_name": "MyAgent", // Must match the class name of the Agent in your code
				// Optional: set this if the Agent is defined in another Worker script
				"script_name": "the-other-worker"
			},
		],
	},
	"migrations": [
		{
			"tag": "v1",
			// Mandatory for the Agent to store state
			"new_sqlite_classes": ["MyAgent"],
		},
	],
	"observability": {
		"enabled": true,
	},
}
```

</WranglerConfig>

The configuration includes:

- A `main` field that points to the entry point of your Agent, which is typically a TypeScript (or JavaScript) file.
- A `durable_objects` field that defines the [Durable Object namespace](/durable-objects/reference/glossary/) that your Agents will run within.
- A `migrations` field that defines the code migrations that your Agent will use. This field is mandatory and must contain at least one migration. The `new_sqlite_classes` field is mandatory for the Agent to store state.

---

# Agents SDK

URL: https://developers.cloudflare.com/agents/api-reference/sdk/

import { MetaInfo, Render, Type, TypeScriptExample, WranglerConfig } from "~/components";

At its most basic, an Agent is a JavaScript class that extends the `Agent` class from the `agents-sdk` package. An Agent encapsulates all of the logic for an Agent, including how clients can connect to it, how it stores state, the methods it exposes, and any error handling.

<TypeScriptExample>

```ts
import { Agent } from "agents-sdk";

class MyAgent extends Agent {
	// Define methods on the Agent
}

export default MyAgent;
```

</TypeScriptExample>

An Agent can have many (millions of) instances: each instance is a separate micro-server that runs independently of the others. This allows Agents to scale horizontally: an Agent can be associated with a single user, or many thousands of users, depending on the agent you're building.

Instances of an Agent are addressed by a unique identifier: that identifier (ID) can be the user ID, an email address, GitHub username, a flight ticket number, an invoice ID, or any other identifier that helps to uniquely identify the instance and for whom it is acting on behalf of.

## The Agent class

Writing an Agent requires you to define a class that extends the `Agent` class from the `agents-sdk` package. An Agent encapsulates all of the logic for an Agent, including how clients can connect to it, how it stores state, the methods it exposes, and any error handling.

An Agent has the following class methods:

<TypeScriptExample>

```ts
import { Agent } from "agents-sdk";

interface Env {
	// Define environment variables & bindings here
}

// Pass the Env as a TypeScript type argument
// Any services connected to your Agent or Worker as Bindings
// are then available on this.env.<BINDING_NAME>
class MyAgent extends Agent<Env> {
	// Called when an Agent is started (or woken up)
	async onStart() {
		// Can access this.env and this.state
		console.log('Agent started');
	}

	// Called when a HTTP request is received
	// Can be connected to routeAgentRequest to automatically route
	// requests to an individual Agent.
	async onRequest(request: Request) {
			console.log("Received request!");
	}

	// Called when a WebSocket connection is established
	async onConnect(connection: Connection, ctx: ConnectionContext) {
		console.log("Connected!");
		// Check the request at ctx.request
		// Authenticate the client
		// Give them the OK.
		connection.accept();
	}

	// Called for each message received on the WebSocket connection
	async onMessage(connection: Connection, message: WSMessage) {
		console.log(`message from client ID: ${connection.id}`)
		// Send messages back to the client
		connection.send("Hello!");
	}

	// WebSocket error and disconnection (close) handling.
	async onError(connection: Connection, error: unknown): Promise<void> {
		console.error(`WS error: ${error}`);
	}

	async onClose(connection: Connection, code: number, reason: string, wasClean: boolean): Promise<void> {
		console.log(`WS closed: ${code} - ${reason} - wasClean: ${wasClean}`);
		connection.close();
	}

	// Called when the Agent's state is updated
	// via this.setState or the useAgent hook from the agents-sdk/react package.
	async onStateUpdate(state: any) {
		// 'state' will be typed if you supply a type parameter to the Agent class.
	}
}

export default MyAgent;
```

</TypeScriptExample>

:::note

To learn more about how to manage state within an Agent, refer to the documentation on [managing and syncing state](/agents/examples/manage-and-sync-state/).

:::

You can also define your own methods on an Agent: it's technically valid to publish an Agent only has your own methods exposed, and create/get Agents directly from a Worker.

Your own methods can access the Agent's environment variables and bindings on `this.env`, state on `this.setState`, and call other methods on the Agent via `this.yourMethodName`.

## Calling Agents from Workers

You can create and run an instance of an Agent directly from a Worker in one of three ways:

1. Using the `routeAgentRequest` helper: this will automatically map requests to an individual Agent based on the `/agents/:agent/:name` URL pattern. The value of `:agent` will be the name of your Agent class converted to `kebab-case`, and the value of `:name` will be the name of the Agent instance you want to create or retrieve.
2. Calling `getAgentByName`, which will create a new Agent instance if none exists by that name, or retrieve a handle to an existing instance.
3. The [Durable Objects stub API](/durable-objects/api/id/), which provides a lower level API for creating and retrieving Agents.

These three patterns are shown below: we recommend using either `routeAgentRequest` or `getAgentByName`, which help avoid some boilerplate.

<TypeScriptExample>

```ts
import { Agent, AgentNamespace, getAgentByName, routeAgentRequest } from 'agents-sdk';

interface Env {
	// Define your Agent on the environment here
	// Passing your Agent class as a TypeScript type parameter allows you to call
	// methods defined on your Agent.
	MyAgent: AgentNamespace<MyAgent>;
}

export default {
	async fetch(request, env, ctx): Promise<Response> {
		// Routed addressing
		// Automatically routes HTTP requests and/or WebSocket connections to /agents/:agent/:name
		// Best for: connecting React apps directly to Agents using useAgent from agents-sdk/react
		(await routeAgentRequest(request, env)) || Response.json({ msg: 'no agent here' }, { status: 404 });

		// Named addressing
		// Best for: convenience method for creating or retrieving an agent by name/ID.
		let namedAgent = getAgentByName<Env, MyAgent>(env.MyAgent, 'my-unique-agent-id');
		// Pass the incoming request straight to your Agent
		let namedResp = (await namedAgent).fetch(request);

		// Durable Objects-style addressing
		// Best for: controlling ID generation, associating IDs with your existing systems,
		// and customizing when/how an Agent is created or invoked
		const id = env.MyAgent.newUniqueId();
		const agent = env.MyAgent.get(id);
		// Pass the incoming request straight to your Agent
		let resp = await agent.fetch(request);

		return Response.json({ hello: 'visit https://developers.cloudflare.com/agents for more' });
	},
} satisfies ExportedHandler<Env>;

export class MyAgent extends Agent<Env> {
	// Your Agent implementation goes here
}
```
</TypeScriptExample>

---

# Browse the web

URL: https://developers.cloudflare.com/agents/examples/browse-the-web/

import { MetaInfo, Render, Type, TypeScriptExample, WranglerConfig } from "~/components";

Agents can browse the web using the [Browser Rendering](/browser-rendering/) API or your preferred headless browser service.

### Browser Rendering API

The [Browser Rendering](/browser-rendering/) allows you to spin up headless browser instances, render web pages, and interact with websites through your Agent.

You can define a method that uses Puppeteer to pull the content of a web page, parse the DOM, and extract relevant information by calling the OpenAI model:

<TypeScriptExample>

```ts
interface Env {
	BROWSER: Fetcher;
}

export class MyAgent extends Agent<Env> {
	async browse(browserInstance: Fetcher, urls: string[]) {
		let responses = [];
		for (const url of urls) {
			const browser = await puppeteer.launch(browserInstance);
			const page = await browser.newPage();
			await page.goto(url);

			await page.waitForSelector('body');
			const bodyContent = await page.$eval('body', (element) => element.innerHTML);
			const client = new OpenAI({
				apiKey: this.env.OPENAI_API_KEY,
			});

			let resp = await client.chat.completions.create({
				model: this.env.MODEL,
				messages: [
					{
						role: 'user',
						content: `Return a JSON object with the product names, prices and URLs with the following format: { "name": "Product Name", "price": "Price", "url": "URL" } from the website content below. <content>${bodyContent}</content>`,
					},
				],
				response_format: {
					type: 'json_object',
				},
			});

			responses.push(resp);
			await browser.close();
		}

		return responses;
	}
}
```

</TypeScriptExample>

You'll also need to add install the `@cloudflare/puppeteer` package and add the following to the wrangler configuration of your Agent:

```sh
npm install @cloudflare/puppeteer --save-dev
```

<WranglerConfig>

```jsonc
{
	// ...
	"browser": {
    "binding": "MYBROWSER"
  }
  // ...
}
```

</WranglerConfig>

### Browserbase

You can also use [Browserbase](https://docs.browserbase.com/integrations/cloudflare/typescript) by using the Browserbase API directly from within your Agent.

Once you have your [Browserbase API key](https://docs.browserbase.com/integrations/cloudflare/typescript), you can add it to your Agent by creating a [secret](/workers/configuration/secrets/):

```sh
cd your-agent-project-folder
npx wrangler@latest secret put BROWSERBASE_API_KEY
```
```sh output
Enter a secret value: ******
Creating the secret for the Worker "agents-example"
Success! Uploaded secret BROWSERBASE_API_KEY
```

Install the `@cloudflare/puppeteer` package and use it from within your Agent to call the Browserbase API:

```sh
npm install @cloudflare/puppeteer
```

<TypeScriptExample>

```ts
interface Env {
	BROWSERBASE_API_KEY: string;
}

export class MyAgent extends Agent<Env> {
	constructor(env: Env) {
		super(env);
	}
}
```

</TypeScriptExample>

---

# Examples

URL: https://developers.cloudflare.com/agents/examples/

import { DirectoryListing, PackageManagers } from "~/components";

Agents running on Cloudflare can:

<DirectoryListing />

---

# Manage and sync state

URL: https://developers.cloudflare.com/agents/examples/manage-and-sync-state/

import { MetaInfo, Render, Type, TypeScriptExample, WranglerConfig } from "~/components";

Every Agent has built-in state management capabilities, including built-in storage and synchronization between the Agent and frontend applications. State within an Agent is:

* Persisted across Agent restarts: data is permanently persisted within the Agent.
* Automatically serialized/deserialized: you can store any JSON-serializable data.
* Immediately consistent within the Agent: read your own writes.
* Thread-safe for concurrent updates

Agent state is stored in a SQL database that is embedded within each individual Agent instance: you can interact with it using the higher-level `this.setState` API (recommended) or by directly querying the database with `this.sql`.

#### State API

Every Agent has built-in state management capabilities. You can set and update the Agent's state directly using `this.setState`:

<TypeScriptExample>

```ts
import { Agent } from "agents-sdk";

export class MyAgent extends Agent {
  // Update state in response to events
  async incrementCounter() {
    this.setState({
      ...this.state,
      counter: this.state.counter + 1,
    });
  }

  // Handle incoming messages
  async onMessage(message) {
    if (message.type === "update") {
      this.setState({
        ...this.state,
        ...message.data,
      });
    }
  }

  // Handle state updates
  onStateUpdate(state, source: "server" | Connection) {
    console.log("state updated", state);
  }
}
```

</TypeScriptExample>

If you're using TypeScript, you can also provide a type for your Agent's state by passing in a type as a [type parameter](https://www.typescriptlang.org/docs/handbook/2/generics.html#using-type-parameters-in-generic-constraints) as the _second_ type parameter to the `Agent` class definition.

<TypeScriptExample>

```ts
import { Agent } from "agents-sdk";

interface Env {}

// Define a type for your Agent's state
interface FlightRecord {
	id: string;
	departureIata: string;
	arrival: Date;;
	arrivalIata: string;
	price: number;
}

// Pass in the type of your Agent's state
export class MyAgent extends Agent<Env, FlightRecord> {
  // This allows this.setState and the onStateUpdate method to
  // be typed:
 	async onStateUpdate(state: FlightRecord) {
  	console.log("state updated", state);
  }

  async someOtherMethod() {
  	this.setState({
  		...this.state,
  		price: this.state.price + 10,
  	});
  }
}
```

</TypeScriptExample>

### Synchronizing state

Clients can connect to an Agent and stay synchronized with its state using the React hooks provided as part of `agents-sdk/react`.

A React application can call `useAgent` to connect to a named Agent over WebSockets at

<TypeScriptExample>

```ts
import { useState } from "react";
import { useAgent } from "agents-sdk/react";

function StateInterface() {
  const [state, setState] = useState({ counter: 0 });

  const agent = useAgent({
    agent: "thinking-agent",
    name: "my-agent",
    onStateUpdate: (newState) => setState(newState),
  });

  const increment = () => {
    agent.setState({ counter: state.counter + 1 });
  };

  return (
    <div>
      <div>Count: {state.counter}</div>
      <button onClick={increment}>Increment</button>
    </div>
  );
}
```

</TypeScriptExample>

The state synchronization system:

* Automatically syncs the Agent's state to all connected clients
* Handles client disconnections and reconnections gracefully
* Provides immediate local updates
* Supports multiple simultaneous client connections

Common use cases:

* Real-time collaborative features
* Multi-window/tab synchronization
* Live updates across multiple devices
* Maintaining consistent UI state across clients
* When new clients connect, they automatically receive the current state from the Agent, ensuring all clients start with the latest data.

### SQL API

Every individual Agent instance has its own SQL (SQLite) database that runs _within the same context_ as the Agent itself. This means that inserting or querying data within your Agent is effectively zero-latency: the Agent doesn't have to round-trip across a continent or the world to access its own data.

You can access the SQL API within any method on an Agent via `this.sql`. The SQL API accepts template literals, and

<TypeScriptExample>

```ts
export class MyAgent extends Agent<Env> {
	async onRequest(request: Request) {
		let userId = new URL(request.url).searchParams.get('userId');

		// 'users' is just an example here: you can create arbitrary tables and define your own schemas
		// within each Agent's database using SQL (SQLite syntax).
		let user = await this.sql`SELECT * FROM users WHERE id = ${userId}`
		return Response.json(user)
	}
}
```

</TypeScriptExample>

You can also supply a [TypeScript type argument](https://www.typescriptlang.org/docs/handbook/2/generics.html#using-type-parameters-in-generic-constraints) the query, which will be used to infer the type of the result:

```ts
type User = {
	id: string;
	name: string;
	email: string;
};

export class MyAgent extends Agent<Env> {
	async onRequest(request: Request) {
		let userId = new URL(request.url).searchParams.get('userId');
		// Supply the type paramter to the query when calling this.sql
		// This assumes the results returns one or more User rows with "id", "name", and "email" columns
		const user = await this.sql<User>`SELECT * FROM users WHERE id = ${userId}`;
		return Response.json(user)
	}
}
```

You do not need to specify an array type (`User[]` or `Array<User>`) as `this.sql` will always return an array of the specified type.

Providing a type parameter does not validate that the result matches your type definition. In TypeScript, properties (fields) that do not exist or conform to the type you provided will be dropped. If you need to validate incoming events, we recommend a library such as [zod](https://zod.dev/) or your own validator logic.

:::note

Learn more about the zero-latency SQL storage that powers both Agents and Durable Objects [on our blog](https://blog.cloudflare.com/sqlite-in-durable-objects/).

:::

The SQL API exposed to an Agent is similar to the one [within Durable Objects](/durable-objects/api/sql-storage/): Durable Object SQL methods available on `this.ctx.storage.sql`. You can use the same SQL queries with the Agent's database, create tables, and query data, just as you would with Durable Objects or [D1](/d1/).

### Use Agent state as model context

You can combine the state and SQL APIs in your Agent with its ability to [call AI models](/agents/examples/using-ai-models/) to include historical context within your prompts to a model. Modern Large Language Models (LLMs) often have very large context windows (up to millions of tokens), which allows you to pull relevant context into your prompt directly.

For example, you can use an Agent's built-in SQL database to pull history, query a model with it, and append to that history ahead of the next call to the model:

<TypeScriptExample>

```ts
export class ReasoningAgent extends Agent<Env> {
	async callReasoningModel(prompt: Prompt) {
		let result = this.sql<History>`SELECT * FROM history WHERE user = ${prompt.userId} ORDER BY timestamp DESC LIMIT 1000`;
		let context = [];
		for await (const row of result) {
			context.push(row.entry);
		}

		const client = new OpenAI({
			apiKey: this.env.OPENAI_API_KEY,
		});

		// Combine user history with the current prompt
		const systemPrompt = prompt.system || 'You are a helpful assistant.';
		const userPrompt = `${prompt.user}\n\nUser history:\n${context.join('\n')}`;

		try {
			const completion = await client.chat.completions.create({
				model: this.env.MODEL || 'o3-mini',
				messages: [
					{ role: 'system', content: systemPrompt },
					{ role: 'user', content: userPrompt },
				],
				temperature: 0.7,
				max_tokens: 1000,
			});

			// Store the response in history
			this
				.sql`INSERT INTO history (timestamp, user, entry) VALUES (${new Date()}, ${prompt.userId}, ${completion.choices[0].message.content})`;

			return completion.choices[0].message.content;
		} catch (error) {
			console.error('Error calling reasoning model:', error);
			throw error;
		}
	}
}
```

</TypeScriptExample>

This works because each instance of an Agent has its _own_ database, the state stored in that database is private to that Agent: whether it's acting on behalf of a single user, a room or channel, or a deep research tool. By default, you don't have to manage contention or reach out over the network to a centralized database to retrieve and store state.

---

# Retrieval Augmented Generation

URL: https://developers.cloudflare.com/agents/examples/rag/

import { MetaInfo, Render, Type, TypeScriptExample, WranglerConfig } from "~/components";

Agents can use Retrieval Augmented Generation (RAG) to retrieve relevant information and use it augment [calls to AI models](/agents/examples/using-ai-models/). Store a user's chat history to use as context for future conversations, summarize documents to bootstrap an Agent's knowledge base, and/or use data from your Agent's [web browsing](/agents/examples/browse-the-web/) tasks to enhance your Agent's capabilities.

You can use the Agent's own [SQL database](/agents/examples/manage-and-sync-state) as the source of truth for your data and store embeddings in [Vectorize](/vectorize/) (or any other vector-enabled database) to allow your Agent to retrieve relevant information.

### Vector search

:::note

If you're brand-new to vector databases and Vectorize, visit the [Vectorize tutorial](/vectorize/get-started/intro/) to learn the basics, including how to create an index, insert data, and generate embeddings.

:::

You can query a vector index (or indexes) from any method on your Agent: any Vectorize index you attach is available on `this.env` within your Agent. If you've [associated metadata](/vectorize/best-practices/insert-vectors/#metadata) with your vectors that maps back to data stored in your Agent, you can then look up the data directly within your Agent using `this.sql`.

Here's an example of how to give an Agent retrieval capabilties:

<TypeScriptExample>

```ts
import { Agent } from "agents-sdk";

interface Env {
	AI: Ai;
	VECTOR_DB: Vectorize;
}

export class RAGAgent extends Agent<Env> {
	// Other methods on our Agent
	// ...
	//
	async queryKnowledge(userQuery: string) {
		// Turn a query into an embedding
		const queryVector = await this.env.AI.run('@cf/baai/bge-base-en-v1.5', {
			text: [userQuery],
		});

		// Retrieve results from our vector index
		let searchResults = await this.env.VECTOR_DB.query(queryVector.data[0], {
			topK: 10,
			returnMetadata: 'all',
		});

		let knowledge = [];
		for (const match of searchResults.matches) {
			console.log(match.metadata);
			knowledge.push(match.metadata);
		}

		// Use the metadata to re-associate the vector search results
		// with data in our Agent's SQL database
		let results = this.sql`SELECT * FROM knowledge WHERE id IN (${knowledge.map((k) => k.id)})`;

		// Return them
		return results;
	}
}
```

</TypeScriptExample>

You'll also need to connect your Agent to your vector indexes:

<WranglerConfig>

```jsonc
{
	// ...
  "vectorize": [
    {
      "binding": "VECTOR_DB",
      "index_name": "your-vectorize-index-name"
    }
  ]
  // ...
}
```

</WranglerConfig>

If you have multiple indexes you want to make available, you can provide an array of `vectorize` bindings.

#### Next steps

* Learn more on how to [combine Vectorize and Workers AI](/vectorize/get-started/embeddings/)
* Review the [Vectorize query API](/vectorize/reference/client-api/)
* Use [metadata filtering](/vectorize/reference/metadata-filtering/) to add context to your results

---

# Run Workflows

URL: https://developers.cloudflare.com/agents/examples/run-workflows/

import { MetaInfo, Render, Type, TypeScriptExample, WranglerConfig } from "~/components";

Agents can trigger asynchronous [Workflows](/workflows/), allowing your Agent to run complex, multi-step tasks in the background. This can include post-processing files that a user has uploaded, updating the embeddings in a [vector database](/vectorize/), and/or managing long-running user-lifecycle email or SMS notification workflows.

Because an Agent is just like a Worker script, it can create Workflows defined in the same project (script) as the Agent _or_ in a different project.

:::note[Agents vs. Workflows]

Agents and Workflows have some similarities: they can both run tasks asynchronously. For straightforward tasks that are linear or need to run to completion, a Workflow can be ideal: steps can be retried, they can be cancelled, and can act on events.

Agents do not have to run to completion: they can loop, branch and run forever, and they can also interact directly with users (over HTTP or WebSockets). An Agent can be used to trigger multiple Workflows as it runs, and can thus be used to co-ordinate and manage Workflows to achieve its goals.

:::

## Trigger a Workflow

An Agent can trigger one or more Workflows from within any method, whether from an incoming HTTP request, a WebSocket connection, on a delay or schedule, and/or from any other action the Agent takes.

Triggering a Workflow from an Agent is no different from [triggering a Workflow from a Worker script](/workflows/build/trigger-workflows/):

<TypeScriptExample>

```ts
interface Env {
	MY_WORKFLOW: Workflow;
	MyAgent: AgentNamespace<MyAgent>;
}

export class MyAgent extends Agent<Env> {
	async onRequest(request: Request) {
		let userId = request.headers.get("user-id");
		// Trigger a schedule that runs a Workflow
		// Pass it a payload
		let { taskId } = await this.schedule(300, "runWorkflow", { id: userId, flight: "DL264", date: "2025-02-23" });
	}

	async runWorkflow(data) {
		let instance = await env.MY_WORKFLOW.create({
			id: data.id,
			params: data,
		})

		// Schedule another task that checks the Workflow status every 5 minutes...
		await this.schedule("*/5 * * * *", "checkWorkflowStatus", { id: instance.id });
	}
}

export class MyWorkflow extends WorkflowEntrypoint<Env> {
	async run(event: WorkflowEvent<Params>, step: WorkflowStep) {
		// Your Workflow code here
	}
}
```

</TypeScriptExample>

You'll also need to make sure your Agent [has a binding to your Workflow](/workflows/build/trigger-workflows/#workers-api-bindings) so that it can call it:

<WranglerConfig>

```jsonc
{
	// ...
	// Create a binding between your Agent and your Workflow
	"workflows": [
		{
			// Required:
			"name": "EMAIL_WORKFLOW",
			"class_name": "MyWorkflow",
			// Optional: set the script_name field if your Workflow is defined in a
			// different project from your Agent
			"script_name": "email-workflows"
		}
	 ],
	// ...
}
```

</WranglerConfig>

## Trigger a Workflow from another project

You can also call a Workflow that is defined in a different Workers script from your Agent by setting the `script_name` property in the `workflows` binding of your Agent:

<WranglerConfig>

```jsonc
{
		// Required:
		"name": "EMAIL_WORKFLOW",
		"class_name": "MyWorkflow",
		// Optional: set tthe script_name field if your Workflow is defined in a
		// different project from your Agent
		"script_name": "email-workflows"
}
```

</WranglerConfig>

Refer to the [cross-script calls](/workflows/build/workers-api/#cross-script-calls) section of the Workflows documentation for more examples.

---

# Schedule tasks

URL: https://developers.cloudflare.com/agents/examples/schedule-tasks/

import { MetaInfo, Render, Type, TypeScriptExample, WranglerConfig } from "~/components";

An Agent can schedule tasks to be run in the future by calling `this.schedule(when, callback, data)`, where `when` can be a delay, a `Date`, or a cron string; `callback` the function name to call, and `data` is an object of data to pass to the function.

Scheduled tasks can do anything a request or message from a user can: make requests, query databases, send emails, read+write state: scheduled tasks can invoke any regular method on your Agent.

### Scheduling tasks

You can call `this.schedule` within any method on an Agent, and schedule tens-of-thousands of tasks per individual Agent:

<TypeScriptExample>

```ts
import { Agent } from "agents-sdk"

export class SchedulingAgent extends Agent {
	async onRequest(request) {
		// Handle an incoming request
		// Schedule a task 5 minutes from now
		// Calls the "checkFlights" method
		let { taskId } = await this.schedule(600, "checkFlights", { flight: "DL264", date: "2025-02-23" });
		return Response.json({ taskId });
	}

	async checkFlights(data) {
		// Invoked when our scheduled task runs
		// We can also call this.schedule here to schedule another task
	}
}
```
</TypeScriptExample>

:::caution

Tasks that set a callback for a method that does not exist will throw an exception: ensure that the method named in the `callback` argument of `this.schedule` exists on your `Agent` class.

:::

You can schedule tasks in multiple ways:

<TypeScriptExample>

```ts
// schedule a task to run in 10 seconds
let task = await this.schedule(10, "someTask", { message: "hello" });

// schedule a task to run at a specific date
let task = await this.schedule(new Date("2025-01-01"), "someTask", {});

// schedule a task to run every 10 seconds
let { id } = await this.schedule("*/10 * * * *", "someTask", { message: "hello" });

// schedule a task to run every 10 seconds, but only on Mondays
let task = await this.schedule("0 0 * * 1", "someTask", { message: "hello" });

// cancel a scheduled task
this.cancelSchedule(task.id);
```

</TypeScriptExample>

Calling `await this.schedule` returns a `Schedule`, which includes the task's randomly generated `id`. You can use this `id` to retrieve or cancel the task in the future. It also provides a `type` property that indicates the type of schedule, for example, one of `"scheduled" | "delayed" | "cron"`.

:::note[Maximum scheduled tasks]

Each task is mapped to a row in the Agent's underlying [SQLite database](/durable-objects/api/sql-storage/), which means that each task can be up to 2 MB in size. The maximum number of tasks must be `(task_size * tasks) + all_other_state < maximum_database_size` (currently 1GB per Agent).

:::

### Managing scheduled tasks

You can get, cancel and filter across scheduled tasks within an Agent using the scheduling API:

<TypeScriptExample>

```ts
// Get a specific schedule by ID
// Returns undefined if the task does not exist
let task = await this.getSchedule(task.id)

// Get all scheduled tasks
// Returns an array of Schedule objects
let tasks = this.getSchedules();

// Cancel a task by its ID
// Returns true if the task was cancelled, false if it did not exist
await this.cancelSchedule(task.id);

// Filter for specific tasks
// e.g. all tasks starting in the next hour
let tasks = this.getSchedules({
	timeRange: {
		start: new Date(Date.now()),
		end: new Date(Date.now() + 60 * 60 * 1000),
	}
});
```

</TypeScriptExample>

---

# Using WebSockets

URL: https://developers.cloudflare.com/agents/examples/websockets/

import { MetaInfo, Render, Type, TypeScriptExample, WranglerConfig } from "~/components";

Users and clients can connect to an Agent directly over WebSockets, allowing long-running, bi-directional communication with your Agent as it operates.

To enable an Agent to accept WebSockets, define `onConnect` and `onMessage` methods on your Agent.

* `onConnect(connection: Connection, ctx: ConnectionContext)` is called when a client establishes a new WebSocket connection. The original HTTP request, including request headers, cookies, and the URL itself, are available on `ctx.request`.
* `onMessage(connection: Connection, message: WSMessage)` is called for each incoming WebSocket message. Messages are one of `ArrayBuffer | ArrayBufferView | string`, and you can send messages back to a client using `connection.send()`. You can distinguish between client connections by checking `connection.id`, which is unique for each connected client.

Here's an example of an Agent that echoes back any message it receives:

<TypeScriptExample>

```ts
import { Agent, Connection } from "agents-sdk";

export class ChatAgent extends Agent {
  async onConnect(connection: Connection, ctx: ConnectionContext) {
  	// Access the request to verify any authentication tokens
    // provided in headers or cookies
    let token = ctx.request.headers.get("Authorization");
    if (!token) {
      await connection.close(4000, "Unauthorized");
      return;
    }

		// Handle auth using your favorite library and/or auth scheme:
 		// try {
		// 	 await jwt.verify(token, env.JWT_SECRET);
		// } catch (error) {
		// 	 connection.close(4000, 'Invalid Authorization header');
		// 	 return;
		// }

		// Accept valid connections
  	connection.accept()
  }

  async onMessage(connection: Connection, message: WSMessage) {
    // const response = await longRunningAITask(message)
    await connection.send(message)
  }
}
```

</TypeScriptExample>

## Connecting clients

The Agent framework includes a useful helper package for connecting directly to your Agent (or other Agents) from a client application. Import `agents-sdk/client`, create an instance of `AgentClient` and use it to connect to an instance of your Agent:

<TypeScriptExample>

```ts
import { AgentClient } from "agents-sdk/client";

const connection = new AgentClient({
  agent: "dialogue-agent",
  name: "insight-seeker",
});

connection.addEventListener("message", (event) => {
  console.log("Received:", event.data);
});

connection.send(
  JSON.stringify({
    type: "inquiry",
    content: "What patterns do you see?",
  })
);
```

</TypeScriptExample>

## React clients

React-based applications can import `agents-sdk/react` and use the `useAgent` hook to connect to an instance of an Agent directly:

<TypeScriptExample>

```ts
import { useAgent } from "agents-sdk/react";

function AgentInterface() {
  const connection = useAgent({
    agent: "dialogue-agent",
    name: "insight-seeker",
    onMessage: (message) => {
      console.log("Understanding received:", message.data);
    },
    onOpen: () => console.log("Connection established"),
    onClose: () => console.log("Connection closed"),
  });

  const inquire = () => {
    connection.send(
      JSON.stringify({
        type: "inquiry",
        content: "What insights have you gathered?",
      })
    );
  };

  return (
    <div className="agent-interface">
      <button onClick={inquire}>Seek Understanding</button>
    </div>
  );
}

```
</TypeScriptExample>

The `useAgent` hook automatically handles the lifecycle of the connection, ensuring that it is properly initialized and cleaned up when the component mounts and unmounts. You can also [combine `useAgent` with `useState`](/agents/examples/manage-and-sync-state/) to automatically synchronize state across all clients connected to your Agent.

## Handling WebSocket events

Define `onError` and `onClose` methods on your Agent to explicitly handle WebSocket client errors and close events. Log errors, clean up state, and/or emit metrics:

<TypeScriptExample>

```ts
import { Agent, Connection } from "agents-sdk";

export class ChatAgent extends Agent {
 	// onConnect and onMessage methods
  // ...

  // WebSocket error and disconnection (close) handling.
  async onError(connection: Connection, error: unknown): Promise<void> {
		console.error(`WS error: ${error}`);
	}
	async onClose(connection: Connection, code: number, reason: string, wasClean: boolean): Promise<void> {
		console.log(`WS closed: ${code} - ${reason} - wasClean: ${wasClean}`);
		connection.close();
	}
}

```

</TypeScriptExample>

---

# Using AI Models

URL: https://developers.cloudflare.com/agents/examples/using-ai-models/

import { AnchorHeading, MetaInfo, Render, Type, TypeScriptExample, WranglerConfig } from "~/components";

Agents can communicate with AI models hosted on any provider, including [Workers AI](/workers-ai/), OpenAI, Anthropic, and Google's Gemini, and use the model routing features in [AI Gateway](/ai-gateway/) to route across providers, eval responses, and manage AI provider rate limits.

Because Agents are built on top of [Durable Objects](/durable-objects/), each Agent or chat session is associated with a stateful compute instance. Tradtional serverless architectures often present challenges for persistent connections needed in real-time applications like chat.

A user can disconnect during a long-running response from a modern reasoning model (such as `o3-mini` or DeepSeek R1), or lose conversational context when refreshing the browser. Instead of relying on request-response patterns and managing an external database to track & store conversation state, state can be stored directly within the Agent. If a client disconnects, the Agent can write to its own distributed storage, and catch the client up as soon as it reconnects: even if it's hours or days later.

## Calling AI Models

You can call models from any method within an Agent, including from HTTP requests using the [`onRequest`](/agents/api-reference/sdk/) handler, when a [scheduled task](/agents/examples/schedule-tasks/) runs, when handling a WebSocket message in the [`onMessage`](/agents/examples/websockets/) handler, or from any of your own methods.

Importantly, Agents can call AI models on their own â€” autonomously â€” and can handle long-running responses that can take minutes (or longer) to respond in full.

### Long-running model requests {/*long-running-model-requests*/}

Modern [reasoning models](https://platform.openai.com/docs/guides/reasoning) or "thinking" model can take some time to both generate a response _and_ stream the response back to the client.

Instead of buffering the entire response, or risking the client disconecting, you can stream the response back to the client by using the [WebSocket API](/agents/examples/websockets/).

<TypeScriptExample filename="src/index.ts">

```ts
import { Agent } from "agents-sdk"
import { OpenAI } from "openai"

export class MyAgent extends Agent<Env> {
	async onConnect(connection: Connection, ctx: ConnectionContext) {
		// Omitted for simplicity: authenticating the user
		connection.accept()
	}

	async onMessage(connection: Connection, message: WSMessage) {
		let msg = JSON.parse(message)
		// This can run as long as it needs to, and return as many messages as it needs to!
		await queryReasoningModel(connection, msg.prompt)
  }

	async queryReasoningModel(connection: Connection, userPrompt: string) {
		const client = new OpenAI({
			apiKey: this.env.OPENAI_API_KEY,
		});

		try {
			const stream = await client.chat.completions.create({
				model: this.env.MODEL || 'o3-mini',
				messages: [{ role: 'user', content: userPrompt }],
				stream: true,
			});

			// Stream responses back as WebSocket messages
			for await (const chunk of stream) {
				const content = chunk.choices[0]?.delta?.content || '';
				if (content) {
					connection.send(JSON.stringify({ type: 'chunk', content }));
				}
			}

			// Send completion message
			connection.send(JSON.stringify({ type: 'done' }));
		} catch (error) {
			connection.send(JSON.stringify({ type: 'error', error: error }));
		}
	}
}
```

</TypeScriptExample>

You can also persist AI model responses back to [Agent's internal state](/agents/examples/manage-and-sync-state/) by using the `this.setState` method. For example, if you run a [scheduled task](/agents/examples/schedule-tasks/), you can store the output of the task and read it later. Or, if a user disconnects, read the message history back and send it to the user when they reconnect.

### Workers AI

### Hosted models

You can use [any of the models available in Workers AI](/workers-ai/models/) within your Agent by [configuring a binding](/workers-ai/configuration/bindings/).

Workers AI supports streaming responses out-of-the-box by setting `stream: true`, and we strongly recommend using them to avoid buffering and delaying responses, especially for larger models or reasoning models that require more time to generate a response.

<TypeScriptExample filename="src/index.ts">

```ts
import { Agent } from "agents-sdk"

interface Env {
	AI: Ai;
}

export class MyAgent extends Agent<Env> {
	async onRequest(request: Request) {
		const response = await env.AI.run(
      "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b",
      {
        prompt: "Build me a Cloudflare Worker that returns JSON.",
        stream: true, // Stream a response and don't block the client!
      }
    );

		// Return the stream
    return new Response(answer, {
        headers: { "content-type": "text/event-stream" }
    })
	}
}
```

</TypeScriptExample>

Your wrangler configuration will need an `ai` binding added:

<WranglerConfig>

```toml
[ai]
binding = "AI"
```

</WranglerConfig>

### Model routing

You can also use the model routing features in [AI Gateway](/ai-gateway/) directly from an Agent by specifying a [`gateway` configuration](/ai-gateway/providers/workersai/) when calling the AI binding.

:::note

Model routing allows you to route requests to different AI models based on whether they are reachable, rate-limiting your client, and/or if you've exceeded your cost budget for a specific provider.

:::

<TypeScriptExample filename="src/index.ts">

```ts
import { Agent } from "agents-sdk"

interface Env {
	AI: Ai;
}

export class MyAgent extends Agent<Env> {
	async onRequest(request: Request) {
		const response = await env.AI.run(
      "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b",
      {
        prompt: "Build me a Cloudflare Worker that returns JSON."
      },
      {
        gateway: {
          id: "{gateway_id}", // Specify your AI Gateway ID here
          skipCache: false,
          cacheTtl: 3360,
        },
      },
    );

    return Response.json(response)
	}
}
```

</TypeScriptExample>

Your wrangler configuration will need an `ai` binding added. This is shared across both Workers AI and AI Gateway.
<WranglerConfig>

```toml
[ai]
binding = "AI"
```

</WranglerConfig>

Visit the [AI Gateway documentation](/ai-gateway/) to learn how to configure a gateway and retrieve a gateway ID.

### AI SDK

The [AI SDK](https://sdk.vercel.ai/docs/introduction) provides a unified API for using AI models, including for text generation, tool calling, structured responses, image generation, and more.

To use the AI SDK, install the `ai` package and use it within your Agent. The example below shows how it use it to generate text on request, but you can use it from any method within your Agent, including WebSocket handlers, as part of a scheduled task, or even when the Agent is initialized.

```sh
npm install ai @ai-sdk/openai
```

<TypeScriptExample filename="src/index.ts">

```ts
import { Agent } from "agents-sdk"
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export class MyAgent extends Agent<Env> {
	async onRequest(request: Request): Promise<Response> {
		const { text } = await generateText({
			model: openai("o3-mini"),
			prompt: "Build me an AI agent on Cloudflare Workers",
			});

		return Response.json({modelResponse: text})
	}
}
```

</TypeScriptExample>

### OpenAI compatible endpoints

Agents can call models across any service, including those that support the OpenAI API. For example, you can use the OpenAI SDK to use one of [Google's Gemini models](https://ai.google.dev/gemini-api/docs/openai#node.js) directly from your Agent.

Agents can stream responses back over HTTP using Server Sent Events (SSE) from within an `onRequest` handler, or by using the native [WebSockets](/agents/examples/websockets/) API in your Agent to responses back to a client, which is especially useful for larger models that can take over 30+ seconds to reply.

<TypeScriptExample filename="src/index.ts">

```ts
import { Agent } from "agents-sdk"
import { OpenAI } from "openai"

export class MyAgent extends Agent<Env> {
	async onRequest(request: Request): Promise<Response> {
		const openai = new OpenAI({
    	apiKey: this.env.GEMINI_API_KEY,
    	baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
		});

		// Create a TransformStream to handle streaming data
    let { readable, writable } = new TransformStream();
    let writer = writable.getWriter();
    const textEncoder = new TextEncoder();

		// Use ctx.waitUntil to run the async function in the background
		// so that it doesn't block the streaming response
    ctx.waitUntil(
      (async () => {
        const stream = await openai.chat.completions.create({
          model: "4o",
          messages: [{ role: "user", content: "Write me a Cloudflare Worker." }],
          stream: true,
        });

        // loop over the data as it is streamed and write to the writeable
        for await (const part of stream) {
          writer.write(
            textEncoder.encode(part.choices[0]?.delta?.content || ""),
          );
        }
        writer.close();
      })(),
    );

		// Return the readable stream back to the client
    return new Response(readable)
	}
}
```

</TypeScriptExample>

---

# Calling LLMs

URL: https://developers.cloudflare.com/agents/concepts/calling-llms/

import { Render } from "~/components";

### Understanding LLM providers and model types

Different LLM providers offer models optimized for specific types of tasks. When building AI systems, choosing the right model is crucial for both performance and cost efficiency.

#### Reasoning Models

Models like OpenAI's o1, Anthropic's Claude, and DeepSeek's R1 are particularly well-suited for complex reasoning tasks. These models excel at:

- Breaking down problems into steps
- Following complex instructions
- Maintaining context across long conversations
- Generating code and technical content

For example, when implementing a travel booking system, you might use a reasoning model to analyze travel requirements and generate appropriate booking strategies.

#### Instruction Models

Models like GPT-4 and Claude Instant are optimized for following straightforward instructions efficiently. They work well for:
- Content generation
- Simple classification tasks
- Basic question answering
- Text transformation

These models are often more cost-effective for straightforward tasks that do not require complex reasoning.

---

# Human in the Loop

URL: https://developers.cloudflare.com/agents/concepts/human-in-the-loop/

import { Render, Note, Aside } from "~/components";

### What is Human-in-the-Loop?

Human-in-the-Loop (HITL) workflows integrate human judgment and oversight into automated processes. These workflows pause at critical points for human review, validation, or decision-making before proceeding. This approach combines the efficiency of automation with human expertise and oversight where it matters most.

![A human-in-the-loop diagram](~/assets/images/agents/human-in-the-loop.svg)

#### Understanding Human-in-the-Loop workflows

In a Human-in-the-Loop workflow, processes are not fully automated. Instead, they include designated checkpoints where human intervention is required. For example, in a travel booking system, a human may want to confirm the travel before an agent follows through with a transaction. The workflow manages this interaction, ensuring that:

1. The process pauses at appropriate review points
2. Human reviewers receive necessary context
3. The system maintains state during the review period
4. Review decisions are properly incorporated
5. The process continues once approval is received

### Best practices for Human-in-the-Loop workflows

#### Long-Term State Persistence

Human review processes do not operate on predictable timelines. A reviewer might need days or weeks to make a decision, especially for complex cases requiring additional investigation or multiple approvals. Your system needs to maintain perfect state consistency throughout this period, including:

- The original request and context
- All intermediate decisions and actions
- Any partial progress or temporary states
- Review history and feedback

:::note[Tip]
[Durable Objects](/durable-objects/) provide an ideal solution for managing state in Human-in-the-Loop workflows, offering persistent compute instances that maintain state for hours, weeks, or months.
:::

#### Continuous Improvement Through Evals

Human reviewers play a crucial role in evaluating and improving LLM performance. Implement a systematic evaluation process where human feedback is collected not just on the final output, but on the LLM's decision-making process. This can include:

- Decision Quality Assessment: Have reviewers evaluate the LLM's reasoning process and decision points, not just the final output.
- Edge Case Identification: Use human expertise to identify scenarios where the LLM's performance could be improved.
- Feedback Collection: Gather structured feedback that can be used to fine-tune the LLM or adjust the workflow. [AI Gateway](/ai-gateway/evaluations/add-human-feedback/) can be a useful tool for setting up an LLM feedback loop.

#### Error handling and recovery

Robust error handling is essential for maintaining workflow integrity. Your system should gracefully handle various failure scenarios, including reviewer unavailability, system outages, or conflicting reviews. Implement clear escalation paths for handling exceptional cases that fall outside normal parameters.

The system should maintain stability during paused states, ensuring that no work is lost even during extended review periods. Consider implementing automatic checkpointing that allows workflows to be resumed from the last stable state after any interruption.

---

# Concepts

URL: https://developers.cloudflare.com/agents/concepts/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Tools

URL: https://developers.cloudflare.com/agents/concepts/tools/

### What are tools?

Tools enable AI systems to interact with external services and perform actions. They provide a structured way for agents and workflows to invoke APIs, manipulate data, and integrate with external systems. Tools form the bridge between AI decision-making capabilities and real-world actions.

### Understanding tools

In an AI system, tools are typically implemented as function calls that the AI can use to accomplish specific tasks. For example, a travel booking agent might have tools for:

- Searching flight availability
- Checking hotel rates 
- Processing payments
- Sending confirmation emails

Each tool has a defined interface specifying its inputs, outputs, and expected behavior. This allows the AI system to understand when and how to use each tool appropriately.

### Common tool patterns

#### API integration tools

The most common type of tools are those that wrap external APIs. These tools handle the complexity of API authentication, request formatting, and response parsing, presenting a clean interface to the AI system.

#### Model Context Protocol (MCP)

The [Model Context Protocol](https://modelcontextprotocol.io/introduction) provides a standardized way to define and interact with tools. Think of it as an abstraction on top of APIs designed for LLMs to interact with external resources. MCP defines a consistent interface for:

- **Tool Discovery**: Systems can dynamically discover available tools
- **Parameter Validation**: Tools specify their input requirements using JSON Schema
- **Error Handling**: Standardized error reporting and recovery
- **State Management**: Tools can maintain state across invocations


#### Data processing tools

Tools that handle data transformation and analysis are essential for many AI workflows. These might include:

- CSV parsing and analysis
- Image processing
- Text extraction
- Data validation

---

# Agents

URL: https://developers.cloudflare.com/agents/concepts/what-are-agents/

import { Render } from "~/components";

### What are agents?

An agent is an AI system that can autonomously execute tasks by making decisions about tool usage and process flow. Unlike traditional automation that follows predefined paths, agents can dynamically adapt their approach based on context and intermediate results. Agents are also distinct from co-pilots (e.g. traditional chat applications) in that they can fully automate a task, as opposed to simply augmenting and extending human input. 

- **Agents** â†’ non-linear, non-deterministic (can change from run to run)
- **Workflows** â†’ linear, deterministic execution paths
- **Co-pilots** â†’ augmentative AI assistance requiring human intervention

### Example: Booking vacations

If this is your first time working with, or interacting with agents, this example will illustrate how an agent works within a context like booking a vacation. If you are already familiar with the topic, read on. 

Imagine you're trying to book a vacation. You need to research flights, find hotels, check restaurant reviews, and keep track of your budget. 

#### Traditional workflow automation

A traditional automation system follows a predetermined sequence:

- Takes specific inputs (dates, location, budget)
- Calls predefined API endpoints in a fixed order
- Returns results based on hardcoded criteria
- Cannot adapt if unexpected situations arise

![Traditional workflow automation diagram](~/assets/images/agents/workflow-automation.svg)

#### AI Co-pilot

A co-pilot acts as an intelligent assistant that:

- Provides hotel and itinerary recommendations based on your preferences
- Can understand and respond to natural language queries
- Offers guidance and suggestions
- Requires human decision-making and action for execution

![A co-pilot diagram](~/assets/images/agents/co-pilot.svg)

#### Agent

An agent combines AI's ability to make judgements and call the relevant tools to execute the task. An agent's output will be nondeterministic given:

- Real-time availability and pricing changes
- Dynamic prioritization of constraints
- Ability to recover from failures
- Adaptive decision-making based on intermediate results

![An agent diagram](~/assets/images/agents/agent-workflow.svg)

An agents can dynamically generate an itinerary and execute on booking reservations, similarly to what you would expect from a travel agent. 

### Three primary components of agent systems:

- **Decision Engine**: Usually an LLM (Large Language Model) that determines action steps
- **Tool Integration**: APIs, functions, and services the agent can utilize
- **Memory System**: Maintains context and tracks task progress

#### How agents work

Agents operate in a continuous loop of:

1. **Observing** the current state or task
2. **Planning** what actions to take, using AI for reasoning
3. **Executing** those actions using available tools (often APIs or [MCPs](https://modelcontextprotocol.io/introduction))
4. **Learning** from the results (storing results in memory, updating task progress, and preparing for next iteration)

---

# Workflows

URL: https://developers.cloudflare.com/agents/concepts/workflows/

import { Render } from "~/components";

## What are workflows?

A workflow is the orchestration layer that coordinates how an agent's components work together. It defines the structured paths through which tasks are processed, tools are called, and results are managed. While agents make dynamic decisions about what to do, workflows provide the underlying framework that governs how those decisions are executed.

### Understanding workflows in agent systems

Think of a workflow like the operating procedures of a company. The company (agent) can make various decisions, but how those decisions get implemented follows established processes (workflows). For example, when you book a flight through a travel agent, they might make different decisions about which flights to recommend, but the process of actually booking the flight follows a fixed sequence of steps.

Let's examine a basic agent workflow:

### Core components of a workflow

A workflow typically consists of several key elements:

1. **Input Processing**
The workflow defines how inputs are received and validated before being processed by the agent. This includes standardizing formats, checking permissions, and ensuring all required information is present.
2. **Tool Integration**
Workflows manage how external tools and services are accessed. They handle authentication, rate limiting, error recovery, and ensuring tools are used in the correct sequence.
3. **State Management**
The workflow maintains the state of ongoing processes, tracking progress through multiple steps and ensuring consistency across operations.
4. **Output Handling**
Results from the agent's actions are processed according to defined rules, whether that means storing data, triggering notifications, or formatting responses.

---

# Getting started

URL: https://developers.cloudflare.com/agents/getting-started/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Testing your Agents

URL: https://developers.cloudflare.com/agents/getting-started/testing-your-agent/

import { Render, PackageManagers, WranglerConfig } from "~/components"

Because Agents run on Cloudflare Workers and Durable Objects, they can be tested using the same tools and techniques as Workers and Durable Objects.

## Writing and running tests

### Setup

:::note

The `agents-sdk-starter` template and new Cloudflare Workers projects already include the relevant `vitest` and `@cloudflare/vitest-pool-workers` packages, as well as a valid `vitest.config.js` file.

:::

Before you write your first test, install the necessary packages:

```sh
npm install vitest@~3.0.0 --save-dev --save-exact
npm install @cloudflare/vitest-pool-workers --save-dev
```

Ensure that your `vitest.config.js` file is identical to the following:

```js
import { defineWorkersConfig } from "@cloudflare/vitest-pool-workers/config";

export default defineWorkersConfig({
  test: {
    poolOptions: {
      workers: {
        wrangler: { configPath: "./wrangler.toml" },
      },
    },
  },
});
```

### Add the Agent configuration

Add a `durableObjects` configuration to `vitest.config.js` with the name of your Agent class:

```js
import { defineWorkersConfig } from '@cloudflare/vitest-pool-workers/config';

export default defineWorkersConfig({
	test: {
		poolOptions: {
			workers: {
				main: './src/index.ts',
				miniflare: {
					durableObjects: {
						NAME: 'MyAgent',
					},
				},
			},
		},
	},
});
```

### Write a test

:::note

Review the [Vitest documentation](https://vitest.dev/) for more information on testing, including the test API reference and advanced testing techniques.

:::

Tests use the `vitest` framework. A basic test suite for your Agent can validate how your Agent responds to requests, but can also unit test your Agent's methods and state.

```ts
import { env, createExecutionContext, waitOnExecutionContext, SELF } from 'cloudflare:test';
import { describe, it, expect } from 'vitest';
import worker from '../src';
import { Env } from '../src';

interface ProvidedEnv extends Env {}

describe('make a request to my Agent', () => {
	// Unit testing approach
	it('responds with state', async () => {
		// Provide a valid URL that your Worker can use to route to your Agent
		// If you are using routeAgentRequest, this will be /agent/:agent/:name
		const request = new Request<unknown, IncomingRequestCfProperties>('http://example.com/agent/my-agent/agent-123');
		const ctx = createExecutionContext();
		const response = await worker.fetch(request, env, ctx);
		await waitOnExecutionContext(ctx);
		expect(await response.text()).toMatchObject({ hello: 'from your agent' });
	});

	it('also responds with state', async () => {
		const request = new Request('http://example.com/agent/my-agent/agent-123');
		const response = await SELF.fetch(request);
		expect(await response.text()).toMatchObject({ hello: 'from your agent' });
	});
});
```

### Run tests

Running tests is done using the `vitest` CLI:

```sh
$ npm run test
# or run vitest directly
$ npx vitest
```
```sh output
  MyAgent
    âœ“ should return a greeting (1 ms)

Test Files  1 passed (1)
```

Review the [documentation on testing](/workers/testing/vitest-integration/get-started/write-your-first-test/) for additional examples and test configuration.

## Running Agents locally

You can also run an Agent locally using the `wrangler` CLI:

```sh
$ npx wrangler dev
```
```sh output
Your Worker and resources are simulated locally via Miniflare. For more information, see: https://developers.cloudflare.com/workers/testing/local-development.

Your worker has access to the following bindings:
- Durable Objects:
  - MyAgent: MyAgent
  Starting local server...
[wrangler:inf] Ready on http://localhost:53645
```

This spins up a local development server that runs the same runtime as Cloudflare Workers, and allows you to iterate on your Agent's code and test it locally without deploying it.

Visit the [`wrangler dev`](https://developers.cloudflare.com/workers/wrangler/commands/#dev) docs to review the CLI flags and configuration options.

---

# Guides

URL: https://developers.cloudflare.com/agents/guides/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Reference

URL: https://developers.cloudflare.com/agents/platform/

import { DirectoryListing } from "~/components";

Build AI Agents on Cloudflare

<DirectoryListing />

---

# Limits

URL: https://developers.cloudflare.com/agents/platform/limits/

import { Render } from "~/components"

Limits that apply to authoring, deploying, and running Agents are detailed below.

Many limits are inherited from those applied to Workers scripts and/or Durable Objects, and are detailed in the [Workers limits](/workers/platform/limits/) documentation.

| Feature                                   | Limit         			    |
| ----------------------------------------- | ----------------------- |
| Max concurrent (running) Agents per account	| Tens of millions+ [^1]
| Max definitions per account         | ~250,000+ [^2]
| Max state stored per unique Agent | 1 GB |
| Max compute time per Agent | 30 seconds (refreshed per HTTP request / incoming WebSocket message) [^3] |
| Duration (wall clock) per step [^3]       | Unlimited (e.g. waiting on a database call or an LLM response) |

---

[^1]: Yes, really. You can have tens of millions of Agents running concurrently, as each Agent is mapped to a [unique Durable Object](/durable-objects/what-are-durable-objects/) (actor).
[^2]: You can deploy up to [500 scripts per account](/workers/platform/limits/), but each script (project) can define multiple Agents. Each deployed script can be up to 10 MB on the [Workers Paid Plan](/workers/platform/pricing/#workers)
[^3]: Compute (CPU) time per Agent is limited to 30 seconds, but this is refreshed when an Agent receives a new HTTP request, runs a [scheduled task](/agents/examples/schedule-tasks/), or an incoming WebSocket message.

<Render file="limits_increase" product="workers" />

---

# Authentication

URL: https://developers.cloudflare.com/ai-gateway/configuration/authentication/

Using an Authenticated Gateway in AI Gateway adds security by requiring a valid authorization token for each request. This feature is especially useful when storing logs, as it prevents unauthorized access and protects against invalid requests that can inflate log storage usage and make it harder to find the data you need. With Authenticated Gateway enabled, only requests with the correct token are processed.

:::note
We recommend enabling Authenticated Gateway when opting to store logs with AI Gateway.

If Authenticated Gateway is enabled but a request does not include the required `cf-aig-authorization` header, the request will fail. This setting ensures that only verified requests pass through the gateway. To bypass the need for the `cf-aig-authorization` header, make sure to disable Authenticated Gateway.
:::

## Setting up Authenticated Gateway using the Dashboard

1. Go to the Settings for the specific gateway you want to enable authentication for.
2. Select **Create authentication token** to generate a custom token with the required `Run` permissions. Be sure to securely save this token, as it will not be displayed again.
3. Include the `cf-aig-authorization` header with your API token in each request for this gateway.
4. Return to the settings page and toggle on Authenticated Gateway.

## Example requests with OpenAI

```bash
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \
  --header 'cf-aig-authorization: Bearer {CF_AIG_TOKEN}' \
  --header 'Authorization: Bearer OPENAI_TOKEN' \
  --header 'Content-Type: application/json' \
  --data '{"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": "What is Cloudflare?"}]}'
```

Using the OpenAI SDK:

```javascript
import OpenAI from "openai";

const openai = new OpenAI({
	apiKey: process.env.OPENAI_API_KEY,
	baseURL: "https://gateway.ai.cloudflare.com/v1/account-id/gateway/openai",
	defaultHeaders: {
		"cf-aig-authorization": `Bearer {token}`,
	},
});
```

## Example requests with the Vercel AI SDK

```javascript
import { createOpenAI } from "@ai-sdk/openai";

const openai = createOpenAI({
	baseURL: "https://gateway.ai.cloudflare.com/v1/account-id/gateway/openai",
	headers: {
		"cf-aig-authorization": `Bearer {token}`,
	},
});
```

## Expected behavior

The following table outlines gateway behavior based on the authentication settings and header status:

| Authentication Setting | Header Info    | Gateway State           | Response                                   |
| ---------------------- | -------------- | ----------------------- | ------------------------------------------ |
| On                     | Header present | Authenticated gateway   | Request succeeds                           |
| On                     | No header      | Error                   | Request fails due to missing authorization |
| Off                    | Header present | Unauthenticated gateway | Request succeeds                           |
| Off                    | No header      | Unauthenticated gateway | Request succeeds                           |

---

# Caching

URL: https://developers.cloudflare.com/ai-gateway/configuration/caching/

import { TabItem, Tabs } from "~/components";

Enable and customize your gateway cache to serve requests directly from Cloudflare's cache, instead of the original model provider, for faster requests and cost savings.

:::note

Currently caching is supported only for text and image responses, and it applies only to identical requests.

This is helpful for use cases when there are limited prompt options - for example, a support bot that asks "How can I help you?" and lets the user select an answer from a limited set of options works well with the current caching configuration.
We plan on adding semantic search for caching in the future to improve cache hit rates.
:::

## Default configuration

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To set the default caching configuration in the dashboard:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Select **AI** > **AI Gateway**.
3. Select **Settings**.
4. Enable **Cache Responses**.
5. Change the default caching to whatever value you prefer.

</TabItem> <TabItem label="API">

To set the default caching configuration using the API:

1. [Create an API token](/fundamentals/api/get-started/create-token/) with the following permissions:

- `AI Gateway - Read`
- `AI Gateway - Edit`

2. Get your [Account ID](/fundamentals/setup/find-account-and-zone-ids/).
3. Using that API token and Account ID, send a [`POST` request](/api/resources/ai_gateway/methods/create/) to create a new Gateway and include a value for the `cache_ttl`.

</TabItem> </Tabs>

This caching behavior will be uniformly applied to all requests that support caching. If you need to modify the cache settings for specific requests, you have the flexibility to override this setting on a per-request basis.

To check whether a response comes from cache or not, **cf-aig-cache-status** will be designated as `HIT` or `MISS`.

## Per-request caching

In order to override the default cache behavior defined on the settings tab, you can, on a per-request basis, set headers for the following options:

:::note

The following headers have been updated to new names, though the old headers will still function. We recommend updating to the new headers to ensure future compatibility:

`cf-cache-ttl` is now `cf-aig-cache-ttl`

`cf-skip-cache` is now `cf-aig-skip-cache`

:::

### Skip cache (cf-aig-skip-cache)

Skip cache refers to bypassing the cache and fetching the request directly from the original provider, without utilizing any cached copy.

You can use the header **cf-aig-skip-cache** to bypass the cached version of the request.

As an example, when submitting a request to OpenAI, include the header in the following manner:

```bash title="Request skipping the cache"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \
  --header "Authorization: Bearer $TOKEN" \
  --header 'Content-Type: application/json' \
  --header 'cf-aig-skip-cache: true' \
  --data ' {
   		 "model": "gpt-4o-mini",
   		 "messages": [
   			 {
   				 "role": "user",
   				 "content": "how to build a wooden spoon in 3 short steps? give as short as answer as possible"
   			 }
   		 ]
   	 }
'
```

### Cache TTL (cf-aig-cache-ttl)

Cache TTL, or Time To Live, is the duration a cached request remains valid before it expires and is refreshed from the original source. You can use **cf-aig-cache-ttl** to set the desired caching duration in seconds. The minimum TTL is 60 seconds and the maximum TTL is one month.

For example, if you set a TTL of one hour, it means that a request is kept in the cache for an hour. Within that hour, an identical request will be served from the cache instead of the original API. After an hour, the cache expires and the request will go to the original API for a fresh response, and that response will repopulate the cache for the next hour.

As an example, when submitting a request to OpenAI, include the header in the following manner:

```bash title="Request to be cached for an hour"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \
  --header "Authorization: Bearer $TOKEN" \
  --header 'Content-Type: application/json' \
  --header 'cf-aig-cache-ttl: 3600' \
  --data ' {
   		 "model": "gpt-4o-mini",
   		 "messages": [
   			 {
   				 "role": "user",
   				 "content": "how to build a wooden spoon in 3 short steps? give as short as answer as possible"
   			 }
   		 ]
   	 }
'
```

### Custom cache key (cf-aig-cache-key)

Custom cache keys let you override the default cache key in order to precisely set the cacheability setting for any resource. To override the default cache key, you can use the header **cf-aig-cache-key**.

When you use the **cf-aig-cache-key** header for the first time, you will receive a response from the provider. Subsequent requests with the same header will return the cached response. If the **cf-aig-cache-ttl** header is used, responses will be cached according to the specified Cache Time To Live. Otherwise, responses will be cached according to the cache settings in the dashboard. If caching is not enabled for the gateway, responses will be cached for 5 minutes by default.

As an example, when submitting a request to OpenAI, include the header in the following manner:

```bash title="Request with custom cache key"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \
  --header 'Authorization: Bearer {openai_token}' \
  --header 'Content-Type: application/json' \
  --header 'cf-aig-cache-key: responseA' \
  --data ' {
   		 "model": "gpt-4o-mini",
   		 "messages": [
   			 {
   				 "role": "user",
   				 "content": "how to build a wooden spoon in 3 short steps? give as short as answer as possible"
   			 }
   		 ]
   	 }
'
```

:::caution[AI Gateway caching behavior]
Cache in AI Gateway is volatile. If two identical requests are sent simultaneously, the first request may not cache in time for the second request to use it, which may result in the second request retrieving data from the original source.
:::

---

# Custom costs

URL: https://developers.cloudflare.com/ai-gateway/configuration/custom-costs/

import { TabItem, Tabs } from "~/components";

AI Gateway allows you to set custom costs at the request level. By using this feature, the cost metrics can accurately reflect your unique pricing, overriding the default or public model costs.

:::note[Note]

Custom costs will only apply to requests that pass tokens in their response. Requests without token information will not have costs calculated.

:::

## Custom cost

To add custom costs to your API requests, use the `cf-aig-custom-cost` header. This header enables you to specify the cost per token for both input (tokens sent) and output (tokens received).

- **per_token_in**: The negotiated input token cost (per token).
- **per_token_out**: The negotiated output token cost (per token).

There is no limit to the number of decimal places you can include, ensuring precise cost calculations, regardless of how small the values are.

Custom costs will appear in the logs with an underline, making it easy to identify when custom pricing has been applied.

In this example, if you have a negotiated price of $1 per million input tokens and $2 per million output tokens, include the `cf-aig-custom-cost` header as shown below.

```bash title="Request with custom cost"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \
  --header "Authorization: Bearer $TOKEN" \
  --header 'Content-Type: application/json' \
  --header 'cf-aig-custom-cost: {"per_token_in":0.000001,"per_token_out":0.000002}' \
  --data ' {
        "model": "gpt-4o-mini",
        "messages": [
          {
            "role": "user",
            "content": "When is Cloudflareâ€™s Birthday Week?"
          }
        ]
      }'
```

:::note

If a response is served from cache (cache hit), the cost is always `0`, even if you specified a custom cost. Custom costs only apply when the request reaches the model provider.
:::

---

# Custom metadata

URL: https://developers.cloudflare.com/ai-gateway/configuration/custom-metadata/

Custom metadata in AI Gateway allows you to tag requests with user IDs or other identifiers, enabling better tracking and analysis of your requests. Metadata values can be strings, numbers, or booleans, and will appear in your logs, making it easy to search and filter through your data.

## Key Features

* **Custom Tagging**: Add user IDs, team names, test indicators, and other relevant information to your requests.
* **Enhanced Logging**: Metadata appears in your logs, allowing for detailed inspection and troubleshooting.
* **Search and Filter**: Use metadata to efficiently search and filter through logged requests.

:::note


AI Gateway allows you to pass up to five custom metadata entries per request. If more than five entries are provided, only the first five will be saved; additional entries will be ignored. Ensure your custom metadata is limited to five entries to avoid unprocessed or lost data.

:::

## Supported Metadata Types

* String
* Number
* Boolean

:::note


Objects are not supported as metadata values.


:::

## Implementations

### Using cURL

To include custom metadata in your request using cURL:

```bash
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \
  --header 'Authorization: Bearer {api_token}' \
  --header 'Content-Type: application/json' \
  --header 'cf-aig-metadata: {"team": "AI", "user": 12345, "test":true}' \
  --data '{"model": "gpt-4o", "messages": [{"role": "user", "content": "What should I eat for lunch?"}]}'
```

### Using SDK

To include custom metadata in your request using the OpenAI SDK:

```javascript
import OpenAI from "openai";

export default {
 async fetch(request, env, ctx) {
   const openai = new OpenAI({
     apiKey: env.OPENAI_API_KEY,
     baseURL: "https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai",
   });

   try {
     const chatCompletion = await openai.chat.completions.create(
       {
         model: "gpt-4o",
         messages: [{ role: "user", content: "What should I eat for lunch?" }],
         max_tokens: 50,
       },
       {
         headers: {
           "cf-aig-metadata": JSON.stringify({
             user: "JaneDoe",
             team: 12345,
             test: true
           }),
         },
       }
     );

     const response = chatCompletion.choices[0].message;
     return new Response(JSON.stringify(response));
   } catch (e) {
     console.log(e);
     return new Response(e);
   }
 },
};
```

### Using Binding

To include custom metadata in your request using [Bindings](/workers/runtime-apis/bindings/):

```javascript
export default {
 async fetch(request, env, ctx) {
   const aiResp = await env.AI.run(
       '@cf/mistral/mistral-7b-instruct-v0.1',
       { prompt: 'What should I eat for lunch?' },
       { gateway: { id: 'gateway_id', metadata: { "team": "AI", "user": 12345, "test": true} } }
   );

   return new Response(aiResp);
 },
};
```

---

# Fallbacks

URL: https://developers.cloudflare.com/ai-gateway/configuration/fallbacks/

import { Render } from "~/components";

Specify model or provider fallbacks with your [Universal endpoint](/ai-gateway/providers/universal/) to handle request failures and ensure reliability.

Cloudflare can trigger your fallback provider in response to [request errors](#request-failures) or [predetermined request timeouts](/ai-gateway/configuration/request-handling#request-timeouts). The [response header `cf-aig-step`](#response-headercf-aig-step) indicates which step successfully processed the request.

## Request failures

By default, Cloudflare triggers your fallback if a model request returns an error.

### Example

In the following example, a request first goes to the [Workers AI](/workers-ai/) Inference API. If the request fails, it falls back to OpenAI. The response header `cf-aig-step` indicates which provider successfully processed the request.

1. Sends a request to Workers AI Inference API.
2. If that request fails, proceeds to OpenAI.

```mermaid
graph TD
    A[AI Gateway] --> B[Request to Workers AI Inference API]
    B -->|Success| C[Return Response]
    B -->|Failure| D[Request to OpenAI API]
    D --> E[Return Response]
```

<br />

You can add as many fallbacks as you need, just by adding another object in the array.

<Render file="universal-gateway-example" />

## Response header(cf-aig-step)

When using the [Universal endpoint](/ai-gateway/providers/universal/) with fallbacks, the response header `cf-aig-step` indicates which model successfully processed the request by returning the step number. This header provides visibility into whether a fallback was triggered and which model ultimately processed the response.

- `cf-aig-step:0` â€“ The first (primary) model was used successfully.
- `cf-aig-step:1` â€“ The request fell back to the second model.
- `cf-aig-step:2` â€“ The request fell back to the third model.
- Subsequent steps â€“ Each fallback increments the step number by 1.

---

# Configuration

URL: https://developers.cloudflare.com/ai-gateway/configuration/

import { DirectoryListing } from "~/components";

Configure your AI Gateway with multiple options and customizations.

<DirectoryListing />

---

# Manage gateways

URL: https://developers.cloudflare.com/ai-gateway/configuration/manage-gateway/

import { Render } from "~/components"

You have several different options for managing an AI Gateway.

## Create gateway

<Render file="create-gateway" />

## Edit gateway

<Render file="edit-gateway" />

:::note


For more details about what settings are available for editing, refer to [Configuration](/ai-gateway/configuration/).


:::

## Delete gateway

Deleting your gateway is permanent and can not be undone.

<Render file="delete-gateway" />

---

# Rate limiting

URL: https://developers.cloudflare.com/ai-gateway/configuration/rate-limiting/

import { TabItem, Tabs } from "~/components";

Rate limiting controls the traffic that reaches your application, which prevents expensive bills and suspicious activity.

## Parameters

You can define rate limits as the number of requests that get sent in a specific time frame. For example, you can limit your application to 100 requests per 60 seconds.

You can also select if you would like a **fixed** or **sliding** rate limiting technique. With rate limiting, we allow a certain number of requests within a window of time. For example, if it is a fixed rate, the window is based on time, so there would be no more than `x` requests in a ten minute window. If it is a sliding rate, there would be no more than `x` requests in the last ten minutes.

To illustrate this, let us say you had a limit of ten requests per ten minutes, starting at 12:00. So the fixed window is 12:00-12:10, 12:10-12:20, and so on. If you sent ten requests at 12:09 and ten requests at 12:11, all 20 requests would be successful in a fixed window strategy. However, they would fail in a sliding window strategy since there were more than ten requests in the last ten minutes.

## Handling rate limits

When your requests exceed the allowed rate, you'll encounter rate limiting. This means the server will respond with a `429 Too Many Requests` status code and your request won't be processed. 

## Default configuration

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To set the default rate limiting configuration in the dashboard:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **AI** > **AI Gateway**.
3. Go to **Settings**.
4. Enable **Rate-limiting**.
5. Adjust the rate, time period, and rate limiting method as desired.

</TabItem> <TabItem label="API">

To set the default rate limiting configuration using the API:

1. [Create an API token](/fundamentals/api/get-started/create-token/) with the following permissions:

- `AI Gateway - Read`
- `AI Gateway - Edit`

2. Get your [Account ID](/fundamentals/setup/find-account-and-zone-ids/).
3. Using that API token and Account ID, send a [`POST` request](/api/resources/ai_gateway/methods/create/) to create a new Gateway and include a value for the `rate_limiting_interval`, `rate_limiting_limit`, and `rate_limiting_technique`.

</TabItem> </Tabs>

This rate limiting behavior will be uniformly applied to all requests for that gateway.

---

# Request handling

URL: https://developers.cloudflare.com/ai-gateway/configuration/request-handling/

import { Render, Aside } from "~/components";

Your AI gateway supports different strategies for handling requests to providers, which allows you to manage AI interactions effectively and ensure your applications remain responsive and reliable.

## Request timeouts

A request timeout allows you to trigger fallbacks or a retry if a provider takes too long to respond.

These timeouts help:

- Improve user experience, by preventing users from waiting too long for a response
- Proactively handle errors, by detecting unresponsive providers and triggering a fallback option

Request timeouts can be set on a Universal Endpoint or directly on a request to any provider.

### Definitions

A timeout is set in milliseconds. Additionally, the timeout is based on when the first part of the response comes back. As long as the first part of the response returns within the specified timeframe - such as when streaming a response - your gateway will wait for the response.

### Configuration

#### Universal Endpoint

If set on a [Universal Endpoint](/ai-gateway/providers/universal/), a request timeout specifies the timeout duration for requests and triggers a fallback.

For a Universal Endpoint, configure the timeout value by setting a `requestTimeout` property within the provider-specific `config` object. Each provider can have a different `requestTimeout` value for granular customization.

```bash title="Provider-level config" {11-13} collapse={15-48}
curl 'https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}' \
	--header 'Content-Type: application/json' \
	--data '[
    {
        "provider": "workers-ai",
        "endpoint": "@cf/meta/llama-3.1-8b-instruct",
        "headers": {
            "Authorization": "Bearer {cloudflare_token}",
            "Content-Type": "application/json"
        },
        "config": {
            "requestTimeout": 1000
        },
        "query": {
            "messages": [
                {
                    "role": "system",
                    "content": "You are a friendly assistant"
                },
                {
                    "role": "user",
                    "content": "What is Cloudflare?"
                }
            ]
        }
    },
    {
        "provider": "workers-ai",
        "endpoint": "@cf/meta/llama-3.1-8b-instruct-fast",
        "headers": {
            "Authorization": "Bearer {cloudflare_token}",
            "Content-Type": "application/json"
        },
        "query": {
            "messages": [
                {
                    "role": "system",
                    "content": "You are a friendly assistant"
                },
                {
                    "role": "user",
                    "content": "What is Cloudflare?"
                }
            ]
        },
				"config": {
            "requestTimeout": 3000
        },
    }
]'
```

#### Direct provider

If set on a [provider](/ai-gateway/providers/) request, request timeout specifies the timeout duration for a request and - if exceeded - returns an error.

For a provider-specific endpoint, configure the timeout value by adding a `cf-aig-request-timeout` header.

```bash title="Provider-specific endpoint example" {4}
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/workers-ai/@cf/meta/llama-3.1-8b-instruct \
 --header 'Authorization: Bearer {cf_api_token}' \
 --header 'Content-Type: application/json' \
 --header 'cf-aig-request-timeout: 5000'
 --data '{"prompt": "What is Cloudflare?"}'
```

---

## Request retries

AI Gateway also supports automatic retries for failed requests, with a maximum of five retry attempts.

This feature improves your application's resiliency, ensuring you can recover from temporary issues without manual intervention.

Request timeouts can be set on a Universal Endpoint or directly on a request to any provider.

### Definitions

With request retries, you can adjust a combination of three properties:

- Number of attempts (maximum of 5 tries)
- How long before retrying (in milliseconds, maximum of 5 seconds)
- Backoff method (constant, linear, or exponential)

On the final retry attempt, your gateway will wait until the request completes, regardless of how long it takes.

### Configuration

#### Universal endpoint

If set on a [Universal Endpoint](/ai-gateway/providers/universal/), a request retry will automatically retry failed requests up to five times before triggering any configured fallbacks.

For a Universal Endpoint, configure the retry settings with the following properties in the provider-specific `config`:

```json
config:{
	maxAttempts?: number;
	retryDelay?: number;
	backoff?: "constant" | "linear" | "exponential";
}
```

As with the [request timeout](/ai-gateway/configuration/request-handling/#universal-endpoint), each provider can have a different retry settings for granular customization.

```bash title="Provider-level config" {11-15} collapse={16-55}
curl 'https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}' \
	--header 'Content-Type: application/json' \
	--data '[
    {
        "provider": "workers-ai",
        "endpoint": "@cf/meta/llama-3.1-8b-instruct",
        "headers": {
            "Authorization": "Bearer {cloudflare_token}",
            "Content-Type": "application/json"
        },
        "config": {
            "maxAttempts": 2,
						"retryDelay": 1000,
						"backoff": "constant"
        },
        "query": {
            "messages": [
                {
                    "role": "system",
                    "content": "You are a friendly assistant"
                },
                {
                    "role": "user",
                    "content": "What is Cloudflare?"
                }
            ]
        }
    },
    {
        "provider": "workers-ai",
        "endpoint": "@cf/meta/llama-3.1-8b-instruct-fast",
        "headers": {
            "Authorization": "Bearer {cloudflare_token}",
            "Content-Type": "application/json"
        },
        "query": {
            "messages": [
                {
                    "role": "system",
                    "content": "You are a friendly assistant"
                },
                {
                    "role": "user",
                    "content": "What is Cloudflare?"
                }
            ]
        },
				"config": {
            "maxAttempts": 4,
						"retryDelay": 1000,
						"backoff": "exponential"
        },
    }
]'
```

#### Direct provider

If set on a [provider](/ai-gateway/providers/) request, a request retry will automatically retry failed requests up to five times. On the final retry attempt, your gateway will wait until the request completes, regardless of how long it takes.

For a provider-specific endpoint, configure the retry settings by adding different header values:

- `cf-aig-max-attempts` (number)
- `cf-aig-retry-delay` (number)
- `cf-aig-backoff` ("constant" | "linear" | "exponential)

---

# WebSockets API

URL: https://developers.cloudflare.com/ai-gateway/configuration/websockets-api/

The AI Gateway WebSockets API provides a single persistent connection, enabling continuous communication. By using WebSockets, you can establish a single connection for multiple AI requests, eliminating the need for repeated handshakes and TLS negotiations, which enhances performance and reduces latency. This API supports all AI providers connected to AI Gateway, including those that do not natively support WebSockets.

## When to use WebSockets?

WebSockets are long-lived TCP connections that enable bi-directional, real-time communication between client and server. Unlike HTTP connections, which require repeated handshakes for each request, WebSockets maintain the connection, supporting continuous data exchange with reduced overhead. WebSockets are ideal for applications needing low-latency, real-time data, such as voice assistants.

## Key benefits

- **Reduced Overhead**: Avoid overhead of repeated handshakes and TLS negotiations by maintaining a single, persistent connection.
- **Provider Compatibility**: Works with all AI providers in AI Gateway. Even if your chosen provider does not support WebSockets, we handle it for you, managing the requests to your preferred AI provider.

## Set up WebSockets API

1. Generate an AI Gateway token with appropriate AI Gateway Run and opt in to using an authenticated gateway.
2. Modify your Universal Endpoint URL by replacing `https://` with `wss://` to initiate a WebSocket connection:
   ```
   wss://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}
   ```
3. Open a WebSocket connection authenticated with a Cloudflare token with the AI Gateway Run permission.

:::note
Alternatively, we also support authentication via the `sec-websocket-protocol` header if you are using a browser WebSocket.
:::

## Example request

```javascript
import WebSocket from "ws";

const ws = new WebSocket(
	"wss://gateway.ai.cloudflare.com/v1/my-account-id/my-gateway/",
	{
		headers: {
			"cf-aig-authorization": "Bearer AI_GATEWAY_TOKEN",
		},
	},
);

ws.send(
	JSON.stringify({
		type: "universal.create",
		request: {
			eventId: "my-request",
			provider: "workers-ai",
			endpoint: "@cf/meta/llama-3.1-8b-instruct",
			headers: {
				Authorization: "Bearer WORKERS_AI_TOKEN",
				"Content-Type": "application/json",
			},
			query: {
				prompt: "tell me a joke",
			},
		},
	}),
);

ws.on("message", function incoming(message) {
	console.log(message.toString());
});
```

## Example response

```json
{
	"type": "universal.created",
	"metadata": {
		"cacheStatus": "MISS",
		"eventId": "my-request",
		"logId": "01JC3R94FRD97JBCBX3S0ZAXKW",
		"step": "0",
		"contentType": "application/json"
	},
	"response": {
		"result": {
			"response": "Why was the math book sad? Because it had too many problems. Would you like to hear another one?"
		},
		"success": true,
		"errors": [],
		"messages": []
	}
}
```

## Example streaming request

For streaming requests, AI Gateway sends an initial message with request metadata indicating the stream is starting:

```json
{
	"type": "universal.created",
	"metadata": {
		"cacheStatus": "MISS",
		"eventId": "my-request",
		"logId": "01JC40RB3NGBE5XFRZGBN07572",
		"step": "0",
		"contentType": "text/event-stream"
	}
}
```

After this initial message, all streaming chunks are relayed in real-time to the WebSocket connection as they arrive from the inference provider. Only the `eventId` field is included in the metadata for these streaming chunks. The `eventId` allows AI Gateway to include a client-defined ID with each message, even in a streaming WebSocket environment.

```json
{
	"type": "universal.stream",
	"metadata": {
		"eventId": "my-request"
	},
	"response": {
		"response": "would"
	}
}
```

Once all chunks for a request have been streamed, AI Gateway sends a final message to signal the completion of the request. For added flexibility, this message includes all the metadata again, even though it was initially provided at the start of the streaming process.

```json
{
	"type": "universal.done",
	"metadata": {
		"cacheStatus": "MISS",
		"eventId": "my-request",
		"logId": "01JC40RB3NGBE5XFRZGBN07572",
		"step": "0",
		"contentType": "text/event-stream"
	}
}
```

---

# Add Human Feedback using API

URL: https://developers.cloudflare.com/ai-gateway/evaluations/add-human-feedback-api/

This guide will walk you through the steps of adding human feedback to an AI Gateway request using the Cloudflare API. You will learn how to retrieve the relevant request logs, and submit feedback using the API.

If you prefer to add human feedback via the dashboard, refer to [Add Human Feedback](/ai-gateway/evaluations/add-human-feedback/).

## 1. Create an API Token

1. [Create an API token](/fundamentals/api/get-started/create-token/) with the following permissions:

- `AI Gateway - Read`
- `AI Gateway - Edit`

2. Get your [Account ID](/fundamentals/setup/find-account-and-zone-ids/).
3. Using that API token and Account ID, send a [`POST` request](/api/resources/ai_gateway/methods/create/) to the Cloudflare API.

## 2. Using the API Token

Once you have the token, you can use it in API requests by adding it to the authorization header as a bearer token. Here is an example of how to use it in a request:

```bash
curl "https://api.cloudflare.com/client/v4/accounts/{account_id}/ai-gateway/gateways/{gateway_id}/logs" \
--header "Authorization: Bearer {your_api_token}"
```

In the request above:

- Replace `{account_id}` and `{gateway_id}` with your specific Cloudflare account and gateway details.
- Replace `{your_api_token}` with the API token you just created.

## 3. Retrieve the `cf-aig-log-id`

The `cf-aig-log-id` is a unique identifier for the specific log entry to which you want to add feedback. Below are two methods to obtain this identifier.

### Method 1: Locate the `cf-aig-log-id` in the request response

This method allows you to directly find the `cf-aig-log-id` within the header of the response returned by the AI Gateway. This is the most straightforward approach if you have access to the original API response.

The steps below outline how to do this.

1. **Make a Request to the AI Gateway**: This could be a request your application sends to the AI Gateway. Once the request is made, the response will contain various pieces of metadata.
2. **Check the Response Headers**: The response will include a header named `cf-aig-log-id`. This is the identifier you will need to submit feedback.

In the example below, the `cf-aig-log-id` is `01JADMCQQQBWH3NXZ5GCRN98DP`.

```json
{
	"status": "success",
	"headers": {
		"cf-aig-log-id": "01JADMCQQQBWH3NXZ5GCRN98DP"
	},
	"data": {
		"response": "Sample response data"
	}
}
```

### Method 2: Retrieve the `cf-aig-log-id` via API (GET request)

If you don't have the `cf-aig-log-id` in the response body or you need to access it after the fact, you can retrieve it by querying the logs using the [Cloudflare API](/api/resources/ai_gateway/subresources/logs/methods/list/).

The steps below outline how to do this.

1. **Send a GET Request to Retrieve Logs**: You can query the AI Gateway logs for a specific time frame or for a specific request. The request will return a list of logs, each containing its own `id`.
   Here is an example request:

```bash
GET https://api.cloudflare.com/client/v4/accounts/{account_id}/ai-gateway/gateways/{gateway_id}/logs
```

Replace `{account_id}` and `{gateway_id}` with your specific account and gateway details.

2. **Search for the Relevant Log**: In the response from the GET request, locate the specific log entry for which you would like to submit feedback. Each log entry will include the `id`.

In the example below, the `id` is `01JADMCQQQBWH3NXZ5GCRN98DP`.

```json
{
	"result": [
		{
			"id": "01JADMCQQQBWH3NXZ5GCRN98DP",
			"cached": true,
			"created_at": "2019-08-24T14:15:22Z",
			"custom_cost": true,
			"duration": 0,
			"id": "string",
			"metadata": "string",
			"model": "string",
			"model_type": "string",
			"path": "string",
			"provider": "string",
			"request_content_type": "string",
			"request_type": "string",
			"response_content_type": "string",
			"status_code": 0,
			"step": 0,
			"success": true,
			"tokens_in": 0,
			"tokens_out": 0
		}
	],
	"result_info": {
		"count": 0,
		"max_cost": 0,
		"max_duration": 0,
		"max_tokens_in": 0,
		"max_tokens_out": 0,
		"max_total_tokens": 0,
		"min_cost": 0,
		"min_duration": 0,
		"min_tokens_in": 0,
		"min_tokens_out": 0,
		"min_total_tokens": 0,
		"page": 0,
		"per_page": 0,
		"total_count": 0
	},
	"success": true
}
```

### Method 3: Retrieve the `cf-aig-log-id` via a binding

You can also retrieve the `cf-aig-log-id` using a binding, which streamlines the process. Here's how to retrieve the log ID directly:

```js
const resp = await env.AI.run('@cf/meta/llama-3-8b-instruct', {
		prompt: 'tell me a joke'
}, {
		gateway: {
				id: 'my_gateway_id'
		}
})

const myLogId = env.AI.aiGatewayLogId
```

:::note[Note:]


The `aiGatewayLogId` property, will only hold the last inference call log id.


:::

## 4. Submit feedback via PATCH request

Once you have both the API token and the `cf-aig-log-id`, you can send a PATCH request to submit feedback. Use the following URL format, replacing the `{account_id}`, `{gateway_id}`, and `{log_id}` with your specific details:

```bash
PATCH https://api.cloudflare.com/client/v4/accounts/{account_id}/ai-gateway/gateways/{gateway_id}/logs/{log_id}
```

Add the following in the request body to submit positive feedback:

```json
{
	"feedback": 1
}
```

Add the following in the request body to submit negative feedback:

```json
{
	"feedback": -1
}
```

## 5. Verify the feedback submission

You can verify the feedback submission in two ways:

- **Through the [Cloudflare dashboard ](https://dash.cloudflare.com)**: check the updated feedback on the AI Gateway interface.
- **Through the API**: Send another GET request to retrieve the updated log entry and confirm the feedback has been recorded.

---

# Add human feedback using Worker Bindings

URL: https://developers.cloudflare.com/ai-gateway/evaluations/add-human-feedback-bindings/

This guide explains how to provide human feedback for AI Gateway evaluations using Worker bindings.

## 1. Run an AI Evaluation

Start by sending a prompt to the AI model through your AI Gateway.

```javascript
const resp = await env.AI.run(
	"@cf/meta/llama-3.1-8b-instruct",
	{
		prompt: "tell me a joke",
	},
	{
		gateway: {
			id: "my-gateway",
		},
	},
);

const myLogId = env.AI.aiGatewayLogId;
```

Let the user interact with or evaluate the AI response. This interaction will inform the feedback you send back to the AI Gateway.

## 2. Send Human Feedback

Use the [`patchLog()`](/ai-gateway/integrations/worker-binding-methods/#31-patchlog-send-feedback) method to provide feedback for the AI evaluation.

```javascript
await env.AI.gateway("my-gateway").patchLog(myLogId, {
	feedback: 1, // all fields are optional; set values that fit your use case
	score: 100,
	metadata: {
		user: "123", // Optional metadata to provide additional context
	},
});
```

## Feedback parameters explanation

- `feedback`: is either `-1` for negative or `1` to positive, `0` is considered not evaluated.
- `score`: A number between 0 and 100.
- `metadata`: An object containing additional contextual information.

### patchLog: Send Feedback

The `patchLog` method allows you to send feedback, score, and metadata for a specific log ID. All object properties are optional, so you can include any combination of the parameters:

```javascript
gateway.patchLog("my-log-id", {
	feedback: 1,
	score: 100,
	metadata: {
		user: "123",
	},
});
```

Returns: `Promise<void>` (Make sure to `await` the request.)

---

# Add Human Feedback using Dashboard

URL: https://developers.cloudflare.com/ai-gateway/evaluations/add-human-feedback/

Human feedback is a valuable metric to assess the performance of your AI models. By incorporating human feedback, you can gain deeper insights into how the model's responses are perceived and how well it performs from a user-centric perspective. This feedback can then be used in evaluations to calculate performance metrics, driving optimization and ultimately enhancing the reliability, accuracy, and efficiency of your AI application.

Human feedback measures the performance of your dataset based on direct human input. The metric is calculated as the percentage of positive feedback (thumbs up) given on logs, which are annotated in the Logs tab of the Cloudflare dashboard. This feedback helps refine model performance by considering real-world evaluations of its output.

This tutorial will guide you through the process of adding human feedback to your evaluations in AI Gateway using the [Cloudflare dashboard](https://dash.cloudflare.com/).

On the next guide, you can [learn how to add human feedback via the API](/ai-gateway/evaluations/add-human-feedback-api/).

## 1. Log in to the dashboard

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **AI** > **AI Gateway**.

## 2. Access the Logs tab

1. Go to **Logs**.
2. The Logs tab displays all logs associated with your datasets. These logs show key information, including:
   - Timestamp: When the interaction occurred.
   - Status: Whether the request was successful, cached, or failed.
   - Model: The model used in the request.
   - Tokens: The number of tokens consumed by the response.
   - Cost: The cost based on token usage.
   - Duration: The time taken to complete the response.
   - Feedback: Where you can provide human feedback on each log.

## 3. Provide human feedback

1. Click on the log entry you want to review. This expands the log, allowing you to see more detailed information.
2. In the expanded log, you can view additional details such as:
   - The user prompt.
   - The model response.
   - HTTP response details.
   - Endpoint information.
3. You will see two icons:
   - Thumbs up: Indicates positive feedback.
   - Thumbs down: Indicates negative feedback.
4. Click either the thumbs up or thumbs down icon based on how you rate the model response for that particular log entry.

## 4. Evaluate human feedback

After providing feedback on your logs, it becomes a part of the evaluation process.

When you run an evaluation (as outlined in the [Set Up Evaluations](/ai-gateway/evaluations/set-up-evaluations/) guide), the human feedback metric will be calculated based on the percentage of logs that received thumbs-up feedback.

:::note[Note]

You need to select human feedback as an evaluator to receive its metrics.

:::

## 5. Review results

After running the evaluation, review the results on the Evaluations tab.
You will be able to see the performance of the model based on cost, speed, and now human feedback, represented as the percentage of positive feedback (thumbs up).

The human feedback score is displayed as a percentage, showing the distribution of positively rated responses from the database.

For more information on running evaluations, refer to the documentation [Set Up Evaluations](/ai-gateway/evaluations/set-up-evaluations/).

---

# Evaluations

URL: https://developers.cloudflare.com/ai-gateway/evaluations/

Understanding your application's performance is essential for optimization. Developers often have different priorities, and finding the optimal solution involves balancing key factors such as cost, latency, and accuracy. Some prioritize low-latency responses, while others focus on accuracy or cost-efficiency.

AI Gateway's Evaluations provide the data needed to make informed decisions on how to optimize your AI application. Whether it's adjusting the model, provider, or prompt, this feature delivers insights into key metrics around performance, speed, and cost. It empowers developers to better understand their application's behavior, ensuring improved accuracy, reliability, and customer satisfaction.

Evaluations use datasets which are collections of logs stored for analysis. You can create datasets by applying filters in the Logs tab, which help narrow down specific logs for evaluation.

Our first step toward comprehensive AI evaluations starts with human feedback (currently in open beta). We will continue to build and expand AI Gateway with additional evaluators.

[Learn how to set up an evaluation](/ai-gateway/evaluations/set-up-evaluations/) including creating datasets, selecting evaluators, and running the evaluation process.

---

# Set up Evaluations

URL: https://developers.cloudflare.com/ai-gateway/evaluations/set-up-evaluations/

This guide walks you through the process of setting up an evaluation in AI Gateway. These steps are done in the [Cloudflare dashboard](https://dash.cloudflare.com/).

## 1. Select or create a dataset

Datasets are collections of logs stored for analysis that can be used in an evaluation. You can create datasets by applying filters in the Logs tab. Datasets will update automatically based on the set filters.

### Set up a dataset from the Logs tab

1. Apply filters to narrow down your logs. Filter options include provider, number of tokens, request status, and more.
2. Select **Create Dataset** to store the filtered logs for future analysis.

You can manage datasets by selecting **Manage datasets** from the Logs tab.

:::note[Note]

Please keep in mind that datasets currently use `AND` joins, so there can only be one item per filter (for example, one model or one provider). Future updates will allow more flexibility in dataset creation.

:::

### List of available filters

| Filter category | Filter options                                               | Filter by description                     |
| --------------- | ------------------------------------------------------------ | ----------------------------------------- |
| Status          | error, status                                                | error type or status.                     |
| Cache           | cached, not cached                                           | based on whether they were cached or not. |
| Provider        | specific providers                                           | the selected AI provider.                 |
| AI Models       | specific models                                              | the selected AI model.                    |
| Cost            | less than, greater than                                      | cost, specifying a threshold.             |
| Request type    | Universal, Workers AI Binding, WebSockets                    | the type of request.                      |
| Tokens          | Total tokens, Tokens In, Tokens Out                          | token count (less than or greater than).  |
| Duration        | less than, greater than                                      | request duration.                         |
| Feedback        | equals, does not equal (thumbs up, thumbs down, no feedback) | feedback type.                            |
| Metadata Key    | equals, does not equal                                       | specific metadata keys.                   |
| Metadata Value  | equals, does not equal                                       | specific metadata values.                 |
| Log ID          | equals, does not equal                                       | a specific Log ID.                        |
| Event ID        | equals, does not equal                                       | a specific Event ID.                      |

## 2. Select evaluators

After creating a dataset, choose the evaluation parameters:

- Cost: Calculates the average cost of inference requests within the dataset (only for requests with [cost data](/ai-gateway/observability/costs/)).
- Speed: Calculates the average duration of inference requests within the dataset.
- Performance:
  - Human feedback: measures performance based on human feedback, calculated by the % of thumbs up on the logs, annotated from the Logs tab.

:::note[Note]

Additional evaluators will be introduced in future updates to expand performance analysis capabilities.

:::

## 3. Name, review, and run the evaluation

1. Create a unique name for your evaluation to reference it in the dashboard.
2. Review the selected dataset and evaluators.
3. Select **Run** to start the process.

## 4. Review and analyze results

Evaluation results will appear in the Evaluations tab. The results show the status of the evaluation (for example, in progress, completed, or error). Metrics for the selected evaluators will be displayed, excluding any logs with missing fields. You will also see the number of logs used to calculate each metric.

While datasets automatically update based on filters, evaluations do not. You will have to create a new evaluation if you want to evaluate new logs.

Use these insights to optimize based on your application's priorities. Based on the results, you may choose to:

- Change the model or [provider](/ai-gateway/providers/)
- Adjust your prompts
- Explore further optimizations, such as setting up [Retrieval Augmented Generation (RAG)](/reference-architecture/diagrams/ai/ai-rag/)

---

# Guardrails

URL: https://developers.cloudflare.com/ai-gateway/guardrails/

import { CardGrid, LinkTitleCard, YouTube } from "~/components";

Guardrails help you deploy AI applications safely by intercepting and evaluating both user prompts and model responses for harmful content. Acting as a proxy between your application and [model providers](/ai-gateway/providers/) (such as OpenAI, Anthropic, DeepSeek, and others), AI Gateway's Guardrails ensure a consistent and secure experience across your entire AI ecosystem.

Guardrails proactively monitor interactions between users and AI models, giving you:

- **Consistent moderation**: Uniform moderation layer that works across models and providers.
- **Enhanced safety and user trust**: Proactively protect users from harmful or inappropriate interactions.
- **Flexibility and control over allowed content**: Specify which categories to monitor and choose between flagging or outright blocking.
- **Auditing and compliance capabilities**: Receive updates on evolving regulatory requirements with logs of user prompts, model responses, and enforced guardrails.

## Video demo

<YouTube id="Its1H0jTxrQ" />

## How Guardrails work

AI Gateway inspects all interactions in real time by evaluating content against predefined safety parameters. Guardrails work by:

1. Intercepting interactions:
   AI Gateway proxies requests and responses, sitting between the user and the AI model.

2. Inspecting content:

   - User prompts: AI Gateway checks prompts against safety parameters (for example, violence, hate, or sexual content). Based on your settings, prompts can be flagged or blocked before reaching the model.
   - Model responses: Once processed, the AI model response is inspected. If hazardous content is detected, it can be flagged or blocked before being delivered to the user.

3. Applying actions:
   Depending on your configuration, flagged content is logged for review, while blocked content is prevented from proceeding.

## Related resource

- [Cloudflare Blog: Keep AI interactions secure and risk-free with Guardrails in AI Gateway](https://blog.cloudflare.com/guardrails-in-ai-gateway/)

---

# Set up Guardrails

URL: https://developers.cloudflare.com/ai-gateway/guardrails/set-up-guardrail/

Add Guardrails to any gateway to start evaluating and potentially modifying responses.

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **AI** > **AI Gateway**.
3. Select a gateway.
4. Go to **Guardrails**.
5. Switch the toggle to **On**.
6. To customize categories, select **Change** > **Configure specific categories**.
7. Update your choices for how Guardrails works on specific prompts or responses (**Flag**, **Ignore**, **Block**).
   - For **Prompts**: Guardrails will evaluate and transform incoming prompts based on your security policies.
   - For **Responses**: Guardrails will inspect the model's responses to ensure they meet your content and formatting guidelines.
8. Select **Save**.

:::note[Usage considerations]
For additional details about how to implement Guardrails, refer to [Usage considerations](/ai-gateway/guardrails/usage-considerations/).
:::

## Viewing Guardrail Results in Logs

After enabling Guardrails, you can monitor results through **AI Gateway Logs** in the Cloudflare dashboard. Guardrail logs are marked with a **green shield icon**, and each logged request includes an `eventID`, which links to its corresponding Guardrail evaluation log(s) for easy tracking. Logs are generated for all requests, including those that **pass** Guardrail checks.

---

# Supported model types

URL: https://developers.cloudflare.com/ai-gateway/guardrails/supported-model-types/

AI Gateway's Guardrails detects the type of AI model being used and applies safety checks accordingly:

- **Text generation models**: Both prompts and responses are evaluated.
- **Embedding models**: Only the prompt is evaluated, as the response consists of numerical embeddings, which are not meaningful for moderation.
- **Unknown models**: If the model type cannot be determined, only the prompt is evaluated, while the response bypass Guardrails.

---

# Usage considerations

URL: https://developers.cloudflare.com/ai-gateway/guardrails/usage-considerations/

Guardrails currently uses [Llama Guard 3 8B](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) on [Workers AI](/workers-ai/) to perform content evaluations. The underlying model may be updated in the future, and we will reflect those changes within Guardrails.

Since Guardrails runs on Workers AI, enabling it incurs usage on Workers AI. You can monitor usage through the Workers AI Dashboard.

## Additional considerations

- **Model availability**: If at least one hazard category is set to `block`, but AI Gateway is unable to receive a response from Workers AI, the request will be blocked. Conversely, if a hazard category is set to `flag` and AI Gateway cannot obtain a response from Workers AI, the request will proceed without evaluation. This approach prioritizes availability, allowing requests to continue even when content evaluation is not possible.
- **Latency impact**: Enabling Guardrails adds some latency. Enabling Guardrails introduces additional latency to requests. Typically, evaluations using Llama Guard 3 8B on Workers AI add approximately 500 milliseconds per request. However, larger requests may experience increased latency, though this increase is not linear. Consider this when balancing safety and performance.
- **Handling long content**: When evaluating long prompts or responses, Guardrails automatically segments the content into smaller chunks, processing each through separate Guardrail requests. This approach ensures comprehensive moderation but may result in increased latency for longer inputs.
- **Supported languages**: Llama Guard 3.3 8B supports content safety classification in the following languages: English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai.  


:::note

Llama Guard is provided as-is without any representations, warranties, or guarantees. Any rules or examples contained in blogs, developer docs, or other reference materials are provided for informational purposes only. You acknowledge and understand that you are responsible for the results and outcomes of your use of AI Gateway.

:::

---

# Workers AI

URL: https://developers.cloudflare.com/ai-gateway/integrations/aig-workers-ai-binding/

import { Render, PackageManagers, WranglerConfig } from "~/components";

This guide will walk you through setting up and deploying a Workers AI project. You will use [Workers](/workers/), an AI Gateway binding, and a large language model (LLM), to deploy your first AI-powered application on the Cloudflare global network.

## Prerequisites

<Render file="prereqs" product="workers" />

## 1. Create a Worker Project

You will create a new Worker project using the create-Cloudflare CLI (C3). C3 is a command-line tool designed to help you set up and deploy new applications to Cloudflare.

Create a new project named `hello-ai` by running:

<PackageManagers type="create" pkg="cloudflare@latest" args={"hello-ai"} />

Running `npm create cloudflare@latest` will prompt you to install the create-cloudflare package and lead you through setup. C3 will also install [Wrangler](/workers/wrangler/), the Cloudflare Developer Platform CLI.

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "hello-world",
		type: "Hello World Worker",
		lang: "TypeScript",
	}}
/>

This will create a new `hello-ai` directory. Your new `hello-ai` directory will include:

- A "Hello World" Worker at `src/index.ts`.
- A [Wrangler configuration file](/workers/wrangler/configuration/)

Go to your application directory:

```bash
cd hello-ai
```

## 2. Connect your Worker to Workers AI

You must create an AI binding for your Worker to connect to Workers AI. Bindings allow your Workers to interact with resources, like Workers AI, on the Cloudflare Developer Platform.

To bind Workers AI to your Worker, add the following to the end of your [Wrangler configuration file](/workers/wrangler/configuration/):

<WranglerConfig>

```toml title="wrangler.toml"
[ai]
binding = "AI"
```

</WranglerConfig>

Your binding is [available in your Worker code](/workers/reference/migrate-to-module-workers/#bindings-in-es-modules-format) on [`env.AI`](/workers/runtime-apis/handlers/fetch/).

You will need to have your `gateway id` for the next step. You can learn [how to create an AI Gateway in this tutorial](/ai-gateway/get-started/).

## 3. Run an inference task containing AI Gateway in your Worker

You are now ready to run an inference task in your Worker. In this case, you will use an LLM, [`llama-3.1-8b-instruct-fast`](/workers-ai/models/llama-3.1-8b-instruct-fast/), to answer a question. Your gateway ID is found on the dashboard.

Update the `index.ts` file in your `hello-ai` application directory with the following code:

```typescript title="src/index.ts" {78-81}
export interface Env {
	// If you set another name in the [Wrangler configuration file](/workers/wrangler/configuration/) as the value for 'binding',
	// replace "AI" with the variable name you defined.
	AI: Ai;
}

export default {
	async fetch(request, env): Promise<Response> {
		// Specify the gateway label and other options here
		const response = await env.AI.run(
			"@cf/meta/llama-3.1-8b-instruct-fast",
			{
				prompt: "What is the origin of the phrase Hello, World",
			},
			{
				gateway: {
					id: "GATEWAYID", // Use your gateway label here
					skipCache: true, // Optional: Skip cache if needed
				},
			},
		);

		// Return the AI response as a JSON object
		return new Response(JSON.stringify(response), {
			headers: { "Content-Type": "application/json" },
		});
	},
} satisfies ExportedHandler<Env>;
```

Up to this point, you have created an AI binding for your Worker and configured your Worker to be able to execute the Llama 3.1 model. You can now test your project locally before you deploy globally.

## 4. Develop locally with Wrangler

While in your project directory, test Workers AI locally by running [`wrangler dev`](/workers/wrangler/commands/#dev):

```bash
npx wrangler dev
```

<Render file="ai-local-usage-charges" product="workers" />

You will be prompted to log in after you run `wrangler dev`. When you run `npx wrangler dev`, Wrangler will give you a URL (most likely `localhost:8787`) to review your Worker. After you go to the URL Wrangler provides, you will see a message that resembles the following example:

````json
{
  "response": "A fascinating question!\n\nThe phrase \"Hello, World!\" originates from a simple computer program written in the early days of programming. It is often attributed to Brian Kernighan, a Canadian computer scientist and a pioneer in the field of computer programming.\n\nIn the early 1970s, Kernighan, along with his colleague Dennis Ritchie, were working on the C programming language. They wanted to create a simple program that would output a message to the screen to demonstrate the basic structure of a program. They chose the phrase \"Hello, World!\" because it was a simple and recognizable message that would illustrate how a program could print text to the screen.\n\nThe exact code was written in the 5th edition of Kernighan and Ritchie's book \"The C Programming Language,\" published in 1988. The code, literally known as \"Hello, World!\" is as follows:\n\n```
main()
{
  printf(\"Hello, World!\");
}
```\n\nThis code is still often used as a starting point for learning programming languages, as it demonstrates how to output a simple message to the console.\n\nThe phrase \"Hello, World!\" has since become a catch-all phrase to indicate the start of a new program or a small test program, and is widely used in computer science and programming education.\n\nSincerely, I'm glad I could help clarify the origin of this iconic phrase for you!"
}
````

## 5. Deploy your AI Worker

Before deploying your AI Worker globally, log in with your Cloudflare account by running:

```bash
npx wrangler login
```

You will be directed to a web page asking you to log in to the Cloudflare dashboard. After you have logged in, you will be asked if Wrangler can make changes to your Cloudflare account. Scroll down and select **Allow** to continue.

Finally, deploy your Worker to make your project accessible on the Internet. To deploy your Worker, run:

```bash
npx wrangler deploy
```

Once deployed, your Worker will be available at a URL like:

```bash
https://hello-ai.<YOUR_SUBDOMAIN>.workers.dev
```

Your Worker will be deployed to your custom [`workers.dev`](/workers/configuration/routing/workers-dev/) subdomain. You can now visit the URL to run your AI Worker.

By completing this tutorial, you have created a Worker, connected it to Workers AI through an AI Gateway binding, and successfully ran an inference task using the Llama 3.1 model.

---

# Vercel AI SDK

URL: https://developers.cloudflare.com/ai-gateway/integrations/vercel-ai-sdk/

The [Vercel AI SDK](https://sdk.vercel.ai/) is a TypeScript library for building AI applications. The SDK supports many different AI providers, tools for streaming completions, and more.

To use Cloudflare AI Gateway inside of the AI SDK, you can configure a custom "Gateway URL" for most supported providers. Below are a few examples of how it works.

## Examples

### OpenAI

If you're using the `openai` provider in AI SDK, you can create a customized setup with `createOpenAI`, passing your OpenAI-compatible AI Gateway URL:

```typescript
import { createOpenAI } from "@ai-sdk/openai";

const openai = createOpenAI({
	baseURL: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai`,
});
```

### Anthropic

If you're using the `anthropic` provider in AI SDK, you can create a customized setup with `createAnthropic`, passing your Anthropic-compatible AI Gateway URL:

```typescript
import { createAnthropic } from "@ai-sdk/anthropic";

const anthropic = createAnthropic({
	baseURL: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/anthropic`,
});
```

### Google AI Studio

If you're using the Google AI Studio provider in AI SDK, you need to append `/v1beta` to your Google AI Studio-compatible AI Gateway URL to avoid errors. The `/v1beta` path is required because Google AI Studio's API includes this in its endpoint structure, and the AI SDK sets the model name separately. This ensures compatibility with Google's API versioning.

```typescript
import { createGoogleGenerativeAI } from '@ai-sdk/google';

const google = createGoogleGenerativeAI({
	baseURL: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/google-ai-studio/v1beta`,
});
```

### Other providers

For other providers that are not listed above, you can follow a similar pattern by creating a custom instance for any AI provider, and passing your AI Gateway URL. For help finding your provider-specific AI Gateway URL, refer to the [Supported providers page](/ai-gateway/providers).

---

# AI Gateway Binding Methods

URL: https://developers.cloudflare.com/ai-gateway/integrations/worker-binding-methods/

import { Render, PackageManagers } from "~/components";

This guide provides an overview of how to use the latest Cloudflare Workers AI Gateway binding methods. You will learn how to set up an AI Gateway binding, access new methods, and integrate them into your Workers.

## Prerequisites

- Install and use the `@cloudflare/workers-types` library, version `4.20250124.3` or above.

## 1. Add an AI Binding to your Worker

To connect your Worker to Workers AI, add the following to your [Wrangler configuration file](/workers/wrangler/configuration/):

import { WranglerConfig } from "~/components";

<WranglerConfig>

```toml title="wrangler.toml"
[ai]
binding = "AI"
```

</WranglerConfig>

This configuration sets up the AI binding accessible in your Worker code as `env.AI`.

## 2. Basic Usage with Workers AI + Gateway

To perform an inference task using Workers AI and an AI Gateway, you can use the following code:

```typescript title="src/index.ts"
const resp = await env.AI.run("@cf/meta/llama-3.1-8b-instruct", {
  prompt: "tell me a joke"
}, {
  gateway: {
    id: "my-gateway"
  }
});
```

Additionally, you can access the latest request log ID with:

```typescript
const myLogId = env.AI.aiGatewayLogId;
```

## 3. Access the Gateway Binding

You can access your AI Gateway binding using the following code:

```typescript
const gateway = env.AI.gateway("my-gateway");
```

Once you have the gateway instance, you can use the following methods:

### 3.1. `patchLog`: Send Feedback

The `patchLog` method allows you to send feedback, score, and metadata for a specific log ID. All object properties are optional, so you can include any combination of the parameters:

```typescript
gateway.patchLog('my-log-id', {
  feedback: 1,
  score: 100,
  metadata: {
    user: "123"
  }
});
```

- **Returns**: `Promise<void>` (Make sure to `await` the request.)
- **Example Use Case**: Update a log entry with user feedback or additional metadata.

### 3.2. `getLog`: Read Log Details

The `getLog` method retrieves details of a specific log ID. It returns an object of type `Promise<AiGatewayLog>`. You can import the `AiGatewayLog` type from the `@cloudflare/workers-types` library.

```typescript
const log = await gateway.getLog("my-log-id");
```

- **Returns**: `Promise<AiGatewayLog>`
- **Example Use Case**: Retrieve log information for debugging or analytics.

### 3.3. `run`: Universal Requests

The `run` method allows you to execute universal requests. Users can pass either a single universal request object or an array of them. This method supports all AI Gateway providers.

Refer to the [Universal endpoint documentation](/ai-gateway/providers/universal/) for details about the available inputs.

```typescript
const resp = await gateway.run({
  provider: "workers-ai",
  endpoint: "@cf/meta/llama-3.1-8b-instruct",
  headers: {
    authorization: "Bearer my-api-token"
  },
  query: {
    prompt: "tell me a joke"
  }
});
```

- **Returns**: `Promise<Response>`
- **Example Use Case**: Perform a [universal request](/ai-gateway/providers/universal/) to any supported provider.

## Conclusion

With the new AI Gateway binding methods, you can now:

- Send feedback and update metadata with `patchLog`.
- Retrieve detailed log information using `getLog`.
- Execute universal requests to any AI Gateway provider with `run`.

These methods offer greater flexibility and control over your AI integrations, empowering you to build more sophisticated applications on the Cloudflare Workers platform.

---

# Analytics

URL: https://developers.cloudflare.com/ai-gateway/observability/analytics/

import { Render, TabItem, Tabs } from "~/components";

Your AI Gateway dashboard shows metrics on requests, tokens, caching, errors, and cost. You can filter these metrics by time.
These analytics help you understand traffic patterns, token consumption, and
potential issues across AI providers. You can
view the following analytics:

- **Requests**: Track the total number of requests processed by AI Gateway.
- **Token Usage**: Analyze token consumption across requests, giving insight into usage patterns.
- **Costs**: Gain visibility into the costs associated with using different AI providers, allowing you to track spending, manage budgets, and optimize resources.
- **Errors**: Monitor the number of errors across the gateway, helping to identify and troubleshoot issues.
- **Cached Responses**: View the percentage of responses served from cache, which can help reduce costs and improve speed.

## View analytics

<Tabs> <TabItem label="Dashboard">

<Render file="analytics-dashboard" />

</TabItem> <TabItem label="graphql">

You can use GraphQL to query your usage data outside of the AI Gateway dashboard. See the example query below. You will need to use your Cloudflare token when making the request, and change `{account_id}` to match your account tag.

```bash title="Request"
curl https://api.cloudflare.com/client/v4/graphql \
  --header 'Authorization: Bearer TOKEN \
  --header 'Content-Type: application/json' \
  --data '{
    "query": "query{\n  viewer {\n	accounts(filter: { accountTag: \"{account_id}\" }) {\n	requests: aiGatewayRequestsAdaptiveGroups(\n    	limit: $limit\n    	filter: { datetimeHour_geq: $start, datetimeHour_leq: $end }\n    	orderBy: [datetimeMinute_ASC]\n  	) {\n    	count,\n    	dimensions {\n        	model,\n        	provider,\n        	gateway,\n        	ts: datetimeMinute\n    	}\n    	\n  	}\n    	\n	}\n  }\n}",
    "variables": {
   	 "limit": 1000,
   	 "start": "2023-09-01T10:00:00.000Z",
   	 "end": "2023-09-30T10:00:00.000Z",
   	 "orderBy": "date_ASC"
    }
}'
```

</TabItem> </Tabs>

---

# Costs

URL: https://developers.cloudflare.com/ai-gateway/observability/costs/

## Supported Providers

AI Gateway currently supports cost metrics from the following providers:

- Anthropic
- Azure OpenAI
- Cohere
- Google AI Studio
- Groq
- Mistral
- OpenAI
- Perplexity
- Replicate

Cost metrics are only available for endpoints where the models return token data and the model name in their responses.

:::note[Note]

The cost metric is an **estimation** based on the number of tokens sent and received in requests. While this metric can help you monitor and predict cost trends, refer to your providerâ€™s dashboard for the most **accurate** cost details.

:::

:::caution[Caution]

Providers may introduce new models or change their pricing. If you notice outdated cost data or are using a model not yet supported by our cost tracking, please [submit a request](https://forms.gle/8kRa73wRnvq7bxL48)

:::

## Custom costs

AI Gateway allows users to set custom costs when operating under special pricing agreements or negotiated rates. Custom costs can be applied at the request level, and when applied, they will override the default or public model costs.
For more information on configuration of custom costs, please visit the [Custom Costs](/ai-gateway/configuration/custom-costs/) configuration page.

---

# Observability

URL: https://developers.cloudflare.com/ai-gateway/observability/

import { DirectoryListing } from "~/components";

Observability is the practice of instrumenting systems to collect metrics, and logs enabling better monitoring, troubleshooting, and optimization of applications.

<DirectoryListing />

---

# Audit logs

URL: https://developers.cloudflare.com/ai-gateway/reference/audit-logs/

[Audit logs](/fundamentals/setup/account/account-security/review-audit-logs/) provide a comprehensive summary of changes made within your Cloudflare account, including those made to gateways in AI Gateway. This functionality is available on all plan types, free of charge, and is enabled by default.

## Viewing Audit Logs

To view audit logs for AI Gateway:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Go to **Manage Account** > **Audit Log**.

For more information on how to access and use audit logs, refer to [review audit logs documentation](https://developers.cloudflare.com/fundamentals/setup/account/account-security/review-audit-logs/).

## Logged Operations

The following configuration actions are logged:

| Operation       | Description                      |
| --------------- | -------------------------------- |
| gateway created | Creation of a new gateway.       |
| gateway deleted | Deletion of an existing gateway. |
| gateway updated | Edit of an existing gateway.     |

## Example Log Entry

Below is an example of an audit log entry showing the creation of a new gateway:

```json
{
 "action": {
     "info": "gateway created",
     "result": true,
     "type": "create"
 },
 "actor": {
     "email": "<ACTOR_EMAIL>",
     "id": "3f7b730e625b975bc1231234cfbec091",
     "ip": "fe32:43ed:12b5:526::1d2:13",
     "type": "user"
 },
 "id": "5eaeb6be-1234-406a-87ab-1971adc1234c",
 "interface": "UI",
 "metadata": {},
 "newValue": "",
 "newValueJson": {
     "cache_invalidate_on_update": false,
     "cache_ttl": 0,
     "collect_logs": true,
     "id": "test",
     "rate_limiting_interval": 0,
     "rate_limiting_limit": 0,
     "rate_limiting_technique": "fixed"
 },
 "oldValue": "",
 "oldValueJson": {},
 "owner": {
     "id": "1234d848c0b9e484dfc37ec392b5fa8a"
 },
 "resource": {
     "id": "89303df8-1234-4cfa-a0f8-0bd848e831ca",
     "type": "ai_gateway.gateway"
 },
 "when": "2024-07-17T14:06:11.425Z"
}
```

---

# Platform

URL: https://developers.cloudflare.com/ai-gateway/reference/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Pricing

URL: https://developers.cloudflare.com/ai-gateway/reference/pricing/

AI Gateway is available to use on all plans.

AI Gateway's core features available today are offered for free, and all it takes is a Cloudflare account and one line of code to [get started](/ai-gateway/get-started/). Core features include: dashboard analytics, caching, and rate limiting.

We will continue to build and expand AI Gateway. Some new features may be additional core features that will be free while others may be part of a premium plan. We will announce these as they become available.

You can monitor your usage in the AI Gateway dashboard.

## Persistent logs (Beta)

:::note[Note]

Billing for persistent log storage will begin on April 15, 2025. Users on paid plans can store logs beyond the included volume of 200,000 logs stored a month without being charged until this date. Users on the free plan remain limited to the 100,000 logs cap for their plan. Please ensure your stored logs are within your plan's included volume before April 14, 2025, if you do not want to be charged. 

:::

Persistent logs are available on all plans, with a free allocation for both free and paid plans. Charges for additional logs beyond those limits are based on the number of logs stored per month.

### Free allocation and overage pricing

| Plan         | Free logs stored   | Overage pricing                      |
| ------------ | ------------------ | ------------------------------------ |
| Workers Free | 100,000 logs total | N/A â€“ Upgrade to Workers Paid        |
| Workers Paid | 200,000 logs total | $8 per 100,000 logs stored per month |

Allocations are based on the total logs stored across all gateways. For guidance on managing or deleting logs, please see our [documentation](/ai-gateway/observability/logging).

For example, if you are a Workers Paid plan user storing 300,000 logs, you will be charged for the excess 100,000 logs (300,000 total logs - 200,000 free logs), resulting in an $8/month charge.

## Logpush

Logpush is only available on the Workers Paid plan.

|          | Paid plan                          |
| -------- | ---------------------------------- |
| Requests | 10 million / month, +$0.05/million |

## Fine print

Prices subject to change. If you are an Enterprise customer, reach out to your account team to confirm pricing details.

---

# Anthropic

URL: https://developers.cloudflare.com/ai-gateway/providers/anthropic/

[Anthropic](https://www.anthropic.com/) helps build reliable, interpretable, and steerable AI systems.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/anthropic
```

## Prerequisites

When making requests to Anthropic, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Anthropic API token.
- The name of the Anthropic model you want to use.

## Examples

### cURL

```bash title="Example fetch request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/anthropic/v1/messages \
 --header 'x-api-key: {anthropic_api_key}' \
 --header 'anthropic-version: 2023-06-01' \
 --header 'Content-Type: application/json' \
 --data  '{
    "model": "claude-3-opus-20240229",
    "max_tokens": 1024,
    "messages": [
      {"role": "user", "content": "What is Cloudflare?"}
    ]
  }'
```

### Use Anthropic SDK with JavaScript

If you are using the `@anthropic-ai/sdk`, you can set your endpoint like this:

```js title="JavaScript"
import Anthropic from "@anthropic-ai/sdk";

const apiKey = env.ANTHROPIC_API_KEY;
const accountId = "{account_id}";
const gatewayId = "{gateway_id}";
const baseURL = `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/anthropic`;

const anthropic = new Anthropic({
	apiKey,
	baseURL,
});

const model = "claude-3-opus-20240229";
const messages = [{ role: "user", content: "What is Cloudflare?" }];
const maxTokens = 1024;

const message = await anthropic.messages.create({
	model,
	messages,
	max_tokens: maxTokens,
});
```

---

# Limits

URL: https://developers.cloudflare.com/ai-gateway/reference/limits/

import { Render } from "~/components";

The following limits apply to gateway configurations, logs, and related features in Cloudflare's platform.

| Feature                                                          | Limit                               |
| ---------------------------------------------------------------- | ----------------------------------- |
| [Cacheable request size](/ai-gateway/configuration/caching/)     | 25 MB per request                   |
| [Cache TTL](/ai-gateway/configuration/caching/#cache-ttl-cf-aig-cache-ttl)     | 1 month               |
| [Custom metadata](/ai-gateway/configuration/custom-metadata/)    | 5 entries per request               |
| [Datasets](/ai-gateway/evaluations/set-up-evaluations/)          | 10 per gateway                      |
| Gateways                                                         | 10 per account                      |
| Gateway name length                                              | 64 characters                       |
| Log storage rate limit                                           | 500 logs per second per gateway     |
| Logs stored [paid plan](/ai-gateway/reference/pricing/)          | 10 million per gateway <sup>1</sup> |
| Logs stored [free plan](/ai-gateway/reference/pricing/)          | 100,000 per account <sup>2</sup>    |
| [Log size stored](/ai-gateway/observability/logging/)            | 10 MB per log <sup>3</sup>          |
| [Logpush jobs](/ai-gateway/observability/logging/logpush/)       | 4 per account                       |
| [Logpush size limit](/ai-gateway/observability/logging/logpush/) | 1MB per log                         |

<sup>1</sup> If you have reached 10 million logs stored per gateway, new logs
will stop being saved. To continue saving logs, you must delete older logs in
that gateway to free up space or create a new gateway. Refer to [Auto Log
Cleanup](/ai-gateway/observability/logging/#auto-log-cleanup) for more details
on how to automatically delete logs.

<sup>2</sup> If you have reached 100,000 logs stored per account, across all
gateways, new logs will stop being saved. To continue saving logs, you must
delete older logs. Refer to [Auto Log
Cleanup](/ai-gateway/observability/logging/#auto-log-cleanup) for more details
on how to automatically delete logs.

<sup>3</sup> Logs larger than 10 MB will not be stored.

<Render file="limits-increase" product="ai-gateway" />

---

# Azure OpenAI

URL: https://developers.cloudflare.com/ai-gateway/providers/azureopenai/

[Azure OpenAI](https://azure.microsoft.com/en-gb/products/ai-services/openai-service/) allows you apply natural language algorithms on your data.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/azure-openai/{resource_name}/{deployment_name}
```

## Prerequisites

When making requests to Azure OpenAI, you will need:

- AI Gateway account ID
- AI Gateway gateway name
- Azure OpenAI API key
- Azure OpenAI resource name
- Azure OpenAI deployment name (aka model name)

## URL structure

Your new base URL will use the data above in this structure: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/azure-openai/{resource_name}/{deployment_name}`. Then, you can append your endpoint and api-version at the end of the base URL, like `.../chat/completions?api-version=2023-05-15`.

## Examples

### cURL

```bash title="Example fetch request"
curl 'https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/azure-openai/{resource_name}/{deployment_name}/chat/completions?api-version=2023-05-15' \
  --header 'Content-Type: application/json' \
  --header 'api-key: {azure_api_key}' \
  --data '{
  "messages": [
    {
      "role": "user",
      "content": "What is Cloudflare?"
    }
  ]
}'
```

### Use `openai-node` with JavaScript

If you are using the `openai-node` library, you can set your endpoint like this:

```js title="JavaScript"
import OpenAI from "openai";

const resource = "xxx";
const model = "xxx";
const apiVersion = "xxx";
const apiKey = env.AZURE_OPENAI_API_KEY;
const accountId = "{account_id}";
const gatewayId = "{gateway_id}";
const baseURL = `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/azure-openai/${resource}/${model}`;

const azure_openai = new OpenAI({
	apiKey,
	baseURL,
	defaultQuery: { "api-version": apiVersion },
	defaultHeaders: { "api-key": apiKey },
});
```

---

# Amazon Bedrock

URL: https://developers.cloudflare.com/ai-gateway/providers/bedrock/

[Amazon Bedrock](https://aws.amazon.com/bedrock/) allows you to build and scale generative AI applications with foundation models.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/aws-bedrock`
```

## Prerequisites

When making requests to Amazon Bedrock, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Amazon Bedrock API token.
- The name of the Amazon Bedrock model you want to use.

## Make a request

When making requests to Amazon Bedrock, replace `https://bedrock-runtime.us-east-1.amazonaws.com/` in the URL youâ€™re currently using with `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/aws-bedrock/bedrock-runtime/us-east-1/`, then add the model you want to run at the end of the URL.

With Bedrock, you will need to sign the URL before you make requests to AI Gateway. You can try using the [`aws4fetch`](https://github.com/mhart/aws4fetch) SDK.

## Examples

### Use `aws4fetch` SDK with TypeScript

```typescript
import { AwsClient } from "aws4fetch";

interface Env {
	accessKey: string;
	secretAccessKey: string;
}

export default {
	async fetch(
		request: Request,
		env: Env,
		ctx: ExecutionContext,
	): Promise<Response> {
		// replace with your configuration
		const cfAccountId = "{account_id}";
		const gatewayName = "{gateway_id}";
		const region = "us-east-1";

		// added as secrets (https://developers.cloudflare.com/workers/configuration/secrets/)
		const accessKey = env.accessKey;
		const secretKey = env.secretAccessKey;

		const requestData = {
			inputText: "What does ethereal mean?",
		};

		const headers = {
			"Content-Type": "application/json",
		};

		// sign the original request
		const stockUrl = new URL(
			"https://bedrock-runtime.us-east-1.amazonaws.com/model/amazon.titan-embed-text-v1/invoke",
		);

		const awsClient = new AwsClient({
			accessKeyId: accessKey,
			secretAccessKey: secretKey,
			region: region,
			service: "bedrock",
		});

		const presignedRequest = await awsClient.sign(stockUrl.toString(), {
			method: "POST",
			headers: headers,
		});

		// change the signed request's host to AI Gateway
		const stockUrlSigned = new URL(presignedRequest.url);
		stockUrlSigned.host = "gateway.ai.cloudflare.com";
		stockUrlSigned.pathname = `/v1/${cfAccountId}/${gatewayName}/aws-bedrock/bedrock-runtime/${region}/model/amazon.titan-embed-text-v1/invoke`;

		// make request
		const response = await fetch(stockUrlSigned, {
			method: "POST",
			headers: presignedRequest.headers,
			body: JSON.stringify(requestData),
		});

		if (
			response.ok &&
			response.headers.get("content-type")?.includes("application/json")
		) {
			const data = await response.json();
			return new Response(JSON.stringify(data));
		} else {
			return new Response("Invalid response", { status: 500 });
		}
	},
};
```

---

# Cartesia

URL: https://developers.cloudflare.com/ai-gateway/providers/cartesia/

[Cartesia](https://docs.cartesia.ai/) provides advanced text-to-speech services with customizable voice models.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/cartesia
```

## URL Structure

When making requests to Cartesia, replace `https://api.cartesia.ai/v1` in the URL you are currently using with `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/cartesia`.

## Prerequisites

When making requests to Cartesia, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Cartesia API token.
- The model ID and voice ID for the Cartesia voice model you want to use.

## Example

### cURL

```bash title="Request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/cartesia/tts/bytes \
  --header 'Content-Type: application/json' \
  --header 'Cartesia-Version: 2024-06-10' \
  --header 'X-API-Key: {cartesia_api_token}' \
  --data '{
    "transcript": "Welcome to Cloudflare - AI Gateway!",
    "model_id": "sonic-english",
    "voice": {
        "mode": "id",
        "id": "694f9389-aac1-45b6-b726-9d9369183238"
    },
    "output_format": {
        "container": "wav",
        "encoding": "pcm_f32le",
        "sample_rate": 44100
    }
}
```

---

# Cerebras

URL: https://developers.cloudflare.com/ai-gateway/providers/cerebras/

[Cerebras](https://inference-docs.cerebras.ai/) offers developers a low-latency solution for AI model inference.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/cerebras-ai
```

## Prerequisites

When making requests to Cerebras, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Cerebras API token.
- The name of the Cerebras model you want to use.

## Examples

### cURL

```bash title="Example fetch request"
curl https://gateway.ai.cloudflare.com/v1/ACCOUNT_TAG/GATEWAY/cerebras/chat/completions \
 --header 'content-type: application/json' \
 --header 'Authorization: Bearer CEREBRAS_TOKEN' \
 --data '{
    "model": "llama3.1-8b",
    "messages": [
        {
            "role": "user",
            "content": "What is Cloudflare?"
        }
    ]
}'
```

---

# Cohere

URL: https://developers.cloudflare.com/ai-gateway/providers/cohere/

[Cohere](https://cohere.com/) build AI models designed to solve real-world business challenges.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/cohere
```

## URL structure

When making requests to [Cohere](https://cohere.com/), replace `https://api.cohere.ai/v1` in the URL youâ€™re currently using with `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/cohere`.

## Prerequisites

When making requests to Cohere, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Cohere API token.
- The name of the Cohere model you want to use.

## Examples

### cURL

```bash title="Request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/cohere/v1/chat \
  --header 'Authorization: Token {cohere_api_token}' \
  --header 'Content-Type: application/json' \
  --data '{
  "chat_history": [
    {"role": "USER", "message": "Who discovered gravity?"},
    {"role": "CHATBOT", "message": "The man who is widely credited with discovering gravity is Sir Isaac Newton"}
  ],
  "message": "What year was he born?",
  "connectors": [{"id": "web-search"}]
}'
```

### Use Cohere SDK with Python

If using the [`cohere-python-sdk`](https://github.com/cohere-ai/cohere-python), set your endpoint like this:

```js title="Python"

import cohere
import os

api_key = os.getenv('API_KEY')
account_id = '{account_id}'
gateway_id = '{gateway_id}'
base_url = f"https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/cohere/v1"

co = cohere.Client(
  api_key=api_key,
  base_url=base_url,
)

message = "hello world!"
model = "command-r-plus"

chat = co.chat(
  message=message,
  model=model
)

print(chat)

```

---

# DeepSeek

URL: https://developers.cloudflare.com/ai-gateway/providers/deepseek/

[DeepSeek](https://www.deepseek.com/) helps you build quickly with DeepSeek's advanced AI models.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/deepseek
```

## Prerequisites

When making requests to DeepSeek, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active DeepSeek AI API token.
- The name of the DeepSeek AI model you want to use.

## URL structure

Your new base URL will use the data above in this structure:

`https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/deepseek/`.

You can then append the endpoint you want to hit, for example: `chat/completions`.

So your final URL will come together as:

`https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/deepseek/chat/completions`.

## Examples

### cURL

```bash title="Example fetch request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/deepseek/chat/completions \
 --header 'content-type: application/json' \
 --header 'Authorization: Bearer DEEPSEEK_TOKEN' \
 --data '{
    "model": "deepseek-chat",
    "messages": [
        {
            "role": "user",
            "content": "What is Cloudflare?"
        }
    ]
}'
```

### Use DeepSeek with JavaScript

If you are using the OpenAI SDK, you can set your endpoint like this:

```js title="JavaScript"
import OpenAI from "openai";

const openai = new OpenAI({
	apiKey: env.DEEPSEEK_TOKEN,
	baseURL:
		"https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/deepseek",
});

try {
	const chatCompletion = await openai.chat.completions.create({
		model: "deepseek-chat",
		messages: [{ role: "user", content: "What is Cloudflare?" }],
	});

	const response = chatCompletion.choices[0].message;

	return new Response(JSON.stringify(response));
} catch (e) {
	return new Response(e);
}
```

---

# ElevenLabs

URL: https://developers.cloudflare.com/ai-gateway/providers/elevenlabs/

[ElevenLabs](https://elevenlabs.io/) offers advanced text-to-speech services, enabling high-quality voice synthesis in multiple languages.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/elevenlabs
```

## Prerequisites

When making requests to ElevenLabs, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active ElevenLabs API token.
- The model ID of the ElevenLabs voice model you want to use.

## Example

### cURL

```bash title="Request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/elevenlabs/v1/text-to-speech/JBFqnCBsd6RMkjVDRZzb?output_format=mp3_44100_128 \
  --header 'Content-Type: application/json' \
  --header 'xi-api-key: {elevenlabs_api_token}' \
  --data '{
    "text": "Welcome to Cloudflare - AI Gateway!",
    "model_id": "eleven_multilingual_v2"
}'
```

---

# Google AI Studio

URL: https://developers.cloudflare.com/ai-gateway/providers/google-ai-studio/

[Google AI Studio](https://ai.google.dev/aistudio) helps you build quickly with Google Gemini models.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/google-ai-studio
```

## Prerequisites

When making requests to Google AI Studio, you will need:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Google AI Studio API token.
- The name of the Google AI Studio model you want to use.

## URL structure

Your new base URL will use the data above in this structure: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/google-ai-studio/`.

Then you can append the endpoint you want to hit, for example: `v1/models/{model}:{generative_ai_rest_resource}`

So your final URL will come together as: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/google-ai-studio/v1/models/{model}:{generative_ai_rest_resource}`.

## Examples

### cURL

```bash title="Example fetch request"
curl "https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_name}/google-ai-studio/v1/models/gemini-1.0-pro:generateContent" \
 --header 'content-type: application/json' \
 --header 'x-goog-api-key: {google_studio_api_key}' \
 --data '{
      "contents": [
          {
            "role":"user",
            "parts": [
              {"text":"What is Cloudflare?"}
            ]
          }
        ]
      }'
```

### Use `@google/generative-ai` with JavaScript

If you are using the `@google/generative-ai` package, you can set your endpoint like this:

```js title="JavaScript example"
import { GoogleGenerativeAI } from "@google/generative-ai";

const api_token = env.GOOGLE_AI_STUDIO_TOKEN;
const account_id = "";
const gateway_name = "";

const genAI = new GoogleGenerativeAI(api_token);
const model = genAI.getGenerativeModel(
	{ model: "gemini-1.5-flash" },
	{
		baseUrl: `https://gateway.ai.cloudflare.com/v1/${account_id}/${gateway_name}/google-ai-studio`,
	},
);

await model.generateContent(["What is Cloudflare?"]);
```

---

# Grok

URL: https://developers.cloudflare.com/ai-gateway/providers/grok/

[Grok](https://docs.x.ai/docs#getting-started) is s a general purpose model that can be used for a variety of tasks, including generating and understanding text, code, and function calling.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/grok
```

## URL structure

When making requests to [Grok](https://docs.x.ai/docs#getting-started), replace `https://api.x.ai/v1` in the URL you are currently using with `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/grok`.

## Prerequisites

When making requests to Grok, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Grok API token.
- The name of the Grok model you want to use.

## Examples

### cURL

```bash title="Request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/grok/v1/chat/completions \
  --header 'content-type: application/json' \
  --header 'Authorization: Bearer {grok_api_token}' \
  --data '{
    "model": "grok-beta",
    "messages": [
        {
            "role": "user",
            "content": "What is Cloudflare?"
        }
    ]
}'
```

### Use OpenAI SDK with JavaScript

If you are using the OpenAI SDK with JavaScript, you can set your endpoint like this:

```js title="JavaScript"
import OpenAI from "openai";

const openai = new OpenAI({
	apiKey: "<api key>",
	baseURL:
		"https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/grok",
});

const completion = await openai.chat.completions.create({
	model: "grok-beta",
	messages: [
		{
			role: "system",
			content:
				"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.",
		},
		{
			role: "user",
			content: "What is the meaning of life, the universe, and everything?",
		},
	],
});

console.log(completion.choices[0].message);
```

### Use OpenAI SDK with Python

If you are using the OpenAI SDK with Python, you can set your endpoint like this:

```python title="Python"
import os
from openai import OpenAI

XAI_API_KEY = os.getenv("XAI_API_KEY")
client = OpenAI(
    api_key=XAI_API_KEY,
    base_url="https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/grok",
)

completion = client.chat.completions.create(
    model="grok-beta",
    messages=[
        {"role": "system", "content": "You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy."},
        {"role": "user", "content": "What is the meaning of life, the universe, and everything?"},
    ],
)

print(completion.choices[0].message)
```

### Use Anthropic SDK with JavaScript

If you are using the Anthropic SDK with JavaScript, you can set your endpoint like this:

```js title="JavaScript"
import Anthropic from "@anthropic-ai/sdk";

const anthropic = new Anthropic({
	apiKey: "<api key>",
	baseURL:
		"https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/grok",
});

const msg = await anthropic.messages.create({
	model: "grok-beta",
	max_tokens: 128,
	system:
		"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.",
	messages: [
		{
			role: "user",
			content: "What is the meaning of life, the universe, and everything?",
		},
	],
});

console.log(msg);
```

### Use Anthropic SDK with Python

If you are using the Anthropic SDK with Python, you can set your endpoint like this:

```python title="Python"
import os
from anthropic import Anthropic

XAI_API_KEY = os.getenv("XAI_API_KEY")
client = Anthropic(
    api_key=XAI_API_KEY,
    base_url="https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/grok",
)

message = client.messages.create(
    model="grok-beta",
    max_tokens=128,
    system="You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.",
    messages=[
        {
            "role": "user",
            "content": "What is the meaning of life, the universe, and everything?",
        },
    ],
)

print(message.content)
```

---

# Groq

URL: https://developers.cloudflare.com/ai-gateway/providers/groq/

[Groq](https://groq.com/) delivers high-speed processing and low-latency performance.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/groq
```

## URL structure

When making requests to [Groq](https://groq.com/), replace `https://api.groq.com/openai/v1` in the URL youâ€™re currently using with `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/groq`.

## Prerequisites

When making requests to Groq, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Groq API token.
- The name of the Groq model you want to use.

## Examples

### cURL

```bash title="Example fetch request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/groq/chat/completions \
  --header 'Authorization: Bearer {groq_api_key}' \
  --header 'Content-Type: application/json' \
  --data '{
    "messages": [
      {
        "role": "user",
        "content": "What is Cloudflare?"
      }
    ],
    "model": "mixtral-8x7b-32768"
}'
```

### Use Groq SDK with JavaScript

If using the [`groq-sdk`](https://www.npmjs.com/package/groq-sdk), set your endpoint like this:

```js title="JavaScript"
import Groq from "groq-sdk";

const apiKey = env.GROQ_API_KEY;
const accountId = "{account_id}";
const gatewayId = "{gateway_id}";
const baseURL = `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/groq`;

const groq = new Groq({
	apiKey,
	baseURL,
});

const messages = [{ role: "user", content: "What is Cloudflare?" }];
const model = "mixtral-8x7b-32768";

const chatCompletion = await groq.chat.completions.create({
	messages,
	model,
});
```

---

# HuggingFace

URL: https://developers.cloudflare.com/ai-gateway/providers/huggingface/

[HuggingFace](https://huggingface.co/) helps users build, deploy and train machine learning models.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/huggingface
```

## URL structure

When making requests to HuggingFace Inference API, replace `https://api-inference.huggingface.co/models/` in the URL youâ€™re currently using with `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/huggingface`. Note that the model youâ€™re trying to access should come right after, for example `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/huggingface/bigcode/starcoder`.

## Prerequisites

When making requests to HuggingFace, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active HuggingFace API token.
- The name of the HuggingFace model you want to use.

## Examples

### cURL

```bash title="Request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/huggingface/bigcode/starcoder \
  --header 'Authorization: Bearer {hf_api_token}' \
  --header 'Content-Type: application/json' \
  --data '{
    "inputs": "console.log"
}'
```

### Use HuggingFace.js library with JavaScript

If you are using the HuggingFace.js library, you can set your inference endpoint like this:

```js title="JavaScript"
import { HfInferenceEndpoint } from "@huggingface/inference";

const accountId = "{account_id}";
const gatewayId = "{gateway_id}";
const model = "gpt2";
const baseURL = `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/huggingface/${model}`;
const apiToken = env.HF_API_TOKEN;

const hf = new HfInferenceEndpoint(baseURL, apiToken);
```

---

# Model providers

URL: https://developers.cloudflare.com/ai-gateway/providers/

Here is a quick list of the providers we support:

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Mistral AI

URL: https://developers.cloudflare.com/ai-gateway/providers/mistral/

[Mistral AI](https://mistral.ai) helps you build quickly with Mistral's advanced AI models.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/mistral
```

## Prerequisites

When making requests to the Mistral AI, you will need:

- AI Gateway Account ID
- AI Gateway gateway name
- Mistral AI API token
- Mistral AI model name

## URL structure

Your new base URL will use the data above in this structure: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/mistral/`.

Then you can append the endpoint you want to hit, for example: `v1/chat/completions`

So your final URL will come together as: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/mistral/v1/chat/completions`.

## Examples

### cURL

```bash title="Example fetch request"
curl -X POST https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/mistral/v1/chat/completions \
 --header 'content-type: application/json' \
 --header 'Authorization: Bearer MISTRAL_TOKEN' \
 --data '{
    "model": "mistral-large-latest",
    "messages": [
        {
            "role": "user",
            "content": "What is Cloudflare?"
        }
    ]
}'
```

### Use `@mistralai/mistralai` package with JavaScript

If you are using the `@mistralai/mistralai` package, you can set your endpoint like this:

```js title="JavaScript example"
import { Mistral } from "@mistralai/mistralai";

const client = new Mistral({
	apiKey: MISTRAL_TOKEN,
	serverURL: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/mistral`,
});

await client.chat.create({
	model: "mistral-large-latest",
	messages: [
		{
			role: "user",
			content: "What is Cloudflare?",
		},
	],
});
```

---

# OpenAI

URL: https://developers.cloudflare.com/ai-gateway/providers/openai/

[OpenAI](https://openai.com/about/) helps you build with ChatGPT.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai
```

## URL structure

When making requests to OpenAI, replace `https://api.openai.com/v1` in the URL youâ€™re currently using with `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai`.

## Prerequisites

When making requests to OpenAI, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active OpenAI API token.
- The name of the OpenAI model you want to use.

## Examples

### cURL

```bash title="Request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \
  --header 'Authorization: Bearer {openai_token}' \
  --header 'Content-Type: application/json' \
  --data ' {
   		 "model": "gpt-4o-mini",
   		 "messages": [
        {
          "role": "user",
          "content": "What is Cloudflare"
        }
   		 ]
   	 }
'
```

### Use OpenAI SDK with JavaScript

If you are using a library like openai-node, set the `baseURL` to your OpenAI endpoint like this:

```js title="JavaScript"
import OpenAI from "openai";

const apiKey = "my api key"; // defaults to process.env["OPENAI_API_KEY"]
const accountId = "{account_id}";
const gatewayId = "{gateway_id}";
const baseURL = `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/openai`;

const openai = new OpenAI({
	apiKey,
	baseURL,
});

try {
	const model = "gpt-3.5-turbo-0613";
	const messages = [{ role: "user", content: "What is a neuron?" }];
	const maxTokens = 100;

	const chatCompletion = await openai.chat.completions.create({
		model,
		messages,
		max_tokens: maxTokens,
	});

	const response = chatCompletion.choices[0].message;

	return new Response(JSON.stringify(response));
} catch (e) {
	return new Response(e);
}
```

---

# OpenRouter

URL: https://developers.cloudflare.com/ai-gateway/providers/openrouter/

[OpenRouter](https://openrouter.ai/) is a platform that provides a unified interface for accessing and using large language models (LLMs).

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openrouter
```

## URL structure

When making requests to [OpenRouter](https://openrouter.ai/), replace `https://openrouter.ai/api/v1/chat/completions` in the URL you are currently using with `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openrouter`.

## Prerequisites

When making requests to OpenRouter, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active OpenRouter API token or a token from the original model provider.
- The name of the OpenRouter model you want to use.

## Examples

### cURL

```bash title="Request"
curl -X POST https://gateway.ai.cloudflare.com/v1/ACCOUNT_TAG/GATEWAY/openrouter/v1/chat/completions \
 --header 'content-type: application/json' \
 --header 'Authorization: Bearer OPENROUTER_TOKEN' \
 --data '{
    "model": "openai/gpt-3.5-turbo",
    "messages": [
        {
            "role": "user",
            "content": "What is Cloudflare?"
        }
    ]
}'

```

### Use OpenAI SDK with JavaScript

If you are using the OpenAI SDK with JavaScript, you can set your endpoint like this:

```js title="JavaScript"
import OpenAI from "openai";

const openai = new OpenAI({
	apiKey: env.OPENROUTER_TOKEN,
	baseURL:
		"https://gateway.ai.cloudflare.com/v1/ACCOUNT_TAG/GATEWAY/openrouter",
});

try {
	const chatCompletion = await openai.chat.completions.create({
		model: "openai/gpt-3.5-turbo",
		messages: [{ role: "user", content: "What is Cloudflare?" }],
	});

	const response = chatCompletion.choices[0].message;

	return new Response(JSON.stringify(response));
} catch (e) {
	return new Response(e);
}
```

---

# Perplexity

URL: https://developers.cloudflare.com/ai-gateway/providers/perplexity/

[Perplexity](https://www.perplexity.ai/) is an AI powered answer engine.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/perplexity-ai
```

## Prerequisites

When making requests to Perplexity, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Perplexity API token.
- The name of the Perplexity model you want to use.

## Examples

### cURL

```bash title="Example fetch request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/perplexity-ai/chat/completions \
     --header 'accept: application/json' \
     --header 'content-type: application/json' \
     --header 'Authorization: Bearer {perplexity_token}' \
     --data '{
      "model": "mistral-7b-instruct",
      "messages": [
        {
          "role": "user",
          "content": "What is Cloudflare?"
        }
      ]
    }'
```

### Use Perplexity through OpenAI SDK with JavaScript

Perplexity does not have their own SDK, but they have compatibility with the OpenAI SDK. You can use the OpenAI SDK to make a Perplexity call through AI Gateway as follows:

```js title="JavaScript"
import OpenAI from "openai";

const apiKey = env.PERPLEXITY_API_KEY;
const accountId = "{account_id}";
const gatewayId = "{gateway_id}";
const baseURL = `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/perplexity-ai`;

const perplexity = new OpenAI({
	apiKey,
	baseURL,
});

const model = "mistral-7b-instruct";
const messages = [{ role: "user", content: "What is Cloudflare?" }];
const maxTokens = 20;

const chatCompletion = await perplexity.chat.completions.create({
	model,
	messages,
	max_tokens: maxTokens,
});
```

---

# Replicate

URL: https://developers.cloudflare.com/ai-gateway/providers/replicate/

[Replicate](https://replicate.com/) runs and fine tunes open-source models.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/replicate
```

## URL structure

When making requests to Replicate, replace `https://api.replicate.com/v1` in the URL youâ€™re currently using with `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/replicate`.

## Prerequisites

When making requests to Replicate, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Replicate API token.
- The name of the Replicate model you want to use.

## Example

### cURL

```bash title="Request"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/replicate/predictions \
  --header 'Authorization: Token {replicate_api_token}' \
  --header 'Content-Type: application/json' \
  --data '{
    "input":
      {
        "prompt": "What is Cloudflare?"
      }
    }'
```

---

# Universal Endpoint

URL: https://developers.cloudflare.com/ai-gateway/providers/universal/

import { Render, Badge } from "~/components";

You can use the Universal Endpoint to contact every provider.

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}
```

AI Gateway offers multiple endpoints for each Gateway you create - one endpoint per provider, and one Universal Endpoint. The Universal Endpoint requires some adjusting to your schema, but supports additional features. Some of these features are, for example, retrying a request if it fails the first time, or configuring a [fallback model/provider](/ai-gateway/configuration/fallbacks/).

You can use the Universal endpoint to contact every provider. The payload is expecting an array of message, and each message is an object with the following parameters:

- `provider` : the name of the provider you would like to direct this message to. Can be OpenAI, workers-ai, or any of our supported providers.
- `endpoint`: the pathname of the provider API youâ€™re trying to reach. For example, on OpenAI it can be `chat/completions`, and for Workers AI this might be [`@cf/meta/llama-3.1-8b-instruct`](/workers-ai/models/llama-3.1-8b-instruct/). See more in the sections that are specific to [each provider](/ai-gateway/providers/).
- `authorization`: the content of the Authorization HTTP Header that should be used when contacting this provider. This usually starts with â€œTokenâ€ or â€œBearerâ€.
- `query`: the payload as the provider expects it in their official API.

## cURL example

<Render file="universal-gateway-example" />

The above will send a request to Workers AI Inference API, if it fails it will proceed to OpenAI. You can add as many fallbacks as you need, just by adding another JSON in the array.

## WebSockets API <Badge text="beta" variant="tip" size="small" />

The Universal Endpoint can also be accessed via a [WebSockets API](/ai-gateway/configuration/websockets-api/) which provides a single persistent connection, enabling continuous communication. This API supports all AI providers connected to AI Gateway, including those that do not natively support WebSockets.

## WebSockets example

```javascript
import WebSocket from "ws";
const ws = new WebSocket(
	"wss://gateway.ai.cloudflare.com/v1/my-account-id/my-gateway/",
	{
		headers: {
			"cf-aig-authorization": "Bearer AI_GATEWAY_TOKEN",
		},
	},
);

ws.send(
	JSON.stringify({
		type: "universal.create",
		request: {
			eventId: "my-request",
			provider: "workers-ai",
			endpoint: "@cf/meta/llama-3.1-8b-instruct",
			headers: {
				Authorization: "Bearer WORKERS_AI_TOKEN",
				"Content-Type": "application/json",
			},
			query: {
				prompt: "tell me a joke",
			},
		},
	}),
);

ws.on("message", function incoming(message) {
	console.log(message.toString());
});
```

## Header configuration hierarchy

The Universal Endpoint allows you to set fallback models or providers and customize headers for each provider or request. You can configure headers at three levels:

1. **Provider level**: Headers specific to a particular provider.
2. **Request level**: Headers included in individual requests.
3. **Gateway settings**: Default headers configured in your gateway dashboard.

Since the same settings can be configured in multiple locations, AI Gateway applies a hierarchy to determine which configuration takes precedence:

- **Provider-level headers** override all other configurations.
- **Request-level headers** are used if no provider-level headers are set.
- **Gateway-level settings** are used only if no headers are configured at the provider or request levels.

This hierarchy ensures consistent behavior, prioritizing the most specific configurations. Use provider-level and request-level headers for fine-tuned control, and gateway settings for general defaults.

## Hierarchy example

This example demonstrates how headers set at different levels impact caching behavior:

- **Request-level header**: The `cf-aig-cache-ttl` is set to `3600` seconds, applying this caching duration to the request by default.
- **Provider-level header**: For the fallback provider (OpenAI), `cf-aig-cache-ttl` is explicitly set to `0` seconds, overriding the request-level header and disabling caching for responses when OpenAI is used as the provider.

This shows how provider-level headers take precedence over request-level headers, allowing for granular control of caching behavior.

```bash
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id} \
  --header 'Content-Type: application/json' \
  --header 'cf-aig-cache-ttl: 3600' \
  --data '[
    {
      "provider": "workers-ai",
      "endpoint": "@cf/meta/llama-3.1-8b-instruct",
      "headers": {
        "Authorization": "Bearer {cloudflare_token}",
        "Content-Type": "application/json"
      },
      "query": {
        "messages": [
          {
            "role": "system",
            "content": "You are a friendly assistant"
          },
          {
            "role": "user",
            "content": "What is Cloudflare?"
          }
        ]
      }
    },
    {
      "provider": "openai",
      "endpoint": "chat/completions",
      "headers": {
        "Authorization": "Bearer {open_ai_token}",
        "Content-Type": "application/json",
        "cf-aig-cache-ttl": "0"
      },
      "query": {
        "model": "gpt-4o-mini",
        "stream": true,
        "messages": [
          {
            "role": "user",
            "content": "What is Cloudflare?"
          }
        ]
      }
    }
  ]'
```

---

# Google Vertex AI

URL: https://developers.cloudflare.com/ai-gateway/providers/vertex/

[Google Vertex AI](https://cloud.google.com/vertex-ai) enables developers to easily build and deploy enterprise ready generative AI experiences.

Below is a quick guide on how to set your Google Cloud Account:

1. Google Cloud Platform (GCP) Account

   - Sign up for a [GCP account](https://cloud.google.com/vertex-ai). New users may be eligible for credits (valid for 90 days).

2. Enable the Vertex AI API

   - Navigate to [Enable Vertex AI API](https://console.cloud.google.com/marketplace/product/google/aiplatform.googleapis.com) and activate the API for your project.

3. Apply for access to desired models.

## Endpoint

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/google-vertex-ai
```

## Prerequisites

When making requests to Google Vertex, you will need:

- AI Gateway account tag
- AI Gateway gateway name
- Google Vertex API key
- Google Vertex Project Name
- Google Vertex Region (for example, us-east4)
- Google Vertex model

## URL structure

Your new base URL will use the data above in this structure: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/google-vertex-ai/v1/projects/{project_name}/locations/{region}`.

Then you can append the endpoint you want to hit, for example: `/publishers/google/models/{model}:{generative_ai_rest_resource}`

So your final URL will come together as: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/google-vertex-ai/v1/projects/{project_name}/locations/{region}/publishers/google/models/gemini-1.0-pro-001:generateContent`

## Example

### cURL

```bash title="Example fetch request"
curl "https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/google-vertex-ai/v1/projects/{project_name}/locations/{region}/publishers/google/models/gemini-1.0-pro-001:generateContent" \
    -H "Authorization: Bearer {vertex_api_key}" \
    -H 'Content-Type: application/json' \
    -d '{
        "contents": {
          "role": "user",
          "parts": [
            {
              "text": "Tell me more about Cloudflare"
            }
          ]
        }'

```

---

# Workers AI

URL: https://developers.cloudflare.com/ai-gateway/providers/workersai/

import { Render } from "~/components";

Use AI Gateway for analytics, caching, and security on requests to [Workers AI](/workers-ai/). Workers AI integrates seamlessly with AI Gateway, allowing you to execute AI inference via API requests or through an environment binding for Workers scripts. The binding simplifies the process by routing requests through your AI Gateway with minimal setup.

## Prerequisites

When making requests to Workers AI, ensure you have the following:

- Your AI Gateway Account ID.
- Your AI Gateway gateway name.
- An active Workers AI API token.
- The name of the Workers AI model you want to use.

## REST API

To interact with a REST API, update the URL used for your request:

- **Previous**:

```txt
https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/{model_id}
```

- **New**:

```txt
https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/workers-ai/{model_id}
```

For these parameters:

- `{account_id}` is your Cloudflare [account ID](/workers-ai/get-started/rest-api/#1-get-api-token-and-account-id).
- `{gateway_id}` refers to the name of your existing [AI Gateway](/ai-gateway/get-started/#create-gateway).
- `{model_id}` refers to the model ID of the [Workers AI model](/workers-ai/models/).

## Examples

First, generate an [API token](/fundamentals/api/get-started/create-token/) with `Workers AI Read` access and use it in your request.

```bash title="Request to Workers AI llama model"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/workers-ai/@cf/meta/llama-3.1-8b-instruct \
 --header 'Authorization: Bearer {cf_api_token}' \
 --header 'Content-Type: application/json' \
 --data '{"prompt": "What is Cloudflare?"}'
```

```bash title="Request to Workers AI text classification model"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/workers-ai/@cf/huggingface/distilbert-sst-2-int8 \
  --header 'Authorization: Bearer {cf_api_token}' \
  --header 'Content-Type: application/json' \
  --data '{ "text": "Cloudflare docs are amazing!" }'
```

### OpenAI compatible endpoints

<Render file="openai-compatibility" product="workers-ai" /> <br />

```bash title="Request to OpenAI compatible endpoint"
curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/workers-ai/v1/chat/completions \
 --header 'Authorization: Bearer {cf_api_token}' \
 --header 'Content-Type: application/json' \
 --data '{
      "model": "@cf/meta/llama-3.1-8b-instruct",
      "messages": [
        {
          "role": "user",
          "content": "What is Cloudflare?"
        }
      ]
    }
'
```

## Workers Binding

You can integrate Workers AI with AI Gateway using an environment binding. To include an AI Gateway within your Worker, add the gateway as an object in your Workers AI request.

```ts
export interface Env {
	AI: Ai;
}

export default {
	async fetch(request: Request, env: Env): Promise<Response> {
		const response = await env.AI.run(
			"@cf/meta/llama-3.1-8b-instruct",
			{
				prompt: "Why should you use Cloudflare for your AI inference?",
			},
			{
				gateway: {
					id: "{gateway_id}",
					skipCache: false,
					cacheTtl: 3360,
				},
			},
		);
		return new Response(JSON.stringify(response));
	},
} satisfies ExportedHandler<Env>;
```

For a detailed step-by-step guide on integrating Workers AI with AI Gateway using a binding, see [Integrations in AI Gateway](/ai-gateway/integrations/aig-workers-ai-binding/).

Workers AI supports the following parameters for AI gateways:

- `id` string
  - Name of your existing [AI Gateway](/ai-gateway/get-started/#create-gateway). Must be in the same account as your Worker.
- `skipCache` boolean(default: false)
  - Controls whether the request should [skip the cache](/ai-gateway/configuration/caching/#skip-cache-cf-aig-skip-cache).
- `cacheTtl` number
  - Controls the [Cache TTL](/ai-gateway/configuration/caching/#cache-ttl-cf-aig-cache-ttl).

---

# Create your first AI Gateway using Workers AI

URL: https://developers.cloudflare.com/ai-gateway/tutorials/create-first-aig-workers/

import { Render } from "~/components";

This tutorial guides you through creating your first AI Gateway using Workers AI on the Cloudflare dashboard. The intended audience is beginners who are new to AI Gateway and Workers AI. Creating an AI Gateway enables the user to efficiently manage and secure AI requests, allowing them to utilize AI models for tasks such as content generation, data processing, or predictive analysis with enhanced control and performance.

## Sign up and log in

1. **Sign up**: If you do not have a Cloudflare account, [sign up](https://cloudflare.com/sign-up).
2. **Log in**: Access the Cloudflare dashboard by logging in to the [Cloudflare dashboard](https://dash.cloudflare.com/login).

## Create gateway

Then, create a new AI Gateway.

<Render file="create-gateway" />

## Connect Your AI Provider

1. In the AI Gateway section, select the gateway you created.
2. Select **Workers AI** as your provider to set up an endpoint specific to Workers AI.
   You will receive an endpoint URL for sending requests.

## Configure Your Workers AI

1. Go to **AI** > **Workers AI** in the Cloudflare dashboard.
2. Select **Use REST API** and follow the steps to create and copy the API token and Account ID.
3. **Send Requests to Workers AI**: Use the provided API endpoint. For example, you can run a model via the API using a curl command. Replace `{account_id}`, `{gateway_id}` and `{cf_api_token}` with your actual account ID and API token:

   ```bash
   curl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/workers-ai/@cf/meta/llama-3.1-8b-instruct \
   --header 'Authorization: Bearer {cf_api_token}' \
   --header 'Content-Type: application/json' \
   --data '{"prompt": "What is Cloudflare?"}'
   ```

The expected output would be similar to :

```bash
{"result":{"response":"I'd be happy to explain what Cloudflare is.\n\nCloudflare is a cloud-based service that provides a range of features to help protect and improve the performance, security, and reliability of websites, applications, and other online services. Think of it as a shield for your online presence!\n\nHere are some of the key things Cloudflare does:\n\n1. **Content Delivery Network (CDN)**: Cloudflare has a network of servers all over the world. When you visit a website that uses Cloudflare, your request is sent to the nearest server, which caches a copy of the website's content. This reduces the time it takes for the content to load, making your browsing experience faster.\n2. **DDoS Protection**: Cloudflare protects against Distributed Denial-of-Service (DDoS) attacks. This happens when a website is overwhelmed with traffic from multiple sources to make it unavailable. Cloudflare filters out this traffic, ensuring your site remains accessible.\n3. **Firewall**: Cloudflare acts as an additional layer of security, filtering out malicious traffic and hacking attempts, such as SQL injection or cross-site scripting (XSS) attacks.\n4. **SSL Encryption**: Cloudflare offers free SSL encryption, which secure sensitive information (like passwords, credit card numbers, and browsing data) with an HTTPS connection (the \"S\" stands for Secure).\n5. **Bot Protection**: Cloudflare has an AI-driven system that identifies and blocks bots trying to exploit vulnerabilities or scrape your content.\n6. **Analytics**: Cloudflare provides insights into website traffic, helping you understand your audience and make informed decisions.\n7. **Cybersecurity**: Cloudflare offers advanced security features, such as intrusion protection, DNS filtering, and Web Application Firewall (WAF) protection.\n\nOverall, Cloudflare helps protect against cyber threats, improves website performance, and enhances security for online businesses, bloggers, and individuals who need to establish a strong online presence.\n\nWould you like to know more about a specific aspect of Cloudflare?"},"success":true,"errors":[],"messages":[]}%
```

## View Analytics

Monitor your AI Gateway to view usage metrics.

1. Go to **AI** > **AI Gateway** in the dashboard.
2. Select your gateway to view metrics such as request counts, token usage, caching efficiency, errors, and estimated costs. You can also turn on additional configurations like logging and rate limiting.

## Optional - Next steps

To build more with Workers, refer to [Tutorials](/workers/tutorials/).

If you have any questions, need assistance, or would like to share your project, join the Cloudflare Developer community on [Discord](https://discord.cloudflare.com) to connect with other developers and the Cloudflare team.

---

# Tutorials

URL: https://developers.cloudflare.com/ai-gateway/tutorials/

import { GlossaryTooltip, ListTutorials } from "~/components";

View <GlossaryTooltip term="tutorial">tutorials</GlossaryTooltip> to help you get started with AI Gateway.

<ListTutorials />

---

# Deploy a Worker that connects to OpenAI via AI Gateway

URL: https://developers.cloudflare.com/ai-gateway/tutorials/deploy-aig-worker/

import { Render, PackageManagers } from "~/components";

In this tutorial, you will learn how to deploy a Worker that makes calls to OpenAI through AI Gateway. AI Gateway helps you better observe and control your AI applications with more analytics, caching, rate limiting, and logging.

This tutorial uses the most recent v4 OpenAI node library, an update released in August 2023.

## Before you start

All of the tutorials assume you have already completed the [Get started guide](/workers/get-started/guide/), which gets you set up with a Cloudflare Workers account, [C3](https://github.com/cloudflare/workers-sdk/tree/main/packages/create-cloudflare), and [Wrangler](/workers/wrangler/install-and-update/).

## 1. Create an AI Gateway and OpenAI API key

On the AI Gateway page in the Cloudflare dashboard, create a new AI Gateway by clicking the plus button on the top right. You should be able to name the gateway as well as the endpoint. Click on the API Endpoints button to copy the endpoint. You can choose from provider-specific endpoints such as OpenAI, HuggingFace, and Replicate. Or you can use the universal endpoint that accepts a specific schema and supports model fallback and retries.

For this tutorial, we will be using the OpenAI provider-specific endpoint, so select OpenAI in the dropdown and copy the new endpoint.

You will also need an OpenAI account and API key for this tutorial. If you do not have one, create a new OpenAI account and create an API key to continue with this tutorial. Make sure to store your API key somewhere safe so you can use it later.

## 2. Create a new Worker

Create a Worker project in the command line:

<PackageManagers type="create" pkg="cloudflare@latest" args={"openai-aig"} />

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "hello-world",
		type: "Hello World Worker",
		lang: "JavaScript",
	}}
/>

Go to your new open Worker project:

```sh title="Open your new project directory"
cd openai-aig
```

Inside of your new opeai-aig directory, find and open the `src/index.js` file. You will configure this file for most of the tutorial.

Initially, your generated `index.js` file should look like this:

```js
export default {
	async fetch(request, env, ctx) {
		return new Response("Hello World!");
	},
};
```

## 3. Configure OpenAI in your Worker

With your Worker project created, we can learn how to make your first request to OpenAI. You will use the OpenAI node library to interact with the OpenAI API. Install the OpenAI node library with `npm`:

```sh title="Install the OpenAI node library"
npm install openai
```

In your `src/index.js` file, add the import for `openai` above `export default`:

```js
import OpenAI from "openai";
```

Within your `fetch` function, set up the configuration and instantiate your `OpenAIApi` client with the AI Gateway endpoint you created:

```js null {5-8}
import OpenAI from "openai";

export default {
	async fetch(request, env, ctx) {
		const openai = new OpenAI({
			apiKey: env.OPENAI_API_KEY,
			baseURL:
				"https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai", // paste your AI Gateway endpoint here
		});
	},
};
```

To make this work, you need to use [`wrangler secret put`](/workers/wrangler/commands/#put) to set your `OPENAI_API_KEY`. This will save the API key to your environment so your Worker can access it when deployed. This key is the API key you created earlier in the OpenAI dashboard:

```sh title="Save your API key to your Workers env"
npx wrangler secret put OPENAI_API_KEY
```

To make this work in local development, create a new file `.dev.vars` in your Worker project and add this line. Make sure to replace `OPENAI_API_KEY` with your own OpenAI API key:

```txt title="Save your API key locally"
OPENAI_API_KEY = "<YOUR_OPENAI_API_KEY_HERE>"
```

## 4. Make an OpenAI request

Now we can make a request to the OpenAI [Chat Completions API](https://platform.openai.com/docs/guides/gpt/chat-completions-api).

You can specify what model you'd like, the role and prompt, as well as the max number of tokens you want in your total request.

```js null {10-22}
import OpenAI from "openai";

export default {
	async fetch(request, env, ctx) {
		const openai = new OpenAI({
			apiKey: env.OPENAI_API_KEY,
			baseURL:
				"https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai",
		});

		try {
			const chatCompletion = await openai.chat.completions.create({
				model: "gpt-4o-mini",
				messages: [{ role: "user", content: "What is a neuron?" }],
				max_tokens: 100,
			});

			const response = chatCompletion.choices[0].message;

			return new Response(JSON.stringify(response));
		} catch (e) {
			return new Response(e);
		}
	},
};
```

## 5. Deploy your Worker application

To deploy your application, run the `npx wrangler deploy` command to deploy your Worker application:

```sh title="Deploy your Worker"
npx wrangler deploy
```

You can now preview your Worker at \<YOUR_WORKER>.\<YOUR_SUBDOMAIN>.workers.dev.

## 6. Review your AI Gateway

When you go to AI Gateway in your Cloudflare dashboard, you should see your recent request being logged. You can also [tweak your settings](/ai-gateway/configuration/) to manage your logs, caching, and rate limiting settings.

---

# Account analytics (beta)

URL: https://developers.cloudflare.com/analytics/account-and-zone-analytics/account-analytics/

Cloudflare account analytics lets you access a wide range of aggregated metrics from all the sites under a specific Cloudflare account.

:::note

For general information about all of Cloudflare's analytics offerings, refer to [About Cloudflare Analytics](/analytics/faq/about-analytics/). 
:::

***

## View your account analytics

To view metrics for your site:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com).

2. Select the appropriate Cloudflare account.

3. Go to **Analytics & Logs** > **Account Analytics**.

Once it loads, the Account Analytics app displays a collection of categorized charts with aggregated metrics for your account.Â To understand the various metrics available, refer to *Review your account metrics* below.

***

## Review your account metrics

This section outlines the aggregated metrics under each category.Â  Before reviewing your metrics, letâ€™s define a couple of concepts used in some panels:

* *Rate* -Â  Reflects the ratio between the amount for a specific data category and the total.
* *Bandwidth* - Refers to the number of bytes sent from the Cloudflare edge network to the requesting client.

Also, note that:

* To filter metrics for a specific time period, use the dropdown in the top right.
* Most metrics are grouped into panels representing different aspects of the underlying data.

### Summary of metrics

Below is a brief description of the major elements comprising the metrics available.

#### HTTP Traffic

These charts aggregate data for HTTP traffic, and include:

![Chart showing last week's data for HTTP traffic](~/assets/images/support/hc-dash-account-analytics-map.png)

* Spark lines for *Requests*, *Bandwidth*, *Page views*, and *Visitors* (*Unique IPs)*
* An interactive map that breaks down the number of requests by country
* A table combining numerical and spark line data, sorted by total number of requests per country

#### Security

![Panel displaying lines highlighting encryption metrics: requests, requests rate, bandwidth, and bandwidth rate](~/assets/images/support/hc-dash-account-analytics_security_panel.png)

This panel features spark lines highlighting various encryption metrics, including: *requests*, *requests rate*, *bandwidth*, and *bandwidth rate*.Â  These also include a comparative percentage change based on the previous period.

#### Cache

![Panel displaying lines for caching metrics: requests, requests rate, bandwidth, and bandwidth rate](~/assets/images/support/hc-dash-account-analytics_cache_card.png)

This panel features spark lines for various caching metrics, including: *requests*, *requests rate*, *bandwidth*, and *bandwidth rate*.Â  These also include a comparative percentage change based on the previous equivalent period.Â  For example, if you selected *Last week* as your time period, the previous period refers to the *week* before.

#### Errors

![Panel displaying lines for 4xx and 5xx error rates](~/assets/images/support/hc-account-analytics_errors_card.png)

This panel displays spark lines for 4xx and 5xx error rates, respectively.Â Learn more about [HTTP Status Codes](/support/troubleshooting/http-status-codes/http-status-codes/).Â 

#### Network

![Statistics showing the percentage of requests that use a specific version of HTTP](~/assets/images/support/hc-dash-account-analytics_network_card.png)

#### Client HTTP Version Used

These statistics show the percentage of requests that use a specific version of HTTP.

#### Traffic Served Over SSL

These statistics show the percentage of traffic that is encrypted using a specific version of SSL or TLS.

#### Content Type Breakdown

These statistics show the number of requests based on the resource content type.

---

# Cloudflare analytics with Workers

URL: https://developers.cloudflare.com/analytics/account-and-zone-analytics/analytics-with-workers/

Learn how Cloudflare analytics tracks requests made by [Cloudflare Workers](/workers/).

## What is a subrequest?

With a no-op Worker (a Worker that simply proxies traffic by passing on the original client request to the origin and proxying the response) running on a particular route, the request to the origin is counted as a â€œsubrequestâ€, separate from initial client to edge request. Thus, unless the Worker responds with a static response and never hits an origin, the eyeball â†’ edge request, and edge â†’ origin request will each be counted separately towards the request or bandwidth count in Analytics.Â  Subrequests are not included in theÂ **Requests**Â orÂ **Bandwidth**Â graphs of the CloudflareÂ **Analytics**Â app.

***

## Zone analytics

In the dashboard, the numbers in zone analytics reflect visitor traffic. That is, the number of requests shown in zone analytics (under the Analytics tabs in the dashboard) is the number of requests that were served to the client.

Similarly, the bandwidth is counted based on the bandwidth that is sent to the client, and status codes reflect the status codes that were served back to the client (so if a subrequest received a 500, but you respond with a 200, a 200 will be shown in the status codes breakdown).

***

## Worker analytics

For a breakdown of subrequest traffic (origin facing traffic), you may go to the CloudflareÂ **Analytics**Â app and select theÂ **Workers**Â tab. Under theÂ **Workers**Â tab, below the Service Workers panel, are aÂ **Subrequests**Â breakdown by count,Â **Bandwidth**Â andÂ **Status Codes**. This will help you spot and debug errors at your origin (such as spikes in 500s), and identify your cache-hit ratio to help you understand traffic going to your origin.

***

## FAQ

**Why donâ€™t I have any analytics for Workers?**

* If you are not currently using Workers (donâ€™t have Workers deployed on any routes or filters), we will not have any information to show you.
* If your Worker sends a static response back to the client without ever calling fetch() to an origin, you are not making any subrequests, thus, all traffic will be shown in zone Analytics

**Will this impact billing?**Â 

No,Â [billing for Workers](/workers/platform/pricing/)Â is based on requests that go through a Worker.Â 

**Why am I seeing such a high cache hit ratio?**

Requests served by a Worker always show as cached. For an accurate cache hit ratio on subrequests, refer to theÂ **Subrequests**Â graph in theÂ **Analytics**Â app under theÂ **Workers**Â analytics tab.

---

# Account and zone analytics

URL: https://developers.cloudflare.com/analytics/account-and-zone-analytics/

import { DirectoryListing } from "~/components"

Visit the following pages to learn more about account and zone analytics:

<DirectoryListing />

---

# Status codes

URL: https://developers.cloudflare.com/analytics/account-and-zone-analytics/status-codes/

:::note

Status Codes analytics by data center is exclusive to the [enterprise level of service](https://www.cloudflare.com/plans/enterprise/contact/). 
:::

Status Codes metrics in the Cloudflare dashboardÂ **Analytics**Â app provide customers with a deeper insight into the distribution of errors that are occurring on their website per data center. A data center facility is where Cloudflare runs its servers that make up our edge network ([current locations](https://www.cloudflare.com/network/)).

HTTP status codes that appear in a response passing through our edge are displayed in analytics. 

The `Origin Status Code` can help you investigate issues on your origin. If your origin returns a `5xx` error, Cloudflare's edge will forward this error to the end user. Comparing the `Edge Status Code` and `Origin Status Code` can help determine whether the issue is occurring on your origin or on the Cloudflare edge.

Errors that originate from our edge servers (blank `502`, `503`, or `504` error page with just `Cloudflare`) are not reported as part of the error analytics.

You can filter out specific error(s) by selecting one or more in the legend. You can also exclude a particular error and it will no longer display as part of the graph.

:::note

Users may also see `100x` errors which are not reported. These will be displayed as either `403` or `409` (edge) errors. 
:::

![Error analytics by Cloudflare data center](~/assets/images/analytics/status-codes.png)

***

## Common edge status codes

- `400` - Bad Request intercepted at the Cloudflare Edge (for example, missing or bad HTTP header)
- `403` - Security functionality (for example,Â Web Application Firewall, Browser Integrity Check, [Cloudflare challenges](/waf/reference/cloudflare-challenges/), and most 1xxx error codes)
- `409` - DNS errors typically in the form of 1000 or 1001 error code
- `413` - File size upload exceeded the maximum size allowed (configured in the dashboard under **Network** > **Maximum Upload Size**.)
- `444` - Used by Nginx to indicate that the server has returned no information to the client, and closed the connection. This error code is internal to Nginx and isÂ **not** returned to the client.
- `499` - Used by Nginx to indicate when a connection has been closed by the client while the server is still processing its request, making the server unable to send a status code back.

For more information, refer to [4xx Client Error](/support/troubleshooting/http-status-codes/4xx-client-error/).

***

## Common origin status codes

- `400` - Origin rejected the request due to bad, or unsupported syntax sent by the application.
- `404` - Only if the origin triggered a 404 response for a request.
- `4xx`
- `50x`

For more information, refer to [4xx Client Error](/support/troubleshooting/http-status-codes/4xx-client-error/) and [Troubleshooting Cloudflare 5XX errors](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/).

***

## 52x errors

- `520` - This is essentially a "catch-all" response for when the origin server returns something unexpected, or something that is not tolerated/cannot be interpreted by our edge (that is, protocol violation or empty response).
- `522` - Our edge could not establish a TCP connection to the origin server.
- `523` - Origin server is unreachable (for example, the origin IP changed but DNS was not updated, or due to network issues between our edge and the origin).
- `524` - Our edge established a TCP connection, but the origin did not reply with a HTTP response before the connection timed out.
- `525` - This error indicates that the SSL handshake between Cloudflare and the origin web server failed, either due to a network issue or a certificare issue at the origin.
- `526` - The certificate configured at the origin is not valid.

For more information, refer to [Troubleshooting Cloudflare 5XX errors](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/).

---

# Total threats stopped

URL: https://developers.cloudflare.com/analytics/account-and-zone-analytics/total-threats-stopped/

Total Threats Stopped measures the number of â€œsuspiciousâ€ and â€œbadâ€ requests that were aimed at your site. Requests receive these labels by our IP Reputation Database as they enter Cloudflareâ€™s network:

* **Legitimate:**Â request pass directly to your site
* **Suspicious:**Â request has been challenged with a [Cloudflare challenge](/waf/reference/cloudflare-challenges/)
* **Bad:**Â request has been blocked because our Browser Integrity Check, or because of user configured settings like WAF rules or IP range block.

Cloudflare uses Threat Scores gathered from sources such as Project Honeypot, as well as our own communities' traffic to determine whether a visitor is legitimate or malicious. When a legitimate visitor passes a challenge, that helps offset the Threat Score against the previous negative behavior seen from that IP address. Our system learns who is a threat from this activity.

In addition to threat analytics you can also monitor search engine crawlers going to your websites. For most websites, threats and crawlers make up 20% to 50% of traffic.

---

# Threat types

URL: https://developers.cloudflare.com/analytics/account-and-zone-analytics/threat-types/

Cloudflare classifies the threats that it blocks or challenges. To help you understand more about your siteâ€™s traffic, the â€œType of Threats Mitigatedâ€ metric on the analytics page measures threats blocked or challenged by the following categories:

## Bad browser

The source of the request was not legitimate or the request itself was malicious. Users would receive aÂ [1010 error page](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-1xxx-errors/#error-1010-the-owner-of-this-website-has-banned-your-access-based-on-your-browsers-signature)Â in their browser.

Cloudflare's [Browser Integrity Check](/waf/tools/browser-integrity-check/) looks for common HTTP headers abused most commonly by spammers and denies them access to your page. It will also challenge visitors that do not have a user agent or a non standard user agent (also commonly used by bots, crawlers, or visitors).

## Blocked hotlink

[Hotlink Protection](/waf/tools/scrape-shield/hotlink-protection/) ensures that other sites cannot use your bandwidth by building pages that link to images hosted on your origin server. This feature can be turned on and off by Cloudflareâ€™s customers.

## Human challenged

Visitors were presented with an interactive challenge page and failed to pass.

*Note: An interactive challenge page is a difficult to read word or set of numbers that only a human can translate. If entered incorrectly or not answered in a timely fashion, the request is blocked.*

## Browser challenge

A bot gave an invalid answer to the JavaScript challenge (in most cases this will not happen, bots typically do not respond to the challenge at all, so "failed" JavaScript challenges would not get logged).

*Note: During a JavaScript challenge you will be shown an interstitial page for about five seconds while Cloudflare performs a series of mathematical challenges to make sure it is a legitimate human visitor.*

## Bad IP

A request that came from an IP address that is not trusted by Cloudflare based on the Threat Score.

Cloudflare uses Threat Scores gathered from sources such as Project Honeypot, as well as our own communities' traffic to determine whether a visitor is legitimate or malicious. When a legitimate visitor passes a challenge, that helps offset the Threat Score against the previous negative behavior seen from that IP address. Our system learns who is a threat from this activity. Site owners may override the Threat Score at any time using Cloudflare's security settings.

## Country block

Requests from countries that were blocked based on the [user configuration](/waf/tools/ip-access-rules/) set in the WAF.

## IP block (user)

Requests from specific IP addresses that were blocked based on the [user configuration](/waf/tools/ip-access-rules/) set in the WAF.

## IP range block (/16)

A /16 IP range that was blocked based on the [user configuration](/waf/tools/ip-access-rules/) set in the WAF.

## IP range block (/24)

A /24 IP range that was blocked based on the [user configuration](/waf/tools/ip-access-rules/) set in the WAF.

## New Challenge (user)

[Challenge](/waf/reference/cloudflare-challenges/) based on user configurations set for visitorâ€™s IP in either WAF managed rules or custom rules, configured in **Security** > **WAF**.

## Challenge error

Requests made by a bot that failed to pass the challenge.

*Note: An interactive challenge page is a difficult to read word or set of numbers that only a human can translate. If entered incorrectly or not answered in a timely fashion, the request is blocked.*

## Bot Request

Request that came from a bot.

## Unclassified

Unclassified threats comprises a number of automatic blocks that are not related to the Browser Integrity Challenge (Bad Browser).Â These threats usually relate to Hotlink Protection, and other actions that happen on Cloudflare's global network based on the composition of the request (and not its content).

Unclassified means a number of conditions under which we group common threats related to Hotlink Protection as well as certain cases of IP reputation and specific requests that are blocked at Cloudflare's global network before reaching your servers.

---

# Zone Analytics

URL: https://developers.cloudflare.com/analytics/account-and-zone-analytics/zone-analytics/

import { Badge } from "~/components";


The Cloudflare zone analytics is a major component of the overall Cloudflare Analytics product line.Â  Specifically, this app gives you access to a wide range of metrics, collected at the website or domain level.

:::note

Read [Cloudflare Analytics](/analytics/faq/about-analytics/)
for general information about all of Cloudflare's analytics offerings.
You can also understand the characteristics of the data that Cloudflare
captures and processes.
:::

***

## View your website analytics

To view metrics for your website:

1. Log in to the Cloudflare dashboard.
2. Select the appropriate Cloudflare **account** for your site, then pick the **domain**.
3. Next, select **Analytics & Logs**.

Once it loads, you can find tabs for **Traffic**, **Security**, **Performance**, **DNS**, **Workers**, and **Logs** (Enterprise domains only). To understand the various metrics available, refer to *Review your website metrics* below.

***

## Review your website metrics

This section outlines the metrics available under each Analytics app tab. Before proceeding, note that each tab may contain:

* One or more panels to further categorize the underlying metrics.
* A dropdown (on the panelâ€™s top right) to filter metrics for a specific time period. The time period you can select may vary based on the Cloudflare plan that your domain is associated with.

Below is a summary of each Analytics app tab.

### Traffic

#### Free plan

These metrics include legitimate user requests as well as crawlers and threats. The Traffic tab features the following panels:Â 

* **Web Traffic** - Displays metrics for *Requests*, *Bandwidth*, and *Unique Visitors*. If you are using Cloudflare Workers, subrequests data will not be visible in zone Traffic Analytics. Instead, you can find subrequests analytics under the **Workers & Pages** tab in the **Overview** section. Refer to [Worker Analytics](/analytics/account-and-zone-analytics/analytics-with-workers/#worker-analytics) for more information.
* **Web Traffic Requests by Country** - Is an interactive map that breaks down the number of requests by country.Â  This panel also includes a data table for **Top Traffic Countries / Regions** that display the countries with the most number of requests (up to five, if the data exists).

#### Pro, Business, or Enterprise plan

:::note

Privacy-first Web Traffic Analytics are available on the Pro, Business,
and Enterprise plans.
:::

Analytics are based on Cloudflareâ€™s edge logs, with no need for third party scripts or trackers. The Traffic tab features the following metrics:

* **Requests** - An HTTP request. A typical page view requires many requests. If you are using Cloudflare Workers, subrequests data will not be visible in zone Traffic Analytics. Instead, you can find subrequests analytics under the **Workers & Pages** tab in the **Overview** section. Refer to [Worker Analytics](/analytics/account-and-zone-analytics/analytics-with-workers/#worker-analytics) for more information.
* **Data Transfer** - Total HTTP data transferred in responses.
* **Page views** - A page view is defined as a successful HTTP response with a content-type of HTML.Â 
* **Visits** - A visit is defined as a page view that originated from a different website, or direct link. Cloudflare checks where the HTTP referer does not match the hostname. One visit can consist of multiple page views.Â 
* **API Requests** - An HTTP request for API data.

To receive more detailed metrics, **Add filter**. You can also filter each metric by **Referer**, **Host**, **Country**, **Path**, **Status code**, **Origin status code**, **Browser**, **Operating system**, or **Device type**.Â 

To change the time period, use the dropdown menu on the right-hand side above the graph. You can also drag to zoom on the graph.

### Security

For this tab, the number and type of charts may vary based on existing data and customer plan. Most of the metrics in this tab come from the Cloudflare Firewall app. The panels available include:

* **Threats** - Displays a data summary and an area chart showing threats against the site.
* **Threats by Country** - Is an interactive map highlighting the countries where threats originated. It also includes data tables with statistics on **Top Threat Countries / Regions** and **Top Crawlers / Bots.**
* **Rate Limiting** (add-on service) - Features a line chart highlighting matching and blocked requests, based on rate limits.Â  To learn more, consult [Rate Limiting Analytics](/waf/reference/legacy/old-rate-limiting/#analytics).
* **Overview** - Displays a set of pie charts for: **Total Threats Stopped**, **Traffic Served Over SSL**, and **Types of Threats Mitigated**. If available, the expandable **Details** link display a table with numerical data.

### Performance

The metrics aggregated under this tab span multiple Cloudflare services.Â  The panels available include:

* **Origin Performance (Argo)** (add-on service) - Displays metrics related to response time between the Cloudflare edge network and origin servers for the last 48 hours.Â  For additional details, refer to [Argo Analytics](/argo-smart-routing/analytics/).
* **Overview** - Displays a set of pie charts for: **Client HTTP Version Used**, **Bandwidth Saved**, and **Content Type Breakdown**. If available, the expandable **Details** link display a table with numerical data.

### Workers

This panel features metrics for Cloudflare Workers. To learn more, read [Cloudflare analytics with Workers](/analytics/account-and-zone-analytics/analytics-with-workers/).

### Logs

The Logs tab is not a metrics feature. Instead, Customers in the Enterprise plan can enable the [Cloudflare Logs Logpush](/logs/about/) service. You can use Logpush to download and analyze data using any analytics tool of your choice.

---

# Get started

URL: https://developers.cloudflare.com/analytics/analytics-engine/get-started/

import { DirectoryListing, WranglerConfig } from "~/components"

## 1. Name your dataset and add it to your Worker

Add the following to your [Wrangler configuration file](/workers/wrangler/configuration/) to create a [binding](/workers/runtime-apis/bindings/) to a Workers Analytics Engine dataset. A dataset is like a table in SQL: the rows and columns should have consistent meaning.

<WranglerConfig>

```toml
[[analytics_engine_datasets]]
binding = "<BINDING_NAME>"
dataset = "<DATASET_NAME>"
```

</WranglerConfig>

## 2. Write data points from your Worker

You can write data points to your Worker by calling the `writeDataPoint()` method that is exposed on the binding that you just created.

```js
async fetch(request, env) {
  env.WEATHER.writeDataPoint({
    'blobs': ["Seattle", "USA", "pro_sensor_9000"], // City, State
    'doubles': [25, 0.5],
    'indexes': ["a3cd45"]
  });
  return new Response("OK!");
}
```

:::note

You do not need to await `writeDataPoint()` â€” it will return immediately, and the Workers runtime handles writing your data in the background.
:::

A data point is a structured event that consists of:

* **Blobs** (strings) â€”Â The dimensions used for grouping and filtering. Sometimes called labels in other metrics systems.
* **Doubles** (numbers) â€”Â The numeric values that you want to record in your data point.
* **Indexes** â€”Â (strings) â€” Used as a [sampling](/analytics/analytics-engine/sql-api/#sampling) key.

In the example above, suppose you are collecting air quality samples. Each data point written represents a reading from your weather sensor. The blobs define city, state, and sensor model â€”Â the dimensions you want to be able to filter queries on later. The doubles define the numeric temperature and air pressure readings. And the index is the ID of your customer. You may want to include [context about the incoming request](/workers/runtime-apis/request/), such as geolocation, to add additional data to your datapoint.

Currently, the `writeDataPoint()` API accepts ordered arrays of values. This means that you must provide fields in a consistent order. While the `indexes` field accepts an array, you currently must only provide a single index. If you attempt to provide multiple indexes, your data point will not be recorded.

## 3. Query data using the SQL API

You can query the data you have written in two ways:

* [**SQL API**](/analytics/analytics-engine/sql-api) â€”Â Best for writing your own queries and integrating with external tools like Grafana.
* [**GraphQL API**](/analytics/graphql-api/) â€”Â This is the same API that powers the Cloudflare dashboard.

For the purpose of this example, we will use the SQL API.

### Create an API token

Create an [API Token](https://dash.cloudflare.com/profile/api-tokens) that has the `Account Analytics Read` permission.

### Write your first query

The following query returns the top 10 cities that had the highest average humidity readings when the temperature was above zero:

```sql
SELECT
  blob1 AS city,
  SUM(_sample_interval * double2) / SUM(_sample_interval) AS avg_humidity
FROM WEATHER
WHERE double1 > 0
GROUP BY city
ORDER BY avg_humidity DESC
LIMIT 10
```

:::note

We are using a custom averaging function to take [sampling](/analytics/analytics-engine/sql-api/#sampling) into account.
:::

You can run this query by making an HTTP request to the SQL API:

```bash
curl "https://api.cloudflare.com/client/v4/accounts/{account_id}/analytics_engine/sql" \
--header "Authorization: Bearer <API_TOKEN>" \
--data "SELECT blob1 AS city, SUM(_sample_interval * double2) / SUM(_sample_interval) AS avg_humidity FROM WEATHER WHERE double1 > 0 GROUP BY city ORDER BY avg_humidity DESC LIMIT 10"
```

Refer to the [Workers Analytics Engine SQL Reference](/analytics/analytics-engine/sql-reference/) for a full list of supported SQL functionality.

### Working with time series data

Workers Analytics Engine is optimized for powering time series analytics that can be visualized using tools like Grafana. Every event written from the runtime is automatically populated with a `timestamp` field. It is expected that most time series will round, and then `GROUP BY` the `timestamp`. For example:

```sql
SELECT
  intDiv(toUInt32(timestamp), 300) * 300 AS t,
  blob1 AS city,
  SUM(_sample_interval * double2) / SUM(_sample_interval) AS avg_humidity
FROM WEATHER
WHERE
  timestamp >= NOW() - INTERVAL '1' DAY
  AND double1 > 0
GROUP BY t, city
ORDER BY t, avg_humidity DESC
```

This query first rounds the `timestamp` field to the nearest five minutes. Then, it groups by that field and city and calculates the average humidity in each city for a five minute period.

Refer to [Querying Workers Analytics Engine from Grafana](/analytics/analytics-engine/grafana/) for more details on how to create efficient Grafana queries against Workers Analytics Engine.

## Further reading

<DirectoryListing
folder="analytics/analytics-engine"
/>

---

# Querying from Grafana

URL: https://developers.cloudflare.com/analytics/analytics-engine/grafana/

Workers Analytics Engine is optimized for powering time series analytics that can be visualized using tools like Grafana. Every event written from the runtime is automatically populated with a `timestamp` field.

## Grafana plugin setup

We recommend the use of the [Altinity plugin for Clickhouse](https://grafana.com/grafana/plugins/vertamedia-clickhouse-datasource/) for querying Workers Analytics Engine from Grafana.

Configure the plugin as follows:

* URL: `https://api.cloudflare.com/client/v4/accounts/<account_id>/analytics_engine/sql`. Replace `<account_id>` with your 32 character account ID (available in the Cloudflare dashboard).
* Leave all auth settings off.
* Add a custom header with a name of `Authorization` and value set to `Bearer <token>`. Replace `<token>` with suitable API token string (refer to the [SQL API docs](/analytics/analytics-engine/sql-api/#authentication) for more information on this).
* No other options need to be set.

## Querying timeseries data

For use in a dashboard, you usually want to aggregate some metric per time interval. This can be achieved by rounding and then grouping by the `timestamp` field. The following query rounds and groups in this way, and then computes an average across each time interval whilst taking into account [sampling](/analytics/analytics-engine/sql-api/#sampling).

```sql
SELECT
    intDiv(toUInt32(timestamp), 60) * 60 AS t,
    blob1 AS label,
    SUM(_sample_interval * double1) / SUM(_sample_interval) AS average_metric
FROM dataset_name
WHERE 
    timestamp <= NOW() 
    AND timestamp > NOW() - INTERVAL '1' DAY
GROUP BY blob1, t
ORDER BY t
```

The Altinity plugin provides some useful macros that can simplify writing queries of this type. The macros require setting `Column:DateTime` to `timestamp` in the query builder, then they can be used like this:

```sql
SELECT
    $timeSeries AS t,
    blob1 AS label,
    SUM(_sample_interval * double1) / SUM(_sample_interval) AS average_metric
FROM dataset_name
WHERE $timeFilter
GROUP BY blob1, t
ORDER BY t
```

This query will automatically adjust the rounding time depending on the zoom level and filter to the correct time range that is currently being displayed.

---

# Workers Analytics Engine

URL: https://developers.cloudflare.com/analytics/analytics-engine/

import { LinkButton } from "~/components"

Workers Analytics Engine provides unlimited-cardinality analytics at scale, via [a built-in API](/analytics/analytics-engine/get-started/) to write data points from Workers, and a [SQL API](/analytics/analytics-engine/sql-api/) to query that data.

You can use Workers Analytics Engine to:

* Expose custom analytics to your own customers
* Build usage-based billing systems
* Understand the health of your service on a per-customer or per-user basis
* Add instrumentation to frequently called code paths, without impacting performance or overwhelming external analytics systems with events

 <LinkButton variant="primary" href="/analytics/analytics-engine/get-started/">Get started</LinkButton>

---

# Limits

URL: https://developers.cloudflare.com/analytics/analytics-engine/limits/

The following limits apply to Workers Analytics Engine:

* Analytics Engine will accept up to twenty blobs, twenty doubles, and one index per call to `writeDataPoint`.
* The total size of all blobs in a request must not exceed 5120 bytes.
* Each index must not be more than 96 bytes.
* You can write a maximum of 25 data points per Worker invocation (client HTTP request). Each call to `writeDataPoint` counts towards this limit.

## Data retention

Data written to Workers Analytics Engine is stored for three months.

Interested in longer retention periods? Join the `#analytics-engine` channel in the [Cloudflare Developers Discord](https://discord.cloudflare.com/) and tell us more about what you are building.

---

# Pricing

URL: https://developers.cloudflare.com/analytics/analytics-engine/pricing/

Workers Analytics Engine is priced based on two metrics â€”Â data points written, and read queries.

| Plan             | Data points written                                                  | Read queries                                                 |
| ---------------- | -------------------------------------------------------------------- | ------------------------------------------------------------ |
| **Workers Paid** | 10 million included per month <br /> (+$0.25 per additional million) | 1 million included per month (+$1.00 per additional million) |
| **Workers Free** | 100,000 included per day                                             | 10,000 included per day                                      |

:::note[Pricing availability]

Currently, you will not be billed for your use of Workers Analytics Engine. Pricing information here is shared in advance, so that you can estimate what your costs will be once Cloudflare starts billing for usage in the coming months.

If you are an Enterprise customer, contact your account team for information about Workers Analytics Engine pricing and billing. 
:::

### Data points written

Every time you call [`writeDataPoint()`](/analytics/analytics-engine/get-started/#2-write-data-points-from-your-worker) in a Worker, this counts as one data point written.

Each data point written costs the same amount. There is no extra cost to add dimensions or cardinality, and no additional cost for writing more data in a single data point.

### Read queries

Every time you post to Workers Analytics Engine's [SQL API](/analytics/analytics-engine/sql-api/), this counts as one read query.

Each read query costs the same amount. There is no extra cost for more or less complex queries, and no extra cost for reading only a few rows of data versus many rows of data.

---

# Sampling with WAE

URL: https://developers.cloudflare.com/analytics/analytics-engine/sampling/

Workers Analytics Engine offers the ability to write an extensive amount of data and retrieve it quickly, at minimal or no cost. To facilitate writing large amounts of data at a reasonable cost, Workers Analytics Engine employs weighted adaptive [sampling](https://en.wikipedia.org/wiki/Sampling_\(statistics\)).

When utilizing sampling, you do not need every single data point to answer questions about a dataset. For a sufficiently large dataset, the [necessary sample size](https://select-statistics.co.uk/blog/importance-effect-sample-size/) does not depend on the size of the original population. Necessary sample size depends on the variance of your measure, the size of the subgroups you analyze, and how accurate your estimate must be.

The implication for Analytics Engine is that we can compress very large datasets into many fewer observations, yet still answer most queries with very high accuracy. This enables us to offer an analytics service that can measure very high rates of usage, with unbounded cardinality, at a low and predictable price.

At a high level, the way sampling works is:

1. At write time, we sample if data points are written too quickly into one index.
2. We sample again at query time if the query is too complex.

In the following sections, you will learn:

* [How sampling works](/analytics/analytics-engine/sampling/#how-sampling-works).
* [How to read sampled data](/analytics/analytics-engine/sampling/#how-to-read-sampled-data).
* [How is data sampled](/analytics/analytics-engine/sampling/#how-is-data-sampled).
* [How Adaptive Bit Rate Sampling works](/analytics/analytics-engine/sampling/#adaptive-bit-rate-sampling-at-read-time).
* [How to pick your index such that your data is sampled in a usable way](/analytics/analytics-engine/sampling/#how-to-select-an-index).

## How sampling works

Cloudflare's data sampling is similar to how online mapping services like Google Maps render maps at different zoom levels. When viewing satellite imagery of a whole continent, the mapping service provides appropriately sized images based on the user's screen and Internet speed.

![The image on the left shows a satellite view from OpenStreetMap. On the right, the same image is zoomed in. In these two images, each pixel represents the same area; however the image on the right has many fewer pixels.](~/assets/images/analytics/zoom-less-pixels.png)

Each pixel on the map represents a large area, such as several square kilometers. If a user tries to zoom in using a screenshot, the resulting image would be blurry. Instead, the mapping service selects higher-resolution images when a user zooms in on a specific city. The total number of pixels remains relatively constant, but each pixel now represents a smaller area, like a few square meters.

![Now the image on the right is of a much higher resolution. Each pixel represents a much smaller area; however, the total number of pixels in both images is roughly the same.](~/assets/images/analytics/zoom-more-pixels.png)

The key point is that the map's quality does not solely depend on the resolution or the area represented by each pixel. It is determined by the total number of pixels used to render the final view.

There are similarities between the how a mapping services handles resolution and Cloudflare Analytics delivers analytics using adaptive samples:

* **How data is stored**:
  * **Mapping service**: Imagery stored at different resolutions.
  * **Cloudflare Analytics**: Events stored at different sample rates.
* **How data is displayed to user**:
  * **Mapping service**: The total number of pixels is \~constant for a given screen size, regardless of the area selected.
  * **Cloudflare Analytics**: A similar number of events are read for each query, regardless of the size of the dataset or length of time selected.
* **How a resolution is selected**:
  * **Mapping service**: The area represented by each pixel will depend on the size of the map being rendered. In a more zoomed out map, each pixel will represent a larger area.
  * **Cloudflare Analytics**: The sample interval of each event in the result depends on the size of the underlying dataset and length of time selected. For a query over a large dataset or long length of time, each sampled event may stand in for many similar events.

## How to read sampled data

To effectively write queries and analyze the data, it is helpful to first learn how sampled data is read in Workers Analytics Engine.

In Workers Analytics Engine, every event is recorded with the `_sample_interval` field. The sample interval is the inverse of the sample rate. For example, if a one percent (1%) sample rate is applied, the `sample_interval` will be set to `100`.

Using the mapping example in simple terms, the sample interval represents the "number of unsampled data points" (kilometers or meters) that a given sampled data point (pixel) represents.

The sample interval is a property associated with each individual row stored in Workers Analytics Engine. Due to the implementation of equitable sampling, the sample interval can vary for each row. As a result, when querying the data, you need to consider the sample interval field. Simply multiplying the query result by a constant sampling factor is not sufficient.

Here are some examples of how to express some common queries over sampled data.

| Use case                           | Example without sampling | Example with sampling                                   |
| ---------------------------------- | ------------------------ | ------------------------------------------------------- |
| Count events in a dataset          | `count()`                | `sum(_sample_interval)`                                 |
| Sum a quantity, for example, bytes | `sum(bytes)`             | `sum(bytes * _sample_interval)`                         |
| Average a quantity                 | `avg(bytes)`             | `sum(bytes * _sample_interval) / sum(_sample_interval)` |
| Compute quantiles                  | `quantile(0.50)(bytes)`  | `quantileWeighted(0.50)(bytes, _sample_interval)`       |

Note that the accuracy of results is not determined by the sample interval, similar to the mapping analogy mentioned earlier. A high sample interval can still provide precise results. Instead, accuracy depends on the total number of data points queried and their distribution.

## How is data sampled

To determine the sample interval for each event, note that most analytics have some important type of subgroup that must be analyzed with accurate results. For example, you may want to analyze user usage or traffic to specific hostnames. Analytics Engine users can define these groups by populating the `index` field when writing an event. This allows for more targeted and precise analysis within the specified groups.

The next observation is that these index values likely have a very different number of events written to them. In fact, the usage of most web services follows a [Pareto distribution](https://en.wikipedia.org/wiki/Pareto_distribution), meaning that the top few users will account for the vast majority of the usage. Pareto distributions are common and look like this:

![In this graphic, each bar represents a user; the height of the bar is their total usage.](~/assets/images/analytics/total-usage.png)

If we took a [simple random sample](https://en.wikipedia.org/wiki/Simple_random_sample) of one percent (1%) of this data, and we applied that to the whole population, you may be able to track your largest customers accurately â€” but you would lose visibility into what your smaller customers are doing:

![The same graphic as above, but now based on a 1% sample of the data.](~/assets/images/analytics/sample-data.png)

Notice that the larger bars look more or less unchanged, and yet they are still quite accurate. But as you analyze smaller customers, results get [quantized](https://en.wikipedia.org/wiki/Quantization_\(signal_processing\)) and may even be rounded to 0 entirely.

This shows that while a one percent (1%) or even smaller sample of a large population may be sufficient, we may need to store a larger proportion of events for a small population to get accurate results.

We do this through a technique called equitable sampling. This means that we will equalize the number of events we store for each unique index value. For relatively uncommon index values, we may write all of the data points that we get via `writeDataPoint()`.  But if you write lots of data points to a single index value, we will start to sample.

Here is the same distribution, but now with (a simulation of) equitable sampling applied:

![This graphic shows the same population, but with equitable sampling.](~/assets/images/analytics/equitable-sampling.png)

You may notice that this graphic is very similar to the first graph. However, it only requires `<10%` of the data to be stored overall. The sample rate is actually much lower than `10%` for the larger series (that is, we store larger sample intervals), but the sample rate is higher for the smaller series.

Refer back to the mapping analogy above. Regardless of the map area shown, the total number of pixels in the map stays constant. Similarly, we always want to store a similar number of data points for each index value. However, the resolution of the map â€” how much area is represented by each pixel â€” will change based on the area being shown. Similarly here, the amount of data represented by each stored data point will vary, based on the total number of data points in the index.

## Adaptive Bit Rate Sampling at Read Time

Equitable sampling ensures that an equal amount of data is maintained for each index within a specific time frame. However, queries can vary significantly in the duration of time they target. Some queries may only require a 10-minute data snapshot, while others might need to analyze data spanning 10 weeks â€” a period which is 10,000 times longer.

To address this issue, we employ a method called [adaptive bit rate](https://blog.cloudflare.com/explaining-cloudflares-abr-analytics/) (ABR). With ABR, queries that cover longer time ranges will retrieve data from a higher sample interval, allowing them to be completed within a fixed time limit. In simpler terms, just as screen size or bandwidth is a fixed resource in our mapping analogy, the time required to complete a query is also fixed. Therefore, irrespective of the volume of data involved, we need to limit the total number of rows scanned to provide an answer to the query. This helps to ensure fairness: regardless of the size of the underlying dataset being queried, we ensure that all queries receive an equivalent share of the available computing time.

To achieve this, we store the data in multiple resolutions (that is, with different levels of detail, for instance, 100%, 10%, 1%) derived from the equitably sampled data. At query time, we select the most suitable data resolution to read based on the query's complexity. The query's complexity is determined by the number of rows to be retrieved and the probability of the query completing within a specified time limit of N seconds. By dynamically selecting the appropriate resolution, we optimize the query performance and ensure it stays within the allotted time budget.

ABR offers a significant advantage by enabling us to consistently provide query results within a fixed query budget, regardless of the data size or time span involved. This sets it apart from systems that struggle with timeouts, errors, or high costs when dealing with extensive datasets.

## How to select an index

In order to get accurate results with sampled data, select an appropriate value to use as your index. The index should match how users will query and view data. For example, if users frequently view data based on a specific device or hostname, it is recommended to incorporate those attributes into your index.

The index has the following properties, which are important to consider when choosing an index:

* Get accurate summary statistics about your entire dataset, across all index values.
* Get an accurate count of the number of unique values of your index.
* Get accurate summary statistics (for example, count, sum) within a particular index value.
* See the `Top N` values of specific fields that are not in your index.
* Filter on most fields.
* Run other aggregations like quantiles.

Some limitations and trade-offs to consider are:

* You may not be able to get accurate unique counts of fields that are not in your index.
  * For example, if you index on `hostname`, you may not be able to count the number of unique URLs.
* You may not be able to observe very rare values of fields not in the index.
  * For example, a particular URL for a hostname, if you index on host and have millions of unique URLs.
* You may not be able to run accurate queries across multiple indices at once.
  * For example, you may only be able to query for one host at a time (or all of them) and expect accurate results.
* There is no guarantee you can retrieve any one individual record.
* You cannot necessarily reconstruct exact sequences of events.

It is not recommended to write a unique index value on every row (like a UUID) for most use cases. While this will make it possible to retrieve individual data points very quickly, it will slow down most queries for aggregations and time series.

Refer to the Workers Analytics Engine FAQs, for common question about [Sampling](/analytics/faq/wae-faqs/#sampling).

---

# SQL API

URL: https://developers.cloudflare.com/analytics/analytics-engine/sql-api/

The Workers Analytics Engine SQL API is an HTTP API that allows executing SQL queries against your Workers Analytics Engine datasets.

The API is hosted at `https://api.cloudflare.com/client/v4/accounts/<account_id>/analytics_engine/sql`.

## Authentication

Authentication is done via bearer token. An `Authorization: Bearer <token>` header must be supplied with every request to the API.

Use the dashboard to create a token with permission to read analytics data on your account:

1. Visit the [API tokens](https://dash.cloudflare.com/profile/api-tokens) page in the Cloudflare dashboard.
2. Select **Create Token**.
3. Select **Create Custom Token**.
4. Complete the **Create Custom Token** form as follows:
   * Give your token a descriptive name.
   * For **Permissions** select *Account* | *Account Analytics* | *Read*
   * Optionally configure account and IP restrictions and TTL.
   * Submit and confirm the form to create the token.
5. Make a note of the token string.

## Querying the API

Submit the query text in the body of a `POST` request to the API address. The format of the data returned can be selected using the [`FORMAT` option](/analytics/analytics-engine/sql-reference/#format-clause) in your query.

You can use cURL to test the API as follows, replacing the `<account_id>` with your 32 character account ID (available in the dashboard) and the `<token>` with the token string you generated above.

```bash
curl "https://api.cloudflare.com/client/v4/accounts/{account_id}/analytics_engine/sql" \
--header "Authorization: Bearer <API_TOKEN>" \
--data "SELECT 'Hello Workers Analytics Engine' AS message"
```

If you have already published some data, you might try executing the following to confirm that the dataset has been created in the DB.

```bash
curl "https://api.cloudflare.com/client/v4/accounts/{account_id}/analytics_engine/sql" \
--header "Authorization: Bearer <API_TOKEN>" \
--data "SHOW TABLES"
```

Refer to the Workers Analytics Engine [SQL reference](/analytics/analytics-engine/sql-reference/), for the full supported query syntax.

## Table structure

A new table will automatically be created for each dataset once you start writing events to it from your worker.

The table will have the following columns:

| Name                         | Type     | Description                                                                                                                                                                                                                                          |
| ---------------------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| dataset                      | string   | This column will contain the dataset name in every row.                                                                                                                                                                                              |
| timestamp                    | DateTime | The timestamp at which the event was logged in your worker.                                                                                                                                                                                          |
| \_sample\_interval           | integer  | In case that the data has been sampled, this column indicates what the sample rate is for this row (that is, how many rows of the original data are represented by this row). Refer to the [sampling](#sampling) section below for more information. |
| index1                       | string   | The index value that was logged with the event. The value in this column is used as the key for sampling.                                                                                                                                            |
| blob1<br/>...<br/>blob20     | string   | The blob values that were logged with the event.                                                                                                                                                                                                     |
| double1<br/>...<br/>double20 | double   | The double values that were logged with the event.                                                                                                                                                                                                   |

## Sampling

At very high volumes of data, Analytics Engine will downsample data in order to be able to maintain performance. Sampling can occur on write and on read.
Sampling is based on the index of your dataset so that only indexes that receive large numbers of events will be sampled. For example, if your worker serves multiple customers, you might consider making customer ID the index field. This would mean that if one customer starts making a high rate of requests then events from that customer could be sampled while other customers data remains unsampled.

We have tested this system of sampling over a number of years at Cloudflare and it has enabled us to scale our web analytics systems to very high throughput, while still providing statistically meaningful results irrespective of the amount of traffic a website receives.

The rate at which the data is sampled is exposed via the `_sample_interval` column. This means that if you are doing statistical analysis of your data, you may need to take this column into account. For example:

| Original query                 | Query taking into account sampling                                        |
| ------------------------------ | ------------------------------------------------------------------------- |
| `SELECT COUNT() FROM ... `     | `SELECT SUM(_sample_interval) FROM ...`                                   |
| `SELECT SUM(double1) FROM ...` | `SELECT SUM(_sample_interval * double1) FROM ...`                         |
| `SELECT AVG(double1) FROM ...` | `SELECT SUM(_sample_interval * double1) / SUM(_sample_interval) FROM ...` |

Additionally, the [QUANTILEWEIGHTED function](/analytics/analytics-engine/sql-reference/#quantileweighted) is designed to be used with sample interval as the third argument.

## Example queries

### Select data with column aliases

Column aliases can be used in queries to give names to the blobs and doubles in your dataset:

```sql
SELECT
    timestamp,
    blob1 AS location_id,
    double1 AS inside_temp,
    double2 AS outside_temp
FROM temperatures
WHERE timestamp > NOW() - INTERVAL '1' DAY
```

### Aggregation taking into account sample interval

Calculate number of readings taken at each location in the last 7 days. In this case, we are grouping by the index field so an exact count can be calculated even in the case that the data has been sampled:

```sql
SELECT
    index1 AS location_id,
    SUM(_sample_interval) AS n_readings
FROM temperatures
WHERE timestamp > NOW() - INTERVAL '7' DAY
GROUP BY index1
```

Calculate the average temperature over the last 7 days at each location. Sample interval is taken into account:

```sql
SELECT
    index1 AS location_id,
    SUM(_sample_interval * double1) / SUM(_sample_interval) AS average_temp
FROM temperatures
WHERE timestamp > NOW() - INTERVAL '7' DAY
GROUP BY index1
```

---

# SQL Reference

URL: https://developers.cloudflare.com/analytics/analytics-engine/sql-reference/

## SHOW TABLES statement

`SHOW TABLES` can be used to list the tables on your account. The table name is the name you specified as `dataset` when configuring the workers binding (refer to [Get started with Workers Analytics Engine](/analytics/analytics-engine/get-started/), for more information). The table is automatically created when you write event data in your worker.

```sql
SHOW TABLES
[FORMAT <format>]
```

Refer to [FORMAT clause](#format-clause) for the available `FORMAT` options.

## SHOW TIMEZONES statement

`SHOW TIMEZONES` can be used to list all of the timezones supported by the SQL API. Most common timezones are supported.

```sql
SHOW TIMEZONES
[FORMAT <format>]
```

## SHOW TIMEZONE statement

`SHOW TIMEZONE` responds with the current default timezone in use by SQL API. This should always be `Etc/UTC`.

```sql
SHOW TIMEZONE
[FORMAT <format>]
```

## SELECT statement

`SELECT` is used to query tables.

Usage:

```sql
SELECT <expression_list>
[FROM <table>|(<subquery>)]
[WHERE <expression>]
[GROUP BY <expression>, ...]
[ORDER BY <expression_list>]
[LIMIT <n>|ALL]
[FORMAT <format>]
```

Below you can find the syntax of each clause. Refer to the [SQL API docs](/analytics/analytics-engine/sql-api/) for some example queries.

### SELECT clause

The `SELECT` clause specifies the list of columns to be included in the result.
Columns can be aliased using the `AS` keyword.

Usage:

```sql
SELECT <expression> [AS <alias>], ...
```

Examples:

```sql
-- return the named columns
SELECT blob2, double3

-- return all columns
SELECT *

-- alias columns to more descriptive names
SELECT
    blob2 AS probe_name,
    double3 AS temperature
```

Additionally, expressions using supported [functions](#supported-functions) and [operators](#supported-operators) can be used in place of column names:

```sql
SELECT
    blob2 AS probe_name,
    double3 AS temp_c,
    double3*1.8+32 AS temp_f -- compute a value

SELECT
    blob2 AS probe_name,
    if(double3 <= 0, 'FREEZING', 'NOT FREEZING') AS description -- use of functions

SELECT
    blob2 AS probe_name,
    avg(double3) AS avg_temp -- aggregation function
```

### FROM clause

`FROM` is used to specify the source of the data for the query.

Usage:

```sql
FROM <table_name>|(subquery)
```

Examples:

```sql
-- query data written to a workers dataset called "temperatures"
FROM temperatures

-- use a subquery to manipulate the table
FROM (
    SELECT
        blob1 AS probe_name,
        count() as num_readings
    FROM
        temperatures
    GROUP BY
        probe_name
)
```

Note that queries can only operate on a single table. `UNION`, `JOIN` etc. are not currently supported.

### WHERE clause

`WHERE` is used to filter the rows returned by a query.

Usage:

```sql
WHERE <condition>
```

`<condition>` can be any expression that evaluates to a boolean.

[Comparison operators](#comparison-operators) can be used to compare values and [boolean operators](#boolean-operators) can be used to combine conditions.

Expressions containing [functions](#supported-functions) and [operators](#supported-operators) are supported.

Examples:

```sql
-- simple comparisons
WHERE blob1 = 'test'
WHERE double1 = 4

-- inequalities
WHERE double1 > 4

-- use of operators (see below for supported operator list)
WHERE double1 + double2 > 4
WHERE blob1 = 'test1' OR blob2 = 'test2'

-- expression using inequalities, functions and operators
WHERE if(unit = 'f', (temp-32)/1.8, temp) <= 0
```

### GROUP BY clause

When using aggregate functions, `GROUP BY` specifies the groups over which the aggregation is run.

Usage:

```sql
GROUP BY <expression>, ...
```

For example. If you had a table of temperature readings:

```sql
-- return the average temperature for each probe
SELECT
    blob1 AS probe_name,
    avg(double1) AS average_temp
FROM temperature_readings
GROUP BY probe_name
```

In the usual case the `<expression>` can just be a column name but it is also possible to supply a complex expression here. Multiple expressions or column names can be supplied separated by commas.

### ORDER BY clause

`ORDER BY` can be used to control the order in which rows are returned.

Usage:

```sql
ORDER BY <expression> [ASC|DESC], ...
```

`<expression>` can just be a column name.

`ASC` or `DESC` determines if the ordering is ascending or descending. `ASC` is the default, and can be omitted.

Examples:

```sql
-- order by double2 then double3, both in ascending order
ORDER BY double2, double3

-- order by double2 in ascending order then double3 is descending order
ORDER BY double2, double3 DESC
```

### LIMIT clause

`LIMIT` specifies a maximum number of rows to return.

Usage:

```sql
LIMIT <n>|ALL
```

Supply the maximum number of rows to return or `ALL` for no restriction.

For example:

```sql
LIMIT 10 -- return at most 10 rows
```

### FORMAT clause

`FORMAT` controls how to the returned data is encoded.

Usage:

```sql
FORMAT [JSON|JSONEachRow|TabSeparated]
```

If no format clause is included then the default format of `JSON` will be used.

Override the default by setting a format. For example:

```sql
FORMAT JSONEachRow
```

The following formats are supported:

#### JSON

Data is returned as a single JSON object with schema data included:

```json
{
    "meta": [
        {
            "name": "<column 1 name>",
            "type": "<column 1 type>"
        },
        {
            "name": "<column 2 name>",
            "type": "<column 2 type>"
        },
        ...
    ],
    "data": [
        {
            "<column 1 name>": "<column 1 value>",
            "<column 2 name>": "<column 2 value>",
            ...
        },
        {
            "<column 1 name>": "<column 1 value>",
            "<column 2 name>": "<column 2 value>",
            ...
        },
        ...
    ],
    "rows": 10
}
```

#### JSONEachRow

Data is returned with a separate JSON object per row. Rows are newline separated and there is no header line or schema data:

```json
{"<column 1 name>": "<column 1 value>", "<column 2 name>": "<column 2 value>"}
{"<column 1 name>": "<column 1 value>", "<column 2 name>": "<column 2 value>"}
...
```

#### TabSeparated

Data is returned with newline separated rows. Columns are separated with tabs. There is no header.

```txt
column 1 value  column 2 value
column 1 value  column 2 value
...
```

## Supported functions

:::note

Note that function names are not case-sensitive, they can be used both in uppercase or in lowercase. 
:::

### count

Usage:

```sql
count()
count(DISTINCT column_name)
```

Count is an aggregation function that returns the number of rows in each group or results set.

Count can also be used to count the number of distinct (unique) values in each column:

Example:

```sql
-- return the total number of rows
count()
-- return the number of different values in the column
count(DISTINCT column_name)
```

### sum

Usage:

```sql
sum([DISTINCT] column_name)
```

Sum is an aggregation function that returns the sum of column values across all rows in each group or results set. Sum also supports `DISTINCT`, but in this case it will only sum the unique values in the column.

Example:

```sql
-- return the total cost of all items
sum(item_cost)
-- return the total of all unique item costs
sum(DISTINCT item_cost)
```

### avg

Usage:

```sql
avg([DISTINCT] column_name)
```

Avg is an aggregation function that returns the mean of column values across all rows in each group or results set. Avg also supports `DISTINCT`, but in this case it will only average the unique values in the column.

Example:

```sql
-- return the mean item cost
avg(item_cost)
-- return the mean of unique item costs
avg(DISTINCT item_cost)
```

### min

Usage:

```sql
min(column_name)
```

Min is an aggregation function that returns the minimum value of a column across all rows.

Example:

```sql
-- return the minimum item cost
min(item_cost)
```

### max

Usage:

```sql
max(column_name)
```

Max is an aggregation function that returns the maximum value of a column across all rows.

Example:

```sql
-- return the maximum item cost
max(item_cost)
```

### quantileWeighted

Usage:

```sql
quantileWeighted(q, column_name, weight_column_name)
```

`quantileWeighted` is an aggregation function that returns the value at the q<sup>th</sup> quantile in the named column across all rows in each group or results set. Each row will be weighted by the value in `weight_column_name`. Typically this would be `_sample_interval` (refer to [how sampling works](/analytics/analytics-engine/sql-api/#sampling), for more information).

Example:

```sql
-- estimate the median value of <double1>
quantileWeighted(0.5, double1, _sample_interval)

-- in a table of query times, estimate the 95th centile query time
quantileWeighted(0.95, query_time, _sample_interval)
```

### if

Usage:

```sql
if(<condition>, <true_expression>, <false_expression>)
```

Returns `<true_expression>` if `<condition>` evaluates to true, else returns `<false_expression>`.

Example:

```sql
if(temp > 20, 'It is warm', 'Bring a jumper')
```

### intDiv

Usage:

```sql
intDiv(a, b)
```

Divide a by b, rounding the answer down to the nearest whole number.

### toUInt32

Usage:

```sql
toUInt32(<expression>)
```

Converts any numeric expression, or expression resulting in a string representation of a decimal, into an unsigned 32 bit integer.

Behaviour for negative numbers is undefined.

### length

Usage:

```sql
length({string})
```

Returns the length of a string. This function is UTF-8 compatible.

Examples:

```sql
SELECT length('a string') AS s;
SELECT length(blob1) AS s FROM your_dataset;
```

### isEmpty

Usage:

```sql
isEmpty({string})
```

Returns a boolean saying whether the string was empty. This computation can also be done as a binary operation: `{string} = ''`.

Examples:

```sql
SELECT isEmpty('a string') AS b;
SELECT isEmpty(blob1) AS b FROM your_dataset;
```

### toLower

Usage:

```sql
toLower({string})
```

Returns the string converted to lowercase. This function is Unicode compatible. This may not be perfect for all languages and users with stringent needs, should do the operation in their own code.

Examples:

```sql
SELECT toLower('STRING TO DOWNCASE') AS s;
SELECT toLower(blob1) AS s FROM your_dataset;
```

### toUpper

Usage:

```sql
toUpper({string})
```

Returns the string converted to uppercase. This function is Unicode compatible. The results may not be perfect for all languages and users with strict needs. These users should do the operation in their own code.

Examples:

```sql
SELECT toUpper('string to uppercase') AS s;
SELECT toUpper(blob1) AS s FROM your_dataset;
```

### startsWith

Usage:

```sql
startsWith({string}, {string})
```

Returns a boolean of whether the first string has the second string at its start.

Examples:

```sql
SELECT startsWith('prefix ...', 'prefix') AS b;
SELECT startsWith(blob1, 'prefix') AS b FROM your_dataset;
```

### endsWith

Usage:

```sql
endsWith({string}, {string})
```

Returns a boolean of whether the first string contains the second string at its end.

Examples:

```sql
SELECT endsWith('prefix suffix', 'suffix') AS b;
SELECT endsWith(blob1, 'suffix') AS b FROM your_dataset;
```

### position

Usage:

```sql
position({needle:string} IN {haystack:string})
```

Returns the position of one string, `needle`, in another, `haystack`. In SQL, indexes are usually 1-based. That means that position returns `1` if your needle is at the start of the haystack. It only returns `0` if your string is not found.

Examples:

```sql
SELECT position(':' IN 'hello: world') AS p;
SELECT position(':' IN blob1) AS p FROM your_dataset;
```

### substring

Usage:

```sql
substring({string}, {offset:integer}[. {length:integer}])
```

Extracts part of a string, starting at the Unicode code point indicated by the offset and returning the number of code points requested by the length. As previously mentioned, in SQL, indexes are usually 1-based. That means that the offset provided to substring should be at least `1`.

Examples:

```sql
SELECT substring('hello world', 6) AS s;
SELECT substring('hello: world', 1, position(':' IN 'hello: world')-1) AS s;
```

### format

Usage:

```sql
format({string}[, ...])
```

This function supports formatting strings, integers, floats, datetimes, intervals, etc, except `NULL`. The function does not support literal `{` and `}` characters in the format string.

Examples:

```sql
SELECT format('blob1: {}', blob1) AS s FROM dataset;
```

See also: [formatDateTime](#formatdatetime)

### toDateTime

Usage:

```sql
toDateTime(<expression>[, 'timezone string'])
```

`toDateTime` converts an expression to a datetime. This function does not support ISO 8601-style timezones; if your time is not in UTC then you must provide the timezone using the second optional argument.

Examples:

```sql
-- double1 contains a unix timestamp in seconds
toDateTime(double1)

-- blob1 contains an datetime in the format 'YYYY-MM-DD hh:mm:ss'
toDateTime(blob1)

-- literal values:
toDateTime(355924804) -- unix timestamp
toDateTime('355924804') -- string containing unix timestamp
toDateTime('1981-04-12 12:00:04') -- string with datetime in 'YYYY-MM-DD hh:mm:ss' format

-- interpret a date relative to New York time
toDateTime('2022-12-01 16:17:00', 'America/New_York')
```

### now

Usage:

```sql
now()
```

Returns the current time as a DateTime.

### toUnixTimestamp

Usage:

```sql
toUnixTimestamp(<datetime>)
```

`toUnixTimestamp` converts a datetime into an integer unix timestamp.

Examples:

```sql
-- get the current unix timestamp
toUnixTimestamp(now())
```

### formatDateTime

Usage:

```sql
formatDateTime(<datetime expression>, <format string>[, <timezone string>])
```

`formatDateTime` prints a datetime as a string according to a provided format string. See
[ClickHouse's docs](https://clickhouse.com/docs/en/sql-reference/functions/date-time-functions/#formatdatetime)
for a list of supported formatting options.

Examples:

```sql
-- prints the current YYYY-MM-DD in UTC
formatDateTime(now(), '%Y-%m-%d')

-- prints YYYY-MM-DD in the datetime's timezone
formatDateTime(<a datetime with a timezone>, '%Y-%m-%d')
formatDateTime(toDateTime('2022-12-01 16:17:00', 'America/New_York'), '%Y-%m-%d')

-- prints YYYY-MM-DD in UTC
formatDateTime(<a datetime with a timezone>, '%Y-%m-%d', 'Etc/UTC')
formatDateTime(toDateTime('2022-12-01 16:17:00', 'America/New_York'), '%Y-%m-%d', 'Etc/UTC')
```

### toStartOfInterval

Usage:

```sql
toStartOfInterval(<datetime>, INTERVAL '<n>' <unit>[, <timezone string>])
```

`toStartOfInterval` rounds down a datetime to the nearest offset of a provided interval. This can
be useful for grouping data into equal-sized time ranges.

Examples:

```sql
-- round the current time down to the nearest 15 minutes
toStartOfInterval(now(), INTERVAL '15' MINUTE)

-- round a timestamp down to the day
toStartOfInterval(timestamp, INTERVAL '1' DAY)

-- count the number of datapoints filed in each hourly window
SELECT
  toStartOfInterval(timestamp, INTERVAL '1' HOUR) AS hour,
  sum(_sample_interval) AS count
FROM your_dataset
GROUP BY hour
ORDER BY hour ASC
```

### extract

Usage:

```sql
extract(<time unit> from <datetime>)
```

`extract` returns an integer number of time units from a datetime. It supports
`YEAR`, `MONTH`, `DAY`, `HOUR`, `MINUTE` and `SECOND`.

Examples:

```sql
-- extract the number of seconds from a timestamp (returns 15 in this example)
extract(SECOND from toDateTime('2022-06-06 11:30:15'))
```

## Supported operators

The following operators are supported:

### Arithmetic operators

| Operator | Description    |
| -------- | -------------- |
| `+`      | addition       |
| `-`      | subtraction    |
| `*`      | multiplication |
| `/`      | division       |
| `%`      | modulus        |

### Comparison operators

| Operator     | Description                                                                                                   |
| ------------ | ------------------------------------------------------------------------------------------------------------- |
| `=`          | equals                                                                                                        |
| `<`          | less than                                                                                                     |
| `>`          | greater than                                                                                                  |
| `<=`         | less than or equal to                                                                                         |
| `>=`         | greater than or equal to                                                                                      |
| `<>` or `!=` | not equal                                                                                                     |
| `IN`         | true if the preceding expression's value is in the list<br/>`column IN ('a', 'list', 'of', 'values')`         |
| `NOT IN`     | true if the preceding expression's value is not in the list<br/>`column NOT IN ('a', 'list', 'of', 'values')` |

We also support the `BETWEEN` operator for checking a value is in an inclusive range: `a [NOT] BETWEEN b AND c`.

### Boolean operators

| Operator | Description                                                          |
| -------- | -------------------------------------------------------------------- |
| `AND`    | boolean "AND" (true if both sides are true)                          |
| `OR`     | boolean "OR" (true if either side or both sides are true)            |
| `NOT`    | boolean "NOT" (true if following expression is false and visa-versa) |

### Unary operators

| Operator | Description                            |
| -------- | -------------------------------------- |
| `-`      | negation operator (for example, `-42`) |

## Literals

| Type          | Syntax                                                                                                   |
| ------------- | -------------------------------------------------------------------------------------------------------- |
| integer       | `42`, `-42`                                                                                              |
| double        | `4.2`, `-4.2`                                                                                            |
| string        | `'so long and thanks for all the fish'`                                                                  |
| boolean       | `true` or `false`                                                                                        |
| time interval | `INTERVAL '42' DAY`<br/>Intervals of `YEAR`, `MONTH`, `DAY`, `HOUR`, `MINUTE` and `SECOND` are supported |

---

# Datadog

URL: https://developers.cloudflare.com/analytics/analytics-integrations/datadog/

This tutorial explains how to analyze Cloudflare metrics using the [Cloudflare Integration tile for Datadog](https://docs.datadoghq.com/integrations/cloudflare/).

## Overview

Before viewing the Cloudflare dashboard in Datadog, note that this integration:

* Is available to all Cloudflare customer plans (Free, Pro, Business and Enterprise)
* Is based on the Cloudflare Analytics API
* Provides Cloudflare web traffic and DNS metrics only
* Does not feature data coming from request logs stored in Cloudflare Logs

## Task 1 - Install the Cloudflare App

To install the Cloudflare App for Datadog:

1. Log in to **Datadog**.

2. Click the **Integrations** tab.

3. In the **search box**, start typing *Cloudflare*. The app tile should appear below the search box.
   ![Searching for Cloudflare App in the Datadog Integrations tab](~/assets/images/fundamentals/datadog/screenshots/datadog-integrations.png)

4. Click the **Cloudflare** tile to begin the installation.

5. Next, click **Configuration** and then complete the following:

   * **Account name**: (Optional) This can be any value. It has not impact on the site data pulled from Cloudflare.

   * **Email**: This value helps keep your account safe. We recommend creating a dedicated Cloudflare user for analytics with the [*Analytics* role](/fundamentals/setup/manage-members/roles/) (read-only). Note that the *Analytics* role is available to Enterprise customers only.

   * **API Key**: Enter your Cloudflare Global API key. For details refer to [API Keys](/fundamentals/api/get-started/keys/).

6. Click **Install Integration**.
   ![Configuring and installing the Datadog integration](~/assets/images/fundamentals/datadog/screenshots/cloudflare-tile-datadog-fill-details.png)

The Cloudflare App for Datadog should be installed now and you can view the dashboard.

## Task 2 - View the dashboard

By default, the dashboard displays metrics for all sites in your Cloudflare account. Use the dashboard filters see metrics for a specific domain.

The dashboard displays the following metrics:

* **Threats** (threats by type, threats by country)
* **Requests** (total requests, cached requests, uncached requests, top countries by request, requests by IP class, top content types)
* **Bandwidth** (total bandwidth, encrypted and unencrypted traffic cached bandwidth, uncached bandwidth)
* **Caching** (Cache hit rate, request caching rate over time)
* **HTTP response status errors**
* **Page views**
* **Search Engine Bot Traffic**
* **DNS** (DNS queries, response time, top hostnames, queries by type, stale vs. uncached queries)

![Dashboard displaying metrics for a site on a Cloudflare account](~/assets/images/fundamentals/datadog/dashboards/cloudflare-dashboard-datadog.png)

---

# Google Cloud

URL: https://developers.cloudflare.com/analytics/analytics-integrations/google-cloud/

## Overview

This tutorial covers how to configure certain Google Cloud Platform (GCP) components so that you can analyze your Cloudflare Logs data.

Before proceeding, you need to enable [Cloudflare Logpush in Google Cloud Storage](/logs/get-started/enable-destinations/google-cloud-storage/) to ensure your log data is available for analyzing.

The components we'll use in this tutorial include:

- **Google Cloud Function** to import logs from Google Cloud Storage to Google BigQuery
- **Google BigQuery** to make log data available to the reporting engine, and
- **Google Data Studio** to run interactive reports

The following diagram depicts how data flows from Cloudflare Logs through the different components of the Google Cloud Platform discussed in this tutorial.

![Data flow from Cloudflare Logpush to Google Cloud Platform](~/assets/images/fundamentals/google/cf-logpush-to-google-cloud-platform.png)

:::note

Google Cloud is offering a credit towards a new Google Cloud account to help you get started. To learn more, visit [Google Cloud Platform Partner Credit](https://cloud.google.com/partners/partnercredit/?PCN=a0n60000003kp9MAAQ).

:::

## Task 1 - Use Google Cloud Function to import log data into Google BigQuery

After you configured Cloudflare Logpush to send your logs to a Google Cloud Storage bucket, your log data updates every five minutes by default.

Google BigQuery makes data available for both querying using Structured Query Language (SQL) and for configuring as a data source for the Google Data Studio reporting engine. BigQuery is a highly scalable cloud database where SQL queries run quite fast.

Importing data from Google Cloud Storage into Google BigQuery requires creating a function using Google Cloud Function and running it in the Google Cloud Shell. This function triggers every time new Cloudflare log data is uploaded to your Google Cloud Storage bucket.

### Clone and deploy a Google Cloud Function

To a create a cloud function to import data from Google Cloud Storage into Google BigQuery, you will need the following GitHub repository from Cloudflare: [https://github.com/cloudflare/GCS-To-Big-Query](https://github.com/cloudflare/GCS-To-Big-Query).

To clone and deploy the cloud function:

1. Run the Google Cloud Platform shell by opening the **Google Cloud Platform** console and clicking the **Google Shell** icon (_Activate Cloud Shell_).

2. Run the following command to download the _master_ zipped archive, uncompress the files to new a directory, and change the command line prompt to the new directory: `curl -LO "https://github.com/cloudflare/cloudflare-gcp/archive/master.zip" && unzip master.zip && cd cloudflare-gcp-master/logpush-to-bigquery`

3. Next, edit the `deploy.sh` file and make sure that:

- **BUCKET_NAME** is set to the bucket you created when you configured Cloudflare Logpush with Google Cloud Platform.

- **DATASET** and **TABLE** are unique names.

  The contents of `deploy.sh` should look similar to this:

  ```bash
  .
  .
  .
  BUCKET_NAME="my_cloudflarelogs_gcp_storage_bucket"
  DATASET="my_cloudflare_logs"
  TABLE="cloudflare_logs"
  .
  .
  .
  ```

4. Then in the **Google Shell**, run the following command to deploy your instance of the cloud function:

   ```sh
   sh ./deploy.sh
   ```

Once you've deployed your new cloud function, verify that it appears in the **Cloud Functions** interface by navigating to **Google Cloud Platform** > **Compute** > **Cloud Functions**.

Also, verify that the data now appears in your table in **BigQuery** by navigating to the appropriate project in **Google Cloud Platform** > **Big Data** > **BigQuery**.

If everything is configured correctly, you can now query any request or visualize data with Google Data Studio or any other analytics tool that supports BigQuery as an input source.

### Add fields in Google Cloud Function

To add fields in Cloud Function, edit the `schema.json` file.

1. Open Google Cloud Function.

2. Select the function you want to update.

3. Click **EDIT** on the Function details page.

4. Select `schema.json` from the list of files.

5. In the file editor, enter the `name`, `type`, and `mode` of any fields you would like to add. Follow the format shown in the file.

6. Click **Deploy**.

To debug in Cloud Function, click **VIEW LOGS** on the Function details page. This will take you to the Logs Viewer, where any errors will appear.

### Add fields in BigQuery

To add fields in BigQuery, edit the schema.

1. Open BigQuery.

2. In the menu, expand **your-project-name**.

3. Expand **cloudflare_data** and click **cf_analytics_logs**.

4. Select the **Schema** tab.

5. Scroll to the bottom of the page, and click **Edit schema**.

6. On the pop-up page, click **Add field**. Enter the field **Name** and select the field **Type** and **Mode** from the dropdowns.

7. Click **Save**.

## Task 2 - Analyze log data with Google Data Studio

To analyze and visualize logs, you can use **Google Data Studio** or any other 3rd party services that supports Google BigQuery as an input source.

With Google Data Studio, you can generate graphs and charts from a Google BigQuery table. You can also refresh the data in your reports and get real-time analytics.

### About the Cloudflare Logs Insights Template

Cloudflare has published a [Logs Insights Template](https://datastudio.google.com/u/0/reporting/1ez3m7Yf8AZLfM6aYRjfgF0pPpRvOwhTh/page/mAzI/preview) in the **Google Data Studio Report Gallery**.

![Cloudflare Logs Insights Template on Google Data Studio Report Gallery](~/assets/images/fundamentals/google/cf-logs-insights-template.png)

The Cloudflare Insights Template features several dashboards, or report pages, to help you analyze your Cloudflare Logs data. You can also use filters within the dashboards to narrow down the analysis by date and time, device type, country, user agent, client IP, hostname, and more. These insights further help with debugging and tracing.

The following dashboards are included in the Insights template:

- **Snapshot**: Gives you an overview of the most important metrics from your Cloudflare logs, including total number of requests, top visitors by geography, IP, user agent, traffic type, total number of threats, and bandwidth usage.

- **Security**: Provides insights on threat identification and mitigations by our **Web Application Firewall**, including **Firewall Rules**, **Rate Limiting**, and **IP Firewall**. Metrics include total threats stopped, threat traffic source, blocked IPs and user agents, top threat requests, security events (SQL injections, XSS, etc.), and rate limiting. Use this data to fine tune the firewall to target obvious threats and avoid false positives.

- **Performance**: Helps you identify and address issues like slow pages and caching misconfigurations. Metrics include total vs. cached bandwidth, cache ratio, top uncached requests, static vs. dynamic content, slowest URIs, and more.

- **Reliability**: Provides insights on the availability of your websites and applications. Metrics include origin response error ratio, origin response status over time, percentage of 3xx/4xx/5xx errors over time, and more.

### Create a report based on the Insights Template

To create a report for your log data based on the Cloudflare template:

1. In Data Studio, open the Cloudflare [template](https://datastudio.google.com/u/0/reporting/1ez3m7Yf8AZLfM6aYRjfgF0pPpRvOwhTh/page/mAzI/preview) and click **Use Template**. A _Create new report_ dialog opens.

2. Under the **New Data Source** dropdown, select **Create New Data Source**. A page opens where you can enter additional configuration details.

3. Under **Google Connectors**, locate the **BigQuery** card and click **Select**.

4. Next under **MY PROJECTS**, select your **Project**, **Dataset**, and **Table**.

5. Click **Connect** in the upper right.

6. In the list of Cloudflare Logs fields, locate _EdgeStartTimestamp_, click the three vertical dots and select **Duplicate**. This creates _Copy of EdgeStartTimestamp_ right below _EdgeStartTimestamp_.

7. Update the **Type** for _Copy of EdgeStartTimestamp_ to set it to _Date & Time_ > _Date Hour (YYYYMMDDHH)_.

8. Next, update the **Type** for each of the following fields as indicated below:

| Cloudflare Log Field       | Type                                 |
| -------------------------- | ------------------------------------ |
| ZoneID                     | Text                                 |
| EdgeColoID                 | Text                                 |
| ClientSrcPort              | Text                                 |
| EdgeResponseStatus         | Number                               |
| Copy of EdgeStartTimestamp | Date & Time > Date Hour (YYYYMMDDHH) |
| OriginResponseStatus       | Number                               |
| ClientASN                  | Text                                 |
| ClientCountry              | Geo > Country                        |

9. Next, add a new field to identify and calculate threat. In the top right corner, click **+ ADD A FIELD**, then in the add field UI:

   - For **Field Name**, type _Threats_.

   - In the **Formula** text box, paste the following code:

   ```sql
   CASE
   WHEN EdgePathingSrc = "user" AND EdgePathingOp = "ban" AND EdgePathingStatus = "ip" THEN "ip block"
   WHEN EdgePathingSrc = "user" AND EdgePathingOp = "ban" AND EdgePathingStatus = "ctry" THEN "country block"
   WHEN EdgePathingSrc = "user" AND EdgePathingOp = "ban" AND EdgePathingStatus = "zl" THEN "routed by zone lockdown"
   WHEN EdgePathingSrc = "user" AND EdgePathingOp = "ban" AND EdgePathingStatus = "ua" THEN "blocked user agent"
   WHEN EdgePathingSrc = "user" AND EdgePathingOp = "ban" AND EdgePathingStatus = "rateLimit" THEN "rate-limiting rule"
   WHEN EdgePathingSrc = "bic" AND EdgePathingOp = "ban" AND EdgePathingStatus = "unknown" THEN "browser integrity check"
   WHEN EdgePathingSrc = "hot" AND EdgePathingOp = "ban" AND EdgePathingStatus = "unknown" THEN "blocked hotlink"
   WHEN EdgePathingSrc = "macro" AND EdgePathingOp = "chl" AND EdgePathingStatus = "captchaFail" THEN "CAPTCHA challenge failed"
   WHEN EdgePathingSrc = "macro" AND EdgePathingOp = "chl" AND EdgePathingStatus = "jschlFail" THEN "java script challenge failed"
   WHEN EdgePathingSrc = "filterBasedFirewall" AND EdgePathingOp = "ban" AND EdgePathingStatus = "unknown" THEN "blocked by filter based firewall"
   WHEN EdgePathingSrc = "filterBasedFirewall" AND EdgePathingOp = "chl" THEN "challenged by filter based firewall"
   Else "Other"
   END
   ```

   - Click **Save** in the lower right corner.

10. Finally, add another new field for grouping status error codes. In the top right corner, click **+ ADD A FIELD**, then in the add field UI:

    - For **Field Name**, type _EdgeResponseStatusClass_.
    - In the **Formula** text box, paste the following code:

    ```sql
    CASE
    WHEN EdgeResponseStatus > 199 AND EdgeResponseStatus < 300 THEN "2xx"
    WHEN EdgeResponseStatus > 299 AND EdgeResponseStatus < 400 THEN "3xx"
    WHEN EdgeResponseStatus > 399 AND EdgeResponseStatus < 500 THEN "4xx"
    WHEN EdgeResponseStatus > 499 AND EdgeResponseStatus < 600 THEN "5xx"
    WHEN EdgeResponseStatus = 0 THEN "0 - Served from CF Edge"
    Else "Other"
    END
    ```

    - Click **Save** in the lower right corner.

11. To finish, click **Add to Report** in the upper right.

### Refreshing fields and filters manually

After you've added your report, you will notice that not all report components render successfully. To fix this, you need to resolve any errors related to invalid dimensions, metrics, or filters that appear in the affected report components.

#### Update Data Studio with new fields

To update Data Studio with fields added to BigQuery, refresh fields for the data source.

1. In Data Studio, open the Cloudflare dashboard in **Edit** mode.

2. Expand the **Resource** menu and select **Manage added data sources**.

3. Click the **EDIT** action for the data source that you want to update.

4. Click **REFRESH FIELDS** below the table. A window with **Field changes found** in BigQuery will pop up.

5. To add the new fields, click **APPLY**.

You can also create custom fields directly in Data Studio.

1. In Data Studio, open the Cloudflare dashboard in **Edit** mode.

2. Expand the **Resource** menu and select **Manage added data sources**.

3. Click the **EDIT** action for the data source that you want to add a custom field to.

4. Click **ADD A FIELD** above the table.

5. Enter a formula in the **Formula** editor.

6. Click **SAVE**.

#### Fix invalid metric or dimension errors

The following table summarizes which specific components require to be fixed:

<table>
	<thead>
		<tr>
			<th>Report page</th>
			<th>Components</th>
			<th>Field to add</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td rowspan="3">2 Security Cloudflare&nbsp;</td>
			<td>
				<p>
					<em>Threats </em>(scorecard)
				</p>
			</td>
			<td>&nbsp;Threats (Metric)</td>
		</tr>
		<tr>
			<td>
				<p>
					<em>Threats - Record Count (table)</em>
				</p>
			</td>
			<td>&nbsp;Threats (Dimension)</td>
		</tr>
		<tr>
			<td>
				&nbsp;<em>Threats Over Time&nbsp;</em>(area chart)
			</td>
			<td>&nbsp;Threats (Breadown Dimension)</td>
		</tr>
		<tr>
			<td>3 Reliability Cloudflare</td>
			<td>
				<em>Status Codes Last 24 hours&nbsp;</em>(bar chart)
			</td>
			<td>
				<em>Copy of EdgeStartTimeStamp&nbsp;</em>(Dimension)
			</td>
		</tr>
		<tr>
			<td>5&nbsp;Last 100s Requests Cloudflare</td>
			<td>
				<em>Last 100 Requests</em> (table)
			</td>
			<td>
				&nbsp;<em>Copy of EdgeStartTimeStamp</em>
			</td>
		</tr>
	</tbody>
</table>

For each of the report components listed above:

1. Select the report component affected.

2. On the menu to the right, under the **Data** tab, locate and click **Invalid Dimension** or **Invalid Metric** (as applicable). The **Field Picker** panel opens.

3. Search or type for the field to add, then click to select it.

4. To finish, click away from the panel to return to the main report.

The component should now render correctly.

#### Update data filters

This fix applies to report page: **3 Reliability Cloudflare**, for the following scorecard components in the report:

- **5xx Errors**
- **4xx Errors**
- **3xx Errors**

To update the filter associated with each scorecard:

1. Select the report component affected.

2. On the menu to the right, under the **Data** tab, locate the **Filters** > **Scorecard Filter** section and click the **pencil** next to the filter name to edit it. The **Edit Filter** panel opens.

3. In the filtering criteria section, click the dropdown and scroll or search for the field _EdgeResponseStatusClass_ and select it.

4. To finish, click **Save** in the lower right corner.

The component should now render correctly.

---

# Querying from a Worker

URL: https://developers.cloudflare.com/analytics/analytics-engine/worker-querying/

import { WranglerConfig } from "~/components";

If you want to access Analytics Engine data from within a Worker you can use `fetch` to access the SQL API. The API can return JSON data that is easy to interact with in JavaScript.

## Authentication

In order that your Worker can authenticate with the API you will need your account ID and an API token.

- Your 32 character account ID can be obtained from the Cloudflare dashboard.
- An API token can also be generated in the dashboard. Refer to the [SQL API docs](/analytics/analytics-engine/sql-api/#authentication) for more information on this.

We recommend storing the account ID as an environment variable and the API token as a secret in your worker. This can be done through the dashboard or through Wrangler. Refer to the [Workers documentation](/workers/configuration/environment-variables/) for more details on this.

## Querying

Use the JavaScript `fetch` API as follows to execute a query:

```js
const query = "SELECT * FROM my_dataset";
const API = `https://api.cloudflare.com/client/v4/accounts/${env.ACCOUNT_ID}/analytics_engine/sql`;
const response = await fetch(API, {
	method: "POST",
	headers: {
		Authorization: `Bearer ${env.API_TOKEN}`,
	},
	body: query,
});
const responseJSON = await response.json();
```

The data will be returned in the format described in the [FORMAT section of the API docs](/analytics/analytics-engine/sql-reference/#json) allowing you to extract meta information about the names and types of returned columns in addition to the data itself and a row count.

## Example Worker

The following is a sample Worker which executes a query against a dataset of weather readings and displays minimum and maximum values for each city.

### Environment variable setup

First the environment variables are set up with the account ID and API token.

The account ID is set in the [Wrangler configuration file](/workers/wrangler/configuration/):

<WranglerConfig>

```toml
[vars]
ACCOUNT_ID = "<account_id>"
```

</WranglerConfig>

The API_TOKEN can be set as a secret, using the wrangler command line tool, by running the following and entering your token string:

```sh
npx wrangler secret put API_TOKEN
```

### Worker script

The worker script itself executes a query and formats the result:

```js
export default {
	async fetch(request, env) {
		// This worker only responds to requests at the root.
		if (new URL(request.url).pathname != "/") {
			return new Response("Not found", { status: 404 });
		}

		// SQL string to be executed.
		const query = `
            SELECT
                blob1 AS city,
                max(double1) as max_temp,
                min(double1) as min_temp
            FROM weather
            WHERE timestamp > NOW() - INTERVAL '1' DAY
            GROUP BY city
            ORDER BY city`;

		// Build the API endpoint URL and make a POST request with the query string
		const API = `https://api.cloudflare.com/client/v4/accounts/${env.ACCOUNT_ID}/analytics_engine/sql`;
		const queryResponse = await fetch(API, {
			method: "POST",
			headers: {
				Authorization: `Bearer ${env.API_TOKEN}`,
			},
			body: query,
		});

		// The API will return a 200 status code if the query succeeded.
		// In case of failure we log the error message and return a failure message.
		if (queryResponse.status != 200) {
			console.error("Error querying:", await queryResponse.text());
			return new Response("An error occurred!", { status: 500 });
		}

		// Read the JSON data from the query response and render the data as HTML.
		const queryJSON = await queryResponse.json();
		return new Response(renderResponse(queryJSON.data), {
			headers: { "content-type": "text/html" },
		});
	},
};

// renderCity renders a table row as HTML from a data row.
function renderCity(row) {
	return `<tr><td>${row.city}</td><td>${row.min_temp}</td><td>${row.max_temp}</td></tr>`;
}

// renderResponse renders a simple HTML table of results.
function renderResponse(data) {
	return `<!DOCTYPE html>
<html>
    <body>
        <table>
            <tr><th>City</th><th>Min Temp</th><th>Max Temp</th></tr>
            ${data.map(renderCity).join("\n")}
        </table>
    </body>
<html>`;
}
```

---

# Graylog

URL: https://developers.cloudflare.com/analytics/analytics-integrations/graylog/

This tutorial explains how to analyze [Cloudflare Logs](https://www.cloudflare.com/products/cloudflare-logs/) using [Graylog](https://github.com/Graylog2/graylog-s3-lambda/blob/master/content-packs/cloudflare/cloudflare-logpush-content-pack.json).

## Overview

If you haven't used Cloudflare Logs before, visit our [Logs documentation](/logs/) for
more details. Contact your Cloudflare Customer Account Team to enable logs for
your account.

### Prerequisites

Before sending your Cloudflare log data to Graylog, make sure that you:

* Have an existing Graylog installation. Both single-node and cluster configurations are supported
* Have a Cloudflare Enterprise account with Cloudflare Logs enabled
* Configure [Logpush](/logs/about/)

:::note[Note]


Cloudflare logs are HTTP/HTTPS request logs in JSON format and are gathered from our 200+ data centers globally. By default, timestamps are returned as UNIX nanosecond integers. All timestamp formats are supported by Graylog.


:::

## Task 1 - Preparation

Before getting Cloudflare logs into Graylog:

1. Configure Cloudflare [Logpush](/logs/about/) to push logs with all desired fields to an AWS S3 bucket of your choice.
2. Download the latest [Graylog Integration for Cloudflare](https://github.com/Graylog2/graylog-s3-lambda/blob/master/content-packs/cloudflare/cloudflare-logpush-content-pack.json).
3. Decompress the zip file.

Once decompressed, the integration package includes:

* *graylog-s3-lambda.jar*
* *content-packs/cloudflare/cloudflare-logpush-content-pack.json*
* *content-packs/cloudflare/threat-lookup.csv*

## Task 2 - Create and configure the AWS Lambda Function

1. Navigate to the Lambda service page in the AWS web console.
2. Create a new Lambda function and specify a *function name* of your choice and the *Java-8 runtime*.
3. Create or specify an execution role with the following permissions. You can also further restrict the resource permissions as desired for your specific set-up.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Policy",
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup"
        "s3:GetObject",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": [
        "arn:aws:logs:your-region:your-account-number:*",
        "arn:aws:s3:your-region::cloudflare-bucket-name/*"
      ]
    }
  ]
}
```

**Note:** If your Graylog cluster is running in a VPC, you may need to add the *AWSLambdaVPCAccessExecutionRole* managed role to allow the Lambda function to route traffic to the VPC.

4. Once you've created the Lambda function, upload the function code ***graylog-s3-lambda.jar*** downloaded in [Task 1](#task-1---preparation). Â Specify the following method for the Handler: *org.graylog.integrations.s3.GraylogS3Function::handleRequest*.

5. Specify at least the following required environment variables to configure the Lambda function for your Graylog cluster:

   * **CONTENT\_TYPE** (required) - *application/x.cloudflare.log* value to indicate that the Lambda function will process Cloudflare logs.
   * **COMPRESSION\_TYPE** ***(required*** **)** - *gzip* since Cloudflare logs are gzip compressed.
   * **GRAYLOG\_HOST** *(required)* - hostname or IP address of the Graylog host or cluster load balancer.
   * **GRAYLOG\_PORT** *(optional - defaults to 12201)* - The Graylog service port.
   * **CONNECT\_TIMEOUT** *(optional - defaults to 10000)* - The number of milliseconds to wait for the connection to be established.
   * **LOG\_LEVEL** *(optional - defaults to INFO)* - The level of detail to include in the CloudWatch logs generated from the Lambda function. Supported values are *OFF*, *ERROR*, *WARN*, *INFO*, *DEBUG*, *TRACE*, and *ALL*. Increase the logging level to help with troubleshooting. See [Defining Custom Log Levels in Code](https://logging.apache.org/log4j/2.0/manual/customloglevels.html) for more information.
   * **CLOUDFLARE\_LOGPUSH\_MESSAGE\_FIELDS** *(optional - defaults to all)* - The fields to parse from the message. Specify as a comma-separated list of field names.
   * **CLOUDFLARE\_LOGPUSH\_MESSAGE\_SUMMARY\_FIELDS** *(optional - defaults to ClientRequestHost, ClientRequestPath, OriginIP, ClientSrcPort, EdgeServerIP, EdgeResponseBytes)* - The fields to include in the message summary that appears above the parsed fields at the top of each message in Graylog. Specify as a comma-separated list of field names.
     ![List of required Graylog environment variables](~/assets/images/fundamentals/graylog/screenshots/graylog-environment-variables.png)

     **Note:** More configuration variables are available to fine-tune the function configuration in the Graylog Lambda S3 [README](https://github.com/Graylog2/graylog-s3-lambda/blob/master/README.md#step-2-specify-configuration) file.

6. Create an AWS S3 Trigger for the Lambda function so that the function can process each Cloudflare log field that is written. Specify the same S3 bucket from [Task 1](#task-1---preparation) and choose the *All object create events* option. Any other desired file filters can be applied here.
   ![Add trigger dialog with an example AWS S3 Trigger](~/assets/images/fundamentals/graylog/screenshots/aws-s3-add-trigger.png)

7. If your Graylog cluster is located within a VPC, you will need to [configure your Lambda function to access resources in a VPC](https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html). You may also need to create a [VPC endpoint for the AWS S3 service](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html#create-vpc-endpoint). This allows the Lambda function to access S3 directly when running in a VPC.

:::note[Note]


By default, all log messages are sent over TCPt. TLS encryption between the
Lambda function and Graylog is not currently supported. We recommend taking
appropriate measures to secure the log messages in transit, such as placing
the Lambda function within a secure VPC subnet where the Graylog node or
cluster is running.


:::

## Task 3 - Import the content pack in Graylog

Importing the Cloudflare Logpush content pack into Graylog loads the
necessary configuration to receive Cloudflare logs and installs the
Cloudflare dashboards.

The following components install with the content pack:

* Cloudflare dashboards ([Task 4](#task-4---view-the-cloudflare-dashboards)).
* A Cloudflare GELF (TCP) input that allows Graylog to receive Cloudflare logs.
* A Cloudflare message [stream](https://docs.graylog.org/en/3.1/pages/streams.html).
* [Pipeline](https://docs.graylog.org/en/3.1/pages/pipelines/pipelines.html) rules that help to process and parse Cloudflare log fields.

To import the content pack:

1. Locate the *cloudflare-logpush-content-pack.json* file that you downloaded and extracted in [Task 1](#task-1---preparation).

2. In Graylog, go to **System** > **Content Packs** and click **Upload** in the top right. Once uploaded, the Cloudflare Logpush content pack will appear in the list of uploaded content packs.
   ![Uploading Graylog content packs](~/assets/images/fundamentals/graylog/screenshots/graylog-content-packs.png)

3. Click **Install**.
   ![Installing Graylog content packs](~/assets/images/fundamentals/graylog/screenshots/graylog-content-packs-uploaded.png)

4. In the **Install** dialog, enter an optional install comment, and verify that the correct values are entered for all configuration parameters.

   * A path is required for the MaxMindâ„¢ï¸ database, available at [https://dev.maxmind.com/geoip/](https://dev.maxmind.com/geoip/).
   * A path is also required for the *Threat Lookup* CSV file, extracted in [Task 1](#task-1---preparation).

   ![Adding an install comment and configuring parameters in Install Dialog screen](~/assets/images/fundamentals/graylog/screenshots/graylog-content-pack-install.png)

5. Once installed, your Graylog cluster will be ready to receive Cloudflare logs from the Lambda function.

Refer to the Graylog Lambda S3 [README](https://github.com/Graylog2/graylog-s3-lambda/blob/master/README.md) for additional information and troubleshooting tips.

## Task 4 - View the Cloudflare Dashboards

You can view your dashboard in the [Graylog Cloudflare integration page](https://go.graylog.com/cloudflare). The dashboards include:

### Cloudflare - Snapshot

This is an at-a-glance overview of the most important metrics from your websites and applications on the Cloudflare network. You can use dashboard filters to further slice and dice the information for granular analysis of events and trends.

Use this dashboard to:

* Monitor the most important web traffic metrics of your websites and applications on the Cloudflare network
* View which countries and IPs your traffic is coming from, and analyze the breakdown between mobile and desktop traffic, protocol, methods, and content types

![Visualizing Cloudflare log metrics in the Graylog dashboard](~/assets/images/fundamentals/graylog/dashboards/snapshot-cloudflare-dashboard-graylog.png)

### Cloudflare - Security

This overview provides insights into threats to your websites and applications, including number of threats stopped,threats over time, top threat countries, and more.

Use this dashboard to:

* Monitor the most important security and threat metrics for your websites and applications
* Fine-tune and configure your IP firewall

![Visualizing an analysis of Cloudflare threat traffic in the Graylog dashboard](~/assets/images/fundamentals/graylog/dashboards/security-cloudflare-dashboard-graylog.png)

### Cloudflare - Performance

This dashboard helps to identify and address performance issues and caching misconfigurations. Metrics include total vs. cached bandwidth, saved bandwidth, total requests, cache ratio, top uncached requests, and more.

Use this dashboard to:

* Monitor caching behavior and identify misconfigurations
* Improve configuration and caching ratio

![Visualizing Cloudflare Performance metrics in the Graylog dashboard](~/assets/images/fundamentals/graylog/dashboards/performance-cloudflare-dashboard-graylog.png)

### Cloudflare - Reliability

This dashboard provides insights on the availability of your websites and applications. Metrics include origin response error ratio, origin response status over time, percentage of 3xx/4xx/5xx errors over time, and more.

Use this dashboard to:

* Investigate errors on your websites and applications by viewing edge and origin response status codes
* Further analyze errors based on status codes by countries, client IPs, hostnames, and other metrics

![Graylog dashboard Cloudflare Reliability](~/assets/images/fundamentals/graylog/dashboards/reliability-cloudflare-dashboard-graylog.png)

### Cloudflare - Bots

Use this dashboard to detect and mitigate bad bots so that you can prevent credential stuffing, spam registration, content scraping, click fraud, inventory hoarding, and other malicious activities.

:::note[Note]


To get bot requests identified correctly, use only one WAF custom rule (or firewall rule), configured with the action *Interactive Challenge*. To learn more about custom rules, refer to the [WAF documentation](/waf/custom-rules/).


:::

Use this dashboard to:

* Investigate bot activity on your website and prevent content scraping, checkout fraud, spam registration, and other malicious activities.
* Use insight to tune Cloudflare to prevent bots from excessive usage and abuse across websites, applications, and API endpoints.

![Graylog dashboard Cloudflare Bot Management](~/assets/images/fundamentals/graylog/dashboards/bot-management-cloudflare-dashboard-graylog.png)

---

# Analytics integrations

URL: https://developers.cloudflare.com/analytics/analytics-integrations/

import { DirectoryListing } from "~/components"

Cloudflare Enterprise customers can use Cloudflare integrations with their preferred analytics provider and configure ready-to-use Cloudflare Dashboards. Most analytics integrations are built on Cloudflare Logs by using Logpush with either Amazon S3 bucket or GCP Storage bucket.

Analyze [Cloudflare Logs](/logs/) data with the following analytics platforms:

<DirectoryListing />

---

# Looker

URL: https://developers.cloudflare.com/analytics/analytics-integrations/looker/

This tutorial explains how to analyze [Cloudflare Logs](https://www.cloudflare.com/products/cloudflare-logs/) using the [Cloudflare Log Analytics for Looker](https://looker.com/platform/blocks/source/cloudflare-log-analytics).

## Overview

If you have not used Cloudflare Logs before, refer to the [Logs documentation](/logs/) for more details. Contact your Cloudflare Customer Account Team to enable logs for your account.

This tutorial uses Cloudflare Logpush to send logs to [Google Cloud Storage Bucket and Cloud Function](/analytics/analytics-integrations/google-cloud/) and then import them into Google Big Query.

### Prerequisites

Before sending your Cloudflare log data to Looker, make sure that you:

* Have an existing Looker account
* Have a Cloudflare Enterprise account with Cloudflare Logs enabled
* Configure [Logpush](/logs/about/) or [Logpull](/logs/logpull/)
* Load your data in a [database supported by Looker](https://looker.com/solutions/other-databases)

:::note[Note]


Cloudflare logs are HTTP/HTTPS request logs in JSON format and are gathered from our 200+ data centers globally. By default, timestamps are returned as Unix nanosecond integers. We recommend using the RFC 3339 format for sending logs to Looker.


:::

## Task 1 - Connect your Database to Looker

Looker connects to a database in order to query the data. In this tutorial, we use Google Big Query as an example. Learn [how to connect Google BigQuery to Looker](https://docs.looker.com/setup-and-management/database-config/google-bigquery#create_a_temporary_dataset_for_persistent_derived_tables).

Once you load Cloudflare logs into your database, [connect the database to Looker](https://docs.looker.com/setup-and-management/connecting-to-db).

## Task 2 - Create a new LookML project from the public Git repository

To create your new LookML project:

1. Log in to your Looker account.

2. In the menu bar, click **Develop** and make sure **Development Mode** is set to *ON*.

3. Next, also under **Develop**, click **Manage LookML Projects**.

4. At the top right of the LookML Projects page, click **New LookML Project**.

5. In the **New Project** dialog, enter a project name.

6. For **Starting Point**, choose \*Clone Public Git Repository\*\*.\*\*\*

7. Enter the *cloudflare\_block* URL for the public Git repository `git://github.com/llooker/cloudflare_block.git`.

8. Click **Create Project**. Looker will pull all of the repository files into a new LookML project.

9. Next, open the project.

10. Click **Deploy from Remote** to pull all remote changes into your local version of the repository.
    ![LookML projects page highlighting Develop menu and Manage LookML Projects and New LookML Project options](~/assets/images/fundamentals/looker/screenshots/develop-look-ml-project.png)

## Task 3 - Update the connection name

To update the connection name in the LookML files:

1. In your LookML **cloudflare\_looker** model file, replace the **connection** name with yours, for example:
   `connection: "bigquery_lpr"`.

2. Check if any table names need to be updated to your database connection names as well. If you decide to rename the filenames for explore, model name, and view, make sure to update all mentions within the other files. Otherwise, you might encounter errors.

## Task 4 - View the Dashboards

In the main menu, click **Browse** and select **LookML Dashboards**. You should see all the Cloudflare dashboards that were pulled from GitHub.

### About the Dashboards

There are five dashboards to help you analyze Cloudflare logs. You can also use filters within the dashboards to help narrow the analysis by date and time, device type, country, user agent, client IP, hostname, and more.

#### Snapshot

This is a quick overview of the most important metrics from your Cloudflare logs, including total number of requests, top visitors by country, client IP, user agent, traffic type, total number of threats, and bandwidth usage.

![Looker dashboard highlighting Cloudflare metrics including Web Traffic Overview and Web Traffic Types](~/assets/images/fundamentals/looker/dashboards/snapshot-cloudflare-dashboard-looker.png)

#### Security

This dashboard provides insights on threat identification and mitigation through our Web Application Firewall (WAF) and IP Firewall. Metrics include total threats stopped, threat traffic source, blocked IPs, and user agents, top threat requests, WAF events (SQL injections, XSS, etc.), and rate limiting. Use this data to fine tune the firewall to target obvious threats and prevent false positives.

![Looker dashboard highlighting Cloudflare metrics including Threats and Threats Over time](~/assets/images/fundamentals/looker/dashboards/security-cloudflare-dashboard-looker.png)

#### Performance

This dashboard helps you identify and address issues like slow pages and caching misconfigurations. Metrics include total vs. cached bandwidth, cache ratio, top uncached requests, static vs. dynamic content, slowest URIs, and more.

![Looker dashboard highlighting Cloudflare metrics including Requests, Bandwidth, and Cache](~/assets/images/fundamentals/looker/dashboards/performance-cloudflare-dashboard-looker.png)

#### Reliability

This dashboard provides insights on the availability of your websites and applications. Metrics include origin response error ratio, origin response status over time, percentage of 3xx/4xx/5xx errors over time, and more.

![Looker dashboard highlighting Cloudflare metrics including Edge and Origin Response Status Over time and Error Ratios](~/assets/images/fundamentals/looker/dashboards/reliability-cloudflare-dashboard-looker.png)

#### Bot Management

This dashboard allows to reliably detect and mitigate bad bots to prevent credential stuffing, spam registration, content scraping, click fraud, inventory hoarding, and other malicious activities. Use these insights to tune Cloudflare and prevent bots from excessive usage and abuse across websites, applications, and API endpoints.

![Looker dashboard highlighting Cloudflare metrics including Global Traffic, False Detected Bots, and Bad Bots](~/assets/images/fundamentals/looker/dashboards/bot-management-cloudflare-dashboard-looker.png)

### Filters

All dashboard have a set of filters that you can apply to the entire dashboard, as shown in the following example. The filters apply across the entire dashboard.

![List of the available filters from the Looker dashboard](~/assets/images/fundamentals/looker/screenshots/cloudflare-looker-dashboard-filters.png)

The default time interval is set to 24 hours. Note that for correct calculations, by default, filters exclude Worker subrequests (**WorkerSubrequest** = *false*) and purge requests (**ClientRequestMethod** is not *PURGE*).

Available Filters:

* Date (EdgeStartTimestamp)

* Device type

* Country

* Client IP

* Hostname

* Request URI

* Origin Response Status

* Edge response status

* Origin IP

* User Agent

* RayID

* Worker Subrequest

* Client Request Method

With the following pre-set filter values in the Looker dashboards all workers subrequests and client request method PURGE are excluded from the calculations:

* **WorkerSubrequet** set to value *False*

* **ClientRequestMethod** doesnâ€™t equal to *PURGE*

You can always adjust your default filters values according to your needs.

---

# New Relic

URL: https://developers.cloudflare.com/analytics/analytics-integrations/new-relic/

This tutorial explains how to analyze Cloudflare metrics using the [New Relic One Cloudflare Quickstart](https://newrelic.com/instant-observability/cloudflare/fc2bb0ac-6622-43c6-8c1f-6a4c26ab5434).

## Prerequisites

Before sending your Cloudflare log data to New Relic, make sure that you:

* Have a Cloudflare Enterprise account with Cloudflare Logs enabled.
* Have a New Relic account.
* Configure [Logpush to New Relic](/logs/get-started/enable-destinations/new-relic/).

## Task 1 - Install the Cloudflare Network Logs quickstart

1. Log in to New Relic.
2. Click the Instant Observability button (top right).
3. Search for **Cloudflare Network Logs**.

![Cloudflare Network Logs install screen](~/assets/images/fundamentals/new-relic/screenshots/cloudflare-network-logs.png)

4. Click **Install this quickstart**.
5. Follow the steps to deploy.

## Task 2 - View the Cloudflare Dashboards

You can view your dashboards on the New Relic dashboard page. The dashboards include the following information:

### Overview

Get a quick overview of the most important metrics from your websites and applications on the Cloudflare network.

![Cloudflare Network Logs install screen](~/assets/images/fundamentals/new-relic/dashboard/dash-1.png)

### Security

Get insights on threats to your websites and applications, including number of threats taken action on by the Web Application Firewall (WAF), threats over time, top threat countries, and more.

![Cloudflare Network security metrics screen](~/assets/images/fundamentals/new-relic/dashboard/dash-2.png)

### Performance

Identify and address performance issues and caching misconfigurations. Metrics include total requests, total versus cached requests, total versus origin requests.

![Cloudflare Network Logs performance metrics screen](~/assets/images/fundamentals/new-relic/dashboard/dash-3.png)

### Reliability

Get insights on the availability of your websites and Applications. Metrics include, edge response status over time, percentage of `3xx`/`4xx`/`5xx` errors over time, and more.

![Cloudflare Network Logs reliability metrics screen](~/assets/images/fundamentals/new-relic/dashboard/dash-4.png)

---

# Sentinel

URL: https://developers.cloudflare.com/analytics/analytics-integrations/sentinel/

Microsoft has developed a Cloudflare connector that allows their customers to integrate [Cloudflare Logs](/logs/) with Microsoft Sentinel.

## How it works

[Logpush](/logs/get-started/enable-destinations/azure/) sends logs from Cloudflare to Azure Blob Storage. From there, the Cloudflare connector, a Microsoft function, ingests these logs into Azure Log Analytics Workspace, making them available for monitoring and analysis in Microsoft Sentinel.

![Sentinel integrations steps](~/assets/images/analytics/sentinel-diagram.png)

For more details, refer to the Microsoft documentation [Cloudflare connector for Microsoft Sentinel](https://learn.microsoft.com/en-us/azure/sentinel/data-connectors/cloudflare).

---

# Splunk

URL: https://developers.cloudflare.com/analytics/analytics-integrations/splunk/

import { Render } from "~/components"

This tutorial explains how to analyze [Cloudflare Logs](https://www.cloudflare.com/products/cloudflare-logs/) using the [Cloudflare App for Splunk](https://splunkbase.splunk.com/app/4501/).

## Prerequisites

Before sending your Cloudflare log data to Splunk, ensure that you:

* Have an existing Splunk Enterprise or Cloud account
* Have a Cloudflare Enterprise account
* Consult the [Splunk documentation](https://splunkbase.splunk.com/app/4501/) for the Cloudflare App

## Task 1 - Install and Configure the Cloudflare App for Splunk

To install the [Cloudflare App for Splunk](https://splunkbase.splunk.com/app/4501/):

1. Log in to your Splunk instance.
2. Under **Apps** > **Find More Apps**, search for *Cloudflare App for Splunk.*
3. Click **Install**.

![Splunk website with Apps menu expanded and Search & Reporting menu item along with Cloudflare App for Splunk](~/assets/images/fundamentals/splunk/screenshots/splunk-cloudflare-app-for-splunk.png)

4. Restart and reopen your Splunk instance.

5. Edit the `cloudflare:json` source type in the Cloudflare App for Splunk. To edit the source type:

   1. Click the **Settings** dropdown and select **Source types**.
   2. Uncheck **Show only popular** and search for *cloudflare*.
   3. Click **Edit** and change the Regex expression to `([\r\n]+)`.
   4. Save your edits.

6. Create an index on Splunk to store the HTTP Event logs. To create an index:

   1. Open the setup screen by clicking the **Settings** dropdown, then click **Indexes**.
   2. Select **New Index**. Note that the **Indexes** page also gives you the status of all your existing indexes so that you can see whether you're about to use up your licensed amount of space.
   3. Name the index **cloudflare**, which is the default index that the Cloudflare App will use.

7. Set up the HTTP Event Collector (HEC) on Splunk. To create an HEC:

   1. Click the **Settings** dropdown and select **Data inputs**.
   2. Click **+Add new** and follow the wizard. When prompted, submit the following responses:
      * Name: Cloudflare
      * Source Type: Select > "cloudflare:json"
      * App Context: Cloudflare App for Splunk (cloudflare)
      * Index: cloudflare
   3. At the end of the wizard you will see a **Token Value**. This token authorizes the Cloudflare Logpush job to send data to your Splunk instance. If you forget to copy it now, Splunk allows you to get the value at any time.

8. Verify whether Splunk is using a self-signed certificate. You'll need this information when creating the Logpush job.

9. Determine the endpoint to use to send the data to. The endpoint should be:

```sql
"<protocol>://input-<host>:<port>/<endpoint>" or "<protocol>://http-inputs-<host>:<port>/<endpoint>"
```

Where:

* `protocol`: HTTP or HTTPS
* `input`: `input` or `http-inputs` based on whether you have a self-service or managed cloud plan
* `host`: The hostname of your Splunk instance. The easiest way to determine the hostname is to look at the URL you went to when you logged in to Splunk.
* `port`: 443 or 8088
* `endpoint`: services/collector/raw

For example: `https://prd-p-0qk3h.splunkcloud.com:8088/services/collector/raw`. Refer to the [Splunk Documentation](https://docs.splunk.com/Documentation/SplunkCloud/latest/Data/UsetheHTTPEventCollector) for more details and examples.

**Post Installation Notes**

You can change the **Index Name** after the initial configuration by clicking on the **Settings** dropdown and navigating to **Advance search**. There you can select **Search macros** and look for the Cloudflare App for Splunk.

![Splunk interface highlighting Apps menu and Manage Apps option along with Enable Acceleration checkbox](~/assets/images/fundamentals/splunk/screenshots/splunk-settings-advanced-search-search-macros.png)

The Cloudflare App for Splunk comes with a custom Cloudflare Data Model that has an acceleration time frame of 1 day but is not accelerated by default. If you enable [Data Model acceleration](https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/Acceleratedatamodels), we recommend that the Data Model is only accelerated for 1 or 7 days to ensure there are no adverse effects within your Splunk environment.

Enable or disable acceleration after the initial configuration by accessing the app Set up page by clicking the **Apps** dropdown, then **Manage Apps** > **Cloudflare Set Up**.

![Splunk Advanced Search page highlighted Search macros and Advanced search](~/assets/images/fundamentals/splunk/screenshots/splunk-apps-manage-apps-cloudflare-set-up-enable-data-model-acceleration.png)

You can also manually configure Data Models by going to **Settings** > **Data models**. Learn more about data model acceleration in the [Splunk documentation](https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/Acceleratedatamodels).

## Task 2 - Make the API call to create the Logpush job

Create the Logpush job by following the instructions on [Enable Logpush to Splunk](/logs/get-started/enable-destinations/splunk/). The API call creates a Logpush job but does not enable it.

Enable the Logpush job through the Cloudflare dashboard or through the API by following the instructions on [Enable Logpush to Splunk](/logs/get-started/enable-destinations/splunk/). To enable through the dashboard:

1. Navigate to the Cloudflare dashboard and select **Analytics & Logs** > **Logs**.
2. Select **Edit** and select the fields referenced in the Dashboard section below to fully populate all tables and graphs.
3. Enable the Logpush job by toggling on the switch next to the Edit link. Data takes a few minutes to populate.

To validate that you are receiving data, search `index=cloudflare` in Splunk.

## Task 3 - View the Dashboards

You can analyze Cloudflare logs with the thirteen (13) dashboards listed below.

You can use filters within these dashboards to help narrow the analysis by date and time, device type, country, user agent, client IP, hostname, and more to further help with debugging and tracing.

### About the Dashboards

The following dashboards outlined below are available as part of the Cloudflare App for Splunk.

#### Cloudflare - Snapshot

*Web Traffic Overview* and *Web Traffic Types*: Get an overview of the most important metrics from your websites and applications on the Cloudflare network.
![Splunk dashboard with Web Traffic Overview metrics](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-snapshot-dashboard.png)

#### Cloudflare - Reliability

*Summary* and *Detailed*: Get insights on the availability of your websites and applications. Metrics include origin response error ratio, origin response status over time, percentage of 3xx/4xx/5xx errors over time, and more.
![Splunk dashboard with a high level summary of Reliability metrics](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-reliability-summary-dashboard.png)

![Splunk dashboard with a detailed summary of Reliability metrics](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-reliability-detailed-dashboard.png)

#### Cloudflare - Security

*Overview*: Get insights on threats to your websites and applications, including number of threats stopped, threats over time, top threat countries, and more.
![Splunk dashboard with an overview of Security metrics](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-security-overview.png)

*WAF*: Get insights on threat identification and mitigation by our Web Application Firewall, including events like SQL injections, XSS, and more. Use this data to fine tune the firewall to target obvious threats and prevent false positives.
![Splunk dashboard with an overview of Security metrics for WAF](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-security-waf-dashboard.png)

*Rate Limiting*: Get insights on rate limiting protection against denial-of-service attacks, brute-force login attempts, and other types of abusive behavior targeted at your websites or applications.
![Splunk dashboard with an overview of Security metrics for Rate Limiting](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-security-rate-limiting-dashboard.png)

*Bots Summary* and *Bots Detailed*: Investigate bot activity on your website to prevent content scraping, checkout fraud, spam registration and other malicious activities.
![Splunk dashboard with a high level summary of Security metrics for Bots](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-security-bot-summary-dashboard.png)

![Splunk dashboard with a detailed summary of Security metrics for Bots](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-security-bots-detailed-dashboard.png)

#### Cloudflare - Performance

*Requests and Cache* and *Bandwidth*: Identify and address performance issues and caching misconfigurations. Metrics include total vs. cached bandwidth, saved bandwidth, total requests, cache ratio, top uncached requests, and more.
![Splunk dashboard with Performance metrics for Requests and Cache](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-performance-requests-and-cache-dashboard.png)

![Splunk dashboard with Performance metrics for Bandwidth](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-performance-bandwidth-dashboard.png)

*Hostname, Content Type, Request Methods, Connection Type*: Get insights into your most popular hostnames, most requested content types, breakdown of request methods, and connection type.

![Splunk dashboard with Cloudflare Performance metrics including for Hostname, Content Type, Request Methods, Connection Type](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-performance-hostname-dashboard.png)

*Static vs. Dynamic Content*: Get insights into the performance of your static and dynamic content, including slowest URLs.
![Splunk dashboard with Cloudflare Performance metrics for Static vs. Dynamic Content](~/assets/images/fundamentals/splunk/dashboards/splunk-cloudflare-performance-static-vs-dynamic-dashboard.png)

### Filters

All dashboard have a set of filters that you can apply to the entire dashboard, as shown in the following example. Filters are applied across the entire dashboard.

![Available dashboard filters from the Splunk dashboard](~/assets/images/fundamentals/splunk/screenshots/splunk-filters.png)

You can use filters to drill down and examine the data at a granular level. Filters include client country, client device type, client IP, client request host, client request URI, client request user agent, edge response status, origin IP, and origin response status.

The default time interval is set to 24 hours. Note that for correct calculations filter will need to exclude Worker subrequests (**WorkerSubrequest** = *false*) and purge requests (**ClientRequestMethod** is not *PURGE*).

Available Filters:

* Time Range (EdgeStartTimestamp)

* Client Country

* Client Device type

* Client IP

* Client Request Host

* Client Request URI

* Client Request User Agent

* Edge response status

* Origin IP

* Origin Response Status

* RayID

* Worker Subrequest

* Client Request Method

## Debugging tips

### Incomplete dashboards

The Splunk Cloudflare App relies on data from the Cloudflare Enterprise Logs fields outlined below. Depending on which fields you have enabled, certain dashboards might not populate fully.

If that is the case, verify and test the Cloudflare App filters below each dashboard (these filters are the same across all dashboards). You can delete any filters that you do not need, even if such filters include data fields already contained in your logs.

Also, you could compare the list of fields you are getting in Cloudflare Logs with the fields listed in **Splunk** > **Settings** > **Data Model** > **Cloudflare**.

The available fields are:

* CacheCacheStatus

* CacheResponseBytes

* CacheResponseStatus

* ClientASN

* ClientCountry

* ClientDeviceType

* ClientIP

* ClientIPClass

* ClientRequestBytes

* ClientRequestHost

* ClientRequestMethod

* ClientRequestPath

* ClientRequestProtocol

* ClientRequestReferer

* ClientRequestURI

* ClientRequestUserAgent

* ClientSSLCipher

* ClientSSLProtocol

* ClientSrcPort

* EdgeColoCode

* EdgeColoID

* EdgeEndTimestamp

* EdgePathingOp

* EdgePathingSrc

* EdgePathingStatus

* EdgeRequestHost

* EdgeResponseBytes

* EdgeResponseContentType

* EdgeResponseStatus

* EdgeServerIP

* EdgeStartTimestamp

* OriginIP

* OriginResponseStatus

* OriginResponseTime

* OriginSSLProtocol

* RayID

* SecurityAction

* SecurityActions

* SecurityRuleDescription

* SecurityRuleID

* SecurityRuleIDs

* SecuritySources

* WAFFlags

* WAFMatchedVar

* WorkerSubrequest

* ZoneID

---

# FAQs

URL: https://developers.cloudflare.com/analytics/faq/

import { DirectoryListing } from "~/components"

Visit the following pages for commonly asked questions:

<DirectoryListing />

---

# About Cloudflare Analytics

URL: https://developers.cloudflare.com/analytics/faq/about-analytics/

In an effort to make analytics an ubiquitous component of all Cloudflare's products, Cloudflare has implemented, and continues to evolve, several ways in which customers can access and gain insights from Internet properties on Cloudflare.

You can access root-level analytics that give you an overview of metadata related to your Cloudflare account, analytics related to specific properties and products, and the GraphQL API that gives you more control over how you visualize the analytics and log information available on the Cloudflare dashboard.

Refer to [Types of analytics](/analytics/types-of-analytics/) for more information regarding this subject.

## How Cloudflare captures and processes analytics data

The underlying datasets that Cloudflare Analytics captures and processes share the following characteristics:

* All metrics reflect traffic proxied through the Cloudflare network (also known as orange-clouded), as configured via DNS records in the Cloudflare DNS app. Note that for a [CNAME setup](/dns/zone-setups/partial-setup/), Cloudflare is unable to offer DNS metrics.
* Cloudflare does not count traffic for unproxied DNS records. However, if your site is not proxied through Cloudflare but Cloudflare is your authoritative DNS server, then we are able to collect DNS metrics.
* Cloudflare can only proxy information for traffic targeting [specific ports](/fundamentals/reference/network-ports/).
* In determining the originating country, Cloudflare uses the IP address associated with each request. Learn about [Configuring Cloudflare IP Geolocation](/network/ip-geolocation/).

## Apparent data discrepancies

It is possible that your Cloudflare metrics do not fully align with data for the same site as reported by other analytics sources, such as Google Analytics and web server logs.

Once Cloudflare identifies a unique IP address for a request, we identify such request as a visit. Therefore, the number of visitors Cloudflare Analytics shows is probably higher than what other analytics services may report.

For example, Google Analytics and other web-based analytics programs use JavaScript on the web browser to track visitors. As a result, Google Analytics does not record threats, bots, and automated crawlers because those requests typically do not trigger JavaScript. Also, these services do not track visitors who disable JavaScript on their browser or who leave a page before it fully loads.

Finally, it is likely that unique visitor data from the Cloudflare Analytics app is greater than your search analytics unique pageviews. This is because pageviews reflect when someone visits a page via a web browser and loads the entire page. However, when another site or service like a bot, plugin, or API is consuming partial content from your site (but not loading a full page), this counts as a unique visitor in Cloudflare and not as a pageview.

## About missing metrics

You may not be seeing metrics on Cloudflare Analytics for the following reasons:

* You only recently signed up for Cloudflare. Metrics are delayed 24 hours for domains on a free Cloudflare plan.
* If you signed up directly with Cloudflare, your nameservers might not be pointing to Cloudflare at your registrar just yet. Registrars can take 24-72 hours to update their nameservers. Metrics will not start gathering until we detect the nameservers pointing to Cloudflare.
* If you signed up through a Cloudflare [hosting partner option](https://www.cloudflare.com/partners/), something might not be configured correctly. Contact the hosting partner for support.
* Some browser extensions designed to block ads may prevent analytics from loading. To address this issue, disable the ad block extension or allow `cloudflare.com` on it.

:::note
Activations through a hosting partner works via a [CNAME setup](/dns/zone-setups/partial-setup/) on the `www` record. If most of your traffic actually goes to `domain.com`, [forward your traffic](/rules/url-forwarding/bulk-redirects/) from `domain.com` to `www.domain.com`.
:::

---

# Other FAQs

URL: https://developers.cloudflare.com/analytics/faq/other-faqs/

## Why do I see a large amount of traffic from CLOUDFLARENET ASN 13335 in Analytics? Does this indicate a DDoS attack?

There is a number of different types of traffic which may originate from **CLOUDFLARENET ASN 13335**; just because there is a lot of traffic from this AS, it likely does not indicate a DDoS attack.

Some sources of traffic from ASN13335 include:

* [Workers subrequests](/workers/runtime-apis/fetch/)
* [WARP](/warp-client/known-issues-and-faq/#does-warp-reveal-my-ip-address-to-websites-i-visit)
* [iCloud Private Relay](https://blog.cloudflare.com/icloud-private-relay/) (For reference, iCloud Private Relayâ€™s egress IP addresses are available in this [CSV form](https://mask-api.icloud.com/egress-ip-ranges.csv))
* [Cloudflare Privacy Proxy](https://blog.cloudflare.com/building-privacy-into-internet-standards-and-how-to-make-your-app-more-private-today/)
* Other Cloudflare features like [Health Checks](/health-checks/)

---

# Workers Analytics Engine FAQs

URL: https://developers.cloudflare.com/analytics/faq/wae-faqs/

Below you will find answers to our most commonly asked questions.

## Sampling

### Could I just use many unique index values to get better unique counts?

No, adding a large number of index values does not come without drawbacks. The tradeoff is that reading across many indices is slow.

In practice, due to how ABR works, reading from many indices in one query will result in low-resolution data â€“ possibly unusably low.

On the other hand, if you pick a good index that aligns with how you read the data, your queries will run faster and you will get higher resolution results.

### What if I need to index on multiple values?


It is possible to concatenate multiple values in your index field. So if you want to index on user ID and hostname, you can write, for example `"$userID:$hostname"` into your index field.

Note that, based on your query pattern, it may make sense to write the same dataset with different indices. It is a common misconception that one should avoid "double-writing" data.

Thanks to sampling, the cost of writing data multiple times can be relatively low. However, reading data inefficiently can result in significant expenses or low-quality results due to sampling.


### How do I know if my data is sampled?

You can use the `_sample_interval` field â€” again, note that this does not tell you if the results are accurate.

You can tell when data is sampled at read time because sample intervals will be multiples of powers of 10, for example `20` or `700`. There is no hard and fast rule for when sampling starts at read time, but in practice reading longer periods (or more index values) will result in a higher sample interval.


### Why is data missing?


Sampling is based largely on the choice of index, as well as other factors like the time range queried and number of indices read. If you are reading from a larger index over a longer time period, and have filtered to a relatively small subgroup within that index, it may not be present due to sampling.

If you need to read accurate results for that subgroup, we suggest that you add that field to your index (refer to [What if I need to index on multiple values](/analytics/faq/wae-faqs/#what-if-i-need-to-index-on-multiple-values)).


### Can I trust sampled data? Are my results accurate?

Sampled data is highly reliable, particularly when a carefully selected index is used.

Admittedly, it is difficult at present to prove that the results returned by ABR queries are within a certain error bound. As a rule of thumb, it is good to check the number of rows read by using count() â€” think of this like the count of pixels in your image. A higher number of rows read will result in more accurate results. (The flipside is that the `_sample_interval` field does not tell you very much about whether your results are accurate). If you are extrapolating from only one or two rows, it is unlikely you have a representative result; if you are extrapolating from thousands of rows, it is very likely that your results are quite accurate.

In the near future, we plan to expose the [margin of error](https://en.wikipedia.org/wiki/Margin_of_error) along with query results so that you can see precisely how accurate your results are.

### How are bursts handled?

Equitable sampling exists both to normalize differences between groups, and also to handle large spikes of traffic to a given index. Equalization happens every few seconds; if you are writing many events very close in time, then it is expected that they will be sampled at write time.  The sample interval for a given index will vary from moment to moment, based on the current rate of data being written.

### How much traffic will trigger sampling?

There is no fixed rule determining when sampling will be triggered.

We have observed that for workloads like our global CDN, which distribute load around our network, each index value needs about 100 data points per second before sampling is noticeable at all.

Depending on your workload and how you use Workers Analytics Engine, sampling may start at a higher or lower threshold than this. For example, if you are writing out many data points from a single worker execution, it is more likely that your data will be sampled.

---

# Error responses

URL: https://developers.cloudflare.com/analytics/graphql-api/errors/

The GraphQL Analytics API is a RESTful API based on HTTPS requests and JSON responses, and will return familiar HTTP status codes (for example, `404`, `500`, `504`). However, in contrast to the common REST approach, a `200` response can contain an error, conforming to the [GraphQL specification](https://graphql.github.io/graphql-spec/June2018/#sec-Errors).

All responses contain an `errors` array, which will be `null` if there are no errors, and include at least one error object if there was an error. Non-null error objects will contain the following fields:

* `message`: a string describing the error.
* `path`: the nodes associated with the error, starting from the root. Note that the number included in the path array, for example, `0` or `1`, specifies to which zone the error applies; `0` indicates the first zone in the list (or only zone, if only one is being queried).
* `timestamp`: UTC datetime when the error occurred.

## Example

```json
{
  "data": null,
  "errors": [
    {
      "message": "cannot request data older than 2678400s",
      "path": ["viewer", "zones", "0", "firewallEventsAdaptiveGroups"],
      "extensions": {
        "timestamp": "2019-12-09T21:27:19.195060142Z"
      }
    }
  ]
}
```

## Common error types

### Dataset accessibility limits (entitlements) exceeded

Sample error messages:

* "cannot request data older than..."
* "number of fields cannot be more than..."

These messages indicate that the query exceeds what is allowed for the particular dataset under your plan. Refer to [Node limits](/analytics/graphql-api/limits/#node-limits-and-availability) for details.

### Parsing issues

Sample error messages:

* "error parsing args..."
* "scalar fields must have not selections"

These messages indicate that the query cannot be processed because it is malformed.

### Rate limits exceeded

Sample error messages:

* "limit reached, please try reduced time period"
* "quota exceeded, please repeat your request in the next minute"
* "rate limiter budget depleted, try again after 5 minutes"

Refer to the [Limits](/analytics/graphql-api/limits/) section for more details about rate limits.

---

# GraphQL Analytics API

URL: https://developers.cloudflare.com/analytics/graphql-api/

The GraphQL Analytics API provides data regarding HTTP requests passing through Cloudflareâ€™s network, as well as data from specific products, such as Firewall or Load Balancing. Network Analytics users also have access to packet-level data. Use the GraphQL Analytics API to select specific datasets and metrics of interest, filter and aggregate the data along various dimensions, and integrate the results with other applications.

The basis of the API is the [GraphQL framework](https://graphql.org/), created and open-sourced by Facebook. There is an active developer community for GraphQL and powerful clients for running queries, which makes it easy to get started. GraphQL is especially useful for building visualizations and powers the analytics in the Cloudflare dashboard.

GraphQL models a business domain as a graph using a schema. In the schema, there are logical definitions for different types of nodes and their connections (edges). These nodes are the datasets you use for your analytics. You write queries in GraphQL much like in SQL: you specify the dataset (table), the metrics to retrieve (such as requests and bytes), and filter or group by dimensions (for example, a time period).

GraphQL differs from a traditional API: it has one single endpoint:

```txt
https://api.cloudflare.com/client/v4/graphql
```

You pass the query parameters as a JSON object in the payload of a `POST` request to this endpoint.

You can use `curl` to make requests to the GraphQL Analytics API. Alternatively, you can use a GraphQL client to construct queries and pass requests to the GraphQL Analytics API.

## Clients

We are using [GraphiQL](https://github.com/skevy/graphiql-app) for our example GraphQL queries. There are many other popular open-source clients that you can find online, such as [Altair](https://altair.sirmuel.design) and [Insomnia](https://insomnia.rest).

---

# Limits

URL: https://developers.cloudflare.com/analytics/graphql-api/limits/

Cloudflare GraphQL API exposes more than 70 datasets representing products with
different configurations and data availability for different zones and accounts
plans.

To support this variety of products, Cloudflare GraphQL API has three layers of
limits:

* global limits
* user limits
* node (dataset) limits

## Global limits

These limits are applied to every query for every plan:

* A zone-scoped query can include up to **10 zones**
* An account-scoped query can include only **1 account**

Additionally, there is a limited number of queries you can make per request. The total number of queries in a request is equal to the number of zone/account scopes, multiplied by the number of nodes to which they are applied.

## User limits

Cloudflare GraphQL API limits the number of GraphQL requests each user can send.
The default quota is **300 GraphQL queries over 5-minute window**. It allows a
user to run at least **1 query every second** or do a burst of 300 queries and
then wait 5 minutes before issuing another query.

That rate limit is applied in addition to the [general rate limits enforced by
the Cloudflare API](/fundamentals/api/reference/limits/).

## Node limits and availability

Each data node has its limits, such as:

* how far back in time can data be requested,
* the maximum time period (in seconds) that can be requested in one query,
* the maximum number of fields that can be requested in one query,
* the maximum number of records that can be returned in one query.

Node limits are tied to requested `zoneTag` or `accountTag`. Larger plans have
access to a greater selection of datasets and can query over broader historical
intervals.

To get exact boundaries and availability for your zone(s) or account, please
refer to [settings][1].

[1]: /analytics/graphql-api/features/discovery/settings/

---

# Get started

URL: https://developers.cloudflare.com/analytics/network-analytics/get-started/

import { GlossaryTooltip, Render } from "~/components"

<Render file="network-analytics-requirements" />

## View the Network Analytics dashboard

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select an account that has access to Magic Transit or Spectrum.

2. Go to Account Home > **Analytics & Logs** > **Network Analytics**.

3. Configure the displayed data. You can [adjust the time range](/analytics/network-analytics/configure/time-range/), [select the main metric](/analytics/network-analytics/configure/displayed-data/#select-high-level-metric) (total packets or total bytes), [apply filters](/analytics/network-analytics/configure/displayed-data/#apply-filters), and more.

## Get Network Analytics data via API

Use the [GraphQL Analytics API](/analytics/graphql-api/) to query data using the available [Network Analytics nodes](/analytics/graphql-api/features/data-sets/).

## Send Network Analytics logs to a third-party service

[Create a Logpush job](/logs/get-started/enable-destinations/) that sends Network analytics logs to your storage service, <GlossaryTooltip term="SIEM">SIEM solution</GlossaryTooltip>, or log management provider.

---

# Sampling

URL: https://developers.cloudflare.com/analytics/graphql-api/sampling/

## Overview

In a small number of cases, the analytics provided on the Cloudflare dashboard and GraphQL Analytics API are based on a **sample** â€” a subset of the dataset. In these cases, Cloudflare Analytics returns an estimate derived from the sampled value. For example, suppose that during an attack the sampling rate is 10% and 5,000 events are sampled. Cloudflare will estimate 50,000 total events (5,000 Ã— 10) and report this value in Analytics.

## Sampled datasets

Cloudflare GraphQL API exposes datasets that powered by adaptive sampling. These
nodes have **Adaptive** in the name and can be discovered through
[introspection][1].

The presence of sampled data is also called out in the Cloudflare dashboard and
in the description of the dataset in the API.

## Why sampling is applied

Analytics is designed to provide requested data, at the appropriate level of detail, as quickly as possible. Sampling allows Cloudflare to deliver analytics within seconds, even when datasets scale quickly and unpredictably, such as a burst of Firewall events generated during an attack. And because the volume of underlying data is large, the value estimated from the sample should still be statistically significant â€“ meaning you can rely on sampled data with a high degree of confidence. Without sampling, it might take several minutes or longer to answer a query â€” a long time to wait when validating mitigation efforts.

## Types of sampling

### Adaptive sampling

Cloudflare almost always uses **adaptive sampling**, which means the sample rate fluctuates depending on the volume of data ingested or queried. If the number of records is relatively small, sampling is not used. However, as the volume of records grows larger, progressively lower sample rates are applied. Security Events (also known as Firewall Events) and the Security Event Log follow this model. Data nodes that use adaptive sampling are easy to identify by the `Adaptive` suffix in the node name, as in `firewallEventsAdaptive`.

### Fixed sampling

The following data nodes are based on fixed sampling, where the sample rate does not vary:



| Data set                                                                                                                                       |   Rate | Notes                                                                                                                                                                                                |
| :--------------------------------------------------------------------------------------------------------------------------------------------- | -----: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Firewall Rules Preview<br /><p><b>Nodes:</b><br />`firewallRulePreviewGroups`</p>                                                              |     1% | Use with caution. A 1% sample rate does not provide accurate estimates for datasets smaller than a certain threshold, a scenario the Cloudflare dashboard calls out explicitly but the API does not. |
| Network Analytics<br /><p><b>Nodes:</b><br />`ipFlows1mGroups`<br />`ipFlows1hGroups`<br />`ipFlows1dGroups`<br />`ipFlows1mAttacksGroups`</p> | 0.012% | Sampling rate is in terms of packet count (1 of every 8,192 packets).                                                                                                                                |



## Access to raw data

Because sampling is primarily adaptive and automatically adjusts to provide an accurate estimate, the sampling rate cannot be directly controlled. Enterprise customers have access to raw data via Cloudflare Logs.

[1]: /analytics/graphql-api/features/discovery/introspection/

---

# Network analytics

URL: https://developers.cloudflare.com/analytics/network-analytics/

import { InlineBadge, Render } from "~/components"

Cloudflare Network Analytics (version 2) provides near real-time visibility into network and transport-layer traffic patterns and DDoS attacks. Network Analytics visualizes packet and bit-level data, the same data available via the Network Analytics dataset of the GraphQL Analytics API.

<Render file="network-analytics-requirements" />

For a technical deep-dive into Network Analytics, refer to our [blog post](https://blog.cloudflare.com/building-network-analytics-v2/).

## Remarks

* The Network Analytics logs refer to IP traffic of Magic Transit customer prefixes/leased IP addresses or Spectrum applications. These logs are not directly associated with the [zones](/fundamentals/setup/accounts-and-zones/#zones) in your Cloudflare account.

* The data retention for Network Analytics is 16 weeks. Additionally, data older than eight weeks might have lower resolution when using narrow time frames.

## Related resources

* [Cloudflare GraphQL API](/analytics/graphql-api/)
* [Cloudflare Logpush](/logs/about/)
* [Migrating from Network Analytics v1 to Network Analytics v2](/analytics/graphql-api/migration-guides/network-analytics-v2/)
* [Cloudflare Network Analytics v1](/analytics/network-analytics/reference/network-analytics-v1/) <InlineBadge preset="deprecated" />

---

# Build developer portals

URL: https://developers.cloudflare.com/api-shield/management-and-monitoring/developer-portal/

import { GlossaryTooltip } from "~/components"

Once <GlossaryTooltip term="API endpoint">endpoints</GlossaryTooltip> are saved into Endpoint Management, API Shield doubles as an API catalog. API Shield can build an interactive documentation portal with the knowledge it has of your APIs, or you can upload a new OpenAPI schema file to build a documentation portal ad-hoc.

## Process

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **API Shield** > **Settings**.
3. Under **Create a developer portal**, select **Create site**.
4. Upload an OpenAPI v3.0 schema file or choose to select an existing schema from API Shield.

  :::note

  If you do not have a schema to upload or to select from a pre-existing schema, export your Endpoint Management schema. For best results, include the learned parameters.

  Only <GlossaryTooltip term="API schema">API schemas</GlossaryTooltip> uploaded to Schema Validation 2.0 are available when selecting existing schemas
  :::

5. Select **Download project files** to save a local copy of the files that will be uploaded to Cloudflare Pages. Downloading the project files can be helpful if you wish to modify the project in any way and then upload the new version manually to Pages.
6. Select **Create pages project** to begin project creation. A new Pages project will be automatically created and your API schema will be automatically uploaded to the project along with other supporting static content.
7. Select **Deploy site**.

### Custom domains

To create a vanity domain instead of using the pages.dev domain, refer to the [Pages custom domain documentation](/pages/configuration/custom-domains/).

## Availability

Building developer portals is available to all API Shield subscribers. This feature uses Cloudflare Pages to host the resulting portal. Refer to [Pages](/pages/) for any limitations of your current subscription plan.

## Limitations

This feature currently uses the open source [Redoc](https://github.com/Redocly/redoc) project from [Redocly](https://redocly.com/). For custom theme and branding options, visit the [Redoc GitHub repository](https://github.com/Redocly/redoc).

To modify the resulting page, download the project files before creating the Pages project. You can create a new Pages project with the modified files you have made to meet your branding guidelines.

---

# Endpoint labeling service

URL: https://developers.cloudflare.com/api-shield/management-and-monitoring/endpoint-labels/

import { Render } from "~/components"

API Shield's labeling service will help you organize your endpoints and address vulnerabilities in your API. The labeling service comes with managed and user-defined labels.

Today, managed labels are useful for organizing endpoints by use case. In a future release, managed labels will automatically label endpoints by use case and those with informative or security risks, alerting you on endpoints that need attention.

User-defined labels can also be added to endpoints in API Shield by creating a label and adding it to an individual endpoint or multiple endpoints. User-defined labels will be useful for organizing your endpoints by owner, version, or type.

You can filter your endpoints based on the labels.

## Managed labels

`cf-log-in`: Add this label to endpoints that accept user credentials. You may have multiple endpoints if you accept username, password, and multi-factor authentication (MFA) across multiple endpoints or requests.

`cf-sign-up`: Add this label to endpoints that are the final step in creating user accounts for your site or application.

`cf-content`: Add this label to endpoints that provide unique content, such as product details, user reviews, pricing, or other unique information.

`cf-purchase`: Add this label to endpoints that are the final step in purchasing goods or services online.

`cf-password-reset`: Add this label to endpoints that participate in the user password reset process. This includes initial password reset requests and final password reset submissions.

`cf-add-cart`: Add this label to endpoints that add items to a user's shopping cart or verify item availability.

`cf-add-payment`: Add this label to endpoints that accept credit card or bank account details where fraudsters may iterate through account numbers to guess valid combinations of payment information.

`cf-check-value`: Add this label to endpoints that check the balance of rewards points, in-game currency, or other stored value products that can be earned, transferred, and redeemed for cash or physical goods.

`cf-add-post`: Add this label to endpoints that post messages in a communication forum, or product or merchant reviews.

`cf-account-update`: Add this label to endpoints that participate in user account or profile updates.

`cf-rss-feed`: Add this label to endpoints that expect traffic from RSS clients.

`cf-risk-missing-auth`: Automatically added when all successful requests lack a session identifier. Refer to the table below for more information.

`cf-risk-mixed-auth`: Automatically added when some successful requests contain a session identifier and some successful requests lack a session identifier. Refer to the table below for more information.

`cf-risk-sensitive`: Automatically added to endpoints when HTTP responses match the WAF's [Sensitive Data Detection](/api-shield/management-and-monitoring/#sensitive-data-detection) ruleset.

`cf-risk-missing-schema`: Automatically added when a learned schema is available for an endpoint that has no active schema.

`cf-risk-error-anomaly`: Automatically added when an endpoint experiences a recent increase in response errors over the last 24 hours. 

`cf-risk-latency-anomaly`: Automatically added when an endpoint experiences a recent increase in response latency over the last 24 hours. 

`cf-risk-size-anomaly`: Automatically added when an endpoint experiences a spike in response body size over the last 24 hours. 

:::note
Cloudflare will only add authentication labels to endpoints with successful response codes. Refer to the below table for more details.
:::

<Render file="label-methodology" />

## Create a label

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **Settings** > **Labels**.
3. Under **Security labels**, select **Create label**.
4. Name the label and add an optional label description.
5. Apply the label to your selected endpoints.
6. Select **Create label**.

Alternatively, you can create a user-defined label via Endpoint Management in API Shield.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **Settings** > **Labels**.
3. Choose the endpoint that you want to label.
4. Select **Edit labels**.
5. Under **User**, select **Create user label**.
6. Enter the label name.
7. Select **Create**.

## Apply a label to an individual endpoint

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **API Shield** > **Endpoint Management**.
3. Choose the endpoint that you want to label.
4. Select **Edit labels**.
5. Add the label(s) that you want to use for the endpoint from the list of managed and user-defined labels.
6. Select **Save labels**.

## Bulk apply labels to multiple endpoints

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **Settings** > **Labels**.
3. On the existing label that you want to apply to multiple endpoints, select **Bulk apply**.
4. Choose the endpoints that you want to label by selecting its checkbox.
5. Select **Save label**.

## Availability

Endpoint Management's labeling service is currently available to Enterprise API Shield subscribers.

---

# Management and Monitoring

URL: https://developers.cloudflare.com/api-shield/management-and-monitoring/

import { GlossaryTooltip, Plan } from "~/components"

<Plan type="all" />

Monitor the health of your <GlossaryTooltip term="API endpoint">API endpoints</GlossaryTooltip> by saving, updating, and monitoring performance metrics using API Shieldâ€™s Endpoint Management.

**Add endpoints** allows customers to save endpoints directly from [API Discovery](/api-shield/security/api-discovery/) or manually by method, path, and host.

This will add the specified endpoints to your list of managed endpoints. You can view your list of saved endpoints in the **Endpoint Management** page.

Cloudflare will start collecting [performance data](/api-shield/management-and-monitoring/#endpoint-analysis) on your endpoint when you save an endpoint.

:::note

When an endpoint is using [Cloudflare Workers](/workers/), the metrics data will not be populated.
:::

## Access

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account and domain.
2. Select **Security** > **API Shield**.
3. Add your endpoints [manually](#add-endpoints-manually), from [Schema Validation](#add-endpoints-from-schema-validation), or from [API Discovery](#add-endpoints-from-api-discovery).

## Add endpoints from API Discovery

There are two ways to add API endpoints from Discovery.

### Add from the Endpoint Management Tab

1. From Endpoint Management, select **Add endpoints** > **Select from Discovery** tab.
2. Select the discovered endpoints you would like to add.
3. Select **Add endpoints**.

### Add from the Discovery Tab

1. From Endpoint Management, select the **Discovery** tab.
2. Select the discovered endpoints you would like to add.
3. Select **Save selected endpoints**.

## Add endpoints from Schema Validation

1. Add a schema by [configuring Schema Validation](/api-shield/security/schema-validation/).
2. On **Review schema endpoints**, save new endpoints to endpoint management by checking the box.
3. Select **Save as draft** or **Save and Deploy**. Endpoints will be saved regardless of whether the Schema is saved as a draft or published.

API Shield will look for duplicate endpoints that have the same host, method, and path. Duplicate endpoints will not be saved to endpoint management.

:::note

If you deselect **Save new endpoints to endpoint management**, the endpoints will not be added.
:::

## Add endpoints manually

1. From Endpoint Management, select **Add endpoints** > **Manually add**.
2. Choose the method from the dropdown menu and add the path and hostname for the endpoint.
3. Select **Add endpoints**.

:::note

By selecting multiple checkboxes, you can add several endpoints from Discovery at once instead of individually.
:::

When adding an endpoint manually, you can specify variable fields in the path or host by enclosing them in braces, `/api/user/{var1}/details` or `{hostVar1}.example.com`.

Cloudflare supports hostname variables in the following formats:

```txt

{hostVar1}.example.com

foo.{hostVar1}.example.com

{hostVar2}.{hostVar1}.example.com
```

Hostname variables must comprise the entire domain field and must not be used with other text in the field.

The following format is not supported:

```txt

foo-{hostVar1}.example.com
```

For more information on how Cloudflare uses variables in API Shield, refer to the examples from [API Discovery](/api-shield/security/api-discovery/).

## Delete endpoints manually

You can delete endpoints one at a time or in bulk.

1. From Endpoint Management, select the checkboxes for the endpoints that you want to delete.
2. Select **Delete endpoints**.

## Endpoint schema learning

Cloudflare learns schema parameters via traffic inspection. For all endpoints saved to Endpoint Management, you can export OpenAPI schemas in `v3.0.0` format by hostname. You can also include learned schema parameters.

To protect your API with a learned schema, refer to [Schema Validation](/api-shield/security/schema-validation/#add-validation-by-applying-a-learned-schema-to-an-entire-hostname).

### Export a schema

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Select **Security** > **API Shield**.
3. Navigate to **Endpoint Management**.
4. Select **Export schema** and choose a hostname to export.
5. Select whether to include [learned parameters](/api-shield/management-and-monitoring/#learned-schemas-will-always-include) and [rate limit recommendations](/api-shield/security/volumetric-abuse-detection/)
6. Select **Export schema** and choose a location to save the file.

:::note

The schema is saved as a JSON file in OpenAPI `v3.0.0` format.
:::

#### Learned schemas will always include:

- The listed hostname in the servers section
- All endpoints by host, method, and path
- Detected path variables

#### Learned schemas can optionally include:

- Detected query parameters and its format
- API Shieldâ€™s rate limit threshold recommendations

## Endpoint Analysis

For each saved endpoint, customers can view:

- **Request count**: The total number of requests to the endpoint over time.
- **Rate limiting recommendation**: per 10 minutes. This is guided by the request count.
- **Latency**: The average origin response time in milliseconds (ms). This metric shows how long it takes from the moment a visitor makes a request to the moment the visitor gets a response back from the origin.
- **Error rate** vs. overall traffic: grouped by 4xx, 5xx, and their sum.
- **Response size**: The average size of the response (in bytes) returned to the request.
- **Labels**: The current [labels](/api-shield/management-and-monitoring/endpoint-labels/) assigned to the endpoint.
- **Authentication status**: The breakdown of which [session identifiers](/api-shield/get-started/#session-identifiers) were seen on successful requests to this endpoint.
- **Sequences**: The number of [Sequence Analytics](/api-shield/security/sequence-analytics/) sequences the endpoint was found in.

:::note

Customers viewing analytics have the ability to toggle detailed metrics view between the last 24 hours and 7 days.
:::

## Using the Cloudflare API

You can interact with Endpoint Management through the Cloudflare API. Refer to [Endpoint Managementâ€™s API documentation](/api/resources/api_gateway/subresources/discovery/subresources/operations/methods/list/) for more information.

## Sensitive Data Detection

Sensitive data comprises various personally identifiable information and financial data. Cloudflare created this ruleset to address common data loss threats, and the WAF can search for this data in HTTP response bodies from your origin.

API Shield will alert users to the presence of sensitive data in the response body of API endpoints listed in Endpoint Management if the zone is also subscribed to the [Sensitive Data Detection managed ruleset](/waf/managed-rules/reference/sensitive-data-detection/).

Sensitive Data Detection is available to Enterprise customers on our Advanced application security plan.

Once Sensitive Data Detection is enabled for your zone, API Shield queries firewall events from the WAF for the last seven days and places a notification icon on the Endpoint Management table row if there are any matched sensitive responses for your endpoint.

API Shield displays the types of sensitive data found if you expand the Endpoint Management table row to view further details. Select **Explore Events** to view the matched events in Security Events.

After Sensitive Data Detection is enabled for your zone, you can [browse the Sensitive Data Detection ruleset](https://dash.cloudflare.com/?to=/:account/:zone/security/data/ruleset/e22d83c647c64a3eae91b71b499d988e/rules). The link will not work if Sensitive Data Detection is not enabled.

---

# Session identifiers

URL: https://developers.cloudflare.com/api-shield/management-and-monitoring/session-identifiers/

import { Render } from "~/components"

<Render file="session-identifiers" />

:::note
<Render file="required-session-identifiers" />
:::

## To set up session identifiers

<Render file="set-up-session-identifiers" />

---

# API Discovery

URL: https://developers.cloudflare.com/api-shield/security/api-discovery/

import { GlossaryTooltip, Render } from "~/components"

Most development teams struggle to keep track of their APIs. Cloudflare API Discovery helps you map out and understand your attack surface area.

## Process

Cloudflare produces a simple, trustworthy map of <GlossaryTooltip term="API endpoint">API endpoints</GlossaryTooltip> through a process of path normalization.

For example, you might have thousands of APIs, but a lot of the calls look similar, such as:

- `api.example.com/profile/238`
- `api.example.com/profile/392`

Both paths serve a similar purpose â€” allowing users to log in to their accounts â€” but they are not identical. To simplify your endpoints, these examples might both map to `api.example.com/profile/*`.

API Discovery runs this process across all your traffic, generating a simple map of endpoints that might look like:

```
/api/login/{customer_identifier}
/api/auth
/api/account/{customer_identifier}
/api/password_reset
/api/logout
```

Similarly, if you have multiple subdomains that share the same set of endpoints, Cloudflare will consolidate subdomains:

```txt
us-api.example.com/api/v1/users/{var1}
de-api.example.com/api/v1/users/{var1}
fr-api.example.com/api/v1/users/{var1}
jp-api.example.com/api/v1/users/{var1}
```

We will consolidate to `{hostVar1}.example.com/api/v1/users/{var1}`.

<Render file="blog-post" />

### Inbox view

API Shield first catalogs your discovered API endpoints in an email inbox-style view. From API Discovery, you can save endpoints to [Endpoint Management](/api-shield/management-and-monitoring/) or ignore endpoints to remove them from view.

You should save all discovered API endpoints to Endpoint Management while ignoring any potential false positives in the API Discovery results by selecting **Save** or **Ignore** on each line. Alternatively, you can bulk-select endpoints to save or ignore. You can get started with saving endpoints by saving all endpoints with a variable. Search for `var1` in the search box and add all the resulting endpoints. You can examine endpoints without path variables for accuracy later on.

By adding endpoints to Endpoint Management, you will unlock further [security](/api-shield/security/), [visibility](/api-shield/management-and-monitoring/#endpoint-analysis), and [management](/api-shield/management-and-monitoring/) features of the platform. Endpoint Management monitors the health of your API endpoints by saving, updating, and monitoring performance metrics.

To restore any errantly ignored endpoints, you can filter by **Ignored** and select **Restore**.

API Discovery is an ongoing process. Check back regularly for new API Discovery results. A badge with the number of endpoints needing review will show in the API Shield dashboard. You may see the quantities in the **Needs Review** and **Ignored** metrics change over time. As your actual API or traffic patterns to your APIs change, API Discovery results that are not saved can disappear.

:::note

Cloudflare will use your feedback on the ignored endpoints to better train the API Discovery Machine Learning model in a future release. 
:::

### Machine Learning-based Discovery

Your API endpoints are discovered with both the Session Identifier-based Discovery and the Machine Learning-based Discovery.

To access Machine Learning-based Discovery, log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain. Go to **API Shield** > **Discovery**. You may filter the source results by `Session Identifier` or `Machine Learning` to view results from each Discovery method.

If all of your zoneâ€™s API traffic contains the <GlossaryTooltip term="session identifier">session identifier</GlossaryTooltip> that you have configured, both sources may deliver the same results due to similarities between their underlying methodology. We expect Machine Learning-based Discovery to excel in discovering API traffic regardless of whether your API uses a session identifier.

You can direct any feedback about your API Discovery results to your account team.

## Availability

API Discovery is only available for Enterprise customers. If you are an Enterprise customer and interested in this product, contact your account team.

---

# Authentication Posture

URL: https://developers.cloudflare.com/api-shield/security/authentication-posture/

import { GlossaryTooltip, Render } from "~/components"

Authentication Posture helps users identify authentication misconfigurations for APIs and alerts of their presence.

For example, a security team member may believe that their API hosted at `/api/v1/users` and `/api/v1/orders` are guarded by the fact that only authenticated users can interact with the endpoints. However, bugs in origin API authentication policies may lead to broken authentication vulnerabilities. Authentication Posture with API Shield details the authentication status of successful requests to your API endpoints, alerting to potential misconfigurations.

Consider a typical e-commerce application. Users can browse items and prices without being logged in. However, to retrieve order details with the `GET /api/v1/orders/{order_id}` endpoint, this example application requires users to log in to their account and pass the subsequent Authorization HTTP header in all requests. Cloudflare will alert via [Security Center Insights](/security-center/security-insights/) and [Endpoint Management labels](/api-shield/management-and-monitoring/endpoint-labels/) if successful requests are sent to the `GET /api/v1/orders/{order_id}` endpoint or any other endpoint without authentication when <GlossaryTooltip term="session identifier">session identifiers</GlossaryTooltip> are configured.

## Process

After configuring [session identifiers](/api-shield/get-started/#session-identifiers), API Shield continuously scans your traffic for successful requests without authentication and labels your endpoints on a daily basis. Refer to the table below for our labeling methodology.

<Render file="label-methodology" />

To examine an endpoint's authentication details in the Cloudflare dashboard:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **API Shield** > **Endpoint Management**.
3. Filter Endpoint Management by the `cf-risk-missing-auth` or `cf-risk-mixed-auth` labels.
4. Select an endpoint to see its authentication posture details on the endpoint details page.
5. Choose between the 24-hour and 7-day view options, and note any authentication changes over time.

The main authentication widget displays how many successful requests over the last seven days had session identifiers included with them, and which identifiers were included with the traffic. 

The authentication-over-time chart shows a detailed breakdown over time of how clients successfully interacted with your API and which identifiers were used. A large increase in unauthenticated traffic may signal a security incident. Similarly, any successful unauthenticated traffic on an endpoint that is expected to be 100% authenticated can be a cause for concern.

Work with your development team to understand which authentication policies may need to be corrected on your API to stop unauthenticated traffic.

## Stop unauthenticated traffic with Cloudflare

You can use the `cf.api_gateway.auth_id_present` field in [custom rules](/waf/custom-rules/) to trigger when the API Shield configured session identifiers are present or absent on a request. Since this rule would cover your entire zone, Cloudflare recommends adding a host and path match in the rule to pinpoint the protection to exactly what is needed.

## Limitations

Authentication Posture can only apply when customers accurately set up session identifiers in API Shield. As a reminder, session identifiers are meant to uniquely identify authenticated users of your API. If you are unsure of your API's session identifier, consult with your development team.

## Availability 

Authentication Posture is available for all Enterprise subscriptions with API Shield.

---

# Security

URL: https://developers.cloudflare.com/api-shield/security/

import { DirectoryListing } from "~/components"

Cloudflare offers the following features to help secure your APIs:

<DirectoryListing />

## Example Cloudflare solutions

Cloudflare's API Shield â€” together with other compatible Cloudflare products â€” helps protect your API from the issues detailed in the [OWASPÂ® API Security Top 10](https://owasp.org/www-project-api-security/).

The following table provides examples of how you might match Cloudflare products to OWASP vulnerabilities:

| OWASP issue                                     | Example Cloudflare solution                                                                                                                            |
| ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Broken Object Level Authorization               | [Sequence Mitigation], [Schema Validation], [JWT Validation], [Rate Limiting]                                                                          |
| Broken Authentication                           | [mTLS](/api-shield/security/mtls/), [JWT Validation], [Exposed Credential Checks](/waf/managed-rules/check-for-exposed-credentials/), [Bot Management](/bots/), [Authentication Posture](/api-shield/security/authentication-posture/) |
| Broken Object Property Level Authorization      | [Schema Validation], [JWT Validation]                                                                                                                  |
| Unrestricted Resource Consumption               | [Rate Limiting], [Sequence Mitigation], [Bot Management], [GraphQL Query Protection]                                                                   |
| Broken Function Level Authorization             | [Schema Validation], [JWT Validation]                                                                                                                  |
| Unrestricted Access to Sensitive Business Flows | [Sequence Mitigation], [Bot Management], [GraphQL Query Protection]                                                                                    |
| Server Side Request Forgery                     | [Schema Validation], [WAF Managed Rules], [WAF Custom Rules](/waf/custom-rules/)                                                                       |
| Security Misconfiguration                       | [Sequence Mitigation], [Schema Validation], [WAF Managed Rules], [GraphQL Query Protection]                                                            |
| Improper Inventory Management                   | [Discovery](/api-shield/security/api-discovery/), [Schema Learning](/api-shield/management-and-monitoring/#endpoint-schema-learning)                   |
| Unsafe Consumption of APIs                      | [JWT Validation], [WAF Managed Rules]                                                                                                                  |

[Schema Validation]: /api-shield/security/schema-validation/

[Sequence Mitigation]: /api-shield/security/sequence-mitigation/

[JWT Validation]: /api-shield/security/jwt-validation/

[GraphQL Query Protection]: /api-shield/security/graphql-protection/

[Bot Management]: /bots/

[Rate Limiting]: /waf/rate-limiting-rules/

[WAF Managed Rules]: /waf/managed-rules/

---

# Sequence Analytics

URL: https://developers.cloudflare.com/api-shield/security/sequence-analytics/

import { GlossaryTooltip } from "~/components"

Sequence Analytics tracks the order of API endpoint requests over time, allowing you to discover how users interact with your API. Sequence Analytics groups and highlights important user journeys (sequences) across your API. You can enforce preferred sequences using [Sequence Mitigation](/api-shield/security/sequence-mitigation/).

## Process

### Sequence building

A sequence is a time-ordered list of HTTP API requests made by a specific visitor as they browse a website, use a mobile app, or interact with a B2B partner via API.

For example, a portion of a sequence made during a bank funds transfer could look like:

| Order | Method | Path                                    | Description                                                                                                                                    |
| ----- | ------ | --------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| 1     | `GET`  | `/api/v1/users/{user_id}/accounts`      | `user_id` is the active user.                                                                                                                  |
| 2     | `GET`  | `/api/v1/accounts/{account_id}/balance` | `account_id` is one of the userâ€™s accounts.                                                                                                    |
| 3     | `GET`  | `/api/v1/accounts/{account_id}/balance` | `account_id` is a different account belonging to the user.                                                                                     |
| 4     | `POST` | `/api/v1/transferFunds`                 | This contains a request body detailing an account to transfer funds from, an account to transfer funds to, and an amount of money to transfer. |

API Shield uses your configured <GlossaryTooltip term="session identifier">session identifier</GlossaryTooltip> to build a set of ordered API operations (HTTP host, method, and path) requested per session. We may surface sequences in various lengths depending how API Shield scores the sequences.

### Sequence scoring

API Shield scores sequences by a metric called precedence score. Sequence Analytics displays sequences by the highest precedence score. High-scoring sequences contain API requests which are likely to occur together in order.

Using the example above, a high score means that the last operation in the sequence `POST /api/v1/transferFunds` is highly likely to be preceded by the other operations in sequence `GET /api/v1/users/{user_id}/accounts` followed by `GET /api/v1/accounts/{account_id}/balance`. The scores are probabilities, which API Shield estimates using data from the last 24 hours.

### Secure your API

To proactively secure your API, you should inspect your highest-scoring sequences. For each high-scoring sequence, you should confirm with your development team if the final operation in the sequence must legitimately always be preceded by the other operations in the sequence.

Using the above example, if `POST /api/v1/transferFunds` must legitimately always be preceded by `GET /api/v1/users/{user_id}/accounts` and `GET /api/v1/accounts/{account_id}/balance?`, you should create an **Allow** rule in Sequence Mitigation on the final operation of the sequence.

You should also consider applying other API Shield protections to these endpoints ([rate limiting suggestions](/api-shield/security/volumetric-abuse-detection/), [Schema Validation](/api-shield/security/schema-validation/), [JWT Validation](/api-shield/security/jwt-validation/), and [mTLS](/api-shield/security/mtls/)).

For more information, refer to our [blog post](https://blog.cloudflare.com/api-sequence-analytics).

### Repeated sequences

True API usage shows many successively repeated operations. To facilitate exploration, Sequence Analytics collapses successively repeated operations into one.

## Availability

Sequence Analytics is available for all API Shield customers. Pro, Business, and Enterprise customers who have not purchased API Shield can get started by [enabling the API Shield trial](https://dash.cloudflare.com/?to=/:account/:zone/security/api-shield) in the Cloudflare dashboard or contacting your account manager.

## Limitations

Sequence Analytics currently requires a session identifier in order to build and track sequences made by an API consumer. Ensure that you have set up your session identifier(s) in the Cloudflare dashboard by going to **API Shield** > **Settings**.

Sequences are currently limited to nine operations in length.

---

# Volumetric Abuse Detection

URL: https://developers.cloudflare.com/api-shield/security/volumetric-abuse-detection/

import { GlossaryTooltip } from "~/components"

Cloudflare Volumetric Abuse Detection helps you set up a system of adaptive rate limiting.

## About

After [API Discovery](/api-shield/security/api-discovery/), Cloudflare looks for <GlossaryTooltip term="API endpoint">endpoint</GlossaryTooltip> abuse based on common user traffic.

For example, your API might see different levels of traffic to a `/reset-password` endpoint than a `/login` endpoint. Additionally, your `/login` endpoint might see higher than average traffic after a successful marketing campaign.

These two scenarios speak to the limitations of traditional rate limiting. Not only does traffic vary between endpoints, but it also can vary over time for the same endpoint. Volumetric Abuse Detection solves these problems with unsupervised learning to develop separate baselines for each API and better adjust to changes in user behavior.

Volumetric Abuse Detection rate limits are generated on a per-session basis. Unlike traditional rate limits, which are based on IP addresses, Volumetric Abuse Detection rate limits are not as susceptible to false positives when traffic to your API increases.

Volumetric Abuse Detection rate limits are a way to prevent blatant volumetric abuse while minimizing false positives. If you are trying to prevent abusive bot traffic altogether, refer to Cloudflareâ€™s [Bot solutions](/bots/).

## Process

Volumetric Abuse Detection analyzes your APIâ€™s individual session traffic statistics to recommend per-endpoint, per-session rate limits.

Volumetric Abuse Detection currently requires a <GlossaryTooltip term="session identifier" link="/api-shield/get-started/#to-set-up-session-identifiers">session identifier</GlossaryTooltip>, like an authorization token available as a request header or cookie.

After adding a session identifier, allow 24 hours for rate limit recommendations to appear on endpoints in **Security** > **API Shield** > **Endpoint Management** on the Cloudflare dashboard. Recommendations will continue to update if your traffic pattern changes.

### Observe rate limits

Once rate limit recommendations appear in **Endpoint Management**, select the endpoint row to view more detail about the recommendation. You will see the overall recommended rate limit value, as well as p99, p90, and p50 rate limit values.

Cloudflare recommends choosing the overall rate limit recommendation, as our analysis includes the variance of the request rate distribution across your API sessions. Choosing a single p-value may cause false positives due to a high number of outliers.

:::note[p-values]

p-values describe what percentile of your traffic fits below the value. For example, if your p90 value is `83`, then 90% of your sessions had maximum request rates less than 83 requests per 10 minutes. 
:::

In **Endpoint Management**, you can review our confidence in the recommendation and how many unique sessions we have seen over the last seven (7) days. In general, endpoints with fewer unique sessions and high variability of user behavior will have lower confidence scores.

Implementing low confidence rate limits can still be helpful to prevent API abuse. If you are hesitant due to the recommendationâ€™s confidence, we suggest starting your rate limit rule in `log` mode and observing violations of the rule for false positives.

### Create rate limits

To create rate limits:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and domain.
2. Go to **Security** > **API Shield**.
3. In **Endpoint Management**, select an endpoint.
4. Select **Create rule** to be automatically redirected to the Advanced Rate Limiting rules dashboard. 

  :::note

  Your endpoint information, session identifier, and recommended rate limit will be pre-filled into the rule. The pre-filled rule will only be enforced when the session identifiers are present on incoming requests.

  To apply rate limits to all requests, remove the `exists` logic in the rule and refer to the disclaimer in the [WAF documentation](/waf/rate-limiting-rules/parameters/#missing-field-versus-empty-value). 
  :::
  
5. Give your rule a name, action, and duration.
6. Select **Deploy** to activate your rule.

## API

[Rate limit recommendations are available via the API](/api/resources/api_gateway/subresources/operations/methods/get/) if you would like to dynamically update rate limits over time.

## Limitations

API Shield will always calculate recommendations when session identifiers are configured. To enable session-based rate limits, [subscribe to Advanced Rate Limiting](/waf/rate-limiting-rules/#availability).

## Availability

Volumetric Abuse Detection is only available for Enterprise customers. If you are an Enterprise customer and interested in this product, contact your account team.

---

# Classic Schema Validation (deprecated)

URL: https://developers.cloudflare.com/api-shield/reference/classic-schema-validation/

import { GlossaryTooltip } from "~/components"

:::caution[Deprecation notice]
Classic Schema Validation has been deprecated. 

Upload all new schemas to [Schema Validation 2.0](/api-shield/security/schema-validation/). 
:::

Use the **API Shield** interface to configure [API Schema Validation](/api-shield/security/schema-validation/), which validates requests according to the <GlossaryTooltip term="API schema">API schema</GlossaryTooltip> you provide.

Before you can configure Schema Validation for an API, you must obtain an API Schema file matching our [specifications](/api-shield/security/schema-validation/#specifications).

If you are in the Schema Validation 2.0, you can make changes to your settings but you cannot add any new Classic Schema Validation schemas.

:::note

This feature is only available for customers on an Enterprise plan. Contact your Cloudflare Customer Success Manager to get access. 
:::

## Create an API Shield with Schema Validation

To configure Schema Validation in the Cloudflare dashboard:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account and domain.
2. Select **Security** > **API Shield**.
3. Go to **Schema Validation** and select **Add schema**.
4. Enter a descriptive name for your policy and optionally edit the expression to trigger Schema Validation. For example, if your API is available at `http://api.example.com/v1`, include a check for the *Hostname* field â€” equal to `api.example.com` â€” and a check for the *URI Path* field using a regular expression â€” matching the regex `^/v1`.
:::caution[Important]

To validate the hostname, you must include the *Hostname* field explicitly in the rule, even if the hostname value is in the schema file. Any hostname value present in the schema file will be ignored. 
:::
5. Select **Next**.
6. Upload your schema file.
7. Select **Save** to validate the content of the schema file and deploy the Schema Validation rule. If you get a validation error, ensure that you are using one of the [supported file formats](/api-shield/security/schema-validation/#specifications) and that each endpoint and method pair has a unique operation ID.

After deploying your API Shield rule, Cloudflare displays a summary of all <GlossaryTooltip term="API endpoint">API endpoints</GlossaryTooltip> organized by their protection level and actions that will occur for non-compliant and unprotected requests.

1. In the **Endpoint action** dropdown, select an action for every request that targets a protected endpoint and fails Schema Validation.
2. In the **Fallthrough action** dropdown, select an action for every request that targets an unprotected endpoint.
3. Optionally, you can save the endpoints to Endpoint Management at the same time the Schema is saved by selecting **Save new endpoints to [endpoint management](/api-shield/management-and-monitoring/)**. Endpoints will be saved regardless of whether the Schema is saved as a draft or published live.
4. Select **Done**.

---

# Reference

URL: https://developers.cloudflare.com/api-shield/reference/

import { DirectoryListing } from "~/components"

Refer to the following pages for more information about API Shield:

<DirectoryListing />

---

# Terraform

URL: https://developers.cloudflare.com/api-shield/reference/terraform/

import { GlossaryTooltip } from "~/components"

Get started with API Shield using Terraform from the examples below. For more information on how to use Terraform with Cloudflare, refer to the [Terraform documentation](/terraform/).

The following resources are available to configure through Terraform:

**Session identifiers**

- [`api_shield`](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/api_shield) for configuring <GlossaryTooltip term="session identifier">session identifiers</GlossaryTooltip> in API Shield.

**Endpoint Management**

- [`api_shield_operation`](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/api_shield_operation) for configuring <GlossaryTooltip term="API endpoint">endpoints</GlossaryTooltip> in Endpoint Management.

**Schema Validation 2.0**

- [`api_shield_schema`](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/api_shield_schema) for configuring a schema in [Schema Validation 2.0](/api-shield/security/schema-validation/).
- [`api_shield_schema_validation_settings`](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/api_shield_schema_validation_settings) for configuring zone-level Schema Validation 2.0 settings.
- [`api_shield_operation_schema_validation_settings`](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/api_shield_operation_schema_validation_settings) for configuring operation-level Schema Validation 2.0 settings.

## Manage API Shield session identifiers

Refer to the example configuration below to set up [session identifiers](/api-shield/get-started/#to-set-up-session-identifiers) on your zone.

```tf title="Example configuration"
resource "cloudflare_api_shield" "my_api_shield" {
  zone_id  = var.zone_id
  auth_id_characteristics {
    name = "authorization"
    type = "header"
  }
}
```

## Manage API Shield Endpoint Management

Refer to the example configuration below to [manage endpoints](/api-shield/management-and-monitoring/) on your zone.

```tf title="Example configuration"
resource "cloudflare_api_shield_operation" "get_image" {
  zone_id  = var.zone_id
  method   = "GET"
  host     = "example.com"
  endpoint = "/api/images/{var1}"
}
 
resource "cloudflare_api_shield_operation" "post_image" {
  zone_id  = var.zone_id
  method   = "POST"
  host     = "example.com"
  endpoint = "/api/images/{var1}"
}
```

## Manage Schema Validation 2.0

:::note

It is required to configure Endpoint Management if you want to set up Schema Validation 2.0 using Terraform. 
:::

Refer to the example configuration below to manage [Schema Validation 2.0](/api-shield/security/schema-validation/configure/) on your zone.

```tf title="Example configuration"
# Schema that should be used for schema validation 2.0
resource "cloudflare_api_shield_schema" "example_schema" {
  zone_id                   = var.zone_id
  name                      = "example-schema"
  kind                      = "openapi_v3"
  validation_enabled        = true
  source                    = file("./schemas/example-schema.json")
}
 
# Block all requests that violate schema by default
resource "cloudflare_api_shield_schema_validation_settings" "zone_level_settings" {
  zone_id                               = var.zone_id
  validation_default_mitigation_action  = "block"
}
 
# For endpoint post_image - only log requests that violate schema
resource "cloudflare_api_shield_operation_schema_validation_settings" "post_image_log_only" {
  zone_id           = var.zone_id
  operation_id      = cloudflare_api_shield_operation.post_image.id
  mitigation_action = "log"
}
```

---

# Test current speed

URL: https://developers.cloudflare.com/automatic-platform-optimization/about/test-current-speed/

Before you begin using APO, we recommend testing your current site speed. Your site speed results give your website a letter grade and performance rating.

1. Visit [GTmetrix](https://gtmetrix.com/).
2. In the text field, enter your website's URL.
3. Click **Test your site**.

After the test runs, you can view your site's grade, performance rating, and other performance metrics.

---

# About

URL: https://developers.cloudflare.com/automatic-platform-optimization/about/

With Automatic Platform Optimization (APO), Cloudflare serves your entire site from our edge network, ensuring customers see improved performance when visiting your site. Cloudflare typically only caches static content, but with APO, we can also cache dynamic content â€” like HTML â€” to serve the entire site from the cache. This process removes round trips from the origin to drastically improve time to first byte (TTFB) along with other site performance metrics. In addition to caching dynamic content, APO caches third-party scripts to further reduce the number of requests that leave Cloudflare's edge network.

With APO, you can manage your WordPress site as normal. Whenever you update content in WordPress, Cloudflare updates content on our edge to prevent serving stale content when you use Cloudflare's WordPress plugin. Additionally, for logged-in or administrator users, we bypass the cache to ensure that private content is not cached and served to other visitors.

## Limitations

Automatic Platform Optimization is not compatible with Enterprise [subdomain setup](/dns/zone-setups/subdomain-setup/) when a subdomain, for example, `www` is in a different zone to the apex domain.

---

# Plugin compatibility

URL: https://developers.cloudflare.com/automatic-platform-optimization/about/plugin-compatibility/

Currently, WordPress offers over 50,000 plugins for download. As a result, testing the compatibility between APO and every available plugin is impossible. However, Cloudflare has a list of officially supported plugins and a list of plugins known to cause issues when APO is enabled.

For questions about a specific plugin not shown in the list, create a thread in the [Cloudflare Community](https://community.cloudflare.com/) to begin the conversation.

:::note


The Cloudflare APO WordPress plugin does not support multisite WordPress installation.


:::

## Compatible plugins

* [NitroPack](https://nitropack.io/)
* [FlyingPress](https://flyingpress.com/)
* [WP Rocket](https://community.cloudflare.com/t/cloudflares-apo-with-wp-rockets-minified-css/225906/3?u=yevgen) **version 3.8.6 or later**
* [BigCommerce](https://wordpress.org/plugins/bigcommerce/)
* [Easy Digital Downloads](https://wordpress.org/plugins/easy-digital-downloads/)
* [WooCommerce](https://wordpress.org/plugins/woocommerce/)
* [Redis Object Cache](https://wordpress.org/plugins/redis-cache/)
* [Object Cache Pro](https://objectcache.pro)
* [YITH WooCommerce Wishlist](https://wordpress.org/plugins/yith-woocommerce-wishlist/)
* [WP EasyCart](https://wordpress.org/plugins/wp-easycart/)
* [Ecwid Ecommerce Shopping Cart](https://wordpress.org/plugins/ecwid-shopping-cart/)
* [WP ECommerce](https://wordpress.org/plugins/wp-e-commerce/)
* [Bookly](https://wordpress.org/plugins/bookly-responsive-appointment-booking-tool/)
* [WPTouch](https://wordpress.org/plugins/wptouch/)
* [Mobile Detect](https://wordpress.org/plugins/tinywp-mobile-detect/)
* [WordPress Mobile Pack](https://wordpress.org/plugins/wordpress-mobile-pack/)
* [WP-Mobilizer](https://wordpress.org/plugins/wp-mobilizer/)
* [Any Mobile Theme Switcher](https://wordpress.org/plugins/any-mobile-theme-switcher/)
* [Easy Social Share Buttons](https://codecanyon.net/item/easy-social-share-buttons-for-wordpress/6394476)
* [Jetpack (Mobile Theme)](https://wordpress.org/plugins/jetpack/)
* [WPML](https://wpml.org/)
* [Hummingbird](https://wordpress.org/plugins/hummingbird-performance/)
* [Imunify360](https://docs.imunify360.com/features/#webshield)
* [Perfmatters](https://perfmatters.io/docs/cloudflare-wordpress-settings/)

---

# Activate the Cloudflare WordPress plugin

URL: https://developers.cloudflare.com/automatic-platform-optimization/get-started/activate-cf-wp-plugin/

After you [change your nameservers](/automatic-platform-optimization/get-started/change-nameservers/), activate the Cloudflare WordPress plugin.

## Prerequisites

Before activating the Cloudflare WordPress plugin, review the following prerequisites.

### Plan type

For users on the free plan, [purchase APO](#purchase-apo) before installing the WordPress plugin.

For users on a Pro plan or higher, continue to [Install and activate](#install-and-activate-the-cloudflare-wordpress-plugin) the Cloudflare WordPress plugin.

### Plugin compatibility

Cloudflare recommends turning off plugins such as WP Rocket Cache Plugin, W3 Total Cache, or similar plugins when first setting up APO. After confirming APO is working, we recommend testing whether turning on the plugins listed above improves results or causes unexpected behavior. In many cases, using APO along with other caching plugins can cause unexpected results.

We also recommend clearing the server cache for the WP Rocket Cache plugin, W3 Total Cache, or similar plugins after APO activation.

For more details, refer to [Plugin compatibility](/automatic-platform-optimization/about/plugin-compatibility/).

### Limitations

The Cloudflare APO WordPress plugin does not support multisite WordPress installation.

## Purchase APO

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com).
2. Select your account and zone.
3. Go to **Speed** > **Optimization** > **Content Optimization**.
4. For **Automatic Platform Optimization for WordPress**, select **Purchase**.
5. Enter your payment information and select **Confirm payment**.

## Install and activate the Cloudflare WordPress plugin

The easiest way to begin using APO is directly from Cloudflareâ€™s WordPress plugin. Before you can use APO, you must first install and activate the plugin and then activate APO.

1. Navigate and log in to your WordPress account.
2. Select **Plugins** > **Add new**.
3. In the search field, enter `Cloudflare`.
4. Locate the Cloudflare plugin and select **Install now**.
5. After the plugin finishes installing, select **Activate**. The Cloudflare plugin now displays in your Plugins list.

## Activate APO

To create the connection between WordPress and Cloudflare, you will create an API token from your Cloudflare dashboard and add it to WordPress. To set up APO on a subdomain, refer to [Subdomains and subdirectories](/automatic-platform-optimization/reference/subdomain-subdirectories/).

## Create the API token from Cloudflare

1. Open your Cloudflare dashboard.
2. Select **My Profile** from the top of the page.
3. Select **API Tokens** > **Create Token**.
4. Locate **WordPress** from the list and select **Use template**.
5. Select **Continue to summary** at the bottom of the page.
6. On the **WordPress API token summary** page, select **Create Token**. Your API token displays.
7. Select the **Copy** button to copy your token. You will need to paste the token in the next section.

:::note


Copy and paste your API token into a document saved on your computer to easily reference it again.


:::

## Add your API token to WordPress

1. Open your WordPress account and navigate to Plugins.
2. Locate the Cloudflare plugin and select **Settings**.
3. Select the option to sign in with an existing account.
4. Enter your email address and paste the token you copied in Step 7 of Create the API token from Cloudflare.
5. Select **Save API Credentials**.
6. For **Apply Recommended Cloudflare Settings for WordPress**, select **Apply**.
7. For **Automatic Platform Optimization**, switch the toggle to **On** to enable APO.

To verify APO is working, see [Verify APO works](/automatic-platform-optimization/get-started/verify-apo-works/).

---

# Change nameservers

URL: https://developers.cloudflare.com/automatic-platform-optimization/get-started/change-nameservers/

After you [confirm your DNS records](/automatic-platform-optimization/get-started/confirm-dns-records/), change your nameservers.

Updating your domain to use Cloudflare's nameservers is a critical step to ensure Cloudflare can optimize and protect your site. Nameservers are your primary DNS controller and identify the location of your domain on the Internet.

Domain registrars can take up to 24 hours to process the nameserver updates. You will receive an email from Cloudflare once your site is activated.

## Lookup domain name registration

1. Visit [WHOIS](https://lookup.icann.org/) to look up your domain name registration.
2. In the text field, enter your domain name without `https://www.` and select **Lookup**.
3. From **Domain Information**, make note of the nameserver information that displays. You will update those nameservers to point to Cloudflare.

We recommend keeping this browser tab or window open and opening a new tab or window for the next section.

## Update your nameserver with your domain registrar

1. Log in to the administrator account for your domain registrar.
2. Navigate to DNS Management.
3. Locate your nameserver information. Your nameservers should match the information from Step 3 of Lookup domain name registration.
4. Replace the existing nameserver information with the Cloudflare nameservers from Step 4 of Create the custom nameserver with Cloudflare.

:::note


You may be prompted to confirm the nameserver change with your domain registrar. Confirm or continue after making the update.


:::

---

# Confirm DNS Records

URL: https://developers.cloudflare.com/automatic-platform-optimization/get-started/confirm-dns-records/

Before you change your nameserver, confirm your DNS records are displaying correctly.

1. Open your Cloudflare dashboard and select your account.
2. Select your domain from the dropdown.
3. Select the **DNS** > **Records** tile. DNS management for your domain displays.
4. To add a record, select **Add record.**
5. To edit an existing record, select **Edit** for the appropriate record.
6. After making your changes, select **Save**.

After you confirm your DNS records, [change your nameservers](/automatic-platform-optimization/get-started/change-nameservers/).

---

# Get started

URL: https://developers.cloudflare.com/automatic-platform-optimization/get-started/

import { DirectoryListing } from "~/components"

Before you configure APO for your WordPress site, complete the following tasks.

<DirectoryListing />

---

# Verify APO works

URL: https://developers.cloudflare.com/automatic-platform-optimization/get-started/verify-apo-works/

You can check whether or not APO is working by verifying APO headers are present. When APO is working, three headers are present: `CF-Cache-Status`, `cf-apo-via`, `cf-edge-cache`.

1. Visit [Uptrends.com](https://www.uptrends.com/tools/http-response-header-check).
2. In the text field, enter the URL for your WordPress homepage including the `https://www.`.
3. Select **Start test**. The **Response Headers** table displays.
4. Locate the three header responses and their description. APO is working correctly when the headers exactly match the headers below.

* `CF-Cache-Status` | `HIT`
  * The `cf-cache-status` header displays if the asset is served from the cache or was considered dynamic and served from the origin.
* `cf-apo-via` | `tcache`
  * The `cf-apo-via` header returns the APO status for the given request.
* `cf-edge-cache` | `cache, platform=wordpress`
  * The `cf-edge-cache` headers confirms the WordPress plugin is installed and enabled.

## Verify the APO integration and WordPress integration work

Open your WordPress site and publish a change. When the integration is working, the page is cached with `cf-cache-status: HIT` and `cf-apo-via: tcache`

---

# FAQs

URL: https://developers.cloudflare.com/automatic-platform-optimization/troubleshooting/faq/

## Do I still need to create "Edge Cache TTL" page rules with "Cache Level: Cache Everything"?

No, you don't need create Edge Cache TTL page rules. When the WordPress plugin is installed, APO automatically caches content for 30 days and invalidates on change within 30 seconds. However, because APO now supports cache-related page rules, make sure existing page rules don't affect the resources served by APO.

## Does Origin Cache Control override APO?

No. APO ignores Origin Cache Control for caching on the Edge, but APO serves original Origin Cache Control to the client.

## Why are my browser cache control headers missing with APO?

The browser cache control headers may be missing with APO if you set your **Browser Cache TTL** to **Respect Existing Headers**. For example:

```sh
curl --silent --verbose --output /dev/null https://example.com/ --header 'Accept: text/html' 2>&1 | grep cache-control
```

```sh output
< cache-control: max-age=86400, stale-while-revalidate=86400, stale-if-error=86400
```

## Is the stale-if-error directive still needed with APO?

No, the `stale-if-error` directive is not needed because the feature is built into APO.

## When I check the posts and homepage cache status, the response header shows `cf-cache-status: BYPASS`. Is APO working?

When Chrome DevTools is open, Chrome sends `Cache-Control: no-cache` by default. You can uncheck the **Disable cache (while DevTools is open)** setting and see that `cf-cache-status: HIT` and `cf-apo-via: cache` headers will be returned.

## When I check `cf-cache-status` via cURL, `MISS` and `DYNAMIC` are always returned. In my browser, I see `HIT` but other tools return `DYNAMIC`. Is this expected behavior?

Yes, this is expected behavior because the requests must contain `accept: "text/html"`.

## Are Google Fonts optimized when APO is activated?

Yes, Google Fonts are also optimized when APO is activated. You can confirm the optimization by checking the font URLs. For example, the URL will change from `https://fonts.gstatic.com/s/...` to `https://example.com/fonts.gstatic.com/s/...` when the site loads. For proxied fonts, the `cf-apo-via:proxy` header is returned.

## Can I customize query string caching with APO?

For more information on query parameters, see [Query parameters and cached responses](/automatic-platform-optimization/reference/query-parameters/).

## Why are my font URLs not being transformed?

APO will skip URL font transformation when the `content-security-policy` response header is present but missing the values described below.

To fix the problem, the `content-security-policy` header value must allow for `unsafe-inline` on either the `style-src` or `default-src` directive. For example, `Content-Security-Policy: style-src unsafe-inline;`.

The header must allow for `self` on either the `font-src` or `default-src` directive. For example, `Content-Security-Policy: font-src self;`.

---

# Troubleshooting

URL: https://developers.cloudflare.com/automatic-platform-optimization/troubleshooting/

## WordPress plugin is undetected on Cloudflare dashboard

The WordPress plugin may go undetected on your Cloudflare dashboard for a few reasons.

* Versions older than 3.8.2 of the WordPress plugin are installed.
  * **Solution:** Install version 4.4.0 of the WordPress plugin.
* Version 3.8.2 of the plugin is installed but existing cache plugins return stale responses, for example, without `cf-edge-cache` header.
  * **Solution:** Enable APO from the WordPress plugin and purge the cache in the existing cache plugins.
* WordPress only runs on a subdomain, but WordPress and the WordPress plugin check against the apex domain.
  * **Solution:** For additional information, see [Subdomains and subdirectories](/automatic-platform-optimization/reference/subdomain-subdirectories/)

If your Cloudflare dashboard can't detect the WordPress plugin after trying the solutions above, ensure you completed all of the steps listed in [Activate the Cloudflare WordPress plugin](/automatic-platform-optimization/get-started/activate-cf-wp-plugin/).

:::note


The Cloudflare APO WordPress plugin does not support multisite WordPress installation.


:::

## WordPress returns stale content

If WordPress is returning stale content, [purge the cache](/cache/how-to/purge-cache/) when APO is enabled.

---

# Cache by device type

URL: https://developers.cloudflare.com/automatic-platform-optimization/reference/cache-device-type/

APO cache by device type provides all of the same benefits of Cloudflare's cache while targeting visitors with content appropriate to their device. Cloudflare evaluates the `User-Agent` header in the HTTP request to identify the device type. Cloudflare then identifies each device type with a case insensitive match to the regex below:

* **Mobile**: `(?:phone|windows\s+phone|ipod|blackberry|(?:android|bb\d+|meego|silk|googlebot) .+? mobile|palm|windows\s+ce|opera mini|avantgo|mobilesafari|docomo|kaios)`
* **Tablet**: `(?:ipad|playbook|(?:android|bb\d+|meego|silk)(?! .+? mobile))`
* **Desktop**: Everything else not matched above.

To enable caching by device type, enable the setting from the Cloudflare dashboard's APO card or from the WordPress plugin version 4.4.0 or later.

Once enabled, Cloudflare sends a `CF-Device-Type` HTTP header to your origin with a value of either `mobile`, `tablet`, `desktop` for every request to specify the visitorâ€™s device type. If your origin responds with the appropriate content for that device type, Cloudflare only caches the resource for that specific device type.

:::note


Changing Cache By Device Type setting will invalidate Cache.


:::

The Cloudflare for WordPress plugin automatically purges all cache variations for updated pages.

Cloudflare recommends that you use plugins that support cache by device type, which you may have to enable on the plugin. You will still need to test your plugins to make sure they behave as expected.

---

# Reference

URL: https://developers.cloudflare.com/automatic-platform-optimization/reference/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Page Rule integration with APO

URL: https://developers.cloudflare.com/automatic-platform-optimization/reference/page-rule-integration/

The following Page Rules can control APO. Any changes to caching via Page Rules require purging the cache for the changes to take effect.

:::caution

Consider using [Cache Rules](/cache/how-to/cache-rules/) instead to control APO due to their enhanced configurability. 
:::

* **Cache Level: Bypass** â€” APO bypasses pages with response header `cf-apo-via: origin,page-rules`

* **Cache Level: Ignore Query String** â€” APO ignores all query strings when serving from Cache.

* **Cache Level: Cache Everything** â€” APO caches pages with all query strings.

  :::caution

  Automatic page purge via the WordPress plugin wonâ€™t clean all cached pages, only pages without query strings. Cached responses will be returned even with request header `cache-control: no-cache`.

  :::

* **Bypass Cache on Cookie (Business and Enterprise plans only)** â€” APO applies custom bypass cookies in addition to the default list.

* **Edge Cache TTL** â€” APO applies custom Edge TTL instead of 30 days. This page rule is helpful for pages that can generate Captchas or nonces.

* **Browser Cache TTL** â€” APO applies custom Browser TTL.

* `CDN-Cache-Control` and `Cloudflare-CDN-Cache-Control` â€“ Enables users to have detailed control over cache TTLs without using a page rule. For more information on the `CDN-Cache-Control` and `Cloudflare-CDN-Cache-Control` headers, refer to [CDN-Cache-Control](/cache/concepts/cache-control/).

---

# Query parameters and cached responses

URL: https://developers.cloudflare.com/automatic-platform-optimization/reference/query-parameters/

Query parameters often signal the presence of dynamic content. As a result, if there are query parameters in the URL, APO bypasses the cache and attempts to get a new version of the page from the origin by default. Because query parameters are also often used for marketing attribution, like UTMs, quick loading times are especially important for users.

To add a query parameter to our allowlist, [create a post in the community](https://community.cloudflare.com/) for consideration.

APO serves cached content as long as the query parameters in the URL are one of the following:

* `ref`
* `utm_source`
* `utm_medium`
* `utm_campaign`
* `utm_term`
* `utm_content`
* `utm_expid`
* `fbclid`
* `fb_action_ids`
* `fb_action_types`
* `fb_source`
* `mc_cid`
* `mc_eid`
* `gclid`
* `dclid`
* `_ga`
* `campaignid`
* `adgroupid`
* `_ke`
* `cn-reloaded`
* `age-verified`
* `ao_noptimize`
* `usqp`
* `mkt_tok`
* `epik`
* `ck_subscriber_id`

## Cookies prefixes that always bypass cache

* `wp-`
* `wordpress`
* `comment_`
* `woocommerce_`
* `xf_`
* `edd_`
* `jetpack`
* `yith_wcwl_session_`
* `yith_wrvp_`
* `wpsc_`
* `ecwid`
* `ec_`
* `bookly_`
* `bookly`

---

# Subdomains and subdirectories

URL: https://developers.cloudflare.com/automatic-platform-optimization/reference/subdomain-subdirectories/

## Run APO on a subdomain

After you enable APO, you configure it to run on the subdomain that uses WordPress. For example, if you have a website called `www.mysite.com` which includes a subdomain running WordPress called `shop.mysite.com`, you would configure APO to run on the `shop.mysite.com` subdomain.

1. Install version 4.4.0 or later of the Cloudflare WordPress plugin.
2. Log in using Cloudflare **API token** or **Global key**.
3. Enable APO. The subdomain displays in the list of hostnames in the card.
4. Repeat the process for each subdomain to enable APO.

By default, APO runs on the apex domain (also known as "root domain" or "naked domain"). If you choose to run APO on a subdomain, the apex domain is automatically disabled. To run APO on a subdomain and the apex domain, upgrade the WordPress plugin to version 4.4.0 or later on the apex domain and re-enable APO.

## Run APO on a subdirectory

After you enable APO, you configure it to run on the subdirectory that uses WordPress. For example, if you have a website called `www.mysite.com` which includes a subdirectory running WordPress called `mysite.com/shop`, you would configure APO to run on the `mysite.com` domain.

1. Install the Cloudflare WordPress plugin.
2. Add your Cloudflare API Token.
3. Activate APO.

Repeat steps 1 and 2 for each subdirectory to activate the WordPress plugin for automatic cache purging.

## Run APO only on a subdirectory

If you choose to run APO only on a subdirectory, the rest of the domain should be configured to bypass APO. You can bypass APO in one of two ways.

### Use the `cf-edge-cache` response header

The `cf-edge-cache: no-cache` instructs the APO service to bypass caching for non-WordPress parts of the site. You can implement this option with Cloudflare Workers using the example below.

```js
export default {
	async fetch(request, env, ctx) {
		const originalResponse = await fetch(request);

		// Response properties are immutable. To change them, construct a new Response object.
    const response = new Response(originalResponse.body, originalResponse);

    // Response headers can be modified through the headers `set` method.
    response.headers.set("cf-edge-cache", "no-cache");

		return response;
	},
};
```

### Use Cache Rules

Create a [cache rule](/cache/how-to/cache-rules/) to exclude non-WordPress portions of the site from caching using **Cache eligibility: Bypass cache**. This option disables all caching, including static assets for those paths. As a result, we recommend disabling APO via the response header.

---

# Business

URL: https://developers.cloudflare.com/bots/bot-analytics/biz-and-ent/

import { GlossaryTooltip, Render } from "~/components"

Business and Enterprise customers without Bot Management can use **Bot Analytics** to dynamically examine bot traffic. These dashboards offer less functionality than Bot Management for Enterprise but still help you understand bot traffic on your domain.

## Access

To use Bot Analytics, open the Cloudflare dashboard and select **Security** > **Bots**.

![View Bot Analytics in the Cloudflare dashboard. For more details, keep reading.](~/assets/images/bots/bot-analytics-dashboard-biz.png)

## Features

For a full tour of Bot Analytics, see [our blog post](https://blog.cloudflare.com/introducing-bot-analytics/). At a high level, the tool includes:

- **Requests by traffic type**: View your total domain traffic segmented vertically by traffic type. Keep an eye on _automated_ and _likely automated_ traffic.
- **Requests by detection source**: Identify the most common detection engines used to score your traffic. Hover over a tooltip to learn more about each engine.
- **Top requests by attribute**: View more detailed information on specific IP addresses and other characteristics.

Bot Analytics shows up to 72 hours of data at a time and can display data up to 30 days old. Bot Analytics displays data in real time in most cases.

<Render file="analytics-features" />

## Common uses

Business and Enterprise customers without Bot Management can use Bot Analytics to:

- Understand <GlossaryTooltip term="bot">bot</GlossaryTooltip> traffic
- Study recent attacks to find trends and detailed information
- Learn more about Cloudflareâ€™s detection engines with real data

For more details and granular control over bot traffic, consider upgrading to [Bot Management for Enterprise](/bots/bot-analytics/bm-subscription/).

---

# Bot Analytics

URL: https://developers.cloudflare.com/bots/bot-analytics/

import { DirectoryListing } from "~/components"

Business and Enterprise customers can use **Bot Analytics** to dynamically examine bot traffic.

<DirectoryListing />

---

# Enterprise Bot Management

URL: https://developers.cloudflare.com/bots/bot-analytics/bm-subscription/

import { GlossaryTooltip, Render } from "~/components"

Enterprise customers with Bot Management can use **Bot Analytics** to dynamically examine bot traffic.

## Access

To use Bot Analytics, open the Cloudflare dashboard and select **Security** > **Bots**.

![View Bot Analytics in the Cloudflare dashboard. For more details, keep reading.](~/assets/images/bots/bot-analytics-dashboard-ent.png)

## Features

<Render file="bm-analytics-features" />

<Render file="analytics-features" />

## Common uses

Bot Management customers can use Bot Analytics to:

- Understand traffic during [your onboarding phase](/bots/get-started/bm-subscription/).
- Tune WAF custom rules to be effective but not overly aggressive.
- Study recent attacks to find trends and detailed information.
- Learn more about Cloudflareâ€™s detection engines with real data.

## API

Data from Bot Analytics is also available via the GraphQL API. You can access <GlossaryTooltip term="bot score">bot scores</GlossaryTooltip>, bot sources, <GlossaryTooltip term="bot tags" link="/bots/concepts/cloudflare-bot-tags/">bot tags</GlossaryTooltip>, and bot _decisions_ (_automated_, _likely automated_, etc.), and more.

Read the [GraphQL Analytics API documentation](/analytics/graphql-api/) for more information about GraphQL and basic querying.

---

# Challenge Solve Rate (CSR)

URL: https://developers.cloudflare.com/bots/concepts/challenge-solve-rate/

import { Render } from "~/components"

<Render file="challenge-solve-rate" />

You can find the CSR of a rule by going to its corresponding dashboard page:

- For [custom rules](/waf/custom-rules/), go to **Security** > **WAF** > **Custom rules**.
- For [rate limiting rules](/waf/rate-limiting-rules/), go to **Security** > **WAF** > **Rate limiting rules**.

---

# Bot Tags

URL: https://developers.cloudflare.com/bots/concepts/cloudflare-bot-tags/

import { Render } from "~/components"

<Render file="bot-tags" />

Use these tags to learn more about your bot traffic and better inform security settings.

:::note

Bot tags are only available to Enterprise customers who have purchased Bot Management. 
:::

## Potential values

Once you [enable Bot Tags](#enable-bot-tags), you can see more information about bot requests, such as whether a request came from a verified bot (like Bing) or a category of verified bot (like SearchEngine).

<Render file="bot-tags-values" />

## Enable bot tags

To enable bot tags, include the `BotTags` log field when using our [Logpush service](/logs/about/).

## Limitations

Currently, Bot Tags are only available in log fields.

Future work will add more values and extend Bot Tags to other Cloudflare products.

---

# Detection IDs

URL: https://developers.cloudflare.com/bots/concepts/detection-ids/

import { Render } from "~/components"

<Render file="detection-ids" />

If you are having an issue with one of our heuristics, detection IDs allow you to decide which heuristics to enforce on your zones using customer configurable heuristics. You can choose unique actions for different bots, detected through Cloudflareâ€™s heuristics engine. You can block, allow, or serve alternate content to specific bots to meet the unique needs of your siteâ€™s traffic.

:::note

A request can trigger multiple detection IDs. 
:::

You can use `cf.bot_management.detection_ids` fields in tools such as:

- [Custom rules](/waf/custom-rules/)
- [Advanced Rate Limiting](/waf/rate-limiting-rules/)
- [Transform Rules](/rules/transform/)
- [Workers](/workers/) (as `request.cf.botManagement.detectionIds`)

Bot Detection IDs and tags are also available in [Bot Analytics](/bots/bot-analytics/) and [Security Analytics](/waf/analytics/security-analytics/).

## Detection tags

Detection tags refer to the category associated with the detection ID at the time that Cloudflare has fingerprinted a bot. For example, if a detection tag is `go`, this means that Cloudflare has observed traffic from that detection ID from a Go programming language bot.

:::note

Detection tags are available in Security Analytics, but not in the Security Events.
:::

## Bot Detection IDs via Logpush

You can create or edit their existing Logpush jobs to include the new Bot Detection IDs field which will provide an array of IDs for each request that has heuristics match on it. The `BotDetectionIDs` field is available as part of the HTTP Requests dataset and you can add it to new or existing jobs via the Logpush API or on the Cloudflare dashboard. This is the primary method to discover Detection IDs.

### Via the Cloudflare dashboard

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Analytics & Logs** > **Logs**.
3. Select **Add Logpush Job**.
4. Select **HTTP Requests** as the dataset.
5. Select **BotDetectionIDs** under the General data field category.
6. Select and enter the destination information.
7. Prove the ownership.
8. Select **Save**.

### Via the API

[Update your logpush job](/logs/reference/log-output-options/) by adding `BotDetectionIDs` to the  `output_options:`  parameters.

## Create or edit an expression

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **Bots**, apply filters and select **Create custom rule** to create a [custom rule](/waf/custom-rules/create-dashboard/) based on those filters. 
   - Alternatively, if you already created a custom rule, go to **Security** > **WAF** > **Custom rules** and edit the expression of an existing custom rule.
3. Use the `cf.bot_management.detection_ids` field in the rule expression.
4. Select **Save**.

## Use cases

### Block requests that match a specific detection ID

```js
any(cf.bot_management.detection_ids[*] eq 3355446)
and not cf.bot_management.verified_bot
and http.request.uri.path eq "/login"
and http.request.method eq "POST"
```

### Run Bot Management without specific detection IDs

```js
cf.bot_management.score lt 30
and not cf.bot_management.verified_bot
and http.request.uri.path eq "/login"
and http.request.method eq "POST"
and not any(cf.bot_management.detection_ids[*] in {3355446 12577893})
```

## Account takeover detections

<Render file="account-takeover-detections" />

### Challenges for account takeover detections

Cloudflare's [Managed Challenge](/waf/reference/cloudflare-challenges/) can limit brute-force attacks on your login endpoints.

To access account takeover detections:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **WAF**.
3. Under **Custom Rules**, select **Create rule**.
4. Fill out the form using **Bot Detection IDs** along with other necessary information.
5. Select **Save as draft** to return to it later, or **Deploy** to deploy the rule.

```js title="Rule example"

(any(cf.bot_management.detection_ids[*] eq 201326593))
```

### Limit logins with account takeover detections

Rate limiting rules can limit the number of logins from a particular IP, JA4 Fingerprint, or country.

To use rate limiting rules with account takeover detections:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to ***Security** > **WAF**.
3. Under **Rate limiting rules**, select **Create rule**.
4. Fill out the form using the **Custom expression builder** and `cf.bot_management_detection_ids` along with other necessary information.
5. Select **Save as draft** to return to it later, or **Deploy** to deploy the rule.

:::note
The rule can be enhanced with Leaked Credential Checks. Refer to the [WAF documentation](/waf/detections/leaked-credentials/) for more information on how to include leaked credentials and account takeover detections in a rate limiting rule. 

:::

---

# Concepts

URL: https://developers.cloudflare.com/bots/concepts/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# JA3/JA4 Fingerprint

URL: https://developers.cloudflare.com/bots/concepts/ja3-ja4-fingerprint/

import { Render } from "~/components"

<Render file="ja3-fingerprint" />

<Render file="ja4-fingerprint" />

:::note

JA3 and JA4 fingerprints are only available to Enterprise customers who have purchased Bot Management. 
:::

<Render file="signals-intelligence-and-ja4" />

```json

{
  "ja4Signals": {
    "h2h3_ratio_1h": 0.98826485872269,
    "heuristic_ratio_1h": 7.288895722013e-05,
    "reqs_quantile_1h": 0.99905741214752,
    "uas_rank_1h": 901,
    "browser_ratio_1h": 0.93640440702438,
    "paths_rank_1h": 655,
    "reqs_rank_1h": 850,
    "cache_ratio_1h": 0.18918327987194,
    "ips_rank_1h": 662,
    "ips_quantile_1h": 0.99926590919495
  },
  "jaSignalsParsed": {
    "ratios": {
      "h2h3_ratio_1h": 0.98826485872269,
      "heuristic_ratio_1h": 7.288895722013e-05,
      "browser_ratio_1h": 0.93640440702438,
      "cache_ratio_1h": 0.18918327987194
    },
    "ranks": {
      "uas_rank_1h": 901,
      "paths_rank_1h": 655,
      "reqs_rank_1h": 850,
      "ips_rank_1h": 662
    },
    "quantiles": {
      "reqs_quantile_1h": 0.99905741214752,
      "ips_quantile_1h": 0.99926590919495
    }
  }
}

```

When JA4 Signals are missing, the output appears as follows:

```json output title="Missing JA4 signals output"
{
  "ja4Signals": {},
  "jaSignalsParsed": {
    "ratios": {},
    "ranks": {},
    "quantiles": {}
  }
}
```

:::note

This sample was generated using [Workers' Cloudflare Object script](/workers/examples/accessing-the-cloudflare-object/). 
:::

## Analytics

To get more information about potential bot requests, use these JA3 and JA4 fingerprints in:

- [Bot Analytics](/bots/bot-analytics/bm-subscription/)
- [Security Events](/waf/analytics/security-events/) and [Security Analytics](/waf/analytics/security-analytics/)
- [Analytics GraphQL API](/analytics/graphql-api/), specifically the **HTTP Requests** dataset
- [Logs](/logs/reference/log-fields/zone/http_requests/)

## Actions

To adjust how your application responds to specific fingerprints, use them with:

- [WAF custom rules](/waf/custom-rules/)
- [Transform Rules](/rules/transform/)
- [Cloudflare Workers](/workers/runtime-apis/request/#incomingrequestcfproperties)

## Use cases

### Block or allow certain traffic

A group of similar requests may share the same JA3 fingerprint. For this reason, JA3 may be useful in blocking an incoming threat. For example, if you notice that a bot attack is not caught by existing defenses, create a [custom rule](/waf/custom-rules/) that blocks or challenges the JA3 used for the attack.

Alternatively, if existing defenses are blocking traffic that is actually legitimate, create a [custom rule](/waf/custom-rules/) with the _Skip_ action allowing the JA3 seen across good requests.

JA3 may also be useful if you want to immediately remedy false positives or false negatives with Bot Management.

### Allow mobile traffic

Often, mobile application traffic will produce the same JA3 fingerprint across devices and users. This means you can identify your mobile application traffic by its JA3 fingerprint.

Use the JA3 fingerprint to [allow traffic](/waf/custom-rules/use-cases/challenge-bad-bots/#adjust-for-mobile-traffic) from your mobile application, but block or challenge remaining traffic.

---

# Bot Feedback Loop

URL: https://developers.cloudflare.com/bots/concepts/feedback-loop/

import { GlossaryTooltip } from "~/components"

The Bot Feedback Loop is a way for customers to send Cloudflare direct feedback in the case of Bot Management potentially <GlossaryTooltip term="bot score" link="/bots/concepts/bot-score/">scoring</GlossaryTooltip> a request incorrectly. When a customer submits a False Negative or a False Positive report, Cloudflare manually analyzes this data and uses it as a training dataset for our next Machine Learning model.

## Availability

Bot Feedback Loop is available for Enterprise Bot Management customers. Visit [Plans](/bots/plans/) for more information.

## False Positive

A false positive can happen if Cloudflare scores a request from a person using a browser, mobile application or desktop application in the _automated_ or _likely automated_ range.

## False Negative

If Cloudflare is unable to detect a portion of automated traffic on your site, submitting a False Negative report will help us catch it in the future.

### Subtypes

| Subtype                | Definition                                                                                                                                                                                                |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Account Creation Abuse | The automated creation of many new accounts in order to gain access to site resources.                                                                                                                    |
| Ad Fraud               | Fraudulent increase in the number of times an advertisement is clicked on or displayed.                                                                                                                   |
| Credit Card Abuse      | Attempts to repeatedly validate many credit card numbers or the same credit card number with different validation details.                                                                                |
| Cashing Out            | Abusing the target Internet application to obtain valuable goods.                                                                                                                                         |
| Login Abuse            | Attempts to gain access to a password protected portion of an Internet application using many different combinations of usernames and passwords.                                                          |
| Inventory Abuse        | Automated abuse related to purchasing limited stock inventory or holding inventory to prevent others from making transactions.                                                                            |
| Denial of Service      | Automated requests with the intent of exhausting server resources to prevent the Internet application from functioning.                                                                                   |
| Expediting             | Automating the use of an Internet application to make transactions faster than a human visitor to gain unfair advantage.                                                                                  |
| Fuzzing                | Finding implementation bugs through the use of malformed data injection in an automated fashion.                                                                                                          |
| Scraping               | Automated retrieval of valuable or proprietary information from an Internet application.                                                                                                                  |
| Spamming               | The abuse of content forms to send spam.                                                                                                                                                                  |
| Token Cracking         | Identification of valid token codes providing some form of user benefit within the application.                                                                                                           |
| Vulnerability Scanning | Systematic enumeration and examination of identifiable, guessable and unknown content locations, paths, file names, parameters, to find weaknesses and points where a security vulnerability might exist. |

## Submit a report

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **Bots**.
3. Apply one or more bot score filters.
4. Select **Report incorrect data** and fill out the form.
5. Select **Submit**.

## Via the API

### Create a feedback report

```sh
curl 'https://api.cloudflare.com/client/v4/zones/{zone_id}/bot_management/feedback' \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "type": "false_positive",
  "description": "Legitimate customers having low score",
  "expression": "(cf.bot_management.score le 46 and ip.geoip.asnum eq 132892 and http.host eq \"api-discovery.theburritobot.com\" and cf.bot_management.ja3_hash eq \"3fed133de60c35724739b913924b6c24\")",
  "first_request_seen_at": "2022-08-01T00:00:00Z",
  "last_request_seen_at": "2022-08-10T00:00:00Z",
  "requests": 100,
  "requests_by_score": {
    "1": 50,
    "10": 50
  },
  "requests_by_score_src": {
    "heuristics": 25,
    "machine_learning": 75
  },
  "requests_by_attribute": {
    "topIPs": [
      {
        "metric": "10.75.34.1",
        "requests": 100
      }
    ],
    "topUserAgents": [
      {
        "metric": "curl/7.68.0",
        "requests": 100
      }
    ]
  }
}'
```

### List feedback reports

```bash
curl 'https://api.cloudflare.com/client/v4/zones/{zone_id}/bot_management/feedback' \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>"
```

```bash output
[
  {
    "created_at": "2022-08-19T00:05:24.749712Z",
    "type": "false_positive",
    "description": "Legitimate customers having low score",
    "expression": "(cf.bot_management.score le 46 and ip.geoip.asnum eq 132892 and http.host eq \"api-discovery.theburritobot.com\" and cf.bot_management.ja3_hash eq \"3fed133de60c35724739b913924b6c24\")",
    "first_request_seen_at": "2022-08-01T00:00:00Z",
    "last_request_seen_at": "2022-08-10T00:00:00Z",
    "requests": 100,
    "requests_by_score": {
      "1": 50,
      "10": 50
    },
    "requests_by_score_src": {
      "heuristics": 25,
      "machine_learning": 75
    },
    "requests_by_attribute": {
      "topIPs": [
        {
          "metric": "10.75.34.1",
          "requests": 100
        }
      ],
      "topUserAgents": [
        {
          "metric": "curl/7.68.0",
          "requests": 100
        }
      ]
    }
  }
]
```

## API Fields

| Field                   | Type    | Description                                                                 | Value Example                                                                                                                                                          |
| ----------------------- | ------- | --------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `type`                  | string  | The feedback report type.                                                   | `false_positive`                                                                                                                                                       |
| `description`           | string  | The feedback report description with  more details on the issue.            | Legitimate customers having low scores.                                                                                                                                |
| `expression`            | string  | The wirefilter expression matching reported requests.                       | `(cf.bot_management.score le 46 and ip.geoip.asnum eq 132892 and http.host eq "app.example.com" and cf.bot_management.ja3_hash eq "3fed133de60c35724739b913924b6c24")` |
| `first_request_seen_at` | string  | The time range start when the first request has been seen, RFC 3339 format. | `2022-08-01T00:00:00Z`                                                                                                                                                 |
| `last_request_seen_at`  | string  | The time range end when the last request has been seen, RFC 3339 format.    | `2022-08-10T00:00:00Z`                                                                                                                                                 |
| `requests`              | integer | The total number of reported requests.                                      | `100`                                                                                                                                                                  |
| `requests_by_score`     | object  | The requests breakdown by score.                                            | See example below.                                                                                                                                                     |
| `requests_by_score_src` | object  | Requests breakdown by score source.                                         | See example below.                                                                                                                                                     |
| `requests_by_attribute` | object  | Requests breakdown by attribute (optional).                                 | See example below.                                                                                                                                                     |

`requests_by_score`

```json
{
  "1": 50,
  "10": 50
}
```

`requests_by_score_src`

```json
{
  "machine_learning": 75,
  "heuristics": 25
}
```

`requests_by_attribute`

```json
  {
    "topIPs": [
      {
        "metric": "10.75.34.1"
        "requests": 100
      }
    ],
    "topUserAgents": [
      {
        "metric": "curl/7.68.0",
        "requests": 100
      }
    ]
  }
```

### Expression fields

| Field                        | Type    | Description                                                                                                                                                                 |
| ---------------------------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `cf.bot_management.ja3_hash` | string  | This provides an SSL/TLS fingerprint to help you identify potential bot requests.                                                                                           |
| `cf.bot_management.score`    | integer | This represents the likelihood that a request originates from a bot using a score from 1-99.                                                                                |
| `http.host`                  | string  | This represents the hostname used in the full request URI.                                                                                                                  |
| `http.request.uri.path`      | string  | This represents the URI path of the request.                                                                                                                                |
| `http.user_agent`            | string  | This represents the HTTP user agent which is a request header that contains a characteristic string to allow identification of the client operating system and web browser. |
| `ip.geoip.asnum`             | integer | This represents the 16- or 32-bit integer representing the Autonomous System (AS) number associated with client IP address.                                                 |
| `ip.geoip.country`           | string  | This represents the 2-letter country code in ISO 3166-1 Alpha 2 format.                                                                                                     |
| `ip.src`                     | string  | The source address of the IP.                                                                                                                                               |

## Recommendations when submitting a report

When you submit a report, use the filters available in the Bot Analytics dashboard to ensure that your report includes only the traffic that received an incorrect score. In addition to filtering by a score (required), you may want to filter by user-agent, IP, ASN or JA3 to more precisely highlight the section of traffic that was scored incorrectly.

If you are not certain if some traffic received an incorrect score, keep this traffic in the report.

We appreciate any comments you wish to leave in the description field that might help our team better understand these requests in the context of typical traffic to your domain.

## Recommendations after submitting a false positive

:::note

The instructions below apply to Enterprise subscription with Bot Management only. 
:::

After submitting a false positive, you can explicitly allow the traffic if you are confident that this traffic source cannot be used for abuse in the future. To allow traffic, you can create a WAF custom rule with a [Skip the remaining custom rules](/waf/custom-rules/skip/options/) action that matches the characteristics of your false positive report. We recommend any skip rule that you create uses the most narrow possible scope, including restricting the request methods and URIs that the expected traffic has access to, to limit potential abuse.

* Allowing a **[JA3/JA4 fingerprint](/bots/concepts/ja3-ja4-fingerprint/)**:  If you want to allow access to a stable software client that does not come from a dedicated IP, you can do so by looking up the JA3 fingerprint(s) used by that client in the Bot Analytics dashboard, and creating a WAF custom rule to allow traffic based on that JA3 fingerprint. JA3 fingerprints will only match a clientâ€™s TLS library, so be cautious in looking for both overlap with other clients and with variation based on the operating system. <br/><br/>Cloudflare does not recommend relying on JA3 rules for mobile applications that may be abused. If you have questions about how to securely allow traffic from your mobile application, please contact your account team.

:::note

The instructions below apply to Enterprise subscription with Bot Management, Bot Fight Mode and Super Bot Fight Mode. 
:::

* Allowing an **IP address**: Only use an IP address to allow traffic if the IP is a dedicated resource that belongs only to the traffic source you wish to allow. <br/>If the traffic you want to allow shares an IP with other traffic sources, or if the IP changes frequently, consider an alternative to allowing by IP address.

## Recommendations after submitting a false negative

After submitting a false negative report, you can explicitly block or rate-limit the incorrectly scored traffic using a combination of characteristics such as IP address, JA3 fingerprint, ASN, and user-agent. Before blocking or rate-limiting based on JA3 fingerprint, please use Bot Analytics to confirm that fingerprint is not being used by legitimate traffic sources.

---

# Sequence rules

URL: https://developers.cloudflare.com/bots/concepts/sequence-rules/

import { Render } from "~/components"

<Render file="sequence-rules" product="bots" params={{ one: "Sequence rules", two: "/bots/concepts/sequence-rules/" }} />

Sequence rules is currently in private beta. If you would like to be included in the beta, contact your account team.

## Prerequisites

- Your account must have the Fraud Detection subscription.
- Each zone must have an API Shield subscription as it relies on [Endpoint Management](/api-shield/management-and-monitoring/).
- Each zone must configure the endpoints to track via Endpoint Management.

---

## Enable sequence rules via the API

1. [Create an API token](/fundamentals/api/get-started/create-token/) if you do not already have one. The API token must include the _Zone_ > _Fraud Detection_ > _Edit_ permission.
2. [Get the zone ID](/fundamentals/setup/find-account-and-zone-ids/) for the zone(s) where you want to enable sequence rules.
3. [Add the endpoints](/api-shield/management-and-monitoring/) that you want to track in your sequence rules using API Shield's Endpoint Management and make note of the short ID. 
  :::note

  The short ID will not be visible until our account team has enabled this feature for you. 
  :::
4. Enable the sequence cookie by adding your API token and zone ID to the following API call.

```bash title="API call"
curl --request PUT \
https://api.cloudflare.com/client/v4/zones/{zone_id}/fraud_detection/sequence_cookies \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{"enabled": true}'
```

5. Use the expression editor to write sequence or timing based rules via [custom rules](/waf/custom-rules/), [rate limiting rules](/waf/rate-limiting-rules/), or [transform rules](/rules/transform/). You can put these rules in log only mode to monitor.

:::note

When you enable sequence rules, Cloudflare will start setting cookies for all requests that match your endpoints. 
:::

Once you have enabled sequence rules, the rules fields will be populated and you can now use the new fields in your rules.

---

## Rules fields

Sequence rules introduces three new fields to Cloudflare Rules. All of these fields reference operations by their short ID. Accounts that have the Fraud Detection subscription can refer to the short ID by viewing the endpoint details via **API Shield** > **Endpoint Management** in the Cloudflare dashboard. Accounts without Fraud Detection do not have access to this field.

Cloudflare only stores up to the 10 most recent operations in a sequence for up to one hour. If there are more than 10 operations in the sequence, older operations will be dropped and will not be included in the following fields. Similarly, if an operation happened more than one hour ago, it will also not be included in the following fields.

## Availability

<Render file="sequence-rules-availability" />

### Example rules

The customer must request endpoint A before endpoint B.

```txt title="Valid sequence"
cf.sequence.current_op eq "bbbbbbbb" and
any(cf.sequence.previous_ops[*] == "aaaaaaaa")
```

```txt title="Invalid sequence"
cf.sequence.current_op eq "bbbbbbbb" and
not any(cf.sequence.previous_ops[*] == "aaaaaaaa")
```

Customer must request endpoint A at least one second before endpoint B.

```txt title="Valid sequence"
cf.sequence.current_op eq "bbbbbbbb" and
cf.sequence.msec_since_op["aaaaaaaa"] ge 1000
```

```txt title="Invalid sequence"
cf.sequence.current_op eq "bbbbbbbb" and
not cf.sequence.msec_since_op["aaaaaaaa"] ge 1000
```

---

## Disable sequence rules via the API

Disabling sequence rules will stop the rules fields from being populated. If you still have rules deployed which depend on these fields, those rules may not behave as intended. Remove or disable any rules that rely on sequence fields before disabling sequence rules.

To disable sequence rules:

1. [Create an API token](/fundamentals/api/get-started/create-token/) if you do not already have one. The API token must include the _Zone_ > _Fraud Detection_ > _Edit_ permission.
2. [Get the zone ID](/fundamentals/setup/find-account-and-zone-ids/) for the zone(s) where you want to enable sequence rules.
3. [Add the endpoints](/api-shield/management-and-monitoring/) that you want to track in your sequence rules using API Shield's Endpoint Management and make note of the short ID.

  :::note

  The short ID will not be visible until our account team has enabled this feature for you. 
  :::

4. Disable the sequence cookie using your API token, zone ID, and by setting `enabled` to `false` on the following API call.

```bash title="API call"
curl --request PUT https://api.cloudflare.com/client/v4/zones/{zone_id}/fraud_detection/sequence_cookies \
--header "Authorization: Bearer <API_TOKEN>" \
--data '{"enabled": false}'
```

---

## Limitations

Cloudflare only supports HTTPS requests since our cookies set the `Secure` attribute.

---

# Signals Intelligence

URL: https://developers.cloudflare.com/bots/concepts/signals-intelligence/

import { Render } from "~/components"

For every available [JA4 fingerprint](/bots/concepts/ja3-ja4-fingerprint/), Bot Management customers can view how Cloudflare sees it on the Internet and what behavior we view with the fingerprint. This data can help you understand why a request is scored in a particular fashion or allow you to use the aggregate data in your own ML models, run in either [Cloudflare Workers](/workers/) or at the origin location.

Specifically, for each JA4 fingerprint, you will be able to access the following information:

- The percentage of traffic associated with browsers that Cloudflare sees.
- The percentage of traffic associated with known bots that Cloudflare sees.
- The number of networks Cloudflare sees actively using this fingerprint.
- The number of Cloudflare sites that see traffic from this fingerprint.
- The frequency that fingerprint requests caches content and generates errors.

This data gives you access to insights only available via the Cloudflare network and generated by our unique edge network that sits behind 20% of all internet traffic. Additionally, you can feed this data into your own [Workers AI](/workers-ai/)-powered custom machine learning models via the Signals Intelligence fields below.

## Signals Intelligence fields

Signals Intelligence fields show observations about a particular JA4 that Cloudflare has seen globally over the last hour.

| <div style="width:170px">Field name</div> | Description                                                                                                                                                                                                                                       |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `h2h3_ratio_1h`                           | The ratio of HTTP/2 and HTTP/3 requests combined with the total number of requests for the JA4 fingerprint in the last hour. Higher values indicate a higher proportion of HTTP/2 and HTTP/3 requests compared to other protocol versions.        |
| `heuristic_ratio_1h`                      | The ratio of requests with a `scoreSrc` value of "heuristics" for the JA4 fingerprint in the last hour. Higher values suggest a larger proportion of requests being flagged by heuristic-based scoring.                                           |
| `reqs_quantile_1h`                        | The quantile position of the JA4 fingerprint based on the number of requests across all fingerprints in the last hour. Higher values indicate a relatively higher number of requests compared to other fingerprints.                              |
| `uas_rank_1h`                             | The rank of the JA4 fingerprint based on the number of distinct user agents across all fingerprints in the last hour. Lower values indicate a higher diversity of user agents associated with the fingerprint.                                    |
| `browser_ratio_1h`                        | The ratio of requests originating from browser-based user agents for the JA4 fingerprint in the last hour. Higher values suggest a higher proportion of browser-based requests.                                                                   |
| `paths_rank_1h`                           | The rank of the JA4 fingerprint based on the number of unique request paths across all fingerprints in the last hour. Lower values indicate a higher diversity of request paths associated with the fingerprint.                                  |
| `reqs_rank_1h`                            | The rank of the JA4 fingerprint based on the number of requests across all fingerprints in the last hour. Lower values indicate a higher number of requests associated with the fingerprint.                                                      |
| `cache_ratio_1h`                          | The ratio of cacheable responses for the JA4 fingerprint in the last hour. Higher values suggest a higher proportion of responses that can be cached.                                                                                             |
| `ips_rank_1h`                             | The rank of the JA4 fingerprint based on the number of unique client IP addresses across all fingerprints in the last hour. Lower values indicate a higher number of distinct client IPs associated with the fingerprint.                         |
| `ips_quantile_1h`                         | The quantile position of the JA4 fingerprint based on the number of unique client IP addresses across all fingerprints in the last hour. Higher values indicate a relatively higher number of distinct client IPs compared to other fingerprints. |

<Render file="signals-intelligence-and-ja4" />

---

# Business

URL: https://developers.cloudflare.com/bots/get-started/biz-and-ent/

import { Render } from "~/components"

Super Bot Fight Mode is included in your Business or Enterprise subscription. When enabled, the product:

- Identifies traffic matching patterns of known bots and likely bots
- Can challenge or block bots
- Offers protection for static resources
- Provides robust analytics to help you understand bot traffic

:::note

If you have an Enterprise subscription without Bot Management, your application will also have Super Bot Fight Mode for Business.
:::

## Enable Super Bot Fight Mode

<Render file="get-started-pro-biz-steps" />

<Render file="flexible-sbfm" />

<Render file="sbfm-upgrade" />

## Disable Super Bot Fight Mode

<Render file="disable-sbfm" />

<Render file="flexible-sbfm" />

## Block AI bots

<Render file="block-ai-bots-enable" params={{ one: "Super Bot Fight Mode" }} />

:::note

You can view blocked AI bot traffic via [Security Analytics](/waf/analytics/security-analytics/). 
:::

## Analytics

For more on analytics, see [Bot Analytics](/bots/bot-analytics/biz-and-ent/).

## Ruleset Engine

<Render file="bfm-ruleset-engine" />

<Render file="bfm-change-notice" />

---

# Enterprise Bot Management

URL: https://developers.cloudflare.com/bots/get-started/bm-subscription/

import { Render } from "~/components"

Bot Management for Enterprise is a paid add-on that provides sophisticated bot protection for your domain. Customers can identify automated traffic, take appropriate action, and view detailed analytics within the dashboard.

This Enterprise product provides the most flexibility to customers by:

- Generating a [bot score](/bots/concepts/bot-score/) of 1-99 for every request. Scores below 30 are commonly associated with bot traffic.
- Allowing customers to take action on this score with [WAF custom rules](/waf/custom-rules/) or [`Workers`](/workers/runtime-apis/request/#incomingrequestcfproperties).
- Allowing customers to view this score in Bot Analytics or Logs.

## Enable Bot Management for Enterprise

Bot Management is automatically enabled for Enterprise zones entitled with the add-on.

To enable a [Bot Management](https://dash.cloudflare.com/?to=/:account/:zone/security/bots) trial on Enterprise zones without the Bot Management add-on entitled:

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Security** > **Bots**.
3. Select **Add Bot Management**.

:::note

If you are not seeing Bot Management enabled on your zone or if you still see **Add Bot Management** on the Cloudflare dashboard, contact your account team for the proper entitlements. 
:::

## Block AI bots

<Render file="block-ai-bots-enable" params={{ one: "Bot Management" }} />

:::note

You can view blocked AI bot traffic via [Security Analytics](/waf/analytics/security-analytics/). 
:::

## Setup

For more guidance on setup, refer to your Customer Success Manager.

---

# Free

URL: https://developers.cloudflare.com/bots/get-started/free/

import { Render } from "~/components"

<Render file="bot-fight-mode-definition" />

## Enable Bot Fight Mode

<Render file="bot-fight-mode-enable" />

<Render file="sbfm-upgrade" />

## Disable Bot Fight Mode

If you find that **Bot Fight Mode** is causing problems with your application traffic, you may want to disable it.

To disable Bot Fight Mode:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account and domain.
2. Go to **Security** > **Bots**.
3. For **Bot Fight Mode**, select **Off**.

## Block AI bots

<Render file="block-ai-bots-enable" params={{ one: "Bot Fight Mode" }} />

:::note

You can view blocked AI bot traffic via [Security Analytics](/waf/analytics/security-analytics/). 
:::

## Visibility

You can see bot-related actions by going to **Security** > **Events**. Any requests challenged by this product will be labeled **Bot Fight Mode** in the **Service** field. This allows you to observe, analyze, and follow trends in your bot traffic over time.

## Limitations

You cannot bypass or skip Bot Fight Mode using the _Skip_ action in WAF custom rules or using Page Rules. _Skip_, _Bypass_, and _Allow_ actions apply to rules or rulesets running on the [Ruleset Engine](/ruleset-engine/). While Super Bot Fight Mode rules are implemented in the Ruleset Engine, Bot Fight Mode checks are not. This is why you can skip Super Bot Fight Mode, but not Bot Fight Mode. If you need to skip Bot Fight Mode, consider using [Super Bot Fight Mode](/bots/get-started/pro/).

Bot Fight Mode can still trigger if you have IP Access rules, but it cannot trigger if an IP Access rule matches the request. For example, the IP Access rule matches the connecting IP.

---

# Get started with Cloudflare bot solutions

URL: https://developers.cloudflare.com/bots/get-started/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Pro

URL: https://developers.cloudflare.com/bots/get-started/pro/

import { Render } from "~/components"

Super Bot Fight Mode is included in your Pro subscription. When enabled, the product:

- Identifies traffic matching patterns of known bots
- Can challenge or block bots
- Offers protection for static resources
- Provides limited analytics to help you understand bot traffic

## Enable Super Bot Fight Mode

<Render file="get-started-pro-biz-steps" />

<Render file="flexible-sbfm" />

<Render file="sbfm-upgrade" />

## Disable Super Bot Fight Mode

<Render file="disable-sbfm" />

<Render file="flexible-sbfm" />

## Block AI bots

<Render file="block-ai-bots-enable" params={{ one: "Super Bot Fight Mode" }} />

:::note

You can view blocked AI bot traffic via [Security Analytics](/waf/analytics/security-analytics/). 
:::

## Analytics

### Bot Report

Use the **Bot Report** to monitor bot traffic for the past 24 hours.

To access the **Bot Report**, go to **Security** > **Bots**. If you see a double-digit percentage of automated traffic, you may want to upgrade to [Bot Management](/bots/plans/bm-subscription/) to save money on origin costs and protect your domain from large-scale attacks.

![Example traffic distribution as part of a bot report](~/assets/images/bots/bot-report-pro.png)

### Security events

You can see bot-related actions by going to **Security** > **Events**. Any requests challenged by this product will be labeled **Super Bot Fight Mode** in the **Service** field. This allows you to observe, analyze, and follow trends in your bot traffic over time.

## Ruleset Engine

<Render file="bfm-ruleset-engine" />

<Render file="bfm-change-notice" />

---

# Business

URL: https://developers.cloudflare.com/bots/plans/biz-and-ent/

import { Render } from "~/components"

<Render file="plan-intro" />

<Render file="buttons-plan-pages" />

## Business features

<Render file="about-plan-biz-and-ent" />

## Bot detection engines

### Heuristics

<Render file="bots-heuristics" />

### Machine learning

<Render file="bots-ml" />

The ML engine identifies _likely automated_ traffic.

### JavaScript detections

<Render file="bots-jsd" />

JSD is completely optional. To adjust your settings, configure Super Bot Fight Mode from **Security** > **Bots**.

### Notes on detection

<Render file="bots-cookie" />

<Render file="disable-cf-bm-cookie" />

## Considerations

<Render file="about-plan-considerations" />

## How do I get started?

<Render file="plan-get-started" />

---

# Enterprise Bot Management

URL: https://developers.cloudflare.com/bots/plans/bm-subscription/

import { Render } from "~/components"

<Render file="plan-intro" />

<Render file="buttons-plan-pages" />

## Bot Management for Enterprise Features

<Render file="about-plan-bm-subscription" />

:::note

Zones that have [Enterprise Bot Management](/bots/get-started/bm-subscription/) enabled will not see Bot Fight Mode or Super Bot Fight Mode under **Security** > **Bots**.
:::

## Bot detection engines

<Render file="bm-bot-detection-engines" />

## How do I get started?

<Render file="plan-get-started" />

---

# Free

URL: https://developers.cloudflare.com/bots/plans/free/

import { Render } from "~/components"

<Render file="plan-intro" />

<Render file="buttons-plan-pages" />

## Free features

<Render file="about-plan-free" />

## Bot detection engines

### Heuristics

<Render file="bots-heuristics" />

### JavaScript detections

<Render file="bots-jsd" />

JSD is automatically enabled with Bot Fight Mode.

### Notes on detection

<Render file="bots-cookie" />

## Considerations

<Render file="about-plan-considerations" />

## How do I get started?

<Render file="plan-get-started" />

---

# Plans

URL: https://developers.cloudflare.com/bots/plans/

import { LinkButton, Render } from "~/components"

<Render file="plan-intro" />

 <LinkButton variant="primary" href="/bots/plans/free/">Free</LinkButton> <LinkButton variant="primary" href="/bots/plans/pro/">Pro</LinkButton> <LinkButton variant="primary" href="/bots/plans/biz-and-ent/">Business</LinkButton> <LinkButton variant="primary" href="/bots/plans/bm-subscription/">Bot Management for Enterprise</LinkButton>


## How do I get started?

<Render file="plan-get-started" />

---

# Pro

URL: https://developers.cloudflare.com/bots/plans/pro/

import { Render } from "~/components"

<Render file="plan-intro" />

<Render file="buttons-plan-pages" />

## Pro features

<Render file="about-plan-pro" />

## Bot detection engines

### Heuristics

<Render file="bots-heuristics" />

### JavaScript detections

<Render file="bots-jsd" />

JSD is completely optional. To adjust your settings, configure Super Bot Fight Mode from **Security** > **Bots**.

### Notes on detection

<Render file="bots-cookie" />

## Considerations

<Render file="about-plan-considerations" />

## How do I get started?

<Render file="plan-get-started" />

---

# Bot Management skips

URL: https://developers.cloudflare.com/bots/reference/bot-management-skips/

import { GlossaryTooltip } from "~/components"

There are instances in which Bot Management does not run and certain fields, such as the [JA3/JA4 field](/bots/concepts/ja3-ja4-fingerprint/), are not populated because it has been determined that running Bot Management would not be necessary.

Refer to <GlossaryTooltip term="bot score" link="/bots/concepts/bot-score/#not-computed">bot scores</GlossaryTooltip> for more information about why a request is not scored.

## Common reasons for Bot Management to not score a request

### Requests to internal endpoints

Requests such as `/cdn-cgi/` are handled individually and will never receive a Bot Management score. Email Obfuscation, Web Analytics, Trace Requests, Challenge Pages, and JavaScript Detections do not receive bot scores. Refer to the table below for some examples of internal endpoints.

| Route                                                             |
| ----------------------------------------------------------------- |
| `/cdn-cgi/rum`                                                    |
| `/cdn-cgi/script_monitor/report`                                  |
| `/cdn-cgi/trace`                                                  |
| `/cdn-cgi/challenge-platform/â€¦`                                   |
| `/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js` |

### Purge requests

All HTTP purge requests will not receive a bot score.

### Early hints cache requests

Early hints cache requests will not receive a bot score.

---

# Bot Management variables

URL: https://developers.cloudflare.com/bots/reference/bot-management-variables/

import { Render } from "~/components"

## Ruleset Engine fields

<Render file="firewall-variables" />

## Workers variables

<Render file="workers-cf-request" />

## Corporate Proxy

<Render file="corporate_proxy" />

## Log fields

<Render file="bot-log-fields" />

---

# Reference

URL: https://developers.cloudflare.com/bots/reference/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Machine Learning models

URL: https://developers.cloudflare.com/bots/reference/machine-learning-models/

## Enable Auto-updates to the Machine Learning Models

Cloudflare allows Enterprise customers to enable Auto-updates to its Machine Learning models for the newest bot detection models as they are released.

To enable Auto-updates:

1. Log in to the Cloudflare dashboard and select your account and domain.
2. Go to **Security** > **Bots**.
3. Select **Configure Bot Management**.
4. Enable **Auto-updates to the Machine Learning Model**.

### What will change

If you are on an older Machine Learning model, you will see a score change to requests scored by the **Machine Learning** source instantly. If you are already on the latest model, you will see changes only after a new Machine Learning model becomes the global default.

Customers will be notified via email and dashboard prior to a new Machine Learning model becoming the global default.

### Risks of not updating

By not updating to the latest version, you will be using a Machine Learning model no longer maintained or monitored by our engineering team. As Internet traffic changes and new trends evolve, scoring accuracy by older versions may degrade.

### Model versions and release notes

| Version | Release Notes                                                                                                                                                                                                                               | Launch Date |
| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- |
| v1      | First Machine Learning Model released.                                                                                                                                                                                                      | Q1 2019     |
| v2      | Introduced dynamic inter-request features to leverage the Cloudflare network to detect new bots more accurately. <br/><br/>Feedback other Bot Management detection mechanisms to the machine learning model to more accurately detect bots. | Q1 2020     |
| v3      | Fixed accuracy issues under some conditions in the previous version.                                                                                                                                                                        | Q2 2020     |
| v4      | Improved scoring for iOS devices. <br/><br/>Fixed scoring inaccuracy in Firefox builds.                                                                                                                                                     | Q1 2021     |
| v5      | Recalibrated model for the [removal of `_cfduid` cookie](https://blog.cloudflare.com/deprecating-cfduid-cookie/). <br/><br/> Introduced new signals to reduce false negatives.                                                              | Q2 2021     |
| v6      | Significantly improved scoring for native Android application traffic. <br/><br/>Improved scoring on the newest versions of Chromium browsers.                                                                                              | Q1 2022     |
| v7      | Increased recognition of distributed botnets. <br/><br/>Improved HTTP/3 scoring.                                                                                                                                                            | Q1 2024     |
| v8      | Improved detection of residential proxies. <br/><br/>Increased weight on network level traffic characteristics.                                                                                                                             | Q2 2024     |

---

# JavaScript detections

URL: https://developers.cloudflare.com/bots/reference/javascript-detections/

import { Render } from "~/components"

<Render file="javascript-detections-definition" params={{ one: " " }} />

## Enable JavaScript detections

For Free customers (Bot Fight Mode), JavaScript detections are automatically enabled and cannot be disabled.

For all other customers (Super Bot Fight Mode and Bot Management for Enterprise), JavaScript detections are optional.

<Render file="javascript-detections-enable" />

For more details on how to set up bot protection, see [Get started](/bots/get-started/).

## Enforcing execution of JavaScript detections

<Render file="javascript-detections-implementation" />

<Render file="cf-clearance-cookie" />

## Limitations

### If you enabled Bot Management before June 2020

Customers who enabled Enterprise Bot Management before June 2020 do not have JavaScript detections enabled by default (unless specifically requested). These customers can still enable the feature in the Cloudflare dashboard.

### If you have a Content Security Policy (CSP)

<Render file="javascript-detections-csp" />

:::caution[Warning]

JavaScript detections are not supported with `nonce` set via `<meta>` tags. 
:::

### If you have ETags

Enabling JavaScript Detections (JSD) will strip [ETags](/cache/reference/etag-headers/) from HTML responses where JSD is injected.

---

# Static resource protection

URL: https://developers.cloudflare.com/bots/reference/static-resources/

import { GlossaryTooltip, Render } from "~/components"

Pro, Business, and Enterprise customers can use Cloudflare's bot solutions to protect their <GlossaryTooltip term="static content">static resources</GlossaryTooltip> from bots.

:::caution[Warning]

If you enable static resource protection, you may block good bots â€” like mail clients â€” that routinely fetch static resources. Make sure you understand your existing infrastructure before enabling this feature. 
:::

## Super Bot Fight Mode

To enable this feature as a Pro or Business customer or an Enterprise customer without Bot Management:

1. Go to **Security** > **Bots**.
2. Select **Configure Super Bot Fight Mode**.
3. For **Static resource protection**, select **On**.

:::caution

The **Static Resource Protection** setting will only activate if at least one of the bot categories (definite, likely, or verified) is _not_ set to `Allow`. If all categories are set to `Allow`, this setting will not have any impact since it works alongside these bot settings as part of the managed rules.
:::

## Bot Management for Enterprise

<Render file="static-resources-bm" /> <br/>

To exclude static resources, you would need to include `not (cf.bot_management.static_resource)` as part of your custom rule.

## Which files are protected?

<Render file="static-resources-list" />

---

# Super Bot Fight Mode for WordPress

URL: https://developers.cloudflare.com/bots/reference/wordpress-loopback-issue/

import { Render } from "~/components"

<Render file="wordpress-loopback-definition" /> <br/>

WordPress relies on making loopback requests to monitor and occasionally administer its websites. Customers can opt-in to optimize Super Bot Fight Mode for WordPress. If this feature is enabled, automated loopback requests made by your WordPress site will be authorized even when Super Bot Fight Mode blocks other bots.

:::note

Loopback requests may also be blocked by [Iâ€™m Under Attack mode](/fundamentals/reference/under-attack-mode/) or certain [WAF custom rules](/waf/custom-rules/). 
:::

## Enable Optimize for WordPress

1. Log in to the [Cloudflare dashboard](http://dash.cloudflare.com) and select your account and domain.
2. Go to **Security** > **Bots**.
3. Select **Configure Bot Management**.
4. Enable **Optimize for WordPress**.

## Availability

This feature is available for all Super Bot Fight Mode customers.

---

# Sample terms

URL: https://developers.cloudflare.com/bots/reference/sample-terms/

Cloudflare recommends that customers consider updating their Terms of Service to address bots specifically related to Artificial Intelligence (AI) training and data scraping. The text below provides an informational example of the kind of language that could be added to a website's terms of use.

> **Artificial Intelligence Restriction**
>
> You may not use automated bots to access, scan, scrape, data mine, copy, or use the materials or content on this website for developing, training, fine-tuning, or otherwise contributing to or improving a machine learning model or artificial intelligence (AI) system or the operation thereof, unless your bot's user agent is (I) explicitly permitted ("allowed") to do so in this website's `robots.txt` file and (II) solely used to identify bots used for AI purposes (i.e., this provision does not apply to user agents that are used for multiple purposes, such as search engine indexing and AI purposes).

:::caution[Disclaimer]

This language is provided for informational purposes only. It does not constitute legal advice, nor does it guarantee any specific outcome. 

This is an illustrative example of language that can be included in a website's terms to put AI providers on notice that they are not authorized to use automated means to scrape content from your website for purposes of training or otherwise contributing to their AI models or systems, unless you have expressly permitted them to do so in your `robots.txt` file. 
:::

---

# Use browser rendering with AI

URL: https://developers.cloudflare.com/browser-rendering/how-to/ai/

import { Aside, WranglerConfig } from "~/components";

The ability to browse websites can be crucial when building workflows with AI. Here, we provide an example where we use Browser Rendering to visit
`https://labs.apnic.net/` and then, using a machine learning model available in [Workers AI](/workers-ai/), extract the first post as JSON with a specified schema.

## Prerequisites

1. Use the `create-cloudflare` CLI to generate a new Hello World Cloudflare Worker script:

```sh
npm create cloudflare@latest -- browser-worker
```

2. Install `@cloudflare/puppeteer`, which allows you to control the Browser Rendering instance:

```sh
npm i @cloudflare/puppeteer
```

2. Install `zod` so we can define our output format and `zod-to-json-schema` so we can convert it into a JSON schema format:

```sh
npm i zod
npm i zod-to-json-schema
```

3. Activate the nodejs compatibility flag and add your Browser Rendering binding to your new Wrangler configuration:

<WranglerConfig>
```toml
compatibility_flags = [ "nodejs_compat" ]
```
</WranglerConfig>

<WranglerConfig>
```toml
[browser]
binding = "MY_BROWSER"
```
</WranglerConfig>

4.  In order to use [Workers AI](/workers-ai/), you need to get your [Account ID and API token](/workers-ai/get-started/rest-api/#1-get-api-token-and-account-id).
Once you have those, create a [`.dev.vars`](/workers/configuration/environment-variables/#add-environment-variables-via-wrangler) file and set them there:

```
ACCOUNT_ID=
API_TOKEN=
```

We use `.dev.vars` here since it's only for local development, otherwise you'd use [Secrets](/workers/configuration/secrets/).

## Load the page using Browser Rendering

In the code below, we launch a browser using `await puppeteer.launch(env.MY_BROWSER)`, extract the rendered text and close the browser.
Then, with the user prompt, the desired output schema and the rendered text, prepare a prompt to send to the LLM.

Replace the contents of `src/index.ts` with the following skeleton script:

```ts
import { z } from "zod";
import puppeteer from "@cloudflare/puppeteer";
import zodToJsonSchema from "zod-to-json-schema";

export default {
  async fetch(request, env) {
    const url = new URL(request.url);
    if (url.pathname != "/") {
      return new Response("Not found");
    }

    // Your prompt and site to scrape
    const userPrompt = "Extract the first post only.";
    const targetUrl = "https://labs.apnic.net/";

    // Launch browser
    const browser = await puppeteer.launch(env.MY_BROWSER);
    const page = await browser.newPage();
    await page.goto(targetUrl);

    // Get website text
    const renderedText = await page.evaluate(() => {
      // @ts-ignore js code to run in the browser context
      const body = document.querySelector("body");
      return body ? body.innerText : "";
    });
    // Close browser since we no longer need it
    await browser.close();

    // define your desired json schema
    const outputSchema = zodToJsonSchema(
      z.object({ title: z.string(), url: z.string(), date: z.string() })
    );

    // Example prompt
    const prompt = `
    You are a sophisticated web scraper. You are given the user data extraction goal and the JSON schema for the output data format.
    Your task is to extract the requested information from the text and output it in the specified JSON schema format:

        ${JSON.stringify(outputSchema)}

    DO NOT include anything else besides the JSON output, no markdown, no plaintext, just JSON.

    User Data Extraction Goal: ${userPrompt}

    Text extracted from the webpage: ${renderedText}`;

    // TODO call llm
    //const result = await getLLMResult(env, prompt, outputSchema);
    //return Response.json(result);
  }

} satisfies ExportedHandler<Env>;

```

## Call an LLM

Having the webpage text, the user's goal and output schema, we can now use an LLM to transform it to JSON according to the user's request.
The example below uses `@hf/thebloke/deepseek-coder-6.7b-instruct-awq` but other [models](/workers-ai/models/), or services like OpenAI, could be used with minimal changes:

```ts
async getLLMResult(env, prompt: string, schema?: any) {
    const model = "@hf/thebloke/deepseek-coder-6.7b-instruct-awq"
    const requestBody = {
        messages: [{
            role: "user",
            content: prompt
        }
        ],
    };
    const aiUrl = `https://api.cloudflare.com/client/v4/accounts/${env.ACCOUNT_ID}/ai/run/${model}`

    const response = await fetch(aiUrl, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${env.API_TOKEN}`,
      },
      body: JSON.stringify(requestBody),
    });
    if (!response.ok) {
      console.log(JSON.stringify(await response.text(), null, 2));
      throw new Error(`LLM call failed ${aiUrl} ${response.status}`);
    }

    // process response
    const data = await response.json();
    const text = data.result.response || '';
    const value = (text.match(/```(?:json)?\s*([\s\S]*?)\s*```/) || [null, text])[1];
    try {
      return JSON.parse(value);
    } catch(e) {
      console.error(`${e} . Response: ${value}`)
    }
  }
```

If you want to use Browser Rendering with OpenAI instead you'd just need to change the `aiUrl` endpoint and `requestBody` (or check out the [llm-scraper-worker](https://www.npmjs.com/package/llm-scraper-worker) package).

## Conclusion

The full Worker script now looks as follows:

```ts
import { z } from "zod";
import puppeteer from "@cloudflare/puppeteer";
import zodToJsonSchema from "zod-to-json-schema";

export default {
  async fetch(request, env) {
    const url = new URL(request.url);
    if (url.pathname != "/") {
      return new Response("Not found");
    }

    // Your prompt and site to scrape
    const userPrompt = "Extract the first post only.";
    const targetUrl = "https://labs.apnic.net/";

    // Launch browser
    const browser = await puppeteer.launch(env.MY_BROWSER);
    const page = await browser.newPage();
    await page.goto(targetUrl);

    // Get website text
    const renderedText = await page.evaluate(() => {
      // @ts-ignore js code to run in the browser context
      const body = document.querySelector("body");
      return body ? body.innerText : "";
    });
    // Close browser since we no longer need it
    await browser.close();

    // define your desired json schema
    const outputSchema = zodToJsonSchema(
      z.object({ title: z.string(), url: z.string(), date: z.string() })
    );

    // Example prompt
    const prompt = `
    You are a sophisticated web scraper. You are given the user data extraction goal and the JSON schema for the output data format.
    Your task is to extract the requested information from the text and output it in the specified JSON schema format:

        ${JSON.stringify(outputSchema)}

    DO NOT include anything else besides the JSON output, no markdown, no plaintext, just JSON.

    User Data Extraction Goal: ${userPrompt}

    Text extracted from the webpage: ${renderedText}`;

    // call llm
    const result = await getLLMResult(env, prompt, outputSchema);
    return Response.json(result);
  }

} satisfies ExportedHandler<Env>;


async function getLLMResult(env, prompt: string, schema?: any) {
    const model = "@hf/thebloke/deepseek-coder-6.7b-instruct-awq"
    const requestBody = {
        messages: [{
            role: "user",
            content: prompt
        }
        ],
    };
    const aiUrl = `https://api.cloudflare.com/client/v4/accounts/${env.ACCOUNT_ID}/ai/run/${model}`

    const response = await fetch(aiUrl, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${env.API_TOKEN}`,
      },
      body: JSON.stringify(requestBody),
    });
    if (!response.ok) {
      console.log(JSON.stringify(await response.text(), null, 2));
      throw new Error(`LLM call failed ${aiUrl} ${response.status}`);
    }

    // process response
    const data = await response.json() as { result: { response: string }};
    const text = data.result.response || '';
    const value = (text.match(/```(?:json)?\s*([\s\S]*?)\s*```/) || [null, text])[1];
    try {
      return JSON.parse(value);
    } catch(e) {
      console.error(`${e} . Response: ${value}`)
    }
}
```

You can run this script to test it using Wrangler's `--remote` flag:

```sh
npx wrangler dev --remote
```

With your script now running, you can go to `http://localhost:8787/` and should see something like the following:

```json
{
  "title": "IP Addresses in 2024",
  "url": "http://example.com/ip-addresses-in-2024",
  "date": "11 Jan 2025"
}
```

For more complex websites or prompts, you might need a better model. Check out the latest models in [Workers AI](/workers-ai/models/).

---

# How To

URL: https://developers.cloudflare.com/browser-rendering/how-to/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Generate PDFs Using HTML and CSS

URL: https://developers.cloudflare.com/browser-rendering/how-to/pdf-generation/

import { Aside, WranglerConfig } from "~/components";

As seen in the [Getting Started guide](/browser-rendering/workers-binding-api/screenshots/), Browser Rendering can be used to generate screenshots for any given URL. Alongside screenshots, you can also generate full PDF documents for a given webpage, and can also provide the webpage markup and style ourselves.

## Prerequisites

1. Use the `create-cloudflare` CLI to generate a new Hello World Cloudflare Worker script:

```sh
npm create cloudflare@latest -- browser-worker
```

2. Install `@cloudflare/puppeteer`, which allows you to control the Browser Rendering instance:

```sh
npm install @cloudflare/puppeteer --save-dev
```

3. Add your Browser Rendering binding to your new Wrangler configuration:

<WranglerConfig>

```toml title="wrangler.toml"
browser = { binding = "BROWSER" }
```
</WranglerConfig>

4. Replace the contents of `src/index.ts` (or `src/index.js` for JavaScript projects) with the following skeleton script:

```ts
import puppeteer from "@cloudflare/puppeteer";

const generateDocument = (name: string) => {};

export default {
	async fetch(request, env) {
		const { searchParams } = new URL(request.url);
		let name = searchParams.get("name");

		if (!name) {
			return new Response("Please provide a name using the ?name= parameter");
		}

		const browser = await puppeteer.launch(env.BROWSER);
		const page = await browser.newPage();

		// Step 1: Define HTML and CSS
		const document = generateDocument(name);

		// Step 2: Send HTML and CSS to our browser
		await page.setContent(document);

		// Step 3: Generate and return PDF

		return new Response();
	},
};
```

## 1. Define HTML and CSS

Rather than using Browser Rendering to navigate to a user-provided URL, manually generate a webpage, then provide that webpage to the Browser Rendering instance. This allows you to render any design you want.

:::note
You can generate your HTML or CSS using any method you like. This example uses string interpolation, but the method is also fully compatible with web frameworks capable of rendering HTML on Workers such as React, Remix, and Vue.
:::

For this example, we're going to take in user-provided content (via a '?name=' parameter), and have that name output in the final PDF document.

To start, fill out your `generateDocument` function with the following:

```ts
const generateDocument = (name: string) => {
	return `
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <style>
      html,
      body,
      #container {
        width: 100%;
        height: 100%;
        margin: 0;
      }
      body {
        font-family: Baskerville, Georgia, Times, serif;
        background-color: #f7f1dc;
      }
      strong {
        color: #5c594f;
        font-size: 128px;
        margin: 32px 0 48px 0;
      }
      em {
        font-size: 24px;
      }
      #container {
        flex-direction: column;
        display: flex;
        align-items: center;
        justify-content: center;
        text-align: center;
      }
    </style>
  </head>

  <body>
    <div id="container">
      <em>This is to certify that</em>
      <strong>${name}</strong>
      <em>has rendered a PDF using Cloudflare Workers</em>
    </div>
  </body>
</html>
`;
};
```

This example HTML document should render a beige background imitating a certificate showing that the user-provided name has successfully rendered a PDF using Cloudflare Workers.

:::note
It is usually best to avoid directly interpolating user-provided content into an image or PDF renderer in production applications. To render contents like an invoice, it would be best to validate the data input and fetch the data yourself using tools like [D1](/d1/) or [Workers KV](/kv/).
:::

## 2. Load HTML and CSS Into Browser

Now that you have your fully styled HTML document, you can take the contents and send it to your browser instance. Create an empty page to store this document as follows:

```ts
const browser = await puppeteer.launch(env.BROWSER);
const page = await browser.newPage();
```

The [`page.setContent()`](https://github.com/cloudflare/puppeteer/blob/main/docs/api/puppeteer.page.setcontent.md) function can then be used to set the page's HTML contents from a string, so you can pass in your created document directly like so:

```ts
await page.setContent(document);
```

## 3. Generate and Return PDF

With your Browser Rendering instance now rendering your provided HTML and CSS, you can use the [`page.pdf()`](https://github.com/cloudflare/puppeteer/blob/main/docs/api/puppeteer.page.pdf.md) command to generate a PDF file and return it to the client.

```ts
let pdf = page.pdf({ printBackground: true });
```

The `page.pdf()` call supports a [number of options](https://github.com/cloudflare/puppeteer/blob/main/docs/api/puppeteer.pdfoptions.md), including setting the dimensions of the generated PDF to a specific paper size, setting specific margins, and allowing fully-transparent backgrounds. For now, you are only overriding the `printBackground` option to allow your `body` background styles to show up.

Now that you have your PDF data, return it to the client in the `Response` with an `application/pdf` content type:

```ts
return new Response(pdf, {
	headers: {
		"content-type": "application/pdf",
	},
});
```

## Conclusion

The full Worker script now looks as follows:

```ts
import puppeteer from "@cloudflare/puppeteer";

const generateDocument = (name: string) => {
	return `
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <style>
	  html, body, #container {
		width: 100%;
	    height: 100%;
		margin: 0;
	  }
      body {
        font-family: Baskerville, Georgia, Times, serif;
        background-color: #f7f1dc;
      }
      strong {
        color: #5c594f;
		font-size: 128px;
		margin: 32px 0 48px 0;
      }
	  em {
		font-size: 24px;
	  }
      #container {
		flex-direction: column;
        display: flex;
        align-items: center;
        justify-content: center;
		text-align: center
      }
    </style>
  </head>

  <body>
    <div id="container">
		<em>This is to certify that</em>
		<strong>${name}</strong>
		<em>has rendered a PDF using Cloudflare Workers</em>
	</div>
  </body>
</html>
`;
};

export default {
	async fetch(request, env) {
		const { searchParams } = new URL(request.url);
		let name = searchParams.get("name");

		if (!name) {
			return new Response("Please provide a name using the ?name= parameter");
		}

		const browser = await puppeteer.launch(env.BROWSER);
		const page = await browser.newPage();

		// Step 1: Define HTML and CSS
		const document = generateDocument(name);

		// // Step 2: Send HTML and CSS to our browser
		await page.setContent(document);

		// // Step 3: Generate and return PDF
		const pdf = await page.pdf({ printBackground: true });

		return new Response(pdf, {
			headers: {
				"content-type": "application/pdf",
			},
		});
	},
};
```

You can run this script to test it using Wranglerâ€™s `--remote` flag:

```sh
npx wrangler@latest dev --remote
```

With your script now running, you can pass in a `?name` parameter to the local URL (such as `http://localhost:8787/?name=Harley`) and should see the following:

![A screenshot of a generated PDF, with the author's name shown in a mock certificate.](~/assets/images/browser-rendering/pdf-generation.png).

---

Dynamically generating PDF documents solves a number of common use-cases, from invoicing customers to archiving documents to creating dynamic certificates (as seen in the simple example here).

---

# Browser close reasons

URL: https://developers.cloudflare.com/browser-rendering/platform/browser-close-reasons/

A browser session may close for a variety of reasons, occasionally due to connection errors or errors in the headless browser instance. As a best practice, wrap `puppeteer.connect` or `puppeteer.launch` in a [`try/catch`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/try...catch) statement.

The reason that a browser closed can be found on the Browser Rendering Dashboard in the [logs tab](https://dash.cloudflare.com/?to=/:account/workers/browser-renderingl/logs). When Cloudflare begins charging for the Browser Rendering API, we will not charge when errors are due to underlying Browser Rendering infrastructure.

| Reasons a session may end                            |
| ---------------------------------------------------- |
| User opens and closes browser normally.              |
| Browser is idle for 60 seconds.                      |
| Chromium instance crashes.                           |
| Error connecting with the client, server, or Worker. |
| Browser session is evicted.                          |

---

# Limits

URL: https://developers.cloudflare.com/browser-rendering/platform/limits/

import { Render, Plan } from "~/components";

<Plan type="workers-paid" />

## Workers Binding API

| Feature                          | Limit               |
| -------------------------------- | ------------------- |
| Concurrent browsers per account  | 10 per account [^1] |
| New browser instances per minute | 10 per minute [^1]  |
| Browser timeout                  | 60 seconds [^1][^2] |

## REST API

| Feature                          | Limit               |
| -------------------------------- | ------------------- |
| Concurrent browsers per account  | 10 per account [^1] |
| New browser instances per minute | 10 per minute [^1]  |
| Browser timeout                  | 60 seconds [^1][^2] |
| Total requests per minute        | 60 per minute [^1]  |

[^1]: Contact our team to request increases to this limit.

[^2]: By default, a browser instance gets killed if it does not get any [devtools](https://chromedevtools.github.io/devtools-protocol/) command for 60 seconds, freeing one instance. Users can optionally increase this by using the `keep_alive` [option](/browser-rendering/platform/puppeteer/#keep-alive). `browser.close()` releases the browser instance.

<Render file="limits-increase" product="browser-rendering" />

---

# Puppeteer

URL: https://developers.cloudflare.com/browser-rendering/platform/puppeteer/

import { TabItem, Tabs } from "~/components";

[Puppeteer](https://pptr.dev/) is one of the most popular libraries that abstract the lower-level DevTools protocol from developers and provides a high-level API that you can use to easily instrument Chrome/Chromium and automate browsing sessions. Puppeteer is used for tasks like creating screenshots, crawling pages, and testing web applications.

Puppeteer typically connects to a local Chrome or Chromium browser using the DevTools port. Refer to the [Puppeteer API documentation on the `Puppeteer.connect()` method](https://pptr.dev/api/puppeteer.puppeteer.connect) for more information.

The Workers team forked a version of Puppeteer and patched it to connect to the Workers Browser Rendering API instead. After connecting, the developers can then use the full [Puppeteer API](https://github.com/cloudflare/puppeteer/blob/main/docs/api/index.md) as they would on a standard setup.

Our version is open sourced and can be found in [Cloudflare's fork of Puppeteer](https://github.com/cloudflare/puppeteer). The npm can be installed from [npmjs](https://www.npmjs.com/) as [@cloudflare/puppeteer](https://www.npmjs.com/package/@cloudflare/puppeteer):

```bash
npm install @cloudflare/puppeteer --save-dev
```

## Use Puppeteer in a Worker

Once the [browser binding](/browser-rendering/platform/wrangler/#bindings) is configured and the `@cloudflare/puppeteer` library is installed, Puppeteer can be used in a Worker:

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import puppeteer from "@cloudflare/puppeteer";

export default {
	async fetch(request, env) {
		const browser = await puppeteer.launch(env.MYBROWSER);
		const page = await browser.newPage();
		await page.goto("https://example.com");
		const metrics = await page.metrics();
		await browser.close();
		return Response.json(metrics);
	},
};
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import puppeteer from "@cloudflare/puppeteer";

interface Env {
	MYBROWSER: Fetcher;
}

export default {
	async fetch(request, env): Promise<Response> {
		const browser = await puppeteer.launch(env.MYBROWSER);
		const page = await browser.newPage();
		await page.goto("https://example.com");
		const metrics = await page.metrics();
		await browser.close();
		return Response.json(metrics);
	},
} satisfies ExportedHandler<Env>;
```

</TabItem> </Tabs>

This script [launches](https://pptr.dev/api/puppeteer.puppeteernode.launch) the `env.MYBROWSER` browser, opens a [new page](https://pptr.dev/api/puppeteer.browser.newpage), [goes to](https://pptr.dev/api/puppeteer.page.goto) [https://example.com/](https://example.com/), gets the page load [metrics](https://pptr.dev/api/puppeteer.page.metrics), [closes](https://pptr.dev/api/puppeteer.browser.close) the browser and prints metrics in JSON.

### Keep Alive

If users omit the `browser.close()` statement, it will stay open, ready to be connected to again and [re-used](/browser-rendering/workers-binding-api/reuse-sessions/) but it will, by default, close automatically after 1 minute of inactivity. Users can optionally extend this idle time up to 10 minutes, by using the `keep_alive` option, set in milliseconds:

```js
const browser = await puppeteer.launch(env.MYBROWSER, { keep_alive: 600000 });
```

Using the above, the browser will stay open for up to 10 minutes, even if inactive.

## Session management

In order to facilitate browser session management, we've added new methods to `puppeteer`:

### List open sessions

`puppeteer.sessions()` lists the current running sessions. It will return an output similar to this:

```json
[
	{
		"connectionId": "2a2246fa-e234-4dc1-8433-87e6cee80145",
		"connectionStartTime": 1711621704607,
		"sessionId": "478f4d7d-e943-40f6-a414-837d3736a1dc",
		"startTime": 1711621703708
	},
	{
		"sessionId": "565e05fb-4d2a-402b-869b-5b65b1381db7",
		"startTime": 1711621703808
	}
]
```

Notice that the session `478f4d7d-e943-40f6-a414-837d3736a1dc` has an active worker connection (`connectionId=2a2246fa-e234-4dc1-8433-87e6cee80145`), while session `565e05fb-4d2a-402b-869b-5b65b1381db7` is free. While a connection is active, no other workers may connect to that session.

### List recent sessions

`puppeteer.history()` lists recent sessions, both open and closed. It's useful to get a sense of your current usage.

```json
[
	{
		"closeReason": 2,
		"closeReasonText": "BrowserIdle",
		"endTime": 1711621769485,
		"sessionId": "478f4d7d-e943-40f6-a414-837d3736a1dc",
		"startTime": 1711621703708
	},
	{
		"closeReason": 1,
		"closeReasonText": "NormalClosure",
		"endTime": 1711123501771,
		"sessionId": "2be00a21-9fb6-4bb2-9861-8cd48e40e771",
		"startTime": 1711123430918
	}
]
```

Session `2be00a21-9fb6-4bb2-9861-8cd48e40e771` was closed explicitly with `browser.close()` by the client, while session `478f4d7d-e943-40f6-a414-837d3736a1dc` was closed due to reaching the maximum idle time (check [limits](/browser-rendering/platform/limits/)).

You should also be able to access this information in the dashboard, albeit with a slight delay.

### Active limits

`puppeteer.limits()` lists your active limits:

```json
{
	"activeSessions": [
		"478f4d7d-e943-40f6-a414-837d3736a1dc",
		"565e05fb-4d2a-402b-869b-5b65b1381db7"
	],
	"allowedBrowserAcquisitions": 1,
	"maxConcurrentSessions": 2,
	"timeUntilNextAllowedBrowserAcquisition": 0
}
```

- `activeSessions` lists the IDs of the current open sessions
- `maxConcurrentSessions` defines how many browsers can be open at the same time
- `allowedBrowserAcquisitions` specifies if a new browser session can be opened according to the rate [limits](/browser-rendering/platform/limits/) in place
- `timeUntilNextAllowedBrowserAcquisition` defines the waiting period before a new browser can be launched.

## Puppeteer API

The full Puppeteer API can be found in the [Cloudflare's fork of Puppeteer](https://github.com/cloudflare/puppeteer/blob/main/docs/api/index.md).

---

# Platform

URL: https://developers.cloudflare.com/browser-rendering/platform/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Wrangler

URL: https://developers.cloudflare.com/browser-rendering/platform/wrangler/

import { Render, WranglerConfig } from "~/components"

[Wrangler](/workers/wrangler/) is a command-line tool for building with Cloudflare developer products.

Use Wrangler to deploy projects that use the Workers Browser Rendering API.

## Install

To install Wrangler, refer to [Install and Update Wrangler](/workers/wrangler/install-and-update/).

## Bindings

[Bindings](/workers/runtime-apis/bindings/) allow your Workers to interact with resources on the Cloudflare developer platform. A browser binding will provide your Worker with an authenticated endpoint to interact with a dedicated Chromium browser instance.

To deploy a Browser Rendering Worker, you must declare a [browser binding](/workers/runtime-apis/bindings/) in your Worker's Wrangler configuration file.

<Render file="nodejs-compat-howto" product="workers" />

<WranglerConfig>

```toml
# Top-level configuration
name = "browser-rendering"
main = "src/index.ts"
workers_dev = true
compatibility_flags = ["nodejs_compat_v2"]

browser = { binding = "MYBROWSER" }
```

</WranglerConfig>

After the binding is declared, access the DevTools endpoint using `env.MYBROWSER` in your Worker code:

```javascript
const browser = await puppeteer.launch(env.MYBROWSER);
```

Run [`npx wrangler dev --remote`](/workers/wrangler/commands/#dev) to test your Worker remotely before deploying to Cloudflare's global network. Local mode support does not exist for Browser Rendering so `--remote` is required. To deploy, run [`npx wrangler deploy`](/workers/wrangler/commands/#deploy).

---

# Deploy a Browser Rendering Worker with Durable Objects

URL: https://developers.cloudflare.com/browser-rendering/workers-binding-api/browser-rendering-with-do/

import { Render, PackageManagers, WranglerConfig } from "~/components";

By following this guide, you will create a Worker that uses the Browser Rendering API along with [Durable Objects](/durable-objects/) to take screenshots from web pages and store them in [R2](/r2/).

Using Durable Objects to persist browser sessions improves performance by eliminating the time that it takes to spin up a new browser session. Since Durable Objects re-uses sessions, it reduces the number of concurrent sessions needed.

<Render file="prereqs" product="workers" />

## 1. Create a Worker project

[Cloudflare Workers](/workers/) provides a serverless execution environment that allows you to create new applications or augment existing ones without configuring or maintaining infrastructure. Your Worker application is a container to interact with a headless browser to do actions, such as taking screenshots.

Create a new Worker project named `browser-worker` by running:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"browser-worker"}
/>

## 2. Enable Durable Objects in the dashboard

To enable Durable Objects, you will need to purchase the Workers Paid plan:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. Go to **Workers & Pages** > **Plans**.
3. Select **Purchase Workers Paid** and complete the payment process to enable Durable Objects.

## 3. Install Puppeteer

In your `browser-worker` directory, install Cloudflareâ€™s [fork of Puppeteer](/browser-rendering/platform/puppeteer/):

```sh
npm install @cloudflare/puppeteer --save-dev
```

## 4. Create a R2 bucket

Create two R2 buckets, one for production, and one for development.

Note that bucket names must be lowercase and can only contain dashes.

```sh
wrangler r2 bucket create screenshots
wrangler r2 bucket create screenshots-test
```

To check that your buckets were created, run:

```sh
wrangler r2 bucket list
```

After running the `list` command, you will see all bucket names, including the ones you have just created.

## 5. Configure your Wrangler configuration file

Configure your `browser-worker` project's [Wrangler configuration file](/workers/wrangler/configuration/) by adding a browser [binding](/workers/runtime-apis/bindings/) and a [Node.js compatibility flag](/workers/configuration/compatibility-flags/#nodejs-compatibility-flag). Browser bindings allow for communication between a Worker and a headless browser which allows you to do actions such as taking a screenshot, generating a PDF and more.

Update your Wrangler configuration file with the Browser Rendering API binding, the R2 bucket you created and a Durable Object:

<WranglerConfig>

```toml
name = "rendering-api-demo"
main = "src/index.js"
compatibility_date = "2023-09-04"
compatibility_flags = [ "nodejs_compat"]
account_id = "<ACCOUNT_ID>"


# Browser Rendering API binding
browser = { binding = "MYBROWSER" }

# Bind an R2 Bucket
[[r2_buckets]]
binding = "BUCKET"
bucket_name = "screenshots"
preview_bucket_name = "screenshots-test"

# Binding to a Durable Object
[[durable_objects.bindings]]
name = "BROWSER"
class_name = "Browser"

[[migrations]]
tag = "v1" # Should be unique for each entry
new_classes = ["Browser"] # Array of new classes

```

</WranglerConfig>

## 6. Code

The code below uses Durable Object to instantiate a browser using Puppeteer. It then opens a series of web pages with different resolutions, takes a screenshot of each, and uploads it to R2.

The Durable Object keeps a browser session open for 60 seconds after last use. If a browser session is open, any requests will re-use the existing session rather than creating a new one. Update your Worker code by copy and pasting the following:

```js
import puppeteer from "@cloudflare/puppeteer";

export default {
	async fetch(request, env) {
		let id = env.BROWSER.idFromName("browser");
		let obj = env.BROWSER.get(id);

		// Send a request to the Durable Object, then await its response.
		let resp = await obj.fetch(request.url);

		return resp;
	},
};

const KEEP_BROWSER_ALIVE_IN_SECONDS = 60;

export class Browser {
	constructor(state, env) {
		this.state = state;
		this.env = env;
		this.keptAliveInSeconds = 0;
		this.storage = this.state.storage;
	}

	async fetch(request) {
		// screen resolutions to test out
		const width = [1920, 1366, 1536, 360, 414];
		const height = [1080, 768, 864, 640, 896];

		// use the current date and time to create a folder structure for R2
		const nowDate = new Date();
		var coeff = 1000 * 60 * 5;
		var roundedDate = new Date(
			Math.round(nowDate.getTime() / coeff) * coeff,
		).toString();
		var folder = roundedDate.split(" GMT")[0];

		//if there's a browser session open, re-use it
		if (!this.browser || !this.browser.isConnected()) {
			console.log(`Browser DO: Starting new instance`);
			try {
				this.browser = await puppeteer.launch(this.env.MYBROWSER);
			} catch (e) {
				console.log(
					`Browser DO: Could not start browser instance. Error: ${e}`,
				);
			}
		}

		// Reset keptAlive after each call to the DO
		this.keptAliveInSeconds = 0;

		const page = await this.browser.newPage();

		// take screenshots of each screen size
		for (let i = 0; i < width.length; i++) {
			await page.setViewport({ width: width[i], height: height[i] });
			await page.goto("https://workers.cloudflare.com/");
			const fileName = "screenshot_" + width[i] + "x" + height[i];
			const sc = await page.screenshot({ path: fileName + ".jpg" });

			await this.env.BUCKET.put(folder + "/" + fileName + ".jpg", sc);
		}

		// Close tab when there is no more work to be done on the page
		await page.close();

		// Reset keptAlive after performing tasks to the DO.
		this.keptAliveInSeconds = 0;

		// set the first alarm to keep DO alive
		let currentAlarm = await this.storage.getAlarm();
		if (currentAlarm == null) {
			console.log(`Browser DO: setting alarm`);
			const TEN_SECONDS = 10 * 1000;
			await this.storage.setAlarm(Date.now() + TEN_SECONDS);
		}

		return new Response("success");
	}

	async alarm() {
		this.keptAliveInSeconds += 10;

		// Extend browser DO life
		if (this.keptAliveInSeconds < KEEP_BROWSER_ALIVE_IN_SECONDS) {
			console.log(
				`Browser DO: has been kept alive for ${this.keptAliveInSeconds} seconds. Extending lifespan.`,
			);
			await this.storage.setAlarm(Date.now() + 10 * 1000);
			// You could ensure the ws connection is kept alive by requesting something
			// or just let it close automatically when there  is no work to be done
			// for example, `await this.browser.version()`
		} else {
			console.log(
				`Browser DO: exceeded life of ${KEEP_BROWSER_ALIVE_IN_SECONDS}s.`,
			);
			if (this.browser) {
				console.log(`Closing browser.`);
				await this.browser.close();
			}
		}
	}
}
```

## 7. Test

Run [`npx wrangler dev --remote`](/workers/wrangler/commands/#dev) to test your Worker remotely before deploying to Cloudflare's global network. Local mode support does not exist for Browser Rendering so `--remote` is required.

## 8. Deploy

Run [`npx wrangler deploy`](/workers/wrangler/commands/#deploy) to deploy your Worker to the Cloudflare global network.

## Related resources

- Other [Puppeteer examples](https://github.com/cloudflare/puppeteer/tree/main/examples)
- Get started with [Durable Objects](/durable-objects/get-started/)
- [Using R2 from Workers](/r2/api/workers/workers-api-usage/)

---

# Workers Binding API

URL: https://developers.cloudflare.com/browser-rendering/workers-binding-api/

import { DirectoryListing } from "~/components";

The Workers Binding API allows you to execute advanced browser rendering scripts within Cloudflare Workers. It provides developers the flexibility to automate and control complex workflows and browser interactions. The following options are available for browser rendering tasks:

<DirectoryListing />

Use the Workers Binding API when you need advanced browser automation, custom workflows, or complex interactions beyond basic rendering. For quick, one-off tasks like capturing screenshots or extracting HTML, the [REST API](/browser-rendering/rest-api/) is the simpler choice.

---

# Reuse sessions

URL: https://developers.cloudflare.com/browser-rendering/workers-binding-api/reuse-sessions/

import { Render, PackageManagers, WranglerConfig } from "~/components";

The best way to improve the performance of your browser rendering Worker is to reuse sessions. One way to do that is via [Durable Objects](/browser-rendering/workers-binding-api/browser-rendering-with-do/), which allows you to keep a long running connection from a Worker to a browser. Another way is to keep the browser open after you've finished with it, and connect to that session each time you have a new request.

In short, this entails using `browser.disconnect()` instead of `browser.close()`, and, if there are available sessions, using `puppeteer.connect(env.MY_BROWSER, sessionID)` instead of launching a new browser session.

## 1. Create a Worker project

[Cloudflare Workers](/workers/) provides a serverless execution environment that allows you to create new applications or augment existing ones without configuring or maintaining infrastructure. Your Worker application is a container to interact with a headless browser to do actions, such as taking screenshots.

Create a new Worker project named `browser-worker` by running:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"browser-worker"}
/>

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "hello-world",
		type: "Hello World Worker",
		lang: "TypeScript",
	}}
/>

## 2. Install Puppeteer

In your `browser-worker` directory, install Cloudflare's [fork of Puppeteer](/browser-rendering/platform/puppeteer/):

```sh
npm install @cloudflare/puppeteer --save-dev
```

## 3. Configure the [Wrangler configuration file](/workers/wrangler/configuration/)

<WranglerConfig>
```toml
name = "browser-worker"
main = "src/index.ts"
compatibility_date = "2023-03-14"
compatibility_flags = [ "nodejs_compat" ]

browser = { binding = "MYBROWSER" }

```
</WranglerConfig>

## 4. Code

The script below starts by fetching the current running sessions. If there are any that don't already have a worker connection, it picks a random session ID and attempts to connect (`puppeteer.connect(..)`) to it. If that fails or there were no running sessions to start with, it launches a new browser session (`puppeteer.launch(..)`). Then, it goes to the website and fetches the dom. Once that's done, it disconnects (`browser.disconnect()`), making the connection available to other workers.

Take into account that if the browser is idle, i.e. does not get any command, for more than the current [limit](/browser-rendering/platform/limits/), it will close automatically, so you must have enough requests per minute to keep it alive.

```ts
import puppeteer from "@cloudflare/puppeteer";

interface Env {
	MYBROWSER: Fetcher;
}

export default {
	async fetch(request: Request, env: Env): Promise<Response> {
		const url = new URL(request.url);
		let reqUrl = url.searchParams.get("url") || "https://example.com";
		reqUrl = new URL(reqUrl).toString(); // normalize

		// Pick random session from open sessions
		let sessionId = await this.getRandomSession(env.MYBROWSER);
		let browser, launched;
		if (sessionId) {
			try {
				browser = await puppeteer.connect(env.MYBROWSER, sessionId);
			} catch (e) {
				// another worker may have connected first
				console.log(`Failed to connect to ${sessionId}. Error ${e}`);
			}
		}
		if (!browser) {
			// No open sessions, launch new session
			browser = await puppeteer.launch(env.MYBROWSER);
			launched = true;
		}

		sessionId = browser.sessionId(); // get current session id

		// Do your work here
		const page = await browser.newPage();
		const response = await page.goto(reqUrl);
		const html = await response!.text();

		// All work done, so free connection (IMPORTANT!)
		await browser.disconnect();

		return new Response(
			`${launched ? "Launched" : "Connected to"} ${sessionId} \n-----\n` + html,
			{
				headers: {
					"content-type": "text/plain",
				},
			},
		);
	},

	// Pick random free session
	// Other custom logic could be used instead
	async getRandomSession(endpoint: puppeteer.BrowserWorker): Promise<string> {
		const sessions: puppeteer.ActiveSession[] =
			await puppeteer.sessions(endpoint);
		console.log(`Sessions: ${JSON.stringify(sessions)}`);
		const sessionsIds = sessions
			.filter((v) => {
				return !v.connectionId; // remove sessions with workers connected to them
			})
			.map((v) => {
				return v.sessionId;
			});
		if (sessionsIds.length === 0) {
			return;
		}

		const sessionId =
			sessionsIds[Math.floor(Math.random() * sessionsIds.length)];

		return sessionId!;
	},
};
```

Besides `puppeteer.sessions()`, we've added other methods to facilitate [Session Management](/browser-rendering/platform/puppeteer/#session-management).

## 5. Test

Run [`npx wrangler dev --remote`](/workers/wrangler/commands/#dev) to test your Worker remotely before deploying to Cloudflare's global network. Local mode support does not exist for Browser Rendering so `--remote` is required.

To test go to the following URL:

`<LOCAL_HOST_URL>/?url=https://example.com`

## 6. Deploy

Run `npx wrangler deploy` to deploy your Worker to the Cloudflare global network and then to go to the following URL:

`<YOUR_WORKER>.<YOUR_SUBDOMAIN>.workers.dev/?url=https://example.com`

---

# Deploy a Browser Rendering Worker

URL: https://developers.cloudflare.com/browser-rendering/workers-binding-api/screenshots/

import { Render, TabItem, Tabs, PackageManagers, WranglerConfig } from "~/components";

By following this guide, you will create a Worker that uses the Browser Rendering API to take screenshots from web pages. This is a common use case for browser automation.

<Render file="prereqs" product="workers" />

## 1. Create a Worker project

[Cloudflare Workers](/workers/) provides a serverless execution environment that allows you to create new applications or augment existing ones without configuring or maintaining infrastructure. Your Worker application is a container to interact with a headless browser to do actions, such as taking screenshots.

Create a new Worker project named `browser-worker` by running:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"browser-worker"}
/>

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "hello-world",
		type: "Hello World Worker",
		lang: "JavaScript / TypeScript",
	}}
/>

## 2. Install Puppeteer

In your `browser-worker` directory, install Cloudflareâ€™s [fork of Puppeteer](/browser-rendering/platform/puppeteer/):

```sh
npm install @cloudflare/puppeteer --save-dev
```

## 3. Create a KV namespace

Browser Rendering can be used with other developer products. You might need a [relational database](/d1/), an [R2 bucket](/r2/) to archive your crawled pages and assets, a [Durable Object](/durable-objects/) to keep your browser instance alive and share it with multiple requests, or [Queues](/queues/) to handle your jobs asynchronous.

For the purpose of this guide, you are going to use a [KV store](/kv/concepts/kv-namespaces/) to cache your screenshots.

Create two namespaces, one for production, and one for development.

```sh
npx wrangler kv namespace create BROWSER_KV_DEMO
npx wrangler kv namespace create BROWSER_KV_DEMO --preview
```

Take note of the IDs for the next step.

## 4. Configure the Wrangler configuration file

Configure your `browser-worker` project's [Wrangler configuration file](/workers/wrangler/configuration/) by adding a browser [binding](/workers/runtime-apis/bindings/) and a [Node.js compatibility flag](/workers/configuration/compatibility-flags/#nodejs-compatibility-flag). Bindings allow your Workers to interact with resources on the Cloudflare developer platform. Your browser `binding` name is set by you, this guide uses the name `MYBROWSER`. Browser bindings allow for communication between a Worker and a headless browser which allows you to do actions such as taking a screenshot, generating a PDF and more.

Update your [Wrangler configuration file](/workers/wrangler/configuration/) with the Browser Rendering API binding and the KV namespaces you created:

<WranglerConfig>

```toml title="wrangler.toml"
name = "browser-worker"
main = "src/index.js"
compatibility_date = "2023-03-14"
compatibility_flags = [ "nodejs_compat" ]

browser = { binding = "MYBROWSER" }
kv_namespaces = [
  { binding = "BROWSER_KV_DEMO", id = "22cf855786094a88a6906f8edac425cd", preview_id = "e1f8b68b68d24381b57071445f96e623" }
]
```

</WranglerConfig>

## 5. Code

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">
Update `src/index.js` with your Worker code:

```js
import puppeteer from "@cloudflare/puppeteer";

export default {
	async fetch(request, env) {
		const { searchParams } = new URL(request.url);
		let url = searchParams.get("url");
		let img;
		if (url) {
			url = new URL(url).toString(); // normalize
			img = await env.BROWSER_KV_DEMO.get(url, { type: "arrayBuffer" });
			if (img === null) {
				const browser = await puppeteer.launch(env.MYBROWSER);
				const page = await browser.newPage();
				await page.goto(url);
				img = await page.screenshot();
				await env.BROWSER_KV_DEMO.put(url, img, {
					expirationTtl: 60 * 60 * 24,
				});
				await browser.close();
			}
			return new Response(img, {
				headers: {
					"content-type": "image/jpeg",
				},
			});
		} else {
			return new Response("Please add an ?url=https://example.com/ parameter");
		}
	},
};
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">
Update `src/index.ts` with your Worker code:

```ts
import puppeteer from "@cloudflare/puppeteer";

interface Env {
	MYBROWSER: Fetcher;
	BROWSER_KV_DEMO: KVNamespace;
}

export default {
	async fetch(request, env): Promise<Response> {
		const { searchParams } = new URL(request.url);
		let url = searchParams.get("url");
		let img: Buffer;
		if (url) {
			url = new URL(url).toString(); // normalize
			img = await env.BROWSER_KV_DEMO.get(url, { type: "arrayBuffer" });
			if (img === null) {
				const browser = await puppeteer.launch(env.MYBROWSER);
				const page = await browser.newPage();
				await page.goto(url);
				img = (await page.screenshot()) as Buffer;
				await env.BROWSER_KV_DEMO.put(url, img, {
					expirationTtl: 60 * 60 * 24,
				});
				await browser.close();
			}
			return new Response(img, {
				headers: {
					"content-type": "image/jpeg",
				},
			});
		} else {
			return new Response("Please add an ?url=https://example.com/ parameter");
		}
	},
} satisfies ExportedHandler<Env>;
```

</TabItem> </Tabs>

This Worker instantiates a browser using Puppeteer, opens a new page, navigates to what you put in the `"url"` parameter, takes a screenshot of the page, stores the screenshot in KV, closes the browser, and responds with the JPEG image of the screenshot.

If your Worker is running in production, it will store the screenshot to the production KV namespace. If you are running `wrangler dev`, it will store the screenshot to the dev KV namespace.

If the same `"url"` is requested again, it will use the cached version in KV instead, unless it expired.

## 6. Test

Run [`npx wrangler dev --remote`](/workers/wrangler/commands/#dev) to test your Worker remotely before deploying to Cloudflare's global network. Local mode support does not exist for Browser Rendering so `--remote` is required.

To test taking your first screenshot, go to the following URL:

`<LOCAL_HOST_URL>/?url=https://example.com`

## 7. Deploy

Run `npx wrangler deploy` to deploy your Worker to the Cloudflare global network.

To take your first screenshot, go to the following URL:

`<YOUR_WORKER>.<YOUR_SUBDOMAIN>.workers.dev/?url=https://example.com`

## Related resources

- Other [Puppeteer examples](https://github.com/cloudflare/puppeteer/tree/main/examples)

---

# Fetch HTML

URL: https://developers.cloudflare.com/browser-rendering/rest-api/content-endpoint/

The `/content` endpoint instructs the browser to navigate to a website and capture the fully rendered HTML of a page, including the `head` section, after JavaScript execution. This is ideal for capturing content from JavaScript-heavy or interactive websites.

## Basic usage

Go to `https://example.com` and return the rendered HTML.

```bash
curl -X 'POST' 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/content' \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer <apiToken>' \
  -d '{"url": "https://example.com"}'
```

## Advanced usage

Navigate to `https://cloudflare.com/` but block images and stylesheets from loading. Undesired requests can be blocked by resource type (`rejectResourceTypes`) or by using a regex pattern (`rejectRequestPattern`). The opposite can also be done, only allow requests that match `allowRequestPattern` or `allowResourceTypes`.

```bash
curl -X POST 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/content' \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
      "url": "https://cloudflare.com/",
      "rejectResourceTypes": ["image"],
      "rejectRequestPattern": ["/^.*\\.(css)"]
		}'

```

Many more options exist, like setting HTTP headers using `setExtraHTTPHeaders`, setting `cookies`, and using `gotoOptions` to control page load behaviour - check the endpoint [reference](/api/resources/browser_rendering/subresources/content/methods/create/) for all available parameters.

---

# REST API

URL: https://developers.cloudflare.com/browser-rendering/rest-api/

The REST API is a RESTful interface that provides endpoints for common browser actions such as capturing screenshots, extracting HTML content, generating PDFs, and more.
The following are the available options:

import { DirectoryListing } from "~/components";

<DirectoryListing />

Use the REST API when you need a fast, simple way to perform common browser tasks such as capturing screenshots, extracting HTML, or generating PDFs without writing complex scripts. If you require more advanced automation, custom workflows, or persistent browser sessions, the [Workers Binding API](/browser-rendering/workers-binding-api/) is the better choice.

## Before you begin

Before you begin, make sure you [create a custom API Token](/fundamentals/api/get-started/create-token/) with the following permissions:

- `Browser Rendering - Edit`

---

# Render PDF

URL: https://developers.cloudflare.com/browser-rendering/rest-api/pdf-endpoint/

The `/pdf` endpoint instructs the browser to render the webpage as a PDF document.

## Basic usage

Navigate to `https://example.com/` and inject custom CSS and an external stylesheet. Then return the rendered page as a PDF.

```bash
curl -X POST 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/pdf' \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
    "url": "https://example.com/",
    "addStyleTag": [
      { "content": "body { font-family: Arial; }" },
      { "url": "https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap.min.css" }
    ]
  }' \
  --output "output.pdf"
```

## Advanced usage

Navigate to `https://example.com`, first setting an additional HTTP request header and configuring the page size (`viewport`). Then, wait until there are no more than 2 network connections for at least 500 ms, or until the maximum timeout of 4500 ms is reached, before considering the page loaded and returning the rendered PDF document.

The `goToOptions` parameter exposes most of [Puppeteer'd API](https://pptr.dev/api/puppeteer.gotooptions).

```bash
curl -X POST 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/pdf' \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
    "url": "https://example.com/",
    "setExtraHTTPHeaders": {
      "X-Custom-Header": "value"
    },
    "viewport": {
      "width": 1200,
      "height": 800
    },
    "gotoOptions": {
      "waitUntil": "networkidle2",
      "timeout": 45000
    }
  }' \
  --output "advanced-output.pdf"
```

## Blocking images and styles when generating a PDF

The options `rejectResourceTypes` and `rejectRequestPattern` can be used to block requests. The opposite can also be done, _only_ allow certain requests using `allowResourceTypes` and `allowRequestPattern`.

```bash
curl -X POST https://api.cloudflare.com/client/v4/accounts/<acccountID>/browser-rendering/pdf \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
  "url": "https://cloudflare.com/",
  "rejectResourceTypes": ["image"],
  "rejectRequestPattern": ["/^.*\\.(css)"]
}' \
  --output "cloudflare.pdf"
```

## Generate PDF from custom HTML

If you have HTML you'd like to generate a PDF from, the `html` option can be used. The option `addStyleTag` can be used to add custom styles.

```bash
curl -X POST https://api.cloudflare.com/client/v4/accounts/<acccountID>/browser-rendering/pdf \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
  "html": "<html><body>Advanced Snapshot</body></html>",
	"addStyleTag": [
      { "content": "body { font-family: Arial; }" },
      { "url": "https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap.min.css" }
    ]
}' \
  --output "invoice.pdf"
```

Many more options exist, like setting HTTP credentials using `authenticate`, setting `cookies`, and using `gotoOptions` to control page load behaviour - check the endpoint [reference](/api/resources/browser_rendering/subresources/pdf/methods/create/) for all available parameters.

---

# Scrape HTML elements

URL: https://developers.cloudflare.com/browser-rendering/rest-api/scrape-endpoint/

The `/scrape` endpoint extracts structured data from specific elements on a webpage, returning details such as element dimensions and inner HTML.

## Basic usage

Go to `https://example.com` and extract metadata from all `h1` and `a` elements in the DOM.

```bash
curl -X POST 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/scrape' \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
  "url": "https://example.com/",
  "elements": [{
    "selector": "h1"
  },
  {
    "selector": "a"
  }]
}'
```

### JSON response

```json title="json response"
{
	"success": true,
	"result": [
		{
			"results": [
				{
					"attributes": [],
					"height": 39,
					"html": "Example Domain",
					"left": 100,
					"text": "Example Domain",
					"top": 133.4375,
					"width": 600
				}
			],
			"selector": "h1"
		},
		{
			"results": [
				{
					"attributes": [
						{ "name": "href", "value": "https://www.iana.org/domains/example" }
					],
					"height": 20,
					"html": "More information...",
					"left": 100,
					"text": "More information...",
					"top": 249.875,
					"width": 142
				}
			],
			"selector": "a"
		}
	]
}
```

Many more options exist, like setting HTTP credentials using `authenticate`, setting `cookies`, and using `gotoOptions` to control page load behaviour - check the endpoint [reference](/api/resources/browser_rendering/subresources/scrape/methods/create/) for all available parameters.

### Response fields

- `results` _(array of objects)_ - Contains extracted data for each selector.
  - `selector` _(string)_ - The CSS selector used.
  - `results` _(array of objects)_ - List of extracted elements matching the selector.
    - `text` _(string)_ - Inner text of the element.
    - `html` _(string)_ - Inner HTML of the element.
    - `attributes` _(array of objects)_ - List of extracted attributes such as `href` for links.
    - `height`, `width`, `top`, `left` _(number)_ - Position and dimensions of the element.

---

# Capture screenshot

URL: https://developers.cloudflare.com/browser-rendering/rest-api/screenshot-endpoint/

The `/screenshot` endpoint renders the webpage by processing its HTML and JavaScript, then captures a screenshot of the fully rendered page.

## Basic usage

Sets the HTML content of the page to `Hello World!` and then takes a screenshot. The option `omitBackground` hides the default white background and allows capturing screenshots with transparency.

```bash
curl -X POST 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/screenshot' \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
    "html": "Hello World!",
    "screenshotOptions": {
      "omitBackground": true
    }
  }' \
  --output "screenshot.png"
```

For more options to control the final screenshot, like `clip`, `captureBeyondViewport`, `fullPage` and others, check the endpoint [reference](/api/resources/browser_rendering/subresources/screenshot/methods/create/).

## Advanced usage

Navigate to `https://cloudflare.com/`, changing the page size (`viewport`) and waiting until there are no active network connections (`waitUntil`) or up to a maximum of `4500ms` (`timeout`). Then take a `fullPage` screenshot.

```bash
curl -X POST 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/screenshot' \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
    "url": "https://cnn.com/",
    "screenshotOptions": {
       "fullPage": true
    },
    "viewport": {
      "width": 1280,
      "height": 720
    },
    "gotoOptions": {
      "waitUntil": "networkidle0",
      "timeout": 45000
    }
  }' \
  --output "advanced-screenshot.png"
```

## Customize CSS and embed custom JavaScript

Instruct the browser to go to `https://example.com`, embed custom JavaScript (`addScriptTag`) and add extra styles (`addStyleTag`), both inline (`addStyleTag.content`) and by loading an external stylesheet (`addStyleTag.url`).

```bash
curl -X POST 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/screenshot' \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
    "url": "https://example.com/",
    "addScriptTag": [
      { "content": "document.querySelector(`h1`).innerText = `Hello World!!!`" }
    ],
    "addStyleTag": [
      {
        "content": "div { background: linear-gradient(45deg, #2980b9  , #82e0aa  ); }"
      },
      {
        "url": "https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap.min.css"
      }
    ]
  }' \
  --output "screenshot.png"
```

Many more options exist, like setting HTTP credentials using `authenticate`, setting `cookies`, and using `gotoOptions` to control page load behaviour - check the endpoint [reference](/api/resources/browser_rendering/subresources/screenshot/methods/create/) for all available parameters.

---

# Take a webpage snapshot

URL: https://developers.cloudflare.com/browser-rendering/rest-api/snapshot/

The `/snapshot` endpoint captures both the HTML content and a screenshot of the webpage in one request. It returns the HTML as a text string and the screenshot as a Base64-encoded image.

## Basic usage

1. Go to `https://example.com/`.
2. Inject custom JavaScript.
3. Capture the rendered HTML.
4. Take a screenshot.

```bash
curl -X POST 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/snapshot' \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
    "url": "https://example.com/",
    "addScriptTag": [
      { "content": "document.body.innerHTML = \"Snapshot Page\";" }
    ]
  }'
```

### JSON response

```json title="json response"
{
	"success": true,
	"result": {
		"screenshot": "Base64EncodedScreenshotString",
		"content": "<html>...</html>"
	}
}
```

## Advanced usage

The `html` property in the JSON payload, it sets the html to `<html><body>Advanced Snapshot</body></html>` then does the following steps:

1. Disable JavaScript.
2. Sets the screenshot to `fullPage`.
3. Changes the page size `(viewport)`.
4. Waits up to `30000ms` or until the `DOMContentLoaded` event fires.
5. Returns the rendered HTML content and a base-64 encoded screenshot of the page.

```bash
curl -X POST 'https://api.cloudflare.com/client/v4/accounts/<accountId>/browser-rendering/snapshot' \
  -H 'Authorization: Bearer <apiToken>' \
  -H 'Content-Type: application/json' \
  -d '{
    "html": "<html><body>Advanced Snapshot</body></html>",
    "setJavaScriptEnabled": false,
    "screenshotOptions": {
       "fullPage": true
    },
    "viewport": {
      "width": 1200,
      "height": 800
    },
    "gotoOptions": {
      "waitUntil": "domcontentloaded",
      "timeout": 30000
    }
  }'
```

### JSON response

```json title="json response"
{
	"success": true,
	"result": {
		"screenshot": "AdvancedBase64Screenshot",
		"content": "<html><body>Advanced Snapshot</body></html>"
	}
}
```

Many more options exist, like setting HTTP credentials using `authenticate`, setting `cookies`, and using `gotoOptions` to control page load behaviour - check the endpoint [reference](/api/resources/browser_rendering/subresources/snapshot/) for all available parameters.

---

# About address maps

URL: https://developers.cloudflare.com/byoip/address-maps/

import { GlossaryDefinition } from "~/components"

<GlossaryDefinition term="address map" prepend="Address map is " />

If you do not have BYOIP or static IPs and you want to use Address Maps, contact your account manager. You can [customize the IPs Cloudflare uses](/fundamentals/concepts/cloudflare-ip-addresses/#customize-cloudflare-ip-addresses) by bringing your own IP addresses to Cloudflare (BYOIP) or by leasing static Cloudflare IPs.

:::note
Both IPv4 and IPv6 addresses are supported.
:::

---

## How Address Maps works

For zones using [Cloudflare's authoritative DNS](/dns/), Cloudflare typically responds to DNS queries for proxied hostnames with [anycast IPs](/fundamentals/concepts/cloudflare-ip-addresses/). However, if you [customize the IPs Cloudflare uses](/fundamentals/concepts/cloudflare-ip-addresses/#customize-cloudflare-ip-addresses) and use Address Maps, Cloudflare will respond with the IP address(es) on the address map.

Address maps do not change [how Cloudflare reaches the configured origin](/fundamentals/concepts/how-cloudflare-works/#how-cloudflare-works-as-a-reverse-proxy). The IP addresses defined on the **DNS** > **Records** under your zone continue to instruct Cloudflare how to reach the origin.

:::caution
Depending on whether you use static IPs or BYOIP, the process to [create an address map](/byoip/address-maps/setup/) is different.
:::

### Static IPs or BYOIP

Leased static IPs allow you to use a set of specifically assigned Cloudflare IPs to ensure they do not change. Cloudflare creates an address map with your static IPs that you may edit. You cannot create another map using your static IPs.

With BYOIP, you use your IPs by bringing an address space that you lease or own and creating an address map.

---

## Immutable address maps

Some customers may only proxy zones through BYOIP addresses, and are prohibited from using Cloudflare IP addresses for proxied DNS names. In this case, Cloudflare will create an immutable, account-wide address map to ensure all zones in your account receive BYOIP addresses as a fallback. These address maps cannot be deleted.

It is still possible to create more specific zone-level address maps with specific BYOIPs, but DNS will fall back to the account-wide address map without one.

To specify different addresses for certain zones, [create a new address map](/byoip/address-maps/setup/).

---

# Set up address maps

URL: https://developers.cloudflare.com/byoip/address-maps/setup/

import { GlossaryTooltip } from "~/components";

Consider the sections below to learn how to set up address maps.

:::note
There is **no expected downtime** when setting up or updating your address maps.
:::

## Create address maps

To avoid any errors if you have [static IPs](/byoip/concepts/static-ips/), Cloudflare creates an address map during the static IP onboarding process, meaning you cannot create a new address map with your static IPs. You may only edit the Cloudflare-created map and add or edit your zones within the existing map.

If you are using BYOIP instead, refer to the following steps:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **IP Addresses** > **Address Maps**.
3. Select **Create an address map**.
4. Choose the scope of the address map.
5. Add the zones and IP addresses that you want to map.
6. Name your address map.
7. Review the information and select **Save and Deploy**.

:::note
Creating an address map does not automatically change DNS configuration. DNS responses only begin to change when a zone or account is added to a map. Additionally, address maps that are not yet enabled will not take effect in DNS responses.
:::

## Manage address maps

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **IP Addresses** > **Address Maps**.
3. Go to your address map and select **Review**.
4. Edit your address map.
5. Review the information and select **Save**.

:::note
You can also enable, disable, and delete address maps. This will likely change the IP addresses used for your zones.
:::

## Non-SNI support

If your visitors use devices that have not been updated since 2011, they may not have <GlossaryTooltip term="Server Name Indication (SNI)">Server Name Indication (SNI)</GlossaryTooltip> support. For further context, refer to [browser compatibility](/ssl/reference/browser-compatibility/#non-sni-support).

Use address maps to specify a hostname as default SNI. This will be used whenever Cloudflare receives a non-SNI TLS handshake.

:::note
Setting up a default SNI is currently only supported via API.
:::

1. If you have not already, create an address map. Refer to the [section above](#create-address-maps) or to the [Create Address Map](/api/resources/addressing/subresources/address_maps/methods/create/) API endpoint.
2. Take note of the address map `id`. If needed, you can use the [List Address Maps](/api/resources/addressing/subresources/address_maps/methods/list/) endpoint to get it.
3. Make sure you add the desired IPs to the address map. Cloudflare will respond with the default SNI on those IPs. Use the dashboard or refer to [Add An IP To An Address Map](/api/resources/addressing/subresources/address_maps/subresources/ips/methods/update/).
4. Configure the `default_sni` value on the address map created in step 1. Refer to the [Update Address Map](/api/resources/addressing/subresources/address_maps/methods/edit/) API endpoint for details. The default SNI can be any valid domain or subdomain owned by your account.

### Spectrum HTTPS applications

Default SNI for Spectrum can only be created via API using the [Create Address Map](/api/resources/addressing/subresources/address_maps/methods/create/) endpoint.

Do not include any membership in your command. Your API command should resemble the following:

```bash
curl https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/addressing/address_maps \
  --header "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
  --header "Content-Type: application/json" \
  --data '{
  "description": "default_sni",
  "default_sni": "sni.example.com",
  "enabled": false,
  "ips": [
    "192.0.0.1"
  ],
  "memberships": []
}'
```

---

# Concepts

URL: https://developers.cloudflare.com/byoip/concepts/

import { DirectoryListing } from "~/components"

Review the topics below for more information about concepts related to BYOIP.

<DirectoryListing />

---

# Letter of Agency

URL: https://developers.cloudflare.com/byoip/concepts/loa/

A Letter of Agency (LOA) - sometimes referred to as a Letter of Authorization - is a document that authorizes Cloudflare to announce a prefix(es) on behalf of another entity. The LOA is required by Cloudflare's transit providers so they can accept the routes Cloudflare advertises on behalf of another entity.

The letter must contain both the prefixes you are authorizing Cloudflare to announce and which ASN they will be announced under. Cloudflare can announce a prefix under your ASN or you can use Cloudflare's ASN, which is AS13335.

:::note
Current customers who are still using Cloudflare's AS209242 can continue using this same ASN. No further change is required.
:::

Cloudflare accepts digital signatures on an LOA, as long as it is clear who is signing the LOA.

:::note[Note]
An LOA is a formal document which should be on company letterhead and contain a wet signature. The Letter of Agency must be a PDF. Transit providers may reject the LOA if it is in a JPG or PNG format.
:::

You can use the below template when creating an LOA document.

```txt title="Letter of Agency template"
[COMPANY LETTERHEAD]

LETTER OF AGENCY ("LOA")

[DATE]


To whom it may concern:

[COMPANY NAME] (the "Company") authorizes Cloudflare, Inc. with AS13335 to advertise the following IP address blocks / originating ASNs:

- - - - - - - - - - - - - - - - - - -
[Subnet & Originating ASN]
[Subnet & Originating ASN]
[Subnet & Originating ASN]
- - - - - - - - - - - - - - - - - - -

As a representative of the Company that is the owner of the aforementioned IP address blocks / originating ASNs, I hereby declare that I am authorized to sign this LOA on the Companyâ€™s behalf.

Should you have any questions please email me at [E-MAIL ADDRESS], or call: [TELEPHONE NUMBER]

Regards,


[SIGNATURE]


[NAME TYPED]
[TITLE]
[COMPANY NAME]
[COMPANY ADDRESS]
[COMPANY STAMP]
```

---

# Prefix delegations

URL: https://developers.cloudflare.com/byoip/concepts/prefix-delegations/

BYOIP supports prefix delegations, which occur when a prefix ownerâ€™s account (Account A) allows another account (Account B) to use all or part of their prefix. The original prefix is still managed by the original account, but a delegation allows another account to use the delegated IP(s) on various services within that account.

Refer to [service bindings](/byoip/service-bindings/) for more information on the services an IP can be bound to.

## CDN


CDN delegations allow you to use the IP(s) with [Address Maps](/byoip/address-maps/) or [Cloudflare for SaaS](/cloudflare-for-platforms/cloudflare-for-saas/) customers.

Address Maps allows you to assign IPs either at the account level or zone level.

In the Cloudflare for SaaS example, Account A is using BYOIP + CDN and Cloudflare for SaaS. Account A can validate and serve traffic for a custom hostname on any of the IPs in its prefix. If Account A delegates some or all of the prefix to Account B, Account B may also validate and serve traffic for custom hostnames on those IPs as well. This is very useful if you use Cloudflare for SaaS but manage different configurations in different accounts. All the accounts can use the IPs through a delegation.

## Spectrum

If Account A delegates use of part or all of a prefix to Account B via a prefix delegation, Account B can also use the [Spectrum API](/spectrum/about/byoip/) with the IPs it was delegated access to.

**Example:** Account A is the primary owner of prefix 1.2.3.0/24. Account A delegates the use of 1.2.3.0/32 to Account B. Account B can now use the Spectrum API to create a Spectrum app with 1.2.3.0/32.

## API calls for prefix delegations

API calls for delegations can be found at [Prefix Delegations](/api/resources/addressing/subresources/prefixes/subresources/delegations/methods/list/).

:::note

The dashboard only supports delegation of an entire prefix. If you want to delegate less than the entire prefix, use the API.

To bind an IP from one service to another, use the API.
:::

## Configure prefix delegations

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **IP Addresses** > **IP Prefixes**.
3. Select **Edit** to modify a prefix. **Edit IP Prefixes** displays.
4. At the bottom of the page, select **Add Delegation**. Other accounts that your user is a part of will auto-load when you create the delegation.
5. Select **Save**.
6. Bind IPs to a service via the [Service Bindings API](/api/resources/addressing/subresources/prefixes/subresources/service_bindings/) as needed.

---

# IP address service bindings

URL: https://developers.cloudflare.com/byoip/service-bindings/

Service bindings are mappings that control whether traffic destined for a given IP address is routed to [Magic Transit](/magic-transit/), the CDN pipeline [^1], or the Spectrum pipeline [^2].

:::note
Service binding operations are currently only available via API. You can find all endpoints and their specifications in the [Cloudflare API documentation](/api/resources/addressing/subresources/prefixes/subresources/service_bindings/).
:::

## Scope

Customers using BYOIP with Magic Transit can leverage the [service binding API endpoints](/api/resources/addressing/subresources/prefixes/subresources/service_bindings/) to selectively route traffic through the CDN [^1] or Spectrum [^2] pipelines on a per-IP address basis.

You can upgrade individual IPs within a Magic Transit prefix to either a CDN IP or a Spectrum IP. For example, if you have `203.0.113.0/24`, you can upgrade `203.0.113.1` to CDN and `203.0.113.2` to Spectrum.

:::caution
Magic Transit customers must ensure that their contract includes CDN and/or Spectrum according to their needs.
:::

### CDN (Cache)

When a service binding of type `CDN` is applied, once the change has propagated across Cloudflare's global network (four to six hours), any HTTP requests are directed into the CDN pipeline for Layer 7 processing.

Refer to [Use BYOIP with Magic Transit and CDN](/byoip/service-bindings/magic-transit-with-cdn/) for detailed guidance.

### Spectrum

When a service binding of type `Spectrum` is applied, once the change has propagated across Cloudflare's global network (four to six hours), any TCP/UDP/HTTP requests are directed into the Spectrum pipeline for Layer 4 or Layer 7 processing.

## Limitations

- You must keep Magic Transit as a common base service, spanning all addresses in your prefix.
- When adding a service binding for a given IP address, it must be either a CDN service binding or a Spectrum service binding. It is not possible (or necessary) to bind both services.
-  Once a service binding is created (or deleted), it will take four to six hours to propagate across Cloudflare's global network. Services for the IP addresses in scope will likely be disrupted during this window.

[^1]: Layer 7 HTTP-based
[^2]: Layer 4 or Layer 7 HTTP with custom ports

---

# Static IPs

URL: https://developers.cloudflare.com/byoip/concepts/static-ips/

Lease static IPs so that you can use a set of specifically assigned Cloudflare IPs. If you need to allowlist your IPs or to communicate your IPs to third parties, allocating static IPs to your account allows you to know them ahead of time.

Cloudflare will not change static IP addresses without notifying you, and will typically only do so at your request.

:::note
Although BYOIP and static IPs are different offerings, both can be managed using [Address Maps](/byoip/address-maps/).
:::

Static IPs are allocated to the account, but can be assigned to a single zone. This means that you can place multiple zones on the same static IPs. You can also specify which zones are mapped to your static IPs and control when the IPs for your zones change.

## Availability

Static IPs are available as an add-on purchase for Enterprise plans.

---

# Use BYOIP with Magic Transit and CDN

URL: https://developers.cloudflare.com/byoip/service-bindings/magic-transit-with-cdn/

import { Details, Example, TabItem, Tabs, GlossaryTooltip } from "~/components";

[Magic Transit](/magic-transit/) customers using BYOIP can also benefit from the performance, reliability, and security that Cloudflare offers for HTTP-based applications.

This documentation covers using the Cloudflare API to configure [service bindings](/byoip/service-bindings/) within Cloudflare's IP Address Management framework. Service bindings allow BYOIP customers to selectively route traffic on a per-IP address basis to the CDN pipeline (which includes [Cache](/cache/), [Web Application Firewall (WAF)](/waf/), and more).

It is also possible to define service bindings to route traffic to the Spectrum pipeline selectively. However, this is not in the scope of this guide.

It is important to note that traffic routed to the CDN pipeline is protected at Layers 3 and 4 by the inherent DDoS protection capabilities native to the CDN pipeline.

## Before you begin

Although it is possible to add discrete bindings for non-contiguous CIDR blocks, implementing service bindings through an **aggregated** CIDR block is **strongly** recommended as it is more efficient.

<Details header="Example">

**Magic Transit protected prefix:** `203.0.113.0/24`

**IPs to upgrade to the CDN:**

`203.0.113.16`<br />
`203.0.113.17`<br />
`203.0.113.18`<br />
`203.0.113.19`<br />
`203.0.113.20`<br />
`203.0.113.21`<br />
`203.0.113.22`<br />
`203.0.113.23`

Add one discrete CDN service binding for `203.0.113.16` with a `/29` netmask.

</Details>

Once a service binding is created (or deleted), it will take **four** to **six** hours to propagate across Cloudflare's global network. Services for the IP addresses in scope will likely be disrupted during this window.

:::note
This guide assumes that the prefix is tied to a single Cloudflare account that has both Magic Transit and CDN properties. If you are using [prefix delegations](/byoip/concepts/prefix-delegations/), the service bindings must be [created](#2-create-service-binding) on the parent account.
:::

## 1. Get account information

1. Log in to your Cloudflare account and get your [account ID](/fundamentals/setup/find-account-and-zone-ids/) and [API token](/fundamentals/api/get-started/create-token/). The token permissions should include `Account` - `IP Prefixes` - `Edit`.
2. Make a `GET` request to the [List Services](/api/resources/addressing/subresources/services/methods/list/) endpoint and take note of the `id` associated with the CDN service.
3. Use the [List Prefixes](/api/resources/addressing/subresources/prefixes/methods/list/) endpoint and take note of the `id` associated with the prefix (`cidr`) you will configure.

<Example>

At this point, continuing the [example](#before-you-begin), you should have a mapping similar to the following:

| Variables      | Description                                                                                                                                       |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| `{service_id}` | The ID of the CDN service within Cloudflare. <br /><br /> Example: `969xxxxxxxx000xxx0000000x00001bf`                                             |
| `{prefix_id}`  | The ID of the Magic Transit protected prefix (`203.0.113.0/24`) you want to configure. <br /><br /> Example: `6b25xxxxxxx000xxx0000000x0000cfc` |

</Example>

4. To confirm you currently only have a Magic Transit service binding and that it spans across your entire prefix, make a `GET` request to the [List Service Bindings](/api/resources/addressing/subresources/prefixes/subresources/service_bindings/methods/list/) endpoint. Replace the `{prefix_id}` in the URI path by the actual prefix ID you got from the previous step.

<Example>

```bash
curl https://api.cloudflare.com/client/v4/accounts/{account_id}/addressing/prefixes/{prefix_id}/bindings \
--header "Authorization: Bearer <API_TOKEN>"
```

</Example>

## 2. Create service binding

:::caution[Caution]
Once a service binding is created (or deleted), it will take four to six hours to propagate across Cloudflare's global network. Services for the IP addresses in scope will likely be disrupted during this window.
:::

1. Make a `POST` request to the [Create service binding](/api/resources/addressing/subresources/prefixes/subresources/service_bindings/methods/create/) endpoint, indicating the IP address you want to bind to the CDN. Specify the **corresponding network mask** as needed.

<Example>

Continuing the example, `203.0.113.100/32` designates an IP address that is within the Magic Transit protected prefix `203.0.113.0/24`.

Replace the `{prefix_id}` in the URI with your prefix ID from previous steps. Within the request body, the `cidr` value should correspond to the IP address or subnet that you are configuring for use with CDN.

```bash

curl https://api.cloudflare.com/client/v4/accounts/{account_id}/addressing/prefixes/{prefix_id}/bindings \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{
  "cidr": "203.0.113.100/32",
  "service_id": <SERVICE_ID>
}'
```

In the response body, the initial provisioning state should be `provisioning`.

```json
{
  "errors": [],
  "messages": [],
  "success": true,
  "result": {
    "cidr": "203.0.113.100/32",
    "id": <CDN_SERVICE_BINDING_ID>,
    "provisioning": {
      "state": "provisioning"
      },
    "service_id": <SERVICE_ID>,
    "service_name": "CDN"
  }
}
```

</Example>

You can periodically check the service binding status using the [List Service Bindings](/api/resources/addressing/subresources/prefixes/subresources/service_bindings/methods/list/) endpoint.

## 3. Create address maps

Once you have configured your IPs to have CDN service, you can use <GlossaryTooltip term="address map" link="/byoip/address-maps/">address maps</GlossaryTooltip> to specify which IPs should be used by Cloudflare in DNS responses when a record is <GlossaryTooltip term="proxy status" link="/dns/proxy-status/">proxied</GlossaryTooltip>.

You can choose between two different scopes:

- Account-level: uses the address map for all proxied DNS records across all of the zones within an account.
- Zone-level: uses the address map for all proxied DNS records within a zone.

:::note
If you need to map only specific subdomains (and not all proxied DNS records) to specific IP addresses, you can use a [Subdomain setup](/dns/zone-setups/subdomain-setup/).
:::

<Tabs labels="Dashboard | API">
<TabItem label="dashboard" no-code="true">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **IP Addresses** > **Address Maps**.
3. Select **Create an address map**.
4. Choose the scope of the address map.
5. Add the zones and IP addresses that you want to map.
6. Name your address map.
7. Review the information and select **Save and Deploy**.

</TabItem>
<TabItem label="api" no-code="true">

Use the [Create Address Map](/api/resources/addressing/subresources/address_maps/methods/create/) endpoint.

Make sure you have the correct Key/Token and permissions.

</TabItem>
</Tabs>

## 4. Create DNS records

<Tabs labels="Dashboard | API">
<TabItem label="dashboard" no-code="true">

To create a DNS record in the dashboard:

1.  Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select an account and domain.
2.  Go to **DNS** > **Records**.
3.  Select **Add record**.
4.  Choose an address (`A`/`AAAA`) [record type](/dns/manage-dns-records/reference/dns-record-types/).
5.  Complete the required fields, setting the **Proxy status** to **proxied**.
6.  Select **Save**.

</TabItem>

<TabItem label="api" no-code="true">

To create records with the API, use a [POST request](/api/resources/dns/subresources/records/methods/create/). For field definitions, select a record type under the request body specification.

</TabItem>
</Tabs>

:::note
As you create the necessary DNS records, [Total TLS](/ssl/edge-certificates/additional-options/total-tls/) can help making sure that you have SSL/TLS certificates in place for all your hostnames.
:::

While the DNS record proxy status and address map will determine how Cloudflare's authoritative DNS responds to requests for your hostnames, the IP addresses specified in `A`/`AAAA` records will determine [how Cloudflare reaches the configured origin](/fundamentals/concepts/how-cloudflare-works/#how-cloudflare-works-as-a-reverse-proxy).

<Details header="Example">

| Type | Name  | IP address      | Proxy status | TTL    |
| ---- | ----- | --------------- | ------------ | ------ |
| `A`  | `www` | `203.0.113.150` | `Proxied`    | `Auto` |

At this point, if an address map for a zone `example.com` specifies that Cloudflare should use `203.0.113.100` for proxied records and the above record exists in the same zone, you can expect the following:

1. Cloudflare responds to DNS requests with `203.0.113.100`.
2. Cloudflare proxies requests through the CDN and then routes the requests via [GRE](/magic-transit/reference/tunnels/) or [CNI](/magic-transit/network-interconnect/) to the origin server `203.0.113.150` (which is within the Magic Transit protected prefix).
3. Depending on whether Magic Transit is implemented with [direct server return model or with Magic Transit egress](/magic-transit/how-to/configure-tunnels/#bidirectional-vs-unidirectional-health-checks), the origin server responds back to Cloudflare either:

   - Directly over the Internet in a Magic Transit direct server return model
   - Back through the Magic GRE tunnel(s) in a Magic Transit egress model

4. As the HTTP response egresses the Cloudflare network back to the client side, the source IP address of the response becomes `203.0.113.100` (the IP address that the HTTP request originally landed on).

</Details>

:::note
Having the same IP address as ingress IP (defined in the address map) and origin IP (listed in the DNS record) will not cause any loops.
:::

<Details header="Example">

Assuming `203.0.113.100` was also the origin IP, the DNS record would look like the following:

| Type | Name  | IP address      | Proxy status | TTL    |
| ---- | ----- | --------------- | ------------ | ------ |
| `A`  | `www` | `203.0.113.100` | `Proxied`    | `Auto` |

</Details>

## 5. (Optional) Add layer 7 functionality

Leverage other features according to your needs:

- [Cache](/cache/)
- [WAF custom rules](/waf/custom-rules/)
- [Security analytics](/waf/analytics/security-analytics/)

---

# Analytics

URL: https://developers.cloudflare.com/calls/turn/analytics/

Cloudflare Calls TURN service counts ingress and egress usage in bytes. You can access this real-time and historical data using the TURN analytics API. You can see TURN usage data in a time series or aggregate that shows traffic in bytes over time.

Cloudflare TURN analytics is available over the GraphQL API only.

:::note[API token permissions]

You will need the "Account Analytics" permission on your API token to make queries to the Calls GraphQL API. 
:::

:::note

See [GraphQL API](/analytics/graphql-api/) for more information on how to set up your GraphQL client. The examples below use the same GraphQL endpoint at `https://api.cloudflare.com/client/v4/graphql`. 
:::

## TURN traffic data filters

You can filter the data in TURN analytics on:

* Datetime range
* TURN Key ID
* TURN Username
* Custom identifier

:::note


[Custom identifiers](/calls/turn/replacing-existing/#tag-users-with-custom-identifiers) are useful for accounting usage for different users in your system.


:::

## Useful TURN analytics queries

Below are some example queries for common usecases. You can modify them to adapt your use case and get different views to the analytics data.

### Top TURN keys by egress

```
query{
  viewer {
    usage: accounts(filter: { accountTag: "8846293bd06d1af8c106d89ec1454fe6" }) {
        callsTurnUsageAdaptiveGroups(
          filter: {
          datetimeMinute_gt: "2024-07-15T02:07:07Z"
          datetimeMinute_lt: "2024-08-10T02:07:05Z"
        }
          limit: 2
          orderBy: [sum_egressBytes_DESC]
        ) {
          dimensions {
            keyId
          }
          sum {
            egressBytes
          }
        }
      }
    }
  }


```

```
{
  "data": {
    "viewer": {
      "usage": [
        {
          "callsTurnUsageAdaptiveGroups": [
            {
              "dimensions": {
                "keyId": "74007022d80d7ebac4815fb776b9d3ed"
              },
              "sum": {
                "egressBytes": 502614982
              }
            },
            {
              "dimensions": {
                "keyId": "6b9e68b07dfee8cc2d116e4c51d6a957"
              },
              "sum": {
                "egressBytes": 4853235
              }
            }
          ]
        }
      ]
    }
  },
  "errors": null
}
```

### Top TURN custom identifiers

```
query{
  viewer {
    usage: accounts(filter: { accountTag: "8846293bd06d1af8c106d89ec1454fe6" }) {
        callsTurnUsageAdaptiveGroups(
          filter: {
          datetimeMinute_gt: "2024-07-15T02:07:07Z"
          datetimeMinute_lt: "2024-08-10T02:07:05Z"
        }
          limit: 100
          orderBy: [sum_egressBytes_DESC]
        ) {
          dimensions {
            customIdentifier
          }
          sum {
            egressBytes
          }
        }
      }
    }
  }
```

```
{
  "data": {
    "viewer": {
      "usage": [
        {
          "callsTurnUsageAdaptiveGroups": [
            {
              "dimensions": {
                "customIdentifier": "custom-id-333"
              },
              "sum": {
                "egressBytes": 269850354
              }
            },
            {
              "dimensions": {
                "customIdentifier": "custom-id-555"
              },
              "sum": {
                "egressBytes": 162641324
              }
            },
            {
              "dimensions": {
                "customIdentifier": "custom-id-112"
              },
              "sum": {
                "egressBytes": 70123304
              }
            }
          ]
        }
      ]
    }
  },
  "errors": null
}
```

### Usage for a specific custom identifier

```
query{
  viewer {
    usage: accounts(filter: { accountTag: "8846293bd06d1af8c106d89ec1454fe6" }) {
        callsTurnUsageAdaptiveGroups(
          filter: {
          datetimeMinute_gt: "2024-07-15T02:07:07Z"
          datetimeMinute_lt: "2024-08-10T02:07:05Z"
          customIdentifier: "tango"
        }
          limit: 100
          orderBy: []
        ) {
          dimensions {
            keyId
            customIdentifier
          }
          sum {
            egressBytes
          }
        }
      }
    }
  }

```

```
{
  "data": {
    "viewer": {
      "usage": [
        {
          "callsTurnUsageAdaptiveGroups": [
            {
              "dimensions": {
                "customIdentifier": "tango",
                "keyId": "74007022d80d7ebac4815fb776b9d3ed"
              },
              "sum": {
                "egressBytes": 162641324
              }
            }
          ]
        }
      ]
    }
  },
  "errors": null
}
```

### Usage as a timeseries (for graphs)

```
query{
  viewer {
    usage: accounts(filter: { accountTag: "8846293bd06d1af8c106d89ec1454fe6" }) {
        callsTurnUsageAdaptiveGroups(
          filter: {
          datetimeMinute_gt: "2024-07-15T02:07:07Z"
          datetimeMinute_lt: "2024-08-10T02:07:05Z"
        }
          limit: 100
          orderBy: [datetimeMinute_ASC]
        ) {
          dimensions {
            datetimeMinute
          }
          sum {
            egressBytes
          }
        }
      }
    }
  }



```

```
{
  "data": {
    "viewer": {
      "usage": [
        {
          "callsTurnUsageAdaptiveGroups": [
            {
              "dimensions": {
                "datetimeMinute": "2024-08-01T17:09:00Z"
              },
              "sum": {
                "egressBytes": 4570704
              }
            },
            {
              "dimensions": {
                "datetimeMinute": "2024-08-01T17:10:00Z"
              },
              "sum": {
                "egressBytes": 27203016
              }
            },
            {
              "dimensions": {
                "datetimeMinute": "2024-08-01T17:11:00Z"
              },
              "sum": {
                "egressBytes": 9067412
              }
            },
            {
              "dimensions": {
                "datetimeMinute": "2024-08-01T17:17:00Z"
              },
              "sum": {
                "egressBytes": 10059322
              }
            },
            ...
           ]
        }
      ]
    }
  },
  "errors": null
}
```

---

# Custom TURN domains

URL: https://developers.cloudflare.com/calls/turn/custom-domains/

Cloudflare Calls TURN service supports using custom domains for UDP, and TCP - but not TLS protocols. Custom domains do not affect any of the performance of Cloudflare Calls TURN and is set up via a simple CNAME DNS record on your domain.

| Protocol      | Custom domains | Primary port | Alternate port |
| ------------- | -------------- | ------------ | -------------- |
| STUN over UDP | âœ…              | 3478/udp     | 53/udp         |
| TURN over UDP | âœ…              | 3478/udp     | 53 udp         |
| TURN over TCP | âœ…              | 3478/tcp     | 80/tcp         |
| TURN over TLS | No             | 5349/tcp     | 443/tcp        |

## Setting up a CNAME record

To use custom domains for TURN, you must create a CNAME DNS record pointing to `turn.cloudflare.com`.

:::caution


Do not resolve the address of `turn.cloudflare.com` or `stun.cloudflare.com` or use an IP address as the value you input to your DNS record. Only CNAME records are supported.


:::

Any DNS provider, including Cloudflare DNS can be used to set up a CNAME for custom domains.

:::note


If Cloudflare's authoritative DNS service is used, the record must be set to [DNS-only or "grey cloud" mode](/dns/proxy-status/#dns-only-records).\`


:::

There is no additional charge to using a custom hostname with Cloudflare Calls TURN.

---

# FAQ

URL: https://developers.cloudflare.com/calls/turn/faq/

## General

### What is Cloudflare Calls TURN pricing? How exactly is it calculated?

Cloudflare TURN pricing is based on the data sent from the Cloudflare edge to the TURN client, as described in [RFC 8656 Figure 1](https://datatracker.ietf.org/doc/html/rfc8656#fig-turn-model). This means data sent from the TURN server to the TURN client and captures all data, including TURN overhead, following successful authentication.

Pricing for Cloudflare Calls TURN service is $0.05 per GB of data used.

Cloudflare's STUN service at `stun.cloudflare.com` is free and unlimited.

There is a free tier of 1,000 GB before any charges start. Cloudflare Calls billing appears as a single line item on your Cloudflare bill, covering both SFU and TURN.

Traffic between Cloudflare Calls TURN and Cloudflare Calls SFU or Cloudflare Stream (WHIP/WHEP) does not incur any charges.

<div class="full-img">

```mermaid
---
title: Cloudflare Calls TURN pricing
---
flowchart LR
    Client[TURN Client]
    Server[TURN Server]

    Client -->|"Ingress (free)"| Server
    Server -->|"Egress (charged)"| Client

    Server <-->|Not part of billing| PeerA[Peer A]
```

</div>

### Is Calls TURN HIPAA/GDPR/FedRAMP compliant?

Please view Cloudflare's [certifications and compliance resources](https://www.cloudflare.com/trust-hub/compliance-resources/) and contact your Cloudflare enterprise account manager for more information.

### Is Calls TURN end-to-end encrypted?

TURN protocol, [RFC 8656](https://datatracker.ietf.org/doc/html/rfc8656), does not discuss encryption beyond wrapper protocols such as TURN over TLS. If you are using TURN with WebRTC will encrypt data at the WebRTC level.

### What regions does Cloudflare Calls TURN operate at?

Cloudflare Calls TURN server runs on [Cloudflare's global network](https://www.cloudflare.com/network) - a growing global network of thousands of machines distributed across hundreds of locations, with the notable exception of the Cloudflare's [China Network](/china-network/).

### Does Cloudflare Calls TURN use the Cloudflare Backbone or is there any "magic" Cloudflare do to speed connection up?

Cloudflare Calls TURN allocations are homed in the nearest available Cloudflare data center to the TURN client via anycast routing. If both ends of a connection are using Cloudflare Calls TURN, Cloudflare will be able to control the routing and, if possible, route TURN packets through the Cloudflare backbone.

### What is the difference between Cloudflare Calls TURN with a enterprise plan vs self-serve (pay with your credit card) plans?

There is no performance or feature level difference for Cloudflare Calls TURN service in enterprise or self-serve plans, however those on [enterprise plans](https://www.cloudflare.com/enterprise/) will get the benefit of priority support, predictable flat-rate pricing and SLA guarantees.

### Does Cloudflare Calls TURN run in the Cloudflare China Network?

Cloudflare's [China Network](/china-network/) does not participate in serving Calls traffic and TURN traffic from China will connect to Cloudflare locations outside of China.

### How long does it take for TURN activity to be available in analytics?

TURN usage shows up in analytics in 30 seconds.

## Technical

### I need to allowlist (whitelist) Cloudflare Calls TURN IP addresses. Which IP addresses should I use?

Cloudflare Calls TURN is easy to use by IT administrators who have strict firewalls because it requires very few IP addresses to be allowlisted compared to other providers. You must allowlist both IPv6 and IPv4 addresses.

Please allowlist the following IP addresses:

- `2a06:98c1:3200::1/128`
- `2606:4700:48::1/128`
- `141.101.90.1/32`
- `162.159.207.1/32`

:::caution[Watch for IP changes]

Cloudflare tries to, but cannot guarantee that the IP addresses used for the TURN service won't change. If you are allowlisting IP addresses and do not have a enterprise contract, you must set up alerting that detects changes the DNS response from `turn.cloudflare.com` (A and AAAA records) and update the hardcoded IP address(es) accordingly within 14 days of the DNS change.

For more details about static IPs, guarantees and other arrangements please discuss with your enterprise account team.

Your enterprise team will be able to provide additional addresses to allowlist as future backup to achieve address diversity while still keeping a short list of IPs.

:::

### I would like to hardcode IP addresses used for TURN in my application to save a DNS lookup

Although this is not recommended, we understand there is a very small set of circumstances where hardcoding IP addresses might be useful. In this case, you must set up alerting that detects changes the DNS response from `turn.cloudflare.com` (A and AAAA records) and update the hardcoded IP address(es) accordingly within 14 days of the DNS change. Note that this DNS response could return more than one IP address. In addition, you must set up a failover to a DNS query if there is a problem connecting to the hardcoded IP address. Cloudflare tries to, but cannot guarantee that the IP address used for the TURN service won't change unless this is in your enterprise contract. For more details about static IPs, guarantees and other arrangements please discuss with your enterprise account team.

### I see that TURN IP are published above. Do you also publish IPs for STUN?

TURN service at `turn.cloudflare.com` will also respond to binding requests ("STUN requests").

### Does Cloudflare Calls TURN support the expired IETF RFC draft "draft-uberti-behave-turn-rest-00"?

The Cloudflare Calls credential generation function returns a JSON structure similar to the [expired RFC draft "draft-uberti-behave-turn-rest-00"](https://datatracker.ietf.org/doc/html/draft-uberti-behave-turn-rest-00), but it does not include the TTL value. If you need a response in this format, you can modify the JSON from the Cloudflare Calls credential generation endpoint to the required format in your backend server or Cloudflare Workers.

### I am observing packet loss when using Cloudflare Calls TURN - how can I debug this?

Packet loss is normal in UDP and can happen occasionally even on reliable connections. However, if you observe systematic packet loss, consider the following:

- Are you sending or receiving data at a high rate (>50-100Mbps) from a single TURN client? Calls TURN might be dropping packets to signal you to slow down.
- Are you sending or receiving large amounts of data with very small packet sizes (high packet rate > 5-10kpps) from a single TURN client? Cloudflare Calls might be dropping packets.
- Are you sending packets to new unique addresses at a high rate resembling to [port scanning](https://en.wikipedia.org/wiki/Port_scanner) behavior?

### I plan to use Calls TURN at scale. What is the rate at which I can issue credentials?

There is no defined limit for credential issuance. Start at 500 credentials/sec and scale up linearly. Ensure you use more than 50% of the issued credentials.

### What is the maximum value I can use for TURN credential expiry time?

You can set a expiration time for a credential up to 48 hours in the future. If you need your TURN allocation to last longer than this, you will need to [update](https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/setConfiguration) the TURN credentials.

### Does Calls TURN support IPv6?

Yes. Cloudflare Calls is available over both IPv4 and IPv6 for TURN Client to TURN server communication, however it does not issue relay addresses in IPv6 as described in [RFC 6156](https://datatracker.ietf.org/doc/html/rfc6156).

### Does Calls TURN issue IPv6 relay addresses?

No. Calls TURN will not respect `REQUESTED-ADDRESS-FAMILY` STUN attribute if specified and will issue IPv4 addresses only.

### Does Calls TURN support TCP relaying?

No. Calls does not implement [RFC6062](https://datatracker.ietf.org/doc/html/rfc6062) and will not respect `REQUESTED-TRANSPORT` STUN attribute.

### I am unable to make CreatePermission or ChannelBind requests with certain IP addresses. Why is that?

Cloudflare Calls denies CreatePermission or ChannelBind requests if private IP ranges (e.g loopback addresses, linklocal unicast or multicast blocks) or IP addresses that are part of [BYOIP](/byoip/) are used.

If you are a Cloudflare BYOIP customer and wish to connect to your BYOIP ranges with Calls TURN, please reach out to your account manager for further details.

### What will happen if TURN credentials expire while the TURN allocation is in use?

Cloudflare Calls will immediately stop billing and recording usage for analytics. After a short delay, the connection will be disconnected.

---

# Generate Credentials

URL: https://developers.cloudflare.com/calls/turn/generate-credentials/

Cloudflare will issue TURN keys, but these keys cannot be used as credentials with `turn.cloudflare.com`. To use TURN, you need to create credentials with a expiring TTL value.

## Create a TURN key

To create a TURN credential, you first need to create a TURN key using [Dashboard](https://dash.cloudflare.com/?to=/:account/calls), or the [API](/api/resources/calls/subresources/turn/methods/create/).

You should keep your TURN key on the server side (don't share it with the browser/app). A TURN key is a long-term secret that allows you to generate unlimited, shorter lived TURN credentials for TURN clients.

With a TURN key you can:

* Generate TURN credentials that expire
* Revoke previously issued TURN credentials

## Create credentials

You should generate short-lived credentials for each TURN user. In order to create credentials, you should have a back-end service that uses your TURN Token ID and API token to generate credentials. It will make an API call like this:

```bash
curl https://rtc.live.cloudflare.com/v1/turn/keys/$TURN_KEY_ID/credentials/generate-ice-servers \
--header "Authorization: Bearer $TURN_KEY_API_TOKEN" \
--header "Content-Type: application/json" \
--data '{"ttl": 86400}'
```

The JSON response below can then be passed on to your front-end application:

```json
{
  "iceServers": [
		{
			"urls": [
				"stun:stun.cloudflare.com:3478",
				"stun:stun.cloudflare.com:53",
				"turn:turn.cloudflare.com:3478?transport=udp",
				"turn:turn.cloudflare.com:53?transport=udp",
				"turn:turn.cloudflare.com:3478?transport=tcp",
				"turn:turn.cloudflare.com:80?transport=tcp",
				"turns:turn.cloudflare.com:5349?transport=tcp",
				"turns:turn.cloudflare.com:443?transport=tcp"
			],
			"username": "bc91b63e2b5d759f8eb9f3b58062439e0a0e15893d76317d833265ad08d6631099ce7c7087caabb31ad3e1c386424e3e",
			"credential": "ebd71f1d3edbc2b0edae3cd5a6d82284aeb5c3b8fdaa9b8e3bf9cec683e0d45fe9f5b44e5145db3300f06c250a15b4a0"
		}
	]
}
```

Use `iceServers` as follows when instantiating the `RTCPeerConnection`:

```js
const myPeerConnection = new RTCPeerConnection({
  iceServers: [
    {
      urls: [
				"stun:stun.cloudflare.com:3478",
				"stun:stun.cloudflare.com:53",
				"turn:turn.cloudflare.com:3478?transport=udp",
				"turn:turn.cloudflare.com:53?transport=udp",
				"turn:turn.cloudflare.com:3478?transport=tcp",
				"turn:turn.cloudflare.com:80?transport=tcp",
				"turns:turn.cloudflare.com:5349?transport=tcp",
				"turns:turn.cloudflare.com:443?transport=tcp"
      ],
			"username": "bc91b63e2b5d759f8eb9f3b58062439e0a0e15893d76317d833265ad08d6631099ce7c7087caabb31ad3e1c386424e3e",
			"credential": "ebd71f1d3edbc2b0edae3cd5a6d82284aeb5c3b8fdaa9b8e3bf9cec683e0d45fe9f5b44e5145db3300f06c250a15b4a0"
    },
  ],
});

```

The `ttl` value can be adjusted to expire the short lived key in a certain amount of time. This value should be larger than the time you'd expect the users to use the TURN service. For example, if you're using TURN for a video conferencing app, the value should be set to the longest video call you'd expect to happen in the app.

When using short-lived TURN credentials with WebRTC, credentials can be refreshed during a WebRTC session using the `RTCPeerConnection` [`setConfiguration()`](https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/setConfiguration) API.

## Revoke credentials

Short lived credentials can also be revoked before their TTL expires with a API call like this:

```bash
curl --request POST \
https://rtc.live.cloudflare.com/v1/turn/keys/$TURN_KEY_ID/credentials/$USERNAME/revoke \
--header "Authorization: Bearer $TURN_KEY_API_TOKEN"
```

---

# TURN Service

URL: https://developers.cloudflare.com/calls/turn/

Separately from the SFU, Calls offers a managed TURN service. TURN acts as a relay point for traffic between WebRTC clients like the browser and SFUs, particularly in scenarios where direct communication is obstructed by NATs or firewalls. TURN maintains an allocation of public IP addresses and ports for each session, ensuring connectivity even in restrictive network environments.

Using Cloudflare Calls TURN service is available free of charge when used together with the Calls SFU. Otherwise, it costs $0.05/real-time GB outbound from Cloudflare to the TURN client.

## Service address and ports

| Protocol      | Primary address     | Primary port | Alternate port |
| ------------- | ------------------- | ------------ | -------------- |
| STUN over UDP | stun.cloudflare.com | 3478/udp     | 53/udp         |
| TURN over UDP | turn.cloudflare.com | 3478/udp     | 53 udp         |
| TURN over TCP | turn.cloudflare.com | 3478/tcp     | 80/tcp         |
| TURN over TLS | turn.cloudflare.com | 5349/tcp     | 443/tcp        |

:::note[Note]
Use of alternate port 53 only by itself is not reccomended. Port 53 is blocked by many ISPs, and by popular browsers such as [Chrome](https://chromium.googlesource.com/chromium/src.git/+/refs/heads/master/net/base/port_util.cc#44) and [Firefox](https://github.com/mozilla/gecko-dev/blob/master/netwerk/base/nsIOService.cpp#L132). It is useful only in certain specific scenerios.
:::

## Regions

Calls TURN service is available in every Cloudflare data center.

When a client tries to connect to `turn.cloudflare.com`, it _automatically_ connects to the Cloudflare location closest to them. We achieve this using anycast routing.

To learn more about the architecture that makes this possible, read this [technical deep-dive about Calls](https://blog.cloudflare.com/cloudflare-calls-anycast-webrtc).

## Protocols and Ciphers for TURN over TLS

TLS versions supported include TLS 1.1, TLS 1.2, and TLS 1.3.

| OpenSSL Name                  | TLS 1.1 | TLS 1.2 | TLS 1.3 |
| ----------------------------- | ------- | ------- | ------- |
| AEAD-AES128-GCM-SHA256        | No      | No      | âœ…      |
| AEAD-AES256-GCM-SHA384        | No      | No      | âœ…      |
| AEAD-CHACHA20-POLY1305-SHA256 | No      | No      | âœ…      |
| ECDHE-ECDSA-AES128-GCM-SHA256 | No      | âœ…      | No      |
| ECDHE-RSA-AES128-GCM-SHA256   | No      | âœ…      | No      |
| ECDHE-RSA-AES128-SHA          | âœ…      | âœ…      | No      |
| AES128-GCM-SHA256             | No      | âœ…      | No      |
| AES128-SHA                    | âœ…      | âœ…      | No      |
| AES256-SHA                    | âœ…      | âœ…      | No      |

## MTU

There is no specific MTU limit for Cloudflare Calls TURN service.

## Limits

Cloudflare Calls TURN service places limits on:

- Unique IP address you can communicate with per relay allocation (>5 new IP/sec)
- Packet rate outbound and inbound to the relay allocation (>5-10 kpps)
- Data rate outbound and inbound to the relay allocation (>50-100 Mbps)

:::note[Limits apply to each TURN allocation independently]

Each limit is for a single TURN allocation (single TURN user) and not account wide. Same limit will apply to each user regardless of the number of unique TURN users.

:::

These limits are suitable for high-demand applications and also have burst rates higher than those documented above. Hitting these limits will result in packet drops.

---

# Replacing existing TURN servers

URL: https://developers.cloudflare.com/calls/turn/replacing-existing/

If you are a existing TURN provider but would like to switch to providing Cloudflare Calls TURN for your customers, there is a few considerations.

## Benefits

Cloudflare Calls TURN service can reduce tangible and untangible costs associated with TURN servers:

* Server costs (AWS EC2 etc)
* Bandwidth costs (Egress, load balancing etc)
* Time and effort to set up a TURN process and maintenance of server
* Scaling the servers up and down
* Maintain the TURN server with security and feature updates
* Maintain high availability

## Recommendations

### Separate environments with TURN keys

When using Cloudflare Calls TURN service at scale, consider separating environments such as "testing", "staging" or "production" with TURN keys. You can create up to 1,000 TURN keys in your account, which can be used to generate end user credentials.

There is no limit to how many end-user credentials you can create with a particular TURN key.

### Tag users with custom identifiers

Cloudflare Calls TURN service lets you tag each credential with a custom identifier as you generate a credential like below:

```bash null {4}
curl https://rtc.live.cloudflare.com/v1/turn/keys/$TURN_KEY_ID/credentials/generate \
--header "Authorization: Bearer $TURN_KEY_API_TOKEN" \
--header "Content-Type: application/json" \
--data '{"ttl": 864000, "customIdentifier": "user4523958"}'
```

Use this field to aggregate usage for a specific user or group of users and collect analytics.

### Monitor usage

You can monitor account wide usage with the [GraphQL analytics API](/calls/turn/analytics/). This is useful for keeping track of overall usage for billing purposes, watching for unexpected changes. You can get timeseries data from TURN analytics with various filters in place.

### Monitor for credential abuse

If you share TURN credentials with end users, credential abuse is possible. You can monitor for abuse by tagging each credential with custom identifiers and monitoring for top custom identifiers in your application via the [GraphQL analytics API](/calls/turn/analytics/).

## How to bill end users for their TURN usage

When billing for TURN usage in your application, it's crucial to understand and account for adaptive sampling in TURN analytics. This system employs adaptive sampling to efficiently handle large datasets while maintaining accuracy.

The sampling process in TURN analytics works on two levels:

* At data collection: Usage data points may be sampled if they are generated too quickly.
* At query time: Additional sampling may occur if the query is too complex or covers a large time range.

To ensure accurate billing, write a single query that sums TURN usage per customer per time period, returning a single value. Avoid using queries that list usage for multiple customers simultaneously.

By following these guidelines and understanding how TURN analytics handles sampling, you can ensure more accurate billing for your end users based on their TURN usage.

:::note


Cloudflare Calls only bills for traffic from Cloudflare's servers to your client, called `egressBytes`.


:::

### Example queries

:::caution[Incorrect approach example]


Querying TURN usage for multiple customers in a single query can lead to inaccurate results. This is because the usage pattern of one customer could affect the sampling rate applied to another customer's data, potentially skewing the results.


:::

```
query{
  viewer {
    usage: accounts(filter: { accountTag: "8846293bd06d1af8c106d89ec1454fe6" }) {
        callsTurnUsageAdaptiveGroups(
          filter: {
          datetimeMinute_gt: "2024-07-15T02:07:07Z"
          datetimeMinute_lt: "2024-08-10T02:07:05Z"
        }
          limit: 100
          orderBy: [customIdentifier_ASC]
        ) {
          dimensions {
            customIdentifier
          }
          sum {
            egressBytes
          }
        }
      }
    }
  }
```

Below is a query that queries usage only for a single customer.

```
query{
  viewer {
    usage: accounts(filter: { accountTag: "8846293bd06d1af8c106d89ec1454fe6" }) {
        callsTurnUsageAdaptiveGroups(
          filter: {
          datetimeMinute_gt: "2024-07-15T02:07:07Z"
          datetimeMinute_lt: "2024-08-10T02:07:05Z"
          customIdentifier: "myCustomer1111"
        }
          limit: 1
          orderBy: [customIdentifier_ASC]
        ) {
          dimensions {
            customIdentifier
          }
          sum {
            egressBytes
          }
        }
      }
    }
  }

```

---

# TURN Feature Matrix

URL: https://developers.cloudflare.com/calls/turn/rfc-matrix/

## TURN client to TURN server protocols

| Protocol | Support | Relevant specification                                                                                    |
| -------- | ------- | --------------------------------------------------------------------------------------------------------- |
| UDP      | âœ…       | [RFC 5766](https://datatracker.ietf.org/doc/html/rfc5766)                                                 |
| TCP      | âœ…       | [RFC 5766](https://datatracker.ietf.org/doc/html/rfc5766)                                                 |
| TLS      | âœ…       | [RFC 5766](https://datatracker.ietf.org/doc/html/rfc5766)                                                 |
| DTLS     | No      | [draft-petithuguenin-tram-turn-dtls-00](http://tools.ietf.org/html/draft-petithuguenin-tram-turn-dtls-00) |

## TURN client to TURN server protocols

| Protocol                                        | Support                                                                                                                        | Relevant specification                                                                                               |
| ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------- |
| TURN (base RFC)                                 | âœ…                                                                                                                              | [RFC 5766](https://datatracker.ietf.org/doc/html/rfc5766)                                                            |
| TURN REST API                                   | âœ… (See [FAQ](/calls/turn/faq/#does-cloudflare-calls-turn-support-the-expired-ietf-rfc-draft-draft-uberti-behave-turn-rest-00)) | [draft-uberti-behave-turn-rest-00](http://tools.ietf.org/html/draft-uberti-behave-turn-rest-00)                      |
| Origin field in TURN (Multi-tenant TURN Server) | âœ…                                                                                                                              | [draft-ietf-tram-stun-origin-06](https://tools.ietf.org/html/draft-ietf-tram-stun-origin-06)                         |
| ALPN support for STUN & TURN                    | âœ…                                                                                                                              | [RFC 7443](https://datatracker.ietf.org/doc/html/rfc7443)                                                            |
| TURN Bandwidth draft specs                      | No                                                                                                                             | [draft-thomson-tram-turn-bandwidth-01](http://tools.ietf.org/html/draft-thomson-tram-turn-bandwidth-01)              |
| TURN-bis (with dual allocation) draft specs     | No                                                                                                                             | [draft-ietf-tram-turnbis-04](http://tools.ietf.org/html/draft-ietf-tram-turnbis-04)                                  |
| TCP relaying TURN extension                     | No                                                                                                                             | [RFC 6062](https://datatracker.ietf.org/doc/html/rfc6062)                                                            |
| IPv6 extension for TURN                         | No                                                                                                                             | [RFC 6156](https://datatracker.ietf.org/doc/html/rfc6156)                                                            |
| oAuth third-party TURN/STUN authorization       | No                                                                                                                             | [RFC 7635](https://datatracker.ietf.org/doc/html/rfc7635)                                                            |
| DTLS support (for TURN)                         | No                                                                                                                             | [draft-petithuguenin-tram-stun-dtls-00](https://datatracker.ietf.org/doc/html/draft-petithuguenin-tram-stun-dtls-00) |
| Mobile ICE (MICE) support                       | No                                                                                                                             | [draft-wing-tram-turn-mobility-02](http://tools.ietf.org/html/draft-wing-tram-turn-mobility-02)                      |

---

# What is TURN?

URL: https://developers.cloudflare.com/calls/turn/what-is-turn/

## What is TURN?

TURN (Traversal Using Relays around NAT) is a protocol that assists in traversing Network Address Translators (NATs) or firewalls in order to facilitate peer-to-peer communications. It is an extension of the STUN (Session Traversal Utilities for NAT) protocol and is defined in [RFC 8656](https://datatracker.ietf.org/doc/html/rfc8656).

## How do I use TURN?

Just like you would use a web browser or cURL to use the HTTP protocol, you need to use a tool or a library to use TURN protocol in your application.

Most users of TURN will use it as part of a WebRTC library, such as the one in their browser or part of [Pion](https://github.com/pion/webrtc), [webrtc-rs](https://github.com/webrtc-rs/webrtc) or [libwebrtc](https://webrtc.googlesource.com/src/).

You can use TURN directly in your application too. [Pion](https://github.com/pion/turn) offers a TURN client library in Golang, so does [webrtc-rs](https://github.com/webrtc-rs/webrtc/tree/master/turn) in Rust.

## Key concepts to know when understanding TURN

1. **NAT (Network Address Translation)**: A method used by routers to map multiple private IP addresses to a single public IP address. This is commonly done by home internet routers so multiple computers in the same network can share a single public IP address.

2. **TURN Server**: A relay server that acts as an intermediary for traffic between clients behind NATs. Cloudflare Calls TURN service is a example of a TURN server.

3. **TURN Client**: An application or device that uses the TURN protocol to communicate through a TURN server. This is your application. It can be a web application using the WebRTC APIs or a native application running on mobile or desktop.

4. **Allocation**: When a TURN server creates an allocation, the TURN server reserves an IP and a port unique to that client.

5. **Relayed Transport Address**: The IP address and port reserved on the TURN server that others on the Internet can use to send data to the TURN client.

## How TURN Works

1. A TURN client sends an Allocate request to a TURN server.
2. The TURN server creates an allocation and returns a relayed transport address to the client.
3. The client can then give this relayed address to its peers.
4. When a peer sends data to the relayed address, the TURN server forwards it to the client.
5. When the client wants to send data to a peer, it sends it through the TURN server, which then forwards it to the peer.

## TURN vs VPN

TURN works similar to a VPN (Virtual Private Network). However TURN servers and VPNs serve different purposes and operate in distinct ways.

A VPN is a general-purpose tool that encrypts all internet traffic from a device, routing it through a VPN server to enhance privacy, security, and anonymity. It operates at the network layer, affects all internet activities, and is often used to bypass geographical restrictions or secure connections on public Wi-Fi.

A TURN server is a specialized tool used by specific applications, particularly for real-time communication. It operates at the application layer, only affecting traffic for applications that use it, and serves as a relay to traverse NATs and firewalls when direct connections between peers are not possible. While a VPN impacts overall internet speed and provides anonymity, a TURN server only affects the performance of specific applications using it.

## Why is TURN Useful?

TURN is often valuable in scenarios where direct peer-to-peer communication is impossible due to NAT or firewall restrictions. Here are some key benefits:

1. **NAT Traversal**: TURN provides a way to establish connections between peers that are both behind NATs, which would otherwise be challenging or impossible.

2. **Firewall Bypassing**: In environments with strict firewall policies, TURN can enable communication that would otherwise be blocked.

3. **Consistent Connectivity**: TURN offers a reliable fallback method when direct or NAT-assisted connections fail.

4. **Privacy**: By relaying traffic through a TURN server, the actual IP addresses of the communicating parties can be hidden from each other.

5. **VoIP and Video Conferencing**: TURN is crucial for applications like Voice over IP (VoIP) and video conferencing, ensuring reliable connections regardless of network configuration.

6. **Online Gaming**: TURN can help online games establish peer-to-peer connections between players behind different types of NATs.

7. **IoT Device Communication**: Internet of Things (IoT) devices can use TURN to communicate when they're behind NATs or firewalls.

---

# Cache Reserve

URL: https://developers.cloudflare.com/cache/advanced-configuration/cache-reserve/

import { Render, TabItem, Tabs } from "~/components";

Cache Reserve is a large, persistent data store [implemented on top of R2](/r2/). By pushing a single button in the dashboard, your websiteâ€™s cacheable content will be written to Cache Reserve. In the same way that [Tiered Cache](/cache/how-to/tiered-cache/) builds a hierarchy of caches between your visitors and your origin, Cache Reserve serves as the ultimate [upper-tier cache](/cache/how-to/tiered-cache/) that will reserve storage space for your assets for as long as you want. This ensures that your content is served from cache longer, shielding your origin from unneeded egress fees.

![Content served from origin and getting cached in Cache Reserve, and Edge Cache Data Centers (T1=upper-tier, T2=lower-tier) on its way back to the client](~/assets/images/cache/content-being-served.png)

How long content in Cache Reserve will be considered â€œfreshâ€ is determined by Edge Cache TTL setting or Cache-Control headers at your origin, if [Edge Cache TTL](/cache/how-to/edge-browser-cache-ttl/#edge-cache-ttl) is not set. After freshness expires, Cloudflare will attempt to revalidate the asset when a subsequent request arrives in Cache Reserve for the asset. This is the same behavior as in Cloudflare's regular CDN.

The retention period of an asset is how long we will keep the asset in Cache Reserve before marking it for eviction. If an asset is not requested within the retention period, it will be evicted from Cache Reserve. Accessing the asset will extend the retention period by one period. By default, the Cache Reserve retention period is 30 days.

Assets must [meet certain criteria](#cache-reserve-asset-eligibility) to use Cache Reserve.

Cache Reserve is a usage-based product and [pricing](#pricing) is detailed below. While Cache Reserve does require a paid plan, users can continue to use Cloudflareâ€™s CDN (without Cache Reserve) for free.

## Enable Cache Reserve

A paid Cache Reserve Plan is required for the enablement.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select a domain.
2. Go to **Caching** > **Cache Reserve**.
3. Select **Enable storage sync**.

</TabItem> <TabItem label="API">

Refer to the [Change Cache Reserve setting API](/api/resources/cache/subresources/cache_reserve/methods/edit/) for more information.

</TabItem> </Tabs>

:::note[Note]
You can pause Cache Reserve at any time. Pausing Cache Reserve means that Cloudflareâ€™s network will no longer use Cache Reserve to serve data, but resources will remain in storage until they are purged or expired.
:::

If you are an Enterprise customer and are interested in Cache Reserve, contact your account team to get help with your configuration.

## Cache Reserve asset eligibility

Not all assets are eligible for Cache Reserve. To be admitted into Cache Reserve, assets must:

- Be cacheable, according to Cloudflare's standard [cacheability factors](/cache),
- Have a freshness time-to-live (TTL) of at least 10 hours (set by any means such as Cache-Control / [CDN-Cache-Control](/cache/concepts/cache-control/) origin response headers, [Edge Cache TTL](/cache/how-to/edge-browser-cache-ttl/#edge-cache-ttl), [Cache TTL By Status](/cache/how-to/configure-cache-status-code/), or [Cache Rules](/cache/how-to/cache-rules/)),
- Have a Content-Length response header.
- When using [Image transformations](/images/manage-images/create-variants/), original files are eligible for Cache Reserve, but resized file variants are not eligible because transformations happen after Cache Reserve in the response flow.

## Limits

- Cache Reserve file limits are the same as [R2 limits](/r2/platform/limits/). Note that [CDN cache limits](/cache/concepts/default-cache-behavior/#customization-options-and-limits) still apply. Assets larger than standard limits will not be stored in the standard CDN cache, so these assets will incur Cache Reserve operations costs far more frequently.
- Origin Range requests are not supported at this time from Cache Reserve.
- Vary for Images is currently not compatible with Cache Reserve.
- Requests to [R2 public buckets linked to a zone's domain](/r2/buckets/public-buckets/) will not use Cache Reserve. Enabling Cache Reserve for the connected zone will use Cache Reserve only for requests not destined for the R2 bucket.
- Cache Reserve makes requests for uncompressed content directly from the origin. Unlike the standard Cloudflare CDN, Cache Reserve does not include the `Accept-Encoding: gzip` header when sending requests to the origin.
- Cache Reserve is bypassed when using the Cloudflare [O2O](/cloudflare-for-platforms/cloudflare-for-saas/saas-customers/how-it-works/) setup.

## Usage

Like the standard CDN, Cache Reserve also uses the `cf-cache-status` header to indicate cache statuses like `MISS`, `HIT`, and `REVALIDATED`. Cache Reserve cache misses and hits are factored into the dashboard's cache hit ratio.

Individual sampled requests that filled or were served by Cache Reserve are viewable via the [CacheReserveUsed](/logs/reference/log-fields/zone/http_requests/) Logpush field.

Cache Reserve monthly operations and storage usage are viewable in the dashboard.

## Pricing

Cache Reserve charges based on the total volume of data stored, along with two classes of operations on that data:

- [Class A operations](/r2/pricing/#class-a-operations) which are more expensive and tend to mutate state.
- [Class B operations](/r2/pricing/#class-b-operations) which tend to read existing state.

In most cases, a Cache Reserve miss will result in both one class A and one class B operation, and a Cache Reserve hit will result in one class B operation. Assets larger than 1 GB will incur more operations proportional to their size.

### Cache Reserve pricing

<table>
	<tbody>
		<th></th>
		<th>Rates</th>
		<tr>
			<td>Storage</td>
			<td>$0.015 / GB-month</td>
		</tr>
		<tr>
			<td>Class A Operations (writes)</td>
			<td>$4.50 / million requests</td>
		</tr>
		<tr>
			<td>Class B Operations (reads)</td>
			<td>$0.36 / million requests</td>
		</tr>
	</tbody>
</table>

<Render file="cache-reserve-billing-note" />

### Storage usage

Storage is billed using gigabyte-month (GB-month) as the billing metric. A GB-month is calculated by recording total bytes stored for the duration of the month.

For example:

- Storing 1 GB for 30 days will be charged as 1 GB-month.
- Storing 2 GB for 15 days will be charged as 1 GB-month.

### Operations

Operations are performed by Cache Reserve on behalf of the user to write data from the origin to Cache Reserve and to pass that data downstream to other parts of Cloudflareâ€™s network. These operations are managed internally by Cloudflare.

#### Class A operations (writes)

Class A operations are performed based on cache misses from Cloudflareâ€™s CDN. When a request cannot be served from cache, it will be fetched from the origin and written to cache reserve as well as our edge caches on the way back to the visitor.

#### Class B operations (reads)

Class B operations are performed when data needs to be fetched from Cache Reserve to respond to a miss in the edge cache.

#### Purge

Asset purges are free operations.

Cache Reserve will be instantly purged along with edge cache when you send a purge by URL request.

Other purge methods, such as purge by tag, host, prefix, or purge everything will force an attempt to revalidate on the subsequent request for the Cache Reserve asset. Note that assets purged this way will still incur storage costs until their retention TTL expires.

:::note
Note this differs from the standard CDN's purge by tag, host, or prefix features which force a cache miss, requiring the origin to deliver the asset in full.
:::

## Cache Reserve billing examples

#### Example 1

Assuming 1,000 assets (each 1 GB) are written to Cache Reserve at the start of the month and each asset is read 1,000 times, the estimated cost for the month would be:

|                    | Usage                                     | Billable Quantity | Price      |
| ------------------ | ----------------------------------------- | ----------------- | ---------- |
| Class B Operations | (1,000 assets) \* (1,000 reads per asset) | 1,000,000         | $0.36      |
| Class A Operations | (1,000 assets) \* (1 write per asset)     | 1,000             | $4.50      |
| Storage            | (1,000 assets) \* (1GB per asset)         | 1,000 GB-months   | $15.00     |
| **TOTAL**          |                                           |                   | **$19.86** |
|                    |                                           |                   |            |

<Render file="cache-reserve-billing-note" />

#### Example 2

Assuming 1,000,000 assets (each 1 MB) are in Cache Reserve, and:

- each asset expires and is rewritten into Cache Reserve 1 time per day
- each asset is read 2 times per day

the estimated cost for the month would be:

|                    | Usage                                                | Billable Quantity | Price       |
| ------------------ | ---------------------------------------------------- | ----------------- | ----------- |
| Class B Operations | (1,000,000 assets) \* (2 reads per day) \* (30 days) | 60,000,000        | $21.60      |
| Class A Operations | (1,000,000 assets) \* (1 write per day) \* (30 days) | 30,000,000        | $135.00     |
| Storage            | (1,000,000 assets) \* (1MB per asset)                | 1,000 GB-months   | $15.00      |
| **TOTAL**          |                                                      |                   | **$171.60** |
|                    |                                                      |                   |             |

<Render file="cache-reserve-billing-note" />

## Tips and best practices

Cache Reserve should be used with [Tiered Cache](/cache/how-to/tiered-cache/) enabled. Cache Reserve is designed for use with Tiered Cache enabled for maximum origin shielding. Using Cache Reserve without Tiered Cache may result in higher storage operation costs. Enabling Cache Reserve via the Cloudflare dashboard will check and provide a warning if you try to use Cache Reserve without Tiered Cache enabled.

## Cache Reserve Analytics

Cache Reserve Analytics provides insights regarding your Cache Reserve usage. It allows you to check what content is stored in Cache Reserve, how often it is being accessed, how long it has been there and how much egress from your origin it is saving you.

In the **Overview** section, under **Cache Reserve**, you have access to the following metrics:

- **Egress savings (bandwidth)** - is an estimation based on response bytes served from Cache Reserve that did not need to be served from your origin server. These are represented as cache hits.
- **Requests served by Cache Reserve** - is the number of requests served by Cache Reserve (total).
- **Data storage summary** - is based on a representative sample of requests. Refer to [Sampling](/analytics/graphql-api/sampling/) for more details about how Cloudflare samples data.
  - **Current data stored** - is the data stored (currently) over time.
  - **Aggregate storage usage** - is the total of storage used for the selected timestamp.
- **Operations** - [Class A](/cache/advanced-configuration/cache-reserve/#class-a-operations-writes) (writes) and [Class B](/cache/advanced-configuration/cache-reserve/#class-b-operations-reads) (reads) operations over time.

## Cache Reserve clear button

You can remove all data stored in Cache Reserve through the dashboard or via API. To clear your cache reserve:

- Cache Reserve must have already been enabled for the zone.
- Cache Reserve needs to be off.

Be aware that the deletion may take up to 24 hours to complete.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select a domain.
2. Go to **Caching** > **Cache Reserve**.
3. In **Delete Cache Reserve Data**, select **Delete Storage**.

</TabItem> <TabItem label="API">

To delete Cache Reserve data via API use the following example requests. For more information, refer to the [API documentation](/api/resources/cache/subresources/cache_reserve/methods/clear/).

```bash title="Request 1: Get Cache Reserve status"
curl https://api.cloudflare.com/client/v4/zones/{zone_id}/cache/cache_reserve \
--header "Authorization: Bearer <API_TOKEN>"
```

```json title="Response"
{
	"result": {
		"editable": true,
		"id": "cache_reserve",
		"value": "off"
	},
	"success": true,
	"errors": [],
	"messages": []
}
```

If Cache Reserve is turned off, you can proceed to the Cache Reserve Clear operation.

```bash title="Request 2: Start Cache Reserve Clear"
curl --request POST \
https://api.cloudflare.com/client/v4/zones/{zone_id}/cache/cache_reserve_clear \
--header "Authorization: Bearer <API_TOKEN>"
```

```json title="Response"
{
	"result": {
		"id": "cache_reserve_clear",
		"start_ts": "2024-06-02T10:00:00.12345Z",
		"state": "In-progress"
	},
	"success": true,
	"errors": [],
	"messages": []
}
```

</TabItem> </Tabs>

---

# Crawler Hints

URL: https://developers.cloudflare.com/cache/advanced-configuration/crawler-hints/

import { FeatureTable } from "~/components"

Crawler Hints aims to increase the proportion of relevant crawls and limit crawls that do not find fresh content to reduce the need for repeated crawls.

## Background

Search engines and similar services operate massive networks of bots that crawl the Internet to identify the content most relevant to a user query. Content on the web is always changing though, and search engine crawlers must continually wander the Internet and guess how frequently they should check a site for content updates.

With Crawler Hints, Cloudflare can proactively tell a crawler about the best time to index or when content changes. Additionally, Crawler Hints supports [IndexNow](https://www.indexnow.org/), which allows websites to notify search engines whenever content on their website content is created, updated, or deleted. Crawler Hints uses cache-status `MISS` to determine when content has likely been updated and sends it to IndexNow's crawler. If an asset's response has an HTTP status code greater than 4xx, the Crawler hints will not report that to [IndexNow](https://www.indexnow.org/).

## Benefits

For a website owner, Crawler Hints ensures that search engines and other bot-powered experiences have the freshest version of your content, translating into happier users and ultimately influencing search rankings.

Crawler Hints also means less traffic hitting your origin, improving resource consumption, site performance, and environmental impact.

## Availability

<FeatureTable id="cache.crawler_hints" />

## Enable Crawler Hints

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com) and select your domain.
2. Go to **Caching** > **Configuration**.
3. Enable **Crawler Hints**.

After enabling Crawler Hints, Cloudflare will begin sending hints to search engines about when they should crawl particular parts of your website.

## Prevent indexing for a specific page

When enabled, Crawler Hints is a global setting for your entire website. You can stop a specific page from being indexed by either:

* Having the origin server send through the header `X-Robots-Tag: noindex` on any pages that should not be indexed.
* Including `<meta name="robots" content="noindex, nofollow" />` in the HTML of any pages that should not be indexed.
* Creating a [Response header Transform Rule](/rules/transform/response-header-modification/) in Cloudflare to add the `X-Robots-Tag: noindex` header instead of doing it from the origin server.

---

# Early Hints

URL: https://developers.cloudflare.com/cache/advanced-configuration/early-hints/

import { FeatureTable } from "~/components"

Early Hints takes advantage of â€œserver think timeâ€ to asynchronously send instructions to the browser to begin loading resources while the origin server is compiling the full response. By sending these hints to a browser before the full response is prepared, the browser can figure out how to load the webpage faster for the end user.

Formally, Early Hints is a [web standard](https://httpwg.org/specs/rfc8297.html) that defines a new HTTP status code (103 Early Hints) that defines new interactions between a client and server. 103s are served to clients while a 200 OK (or error) response is prepared, which is the â€œserver think time.â€ You can enable Cloudflare's edge to cache and send 103 Early Hints responses with Link headers from your HTML pages. The response contains hints about which assets will likely be needed to fully render the webpage. This "hinting" speeds up page loads and generally reduces user-perceived latency.

:::note[Note]


Early Hints is currently only supported over HTTP/2 and HTTP/3.


:::

For more information about Early Hints, refer to the [Cloudflare](https://blog.cloudflare.com/early-hints) and [Google Chrome](https://developer.chrome.com/en/blog/early-hints/) blogs.

## Availability

<FeatureTable id="speed.early_hints" />

## Enable Early Hints

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com) and select your domain.
2. From the dashboard, select **Speed** > **Optimization**.
3. Go to the **Content Optimization** tab.
4. For **Early Hints**, toggle the switch to **On**.

## Generate Early Hints

Early Hints are only generated and cached:

* For URIs with `.html`, `.htm`, or `.php` file extensions, or no file extension
* On 200, 301, or 302 response return codes
* When the response contains [link headers](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Link) with preconnect or preload rel types, such as `Link: </img/preloaded.png>; rel=preload`

:::note


Early Hints cache entries are keyed by request URI and ignore query strings.


:::

## Emit Early Hints

Cloudflare will asynchronously look up and emit a cached 103 Early Hints response ahead of a main response.

Currently, only certain browser versions will take action to preload or preconnect on receiving Early Hints, such as Google Chrome M94 and higher. Instructions for running WebPageTest to experiment with compatible client browsers can be found in the [blog post](https://blog.cloudflare.com/early-hints/#testing-early-hints-with-web-page-test).

Additionally, keep the following in mind:

* Early Hints responses may be emitted before reaching the origin server or Worker. When Early Hints is enabled and pages on your site require authentication, unauthenticated visitors may receive a 103 response. The 103 response would contain cached Link headers and be sent before a 403 Forbidden response from your origin.
* Early Hints may be emitted less frequently on requests where the content is cacheable. Cloudflare CDN is more likely to retrieve a response header before the asynchronous Early Hints lookup finishes if the response has been cached. Cloudflare will not send a 103 response if the main response header is already available.
* Cloudflare currently disables Early Hints on some User-Agents, for example, select search crawler bots that show incompatibility with 1xx responses.
* You may see an influx of `504` responses with the `RequestSource` of `earlyHintsCache` in Cloudflare Logs when Early Hints is enabled, which is expected and benign. Requests from `earlyHintsCache` are internal subrequests for cached Early Hints, and they are neither end user requests, nor do they go to your origin. Their response status only indicates whether there are cached Early Hints for the request URI (`200` on cache HIT, `504` on cache MISS). These requests are already filtered out in other views, such as Cache Analytics. To filter out these requests or to filter requests by end users of your website only, please refer to [Filter end users](/analytics/graphql-api/features/filtering/#filter-end-users).

---

# Advanced configuration

URL: https://developers.cloudflare.com/cache/advanced-configuration/

import { DirectoryListing } from "~/components"

Learn how to complete different configuration options to customize your cache behavior.

<DirectoryListing />

---

# Query String Sort

URL: https://developers.cloudflare.com/cache/advanced-configuration/query-string-sort/

import { FeatureTable } from "~/components"

**Query String Sort** increases cache-hit rates by first sorting query strings into a consistent order before checking the Cloudflare cache.

By default, Cloudflareâ€™s cache treats resources as distinct if their URL query strings are in a different order.Â For instance, these resources are cached separately:

* `/video/48088296?title=0&byline=0&portrait=0&color=51a516`
* `/video/48088296?byline=0&color=51a516&portrait=0&title=0`

Query String Sort changes this behavior. If two query strings exist with the same name, the URL is sorted by the parameter value.Â For example:

`/example/file?word=alpha&word=beta` and `/example/file?word=beta&word=alpha`

would be sorted to:

`/example/file?word=alpha&word=beta`

## Availability

<FeatureTable id="cache.query_string_sort" />

***

## Enable Query String Sort

To enable Query String Sort:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com).
2. Select your account and zone.
3. Go to **Caching** > **Configuration**.
4. For **Enable Query String Sort**, switch the toggle to **On**.

***

## Unexpected behavior with WordPress admin pages

When a site or an application requires exact query string ordering, enabling Query String Sort might cause unexpected behavior.

For example in the WordPress admin UI, you might notice any of the following behaviors:

* No media appear in the Media Library
* Inability to customize the site via **Appearance** > **Customize**
* Inability to drag any widget to a sidebar in **Appearance** > **Widgets**
* Inability to edit menus in **Appearance** > **Menus**

To understand why this happens, note that WordPressÂ [concatenates JavaScript files](https://developer.wordpress.org/advanced-administration/wordpress/wp-config/#disable-javascript-concatenation)Â to speed up the administration interface. The way WordPress implements this involves multiple occurrences ofÂ `load[]`Â parameters in the query string, where the order of those parameters is crucial.

:::note


Note that more recent versions of WordPress may not experience this issue, as a patch has been implemented in WordPress since 2019. The patch can be found at [WordPress Core Trac Changeset 45456](https://core.trac.wordpress.org/changeset/45456).


:::

### Identify the problem

The screenshot below shows an example where resources in the Media Library are not rendered correctly and the browser debugging console reveals that the page is throwing an error:

![Resources in the Media Library are not rendered correctly](~/assets/images/support/media_library_enabling_query.png)

When the pageÂ `load-scripts.php`Â loads, the browser sends a request to Cloudflare for:

```txt
/wp-admin/load-scripts.php?c=0&load%5B%5D=hoverIntent,common,admin-bar,underscore,shortcode,backbone,wp-util,wp-backbone,media-models,wp-plupload,wp-mediaelement,wp-api-r&load%5B%5D=equest,media-views,media-editor,media-audiovideo,mce-view,imgareaselect,image-edit,media-grid,media,svg-painter&ver=5.0.3
```

With Query String Sort enabled, Cloudflare will then sort the parameters and values in the request query string, resulting in the following:

```txt
/wp-admin/load-scripts.php?c=0&load%5B%5D=equest,media-views,media-editor,media-audiovideo,mce-view,imgareaselect,image-edit,media-grid,media,svg-painter&load%5B%5D=hoverIntent,common,admin-bar,underscore,shortcode,backbone,wp-util,wp-backbone,media-models,wp-plupload,wp-mediaelement,wp-api-r&ver=5.0.3
```

Note that theÂ `load[]`Â parameters were swapped, asÂ `equest`Â should come beforeÂ `hoverIntent`Â when alphabetically ordered.

When this happens, you will most likely find errors in the browser console, such as:

`_____ is not defined at load-scripts.php?c=0&load[]=...`

This type of error indicates that Query String Sort is inadvertently breaking some WordPress admin page functionality.

After sorting, the query then goes to Cloudflare's cache infrastructure (and to the origin server, if the resource is not in the Cloudflare cache or is not cacheable). The origin server then serves the concatenated scripts, which are ordered differently. Because scripts might depend on other scripts, this process might break dependencies.

### Respond to the issue

Start by analyzing your site or application behavior around the use of query strings. Do you have assets served with multiple possible arrangements of query strings?

For example, you might have an image resizing endpoint or a search form, where the order of query parameters might vary - such as width, height, and version -Â yet a unique parameter combination points to a single relevant asset.

To minimize problems, consider:

* Disabling **Query String Sort** for the site if youâ€™re sure that this feature does not add value to any part of your site. Cloudflare disables this option by default in the **Caching** app.
* Use Cache Rules to enable **Query String Sort** (set **Cache key** > **Sort query string**: `On`) for URLs where preserving the query string parameter order is not important.
* Alternatively, use Cache Rules to disable **Query String Sort** for URLs where a specific parameter order is required. For example, set **Cache key** > **Sort query string**: `Off` for URI paths starting with `/wp-admin/load-scripts.php`, or for any URLs with similar requirements.

To learn more about Cache Rules, visitÂ [Cache Rules](/cache/how-to/cache-rules/).

***

## Related resources

* [Increasing Cache Hit Rates with Query String Sort](https://blog.cloudflare.com/increasing-cache-hit-rates-with-query-string-sort/)
* [Best Practice: Caching Everything While Ignoring Query Strings](/cache/how-to/cache-rules/examples/cache-everything-ignore-query-strings/)

---

# Vary for images

URL: https://developers.cloudflare.com/cache/advanced-configuration/vary-for-images/

import { Details, FeatureTable } from "~/components"

`Vary` is an HTTP response header that allows origins to serve variants of the same content that can be used depending on the browser sending the request.

Cloudflare sits in between the browser and the origin. When Cloudflare receives the originâ€™s response, the specific image variant is cached so that subsequent requests from browsers with the same image preferences can be served from cache. This also means that serving multiple image variants for the same asset will create distinct cache entries.

`Vary` for Images reduces the content-negotiation process by parsing a requestâ€™s `Accept` header, which is sent to the origin to deliver the correct content to the browser.

Vary for images is available for Pro, Business, and Enterprise customers.

## Availability

<FeatureTable id="cache.vary_for_images" />

## File extensions

You can use vary for images on the file extensions below if the origin server sends the `Vary: Accept` response header. If the origin server sends `Vary: Accept` but does not serve the set variant, the response is not cached and displays `BYPASS` in the cache status in the response header. Additionally, the list of variant types the origin serves for each extension must be configured so that Cloudflare decides which variant to serve without contacting the origin server.


<Details header="File extensions enabled for varying">
  <ul>
    <li>.avif</li>
    <li>.bmp</li>
    <li>.gif</li>
    <li>.jpg</li>
    <li>.jpeg</li>
    <li>.jp2</li>
    <li>.png</li>
    <li>.tif</li>
    <li>.tiff</li>
    <li>.webp</li>
  </ul>
</Details>

## Enable vary for images

Vary for Images is enabled through Cloudflareâ€™s API by creating a variants rule. In the examples below, learn how to serve JPEG, WebP, and AVIF variants for `.jpeg` and `.jpg` extensions.

### Create a variants rule

```bash
curl --request PATCH \
"https://api.cloudflare.com/client/v4/zones/{zone_id}/cache/variants" \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{"value":{"jpeg":["image/webp","image/avif"],"jpg":["image/webp","image/avif"]}}'
```

### Modify to only allow WebP variants

```bash
curl --request PATCH \
"https://api.cloudflare.com/client/v4/zones/{zone_id}/cache/variants" \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{"value":{"jpeg":["image/webp"],"jpg":["image/webp"]}}'
```

### Delete the rule

```bash
curl --request DELETE \
"https://api.cloudflare.com/client/v4/zones/{zone_id}/cache/variants" \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>"
```

### Get the rule

```bash
curl "https://api.cloudflare.com/client/v4/zones/{zone_id}/cache/variants" \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>"
```

To learn more about purging varied images, refer to [Purge varied images](/cache/how-to/purge-cache/purge-varied-images/).

## Limitations

* For Vary for images to work, your image URLs must include the file extension in the path and not the query string. For example the URL `https://example.com/image.jpg` is compatible but `https://example.com/index.php?file=image.jpg` is not compatible.
* Your origin must return an image type matching the file extension in the URL when a HTTP client sends no `Accept` header, or an `Accept: */*` header. Otherwise, you will see `CF-Cache-Status: BYPASS` in the HTTP response headers.

---

# Avoid web cache poisoning

URL: https://developers.cloudflare.com/cache/cache-security/avoid-web-poisoning/

A cache poisoning attack uses an HTTP request to trick an origin web server into responding with a harmful resource that has the same cache key as a clean request. As a result, the poisoned resource gets cached and served to other users.

A Content Delivery Network (CDN) like Cloudflare relies on cache keys to compare new requests against cached resources. The CDN then determines whether the resource should be served from the cache or requested directly from the origin web server.

## Learn about Cache Poisoning

To deepen your understanding of the risks and vulnerabilities associated with cache poisoning, consult the following resources:

* [Practical Web Cache Poisoning](https://portswigger.net/blog/practical-web-cache-poisoning)
* [How Cloudflare protects customers from cache poisoning](https://blog.cloudflare.com/cache-poisoning-protection/)

## Only cache files that are truly static

Review the caching configuration for your origin web server and ensure you are caching files that are static and do not depend on user input in any way. To learn more about Cloudflare caching, review:

* [Which file extensions does Cloudflare cache for static content?](/cache/concepts/default-cache-behavior/)
* [How Do I Tell Cloudflare What to Cache?](/cache/how-to/cache-rules/)

## Do not trust data in HTTP headers

Client-side vulnerabilities are often exploited through HTTP headers, including cross-site scripting (XSS). In general, you should not trust the data in HTTP headers and as such:

* Do not rely on values in HTTP headers if they are not part of your [cache key](/cache/how-to/cache-keys/).
* Never return HTTP headers to users in cached content.

## Do not trust GET request bodies

Cloudflare caches contents of GET request bodies, but they are not included in the cache key. GET request bodies should be considered untrusted and should not modify the contents of a response. If a GET body can change the contents of a response, consider bypassing cache or using a POST request.

## Monitor web security advisories

To keep informed about Internet security threats, Cloudflare recommends that you monitor web security advisories on a regular basis. Some of the more popular advisories include:

* [Drupal Security Advisories](https://www.drupal.org/security)
* [Symfony Security Advisories](https://symfony.com/blog/category/security-advisories)
* [Laminas Security Advisories](https://getlaminas.org/security/advisories)

---

# Cache Deception Armor

URL: https://developers.cloudflare.com/cache/cache-security/cache-deception-armor/

Before learning about Cache Deception Armor, you should first understand how Web Cache Deception attacks work.

## Web Cache Deception attacks

Web Cache Deceptions attacks occur when an attacker tricks a user into opening a link in the format of `http://www.example.com/newsfeed/foo.jpg`, when `http://www.example.com/newsfeed` is the location of a dynamic script that returns different content for different users.

This scenario becomes problematic when your website is configured to be flexible about what kinds of paths it can handle. To be more specific, when requests to a path that do not exist, such as `/x/y/z` are treated as equivalent to requests to a parent path that does exist `/x`.

For example, an attacker could send a user a link to `http://www.example.com/newsfeed/foo.jpg` so that the user could be taken to their newsfeed. When the request passes through Cloudflare, the request would be cached because the path ends in `.jpg`. The attacker can then visit the same URL themselves, and their request will be served from Cloudflare's cache, exposing your user's sensitive content.

## Cache Deception Armor protects against attacks

You can protect users from Web Cache Deception attacks by [creating a cache rule](/cache/cache-security/cache-deception-armor/#enable-cache-deception-armor). With this rule, you can continue to cache static assets, but the rule will verify a URL's extension matches the returned `Content-Type`.

In the newsfeed example above, if `http://www.example.com/newsfeed` is a script that outputs a webpage, the `Content-Type` is `text/html`. On the other hand, `http://www.example.com/newsfeed/foo.jpg` is expected to have `image/jpeg` as `Content-Type`. When a mismatch that could result in a Web Cache Deception attack is found, Cloudflare does not cache the response.

### Exceptions

* If the returned `Content-Type` is `application/octet-stream`, the extension does not matter because that is typically a signal to instruct the browser to save the asset instead of to display it.
* Cloudflare allows `.jpg` to be served as `image/webp` or `.gif` as `video/webm` and other cases that we think are unlikely to be attacks.
* Keep in mind that Cache Deception Armor depends upon [Origin Cache Control](/cache/concepts/cache-control/). A `Cache-Control` header from the origin, [Edge Cache TTL Cache Rule](/cache/how-to/cache-rules/settings/#edge-ttl) or [Browser Cache TTL](/cache/how-to/edge-browser-cache-ttl/set-browser-ttl/) zone setting may override the protection.

## Enable Cache Deception Armor

To enable Cache Deception Armor, you need to start by creating a [cache rule](/cache/how-to/cache-rules/). Follow the steps below for guidance:

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com), and select your account and domain.
2. Go to **Caching** > **Cache Rules**.
3. Select **Create rule**.
4. Under **When incoming requests match**, define the [rule expression](/ruleset-engine/rules-language/expressions/edit-expressions/#expression-builder).
5. Under **Then**, in the **Cache eligibility** section, select **Eligible for cache**.
6. Add the **Cache Key** setting to the rule and turn on **Cache deception armor**.
7. To save and deploy your rule, select **Deploy**. If you are not ready to deploy your rule, select **Save as Draft**.

---

# Cross-Origin Resource Sharing (CORS)

URL: https://developers.cloudflare.com/cache/cache-security/cors/

A cross-origin request is a request for website resources external to the origin. For example, `a.example.com` attempts to serve resources from `b.secondexample.com`. CORS instructs the browser to determine if a cross-origin request, such as an image or JavaScript from `b.secondexample.com`, is allowed by `a.example.com`. The browser does not load resources that are disallowed by CORS.

Cloudflare supports CORS by:

* Identifying cached assets based on the `Host` Header, `Origin` Header, URL path, and query. This allows different resources to use the same `Host` header but different `Origin` headers.
* Passing `Access-Control-Allow-Origin` headers from the origin server to the browser.

The `Access-Control-Allow-Origin` header allows servers to specify rules for sharing their resources with external domains. When a server receives a request to access a resource, it responds with a value for the `Access-Control-Allow-Origin` header. `Access-Control-Allow-Origin` headers are often applied to [cacheable content](/cache/concepts/default-cache-behavior/). A web server may respond with different `Access-Control` headers depending on the `Origin` header sent in the request.

## Add or change CORS headers

If you add or change CORS configuration at your origin web server, purging the Cloudflare cache by URL does not update the CORS headers. Force Cloudflare to retrieve the new CORS headers via one of the following options:

* Change the filename or URL to bypass cache to instruct Cloudflare to retrieve the latest CORS headers.
* Use the [single-file purge API](/api/resources/cache/methods/purge/#purge-cached-content-by-url) to specify the appropriate CORS headers along with the purge request.
* Update the resourceâ€™s last-modified time at your origin web server. Then, complete a [full purge](/cache/how-to/purge-cache/purge-everything/) to retrieve the latest version of your assets including updated CORS headers.

---

# Cache security

URL: https://developers.cloudflare.com/cache/cache-security/

import { DirectoryListing } from "~/components"

Review the following content to learn more about cache security.

<DirectoryListing />

---

# Head Requests and Set-Cookie Headers

URL: https://developers.cloudflare.com/cache/concepts/cache-behavior/

In this page, we document how Cloudflare's cache system behaves in interaction with:

* `HEAD` requests
* `Set-Cookie` response headers

## Interaction of `HEAD` requests with Cache

Cloudflare converts `HEAD` requests to `GET` requests for cacheable requests.

When you make a `HEAD` request for a cacheable resource and Cloudflare does not have that resource in the edge cache, a cache miss happens. Cloudflare will send a `GET` request to your origin, cache the full response and return the response headers only. Make sure the origin server is setup to handle `GET` requests, even if only `HEAD` requests are expected, so that compatibility with this behavior is ensured.

## Interaction of `Set-Cookie` response header with Cache

For non-cacheable requests, `Set-Cookie` is always preserved. For cacheable requests, there are three possible behaviors:

* `Set-Cookie` is returned from origin and the default cache level is used. If [origin cache control](/cache/concepts/cache-control/) is not enabled, Cloudflare removes the `Set-Cookie` and caches the asset. If origin cache control is enabled, Cloudflare does not cache the asset and preserves the `Set-Cookie`. A cache status of `BYPASS` is returned.

* `Set-Cookie` is returned from origin and the cache level is set to `Cache Everything` in Page Rules, or `Eligible for cache` in Cache Rules. In this case, Cloudflare preserves the `Set-Cookie` but does not cache the asset. A cache `MISS` will be returned every time.

* `Set-Cookie` is returned from origin, the cache level is set to `Cache Everything` in Page Rules, or `Eligible for cache` in Cache Rules, and edge cache TTL is set. In this case, Cloudflare removes the `Set-Cookie` and the asset is cached.

---

# Origin Cache Control

URL: https://developers.cloudflare.com/cache/concepts/cache-control/

import { Details } from "~/components"

Origin Cache Control is a Cloudflare feature. When enabled on an Enterprise customer's website, it indicates that Cloudflare should strictly respect `Cache-Control` directives received from the origin server. Free, Pro and Business customers have this feature enabled by default.

`Cache-Control` directives in the HTTP response from your origin server provide specific [caching instructions](https://datatracker.ietf.org/doc/html/rfc7234) to intermediary services like Cloudflare.

With the Origin Cache Control feature enabled, `Cache-Control` directives present in the origin server's response will be followed as specified. For example, if the response includes a `max-age` directive of 3,600 seconds, Cloudflare will cache the resource for that duration before checking the origin server again for updates.

Cloudflare's [Cache Rules](/cache/how-to/cache-rules/) allows users to either augment or override an origin server's `Cache-Control` headers or [default policies](/cache/concepts/default-cache-behavior/) set by Cloudflare.

In the following sections, we will provide more details regarding:

* The most common `Cache-Control` directives.
* How to enable Origin Cache Control.
* How Origin Cache Control behaves with `Cache-Control` directives.
* How other Cloudflare products interact with `Cache-Control` directives.

## `Cache-control` directives

A `Cache-Control` header can include a number of directives, and the directive dictates who can cache a resource along with how long those resources can be cached before they must be updated.

:::note[Note]


For more information about `Cache-Control` directives at origin servers, refer to the [Mozilla Cache-Control documentation](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control).


:::

If multiple directives are passed together, each directive is separated by a comma. If the directive takes an argument, it follows the directive separated by an equal sign. For example: `max-age=86400`.

Directives can be broken down into four groups: [cacheability](/cache/concepts/cache-control/#cacheability), [expiration](/cache/concepts/cache-control/#expiration), [revalidation](/cache/concepts/cache-control/#revalidation), and [other](/cache/concepts/cache-control/#other).

### Cacheability

Cacheability refers to whether or not a resource should enter a cache, and the directives below indicate a resource's cacheability.

* `public` â€” Indicates any cache may store the response, even if the response is normally non-cacheable or cacheable only within a private cache.
* `private` â€” Indicates the response message is intended for a single user, such as a browser cache, and must not be stored by a shared cache like Cloudflare or a corporate proxy.
* `no-store` â€” Indicates any cache, such as a client or proxy cache, must not store any part of either the immediate request or response.

### Expiration

Expiration refers to how long a resource should remain in the cache, and the directives below affect how long a resource stays in the cache.

:::note[Note]


Cloudflare respects whichever value is higher: the [Browser Cache TTL](/cache/how-to/edge-browser-cache-ttl/) in Cloudflare or the `max-age` header. You can also simultaneously specify a Cloudflare Edge Cache TTL different than a Browser's Cache TTL respectively via the `s-maxage` and `max-age` `Cache-Control` headers.

When using Origin Cache Control and setting `max-age=0`, Cloudflare prefers to cache and revalidate. With Origin Cache Control off and `max-age=0`, Cloudflare will bypass cache.

When setting `no-cache` with Origin Cache Control off, Cloudflare does not cache. When setting `no-cache` with Origin Cache Control on, Cloudflare caches and always revalidates.


:::

* `max-age=seconds` â€” Indicates the response is stale after its age is greater than the specified number of seconds. Age is defined as the time in seconds since the asset was served from the origin server. The `seconds` argument is an unquoted integer.
* `s-maxage=seconds` â€” Indicates that in shared caches, the maximum age specified by this directive overrides the maximum age specified by either the `max-age` directive or the `Expires` header field. The `s-maxage` directive also implies the semantics of the proxy-revalidate response directive. Browsers ignore `s-maxage`.
* `no-cache` â€” Indicates the response cannot be used to satisfy a subsequent request without successful validation on the origin server. This allows an origin server to prevent a cache from using the origin to satisfy a request without contacting it, even by caches that have been configured to send stale responses.

Ensure the HTTP `Expires` header is set in your origin server to use Greenwich Mean Time (GMT) as stipulated in [RFC 2616](https://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.3 "3.3.1 Full Date").

### Revalidation

Revalidation determines how the cache should behave when a resource expires, and the directives below affect the revalidation behavior.

* `must-revalidate` â€” Indicates that once the resource is stale, a cache (client or proxy) must not use the response to satisfy subsequent requests without successful validation on the origin server.
* `proxy-revalidate` â€” Has the same meaning as the `must-revalidate` response directive except that it does not apply to private client caches.
* `stale-while-revalidate=<seconds>` â€” When present in an HTTP response, indicates caches may serve the response in which it appears after it becomes stale, up to the indicated number of seconds since the resource expired. If [Always Online](/cache/how-to/always-online/) is enabled, then the `stale-while-revalidate` and `stale-if-error` directives are ignored. This directive is not supported when using the Cache API methods `cache.match` or `cache.put`. For more information, refer to the [Worker's documentation for Cache API](/workers/platform/limits/#cache-api-limits).
* `stale-if-error=<seconds>` â€” Indicates that when an error is encountered, a cached stale response may be used to satisfy the request, regardless of other freshness information. To avoid this behavior, include `stale-if-error=0` directive with the object returned from the origin. This directive is not supported when using the Cache API methods `cache.match` or `cache.put`. For more information, refer to the [Worker's documentation for Cache API](/workers/platform/limits/#cache-api-limits).

The `stale-if-error` directive is ignored if [Always Online](/cache/how-to/always-online/) is enabled or if an explicit in-protocol directive is passed. Examples of explicit in-protocol directives include a `no-store` or `no-cache cache` directive, a `must-revalidate` cache-response-directive, or an applicable `s-maxage` or `proxy-revalidate` cache-response-directive.

### Other

Additional directives that influence cache behavior are listed below.

* `no-transform` â€” Indicates that an intermediary â€” regardless of whether it implements a cache â€” must not transform the payload.
* `vary` â€” Cloudflare does not consider vary values in caching decisions. Nevertheless, vary values are respected when [Vary for images](/cache/advanced-configuration/vary-for-images/) is configured and when the vary header is [`vary: accept-encoding`](/speed/optimization/content/compression/).
* `immutable` â€” Indicates to clients the response body does not change over time. The resource, if unexpired, is unchanged on the server. The user should not send a conditional revalidation request, such as `If-None-Match` or `If-Modified-Since`, to check for updates, even when the user explicitly refreshes the page. This directive has no effect on public caches like Cloudflare, but does change browser behavior.

## Enable Origin Cache Control

If you enable Origin Cache Control, Cloudflare will aim to strictly adhere to [RFC 7234](https://datatracker.ietf.org/doc/html/rfc7234). Enterprise customers have the ability to select if Cloudflare will adhere to this behavior, enabling or disabling Origin Cache Control for their websites through cache rules in the [dashboard](/cache/how-to/cache-rules/settings/#origin-cache-control-enterprise-only) or via [API](/cache/how-to/cache-rules/settings/#origin-cache-control-enterprise-only). Free, Pro, and Business customers have this option enabled by default and cannot disable it.

## Origin Cache Control behavior

The following section covers the directives and behavioral conditions associated with enabling or disabling Origin Cache Control.

### Directives

The table below lists directives and their behaviors when Origin Cache Control is disabled and when it is enabled.

<table>
  <tbody>
    <th colspan="5" rowspan="1">
      Directive
    </th>
    <th colspan="5" rowspan="1">
      Origin Cache Control disabled behavior
    </th>
    <th colspan="5" rowspan="1">
      Origin Cache Control enabled behavior
    </th>
    <tr>
      <td colspan="5" rowspan="1">
        <code>s-maxage=0</code>
      </td>
      <td colspan="5" rowspan="1">
        Will not cache.
      </td>
      <td colspan="5" rowspan="1">
        Caches and always revalidates
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        <code>max-age=0</code>
      </td>
      <td colspan="5" rowspan="1">
        Will not cache.
      </td>
      <td colspan="5" rowspan="1">
        Caches and always revalidates.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        <code>no-cache</code>
      </td>
      <td colspan="5" rowspan="1">
        Will not cache.
      </td>
      <td colspan="5" rowspan="1">
        Caches and always revalidates. Does not serve stale.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        <code>no-cache=&lt;headers&gt;</code>
      </td>
      <td colspan="5" rowspan="1">
        Will not cache.
      </td>
      <td colspan="5" rowspan="1">
        Caches if headers mentioned in <code>no-cache=&lt;headers&gt;</code> do not exist. Always
        revalidates if any header mentioned in <code>no-cache=&lt;headers&gt;</code> is present.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        <code>Private=&lt;headers&gt;</code>
      </td>
      <td colspan="5" rowspan="1">
        Will not cache.
      </td>
      <td colspan="5" rowspan="1">
        Does not cache <code>&lt;headers&gt;</code> values mentioned in <code>Private=&lt;headers&gt;</code> directive.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        <code>must-revalidate</code>
      </td>
      <td colspan="5" rowspan="1">
        Cache directive is ignored and stale is served.
      </td>
      <td colspan="5" rowspan="1">
        Does not serve stale. Must revalidate for CDN and for browser.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        <code>proxy-revalidate</code>
      </td>
      <td colspan="5" rowspan="1">
        Cache directive is ignored and stale is served.
      </td>
      <td colspan="5" rowspan="1">
        Does not serve stale. Must revalidate for CDN but not for browser.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        <code>no-transform</code>
      </td>
      <td colspan="5" rowspan="1">
        May (un)Gzip, Polish, email filter, etc.
      </td>
      <td colspan="5" rowspan="1">
        Does not transform body.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        <code>s-maxage=delta, delta>1</code>
      </td>
      <td colspan="5" rowspan="1">
        Same as <code>max-age</code>.
      </td>
      <td colspan="5" rowspan="1">
        <code>Max-age</code> and <code>proxy-revalidate</code>.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        <code>immutable</code>
      </td>
      <td colspan="5" rowspan="1">
        Not proxied downstream.
      </td>
      <td colspan="5" rowspan="1">
        Proxied downstream. Browser facing, does not impact caching proxies.
      </td>
    </tr>
  </tbody>
</table>

### Conditions

Certain scenarios also affect Origin Cache Control behavior when it is enabled or disabled.

<table>
  <tbody>
    <th colspan="5" rowspan="1">
      Condition
    </th>
    <th colspan="5" rowspan="1">
      Origin Cache Control disabled behavior
    </th>
    <th colspan="5" rowspan="1">
      Origin Cache Control enabled behavior
    </th>
    <tr>
      <td colspan="5" rowspan="1">
        Presence of <code>Authorization</code> header.
      </td>
      <td colspan="5" rowspan="1">
        Content may be cached.
      </td>
      <td colspan="5" rowspan="1">
        Content is cached only if <code>must-revalidate</code>, <code>public</code>, or <code>s-maxage</code> is also present.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        Use of <code>no-cache</code> header.
      </td>
      <td colspan="5" rowspan="1">
        In logs, <code>cacheStatus=miss</code>.
      </td>
      <td colspan="5" rowspan="1">
        In logs, <code>cacheStatus=bypass</code>.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        Origin response has <code>Set-Cookie</code> header and default cache level is used.
      </td>
      <td colspan="5" rowspan="1">
        Content may be cached with stripped <code>set-cookie</code> header.
      </td>
      <td colspan="5" rowspan="1">
        Content is not cached.
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        Browser Cache TTL is set.
      </td>
      <td colspan="5" rowspan="1">
        `Cache-Control` returned to eyeball does not include <code>private</code>.
      </td>
      <td colspan="5" rowspan="1">
        If origin returns <code>private</code> in `Cache-Control` then preserve it.
      </td>
    </tr>
  </tbody>
</table>

## Examples

Review the examples below to learn which directives to use with the `Cache-Control` header to control specific caching behavior.


<Details header="Cache a static asset.">

`Cache-Control: public, max-age=86400`


</Details>


<Details header="Ensure a secret asset is never cached.">

`Cache-Control: no-store`


</Details>


<Details header="Cache assets on browsers but not on proxy cache.">

`Cache-Control: private, max-age=3600`


</Details>


<Details header="Cache assets in client and proxy caches, but prefer revalidation when serve.">

`Cache-Control: public, no-cache`


</Details>


<Details header="Cache assets in proxy caches but REQUIRE revalidation by the proxy when serve.">

`Cache-Control: public, no-cache, proxy-revalidate` or `Cache-Control: public, s-maxage=0`


</Details>


<Details header="Cache assets in proxy caches, but REQUIRE revalidation by any cache when serve.">

`Cache-Control: public, no-cache, must-revalidate`


</Details>


<Details header="Cache assets, but ensure the proxy does not modify it.">

`Cache-Control: public, no-transform`

This configuration also disables transformation like gzip or brotli compression from our edge to your visitors if the original payload was served uncompressed.


</Details>


<Details header="Cache assets with revalidation, but allow stale responses if origin server is unreachable.">

`Cache-Control: public, max-age=3600, stale-if-error=60`

With this configuration, Cloudflare attempts to revalidate the content with the origin server after it has been in cache for 3600 seconds (one hour). If the server returns an error instead of proper revalidation responses, Cloudflare continues serving the stale resource for a total of one minute beyond the expiration of the resource.


</Details>


<Details header="Cache assets for different amounts of time on Cloudflare and in visitor browsers.">

`Cache-Control: public, max-age=7200, s-maxage=3600`


</Details>


<Details header="Cache an asset and serve while asset is being revalidated.">

`Cache-Control: max-age=600, stale-while-revalidate=30`

This configuration indicates the asset is fresh for 600 seconds. The asset can be served stale for up to an additional 30 seconds to parallel requests for the same resource while the initial synchronous revalidation is attempted.


</Details>

## Interaction with other Cloudflare features

In this section, we provide details regarding how other Cloudflare features interact with `Cache-Control` directives.

### Edge Cache TTL

[Edge Cache TTL](/cache/how-to/edge-browser-cache-ttl/#edge-cache-ttl) Cache Rules override `s-maxage` and disable revalidation directives if present. When Origin Cache Control is enabled at Cloudflare, the original `Cache-Control` header passes downstream from our edge even if Edge Cache TTL overrides are present. Otherwise, when Origin Cache Control is disabled at Cloudflare, Cloudflare overrides the Origin Cache Control.

### Browser Cache TTL

[Browser Cache TTL](/cache/how-to/edge-browser-cache-ttl/#browser-cache-ttl) Cache Rules override `max-age` settings passed downstream from our edge, typically to your visitor's browsers.

### Polish

[Polish](/images/polish/) is disabled when the `no-transform` directive is present.

### Gzip and Other Compression

Compression is disabled when the `no-transform` directive is present. If the original asset fetched from the origin is compressed, it is served compressed to the visitor. If the original asset is uncompressed, compression is not applied.

---

# Cloudflare cache responses

URL: https://developers.cloudflare.com/cache/concepts/cache-responses/

The `CF-Cache-Status` header output indicates whether a resource is cached or not. To investigate cache responses returned by this header, use services like [Redbot](https://redbot.org/), [webpagetest.org](http://www.webpagetest.org/), or a visual tool like [Chrome's Dr. Flare plugin](https://community.cloudflare.com/t/community-tip-dr-flare-debug-tool-for-cloudflare-chrome-extension/110166).

Below you can find a comprehensive breakdown of Cloudflare's cache response statuses.

## HIT

The resource was found in Cloudflare's cache.

## MISS

The resource was not found in Cloudflare's cache and was served from the origin web server.

## NONE/UNKNOWN

Cloudflare generated a response that denotes the asset is not eligible for caching. This may have happened because:

* A Worker generated a response without sending any subrequests. In this case, the response did not come from cache, so the cache status will be `none/unknown`.

* A Worker request made a subrequest (`fetch`). In this case, the subrequest will be logged with a cache status, while the main request will be logged with `none/unknown` status (the main request did not hit cache, since Workers sits in front of cache).

* A WAF custom rule was triggered to block a request. The response will come from the Cloudflare global network before it hits cache. Since there is no cache status, Cloudflare will log as `none/unknown`.

* A [redirect rule](/rules/url-forwarding/) or [Always Use HTTPS](/ssl/edge-certificates/additional-options/always-use-https/) caused the global network to respond with a redirect to another asset/URL. This redirect response happens before the request reaches cache, so the cache status is `none/unknown`.

## EXPIRED

The resource was found in Cloudflare's cache but was expired and served from the origin web server.

## STALE

The resource was served from Cloudflare's cache but was expired. Cloudflare could not contact the origin to retrieve an updated resource.

## BYPASS

The origin server instructed Cloudflare to bypass cache via a `Cache-Control` header set to `no-cache`, `private`, or `max-age=0` even though Cloudflare originally preferred to cache the asset. BYPASS is returned when enabling [Origin Cache-Control](/cache/concepts/cache-control/). Cloudflare also sets BYPASS when your origin web server sends cookies in the response header. If the Request to your origin includes an `Authorization` header, in some cases the response will also be BYPASS. Refer to [Conditions](/cache/concepts/cache-control/#conditions) in the Origin Cache-Control behavior section for more details.

## REVALIDATED

The resource is served from Cloudflare's cache but is stale. The resource was revalidated by either an `If-Modified-Since` header or an `If-None-Match` header.

## UPDATING

The resource was served from Cloudflare's cache and was expired, but the origin web server is updating the resource. `UPDATING` is typically only seen for very popular cached resources.

## DYNAMIC

Cloudflare does not consider the asset eligible to cache and your Cloudflare settings do not explicitly instruct Cloudflare to cache the asset. Instead, the asset was requested from the origin web server. Use [Cache Rules](/cache/how-to/cache-rules/) to implement custom caching options.

---

# CDN-Cache-Control

URL: https://developers.cloudflare.com/cache/concepts/cdn-cache-control/

`CDN-Cache-Control` is a response header field set on the origin to separately control the behavior of CDN caches from other intermediaries that might handle a response. You can set the `CDN-Cache-Control` or `Cloudflare-CDN-Cache-Control` response header using the same directives used with the [Cache-Control](/cache/concepts/cache-control/).

## Header precedence

You have several options available to determine how `CDN-Cache-Control` directives interact with `Cache-Control` directives.

An origin can:

* Return the `CDN-Cache-Control` response header which Cloudflare evaluates to make caching decisions. `Cache-Control`, if also returned by the origin, is proxied as is and does not affect caching decisions made by Cloudflare. Additionally, `CDN-Cache-Control` is proxied downstream in case there are other CDNs between Cloudflare and the browser.

* Return the `Cloudflare-CDN-Cache-Control` response header. This results in the same behavior as the origin returning `CDN-Cache-Control` except Cloudflare does not proxy `Cloudflare-CDN-Cache-Control` downstream because itâ€™s a header only used to control Cloudflare. This option is beneficial if you want only Cloudflare to have a different caching behavior while all other downstream servers rely on `Cache-Control` or if you do not want Cloudflare to proxy the `CDN-Cache-Control` header downstream.

* Return both `Cloudflare-CDN-Cache-Control` and `CDN-Cache-Control` response headers. In this case, Cloudflare only looks at `Cloudflare-CDN-Cache-Control` when making caching decisions because it is the most specific version of `CDN-Cache-Control` and proxies `CDN-Cache-Control` downstream. Only forwarding `CDN-Cache-Control` in this situation is beneficial if you want Cloudflare to have a different caching behavior than other CDNs downstream.

Additionally, surrogates will not honor `Cache-Control` headers in the response from an origin. For example, if the `Surrogate-Control` header is present within the response, Cloudflare ignores any `Cache-Control` directives, even if the `Surrogate-Control` header does not contain directives.

## Interaction with other Cloudflare features

### Edge Cache TTL cache rule

The [Edge Cache TTL cache rule](/cache/how-to/cache-rules/settings/#edge-ttl) overrides the amount of time an asset is cached on the edge (Cloudflare data centers). This cache rule overrides directives in `Cloudflare-CDN-Cache-Control/CDN-Cache-Control` which manage how long an asset is cached on the edge. You can create this rule in the dashboard in **Cache** > **Cache Rules**.

### Browser Cache TTL cache rule

The [Browser Cache TTL cache rule](/cache/how-to/cache-rules/settings/#browser-ttl) overrides the amount of time an asset is cached by browsers/servers downstream of Cloudflare. Browser Cache TTL only modifies the `Cache-Control` response header. This cache rule does not modify `Cloudflare-CDN-Cache-Control/CDN-Cache-Control` response headers.

### Other Origin Response Headers

The origin returns the `Expires` response header which specifies the amount of time before an object is considered stale to the browser. This response header does not affect the caching decision at Cloudflare when `Cloudflare-CDN-Cache-Control/CDN-Cache-Control` is in use.

### Cloudflare Default cache values

In situations where Cloudflare does not receive `Cloudflare-CDN-Cache-Control`, `CDN-Cache-Control`, or `Cache-Control` values, cacheable assets use the general [default values](/cache/concepts/default-cache-behavior/).

## When to use CDN-Cache-Control

### Manage cached assets TTLs

Use `CDN-Cache-Control` when you want to manage cached assetâ€™s TTLs separately for origin caches, CDN caches, and browser caches. The example below shows how you can manage your cached assetâ€™s TTLs using origin-set response headers.

Headers:

* `Cache-Control: max-age=14400, s-maxage=84000`
* `Cloudflare-CDN-Cache-Control: max-age=24400`
* `CDN-Cache-Control: max-age=18000`

Cache behavior:

<table>
  <tbody>
    <th colspan="5" rowspan="1">
      Caches
    </th>
    <th colspan="5" rowspan="1">
      Cache TTL (seconds)
    </th>
    <tr>
      <td colspan="5" rowspan="1">
        Origin Server Cache
      </td>
      <td colspan="5" rowspan="1">
        14400
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        Network Shared Cache
      </td>
      <td colspan="5" rowspan="1">
        84000
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        Cloudflare Edge
      </td>
      <td colspan="5" rowspan="1">
        24400
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        Other CDNs
      </td>
      <td colspan="5" rowspan="1">
        18000
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        Browser Cache
      </td>
      <td colspan="5" rowspan="1">
        14400
      </td>
    </tr>
  </tbody>
</table>

### Specify when to serve stale content

Use `CDN-Cache-Control` headers in conjunction with `Cache-Control` headers to specify when to serve stale content in the case of error or during revalidation. The example below shows how you might set your headers and directives to apply to CDNs when handling errors.

Headers:

* `Cache-Control: stale-if-error=400`
* `Cloudflare-CDN-Cache-Control: stale-if-error=60`
* `CDN-Cache-Control: stale-if-error=200`

Behavior in response to 5XX error:

<table>
  <tbody>
    <th colspan="5" rowspan="1">
      Caches
    </th>
    <th colspan="5" rowspan="1">
      Stale served (seconds) in response to error
    </th>
    <tr>
      <td colspan="5" rowspan="1">
        Origin Cache Layer/Network Cache/Browser Cache
      </td>
      <td colspan="5" rowspan="1">
        400 (if it assumes the directive applies)
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        Cloudflare Edge
      </td>
      <td colspan="5" rowspan="1">
        60
      </td>
    </tr>
    <tr>
      <td colspan="5" rowspan="1">
        Other CDN
      </td>
      <td colspan="5" rowspan="1">
        200
      </td>
    </tr>
  </tbody>
</table>

---

# Default cache behavior

URL: https://developers.cloudflare.com/cache/concepts/default-cache-behavior/

import { FeatureTable } from "~/components"

Cloudflare respects the origin web serverâ€™s cache headers in the following order unless an [Edge Cache TTL cache rule](/cache/how-to/cache-rules/settings/#edge-ttl) overrides the headers. Refer to the [Edge TTL](/cache/how-to/configure-cache-status-code/#edge-ttl) section for details on default TTL behavior.

* Cloudflare **does not** cache the resource when:
  * The `Cache-Control` header is set to `private`, `no-store`, `no-cache`, or `max-age=0`.
  * The [Set-Cookie header](/cache/concepts/cache-behavior/#interaction-of-set-cookie-response-header-with-cache) exists.
  * The HTTP request method is anything other than a `GET`.
* Cloudflare **does** cache the resource when:
  * The `Cache-Control` header is set to `public` and `max-age` is greater than 0.
  * The `Expires` header is set to a future date.

:::note
Cloudflare does cache the resource even if there is no `Cache-Control` header based on [status codes](/cache/how-to/configure-cache-status-code/#edge-ttl).
:::

:::note
If both `max-age` and an `Expires` header are set, `max-age` will be used by Cloudflare.
:::

When [Origin Cache Control](/cache/concepts/cache-control/) is enabled on an Enterprise customerâ€™s website, it indicates that Cloudflare should strictly respect `Cache-Control` directives received from the origin server. Free, Pro and Business customers have this feature enabled by default. For a list of directives and behaviors when Origin Cache-Control is enabled or disabled, refer to [Cache-Control directives](/cache/concepts/cache-control/#cache-control-directives).

## Default cached file extensions

Cloudflare only caches based on file extension and not by MIME type. The Cloudflare CDN does not cache HTML or JSON by default. Additionally, by default Cloudflare caches a website's robots.txt.

|       |      |      |      |      |       |     |
| ----- | ---- | ---- | ---- | ---- | ----- | --- |
| 7Z    | CSV  | GIF  | MIDI | PNG  | TIF   | ZIP |
| AVI   | DOC  | GZ   | MKV  | PPT  | TIFF  | ZST |
| AVIF  | DOCX | ICO  | MP3  | PPTX | TTF   |     |
| APK   | DMG  | ISO  | MP4  | PS   | WEBM  |     |
| BIN   | EJS  | JAR  | OGG  | RAR  | WEBP  |     |
| BMP   | EOT  | JPG  | OTF  | SVG  | WOFF  |     |
| BZ2   | EPS  | JPEG | PDF  | SVGZ | WOFF2 |     |
| CLASS | EXE  | JS   | PICT | SWF  | XLS   |     |
| CSS   | FLAC | MID  | PLS  | TAR  | XLSX  |     |

To cache additional content, refer to [Cache Rules](/cache/how-to/cache-rules/) to create a rule to cache everything.

## Customization options and limits

Cloudflareâ€™s CDN provides several cache customization options:

* Caching behavior for individual URLs via [Cache Rules](/cache/how-to/cache-rules/)
* Customize caching with [Cloudflare Workers](/workers/reference/how-the-cache-works/)
* Adjust caching level, cache TTL, and more via the Cloudflare Caching app

### Upload limits

<FeatureTable id="network.max_upload_size" />

If you require a larger upload, group requests smaller than the upload thresholds or upload the full resource through an [unproxied (grey-clouded) DNS record](/dns/proxy-status/).

### Cacheable size limits

Cloudflare cacheable file limits:

* Free, Pro and Business customers have a limit of 512 MB.
* For Enterprise customers the default maximum cacheable file size is 5 GB. Contact your account team to request a limit increase.

## When does Cloudflare cache successfully?

The connection status between visitors and Cloudflare can vary, affecting whether Cloudflare caches the content or not. If Cloudflare has already established a connection to the origin and started fetching the content, it will continue to retrieve and cache the entire content, even if the visitor disconnects midway. However, if a visitor disconnects before the origin responds to Cloudflare's request, no content will have been fetched yet, so Cloudflare will not start caching the content.

---

# Concepts

URL: https://developers.cloudflare.com/cache/concepts/

import { DirectoryListing } from "~/components"

Review the following topics to learn more about the basic concepts and components of Cloudflare Cache.

<DirectoryListing />

---

# Customize cache

URL: https://developers.cloudflare.com/cache/concepts/customize-cache/

Some possible combinations of origin web server settings and Cloudflare [Cache Rules](/cache/how-to/cache-rules/) include:

## Create a directory for static content at your origin web server

For example, create a `/static/` subdirectory at your origin web server and a Cache Everything Cache Rule matching the following expression:

* Using the Expression Builder: `Hostname contains "example.com" AND URI Path starts with "/static"`
* Using the Expression Editor: `(http.host contains "example.com" and starts_with(http.request.uri.path, "/static"))`

## Append a unique file extension to static pages

For example, create a `.shtml` file extension for resources at your origin web server and a Cache Everything Cache Rule matching the following expression:

* Using the Expression Builder: `Hostname contains "example.com" AND URI Path ends with ".shtml"`
* Using the Expression Editor: `(http.host contains "example.com" and ends_with(http.request.uri.path, ".shtml"))`

## Add a query string to a resourceâ€™s URL to mark the content as static

For example, add a `static=true` query string for resources at your origin web server and a Cache Everything Cache Rule matching the following expression:

* Using the Expression Builder: `Hostname contains "example.com" AND URI Query String contains "static=true"`
* Using the Expression Editor: `(http.host contains "example.com" and http.request.uri.query contains "static=true")`

Resources that match a Cache Everything Cache Rule are still not cached if the origin web server sends a Cache-Control header of `max-age=0`, `private`, `no-cache`, or an `Expires` header with an already expired date. Include the [Edge Cache TTL](/cache/how-to/cache-rules/settings/#edge-ttl) setting within the Cache Everything Cache Rule to additionally override the `Cache-Control` headers from the origin web server.

---

# Retention vs Freshness (TTL)

URL: https://developers.cloudflare.com/cache/concepts/retention-vs-freshness/

In the context of Cloudflare CDN (Content Delivery Network), retention and freshness refer to two separate but related concepts. For an object in cache, freshness is how long it should be considered valid without consulting its source, while retention refers to how long it stays in cache before being removed.

## Retention

When a file or resource is requested from a website, Cloudflare caches it to avoid having to ask the origin server for it again the next time it is requested, reducing latency and improving delivery speed. But if an object in cache does not get requested again, eventually it will be removed to make room for newer, more popular objects. This process is called eviction and is a standard part of cache management. When the cache wants to store a new object but does not have room, it uses an algorithm called Least Recently Used, or LRU, to nominate an object to evict and replace it with the new one. An objectâ€™s cache retention period refers to the duration the object is stored in Cloudflareâ€™s cache before being evicted. It is worth noting that an objectâ€™s retention period is a function of its relative popularity and the size of Cloudflareâ€™s caches, and therefore is not configurable.

## Freshness (TTL)

The time window that an object should be considered safe for a cache to use is dictated by its freshness, also known as Time to Live (TTL). If an object has a TTL of five minutes that means that, starting from the moment the cache first receives the object, for the next five minutes the cache can use that object without checking with the origin again. After five minutes have passed, if Cloudflare gets another request for that object, we cannot use what is stored in the cache without first checking the origin to see if the object is still valid. Those first five minutes in this objectâ€™s case are its freshness period. There are a few ways to configure TTLs for resources served through Cloudflareâ€™s Content Delivery Network:

* Include [Origin Cache Control](/cache/concepts/cache-control/) or [CDN Cache Control](/cache/concepts/cache-control/) directives, like `max-age` or `s-maxage`, in the origin cache-control response header.

* Use [Cache Rules](/cache/how-to/cache-rules/) or [Workers](/cache/interaction-cloudflare-products/workers/).

If an object in cache is no longer fresh and Cloudflare receives a request for it, we ask the origin to revalidate the object we have in cache. The Origin can then either send a new version of the object which will replace the old in cache, or tell us the object we have is valid and to refresh its TTL. This revalidation will happen any time an objectâ€™s retention period is greater than its freshness period.

---

# Revalidation and request collapsing

URL: https://developers.cloudflare.com/cache/concepts/revalidation/

import { GlossaryTooltip } from "~/components"

Revalidation is a caching mechanism that checks the [freshness](/cache/concepts/retention-vs-freshness/#freshness-ttl) of cached data before serving it to users. If a cached object is no longer fresh and Cloudflare receives a request for it, the system makes a request to the origin to revalidate the object in the Cloudflare cache. By using headers like `If-Modified-Since` and `ETag`, Cloudflare validates content without fully re-fetching it. When these headers are missing, Smart Edge Revalidation generates a `Last-Modified` header, ensuring efficient updates and delivery of fresh content while reducing origin traffic.

## Revalidation towards origin

For stale (expired TTL) content, Cloudflare will send a revalidation request to the origin. If the stale content is still valid Cloudflare will set a new TTL. If the content is expired, then the origin will provide new fresh content to replace the old.

Consider the following example scenarios.

### Example 1

One-thousand (1,000) requests arrive simultaneously at Cloudflare's network, and the requested resource was in Cloudflare but cannot be served because its TTL configuration indicates it is no longer fresh. This means that the resource needs to be served from the origin server. In this case, one request will go to the origin to be revalidated, while the other 999 requests will be served from cache with the status of [UPDATING](/cache/concepts/cache-responses/#updating). This means that the resource, although expired, is served stale from Cloudflare's cache, while the origin server is updating it. This behavior is defined by the [`stale-while-revalidate`](/cache/concepts/cache-control/#revalidation) directive in `cache-control`. If you do not wish to serve stale content, set the directive to zero seconds, `stale-while-revalidate=0`.

### Example 2

One-thousand (1,000) requests arrive simultaneously at a single Cloudflare data center, and the requested asset is not in Cloudflare's cache (a cache miss). These requests will use a <GlossaryTooltip term="cache lock">cache lock</GlossaryTooltip> to communicate with your origin. This means that only the first request will go to origin to fetch the asset. The remaining 999 requests wait for the first request to fetch the data, after which the response is [streamed](https://blog.cloudflare.com/introducing-concurrent-streaming-acceleration/) to all the waiting requests. The cache lock ensures that Cloudflare only sends the origin one request at a time for a given asset from a location in Cloudflare's network, preventing the origin from getting too much traffic.

## Smart revalidation towards users

When both [`Last-Modified`](https://datatracker.ietf.org/doc/html/rfc7232?cf_history_state=%7B%22guid%22%3A%22C255D9FF78CD46CDA4F76812EA68C350%22%2C%22historyId%22%3A15%2C%22targetId%22%3A%226C8153BAEF7BC0C5A331E28F8BCF1ABA%22%7D#section-2.2) and [`Etag`](https://datatracker.ietf.org/doc/html/rfc7232?cf_history_state=%7B%22guid%22%3A%22C255D9FF78CD46CDA4F76812EA68C350%22%2C%22historyId%22%3A13%2C%22targetId%22%3A%226C8153BAEF7BC0C5A331E28F8BCF1ABA%22%7D#section-2.3) headers are absent from the origin server response, Smart Edge Revalidation will use the time the object was cached on Cloudflare's global network as the `Last-Modified` header value. When a browser sends a revalidation request to Cloudflare using `If-Modified-Since` or `If-None-Match`, our global network can answer those revalidation questions using the `Last-Modified` header generated from Smart Edge Revalidation. In this way, our global network can ensure efficient revalidation even if the headers are not sent from the origin.

---

# Always Online

URL: https://developers.cloudflare.com/cache/how-to/always-online/

import { FeatureTable } from "~/components"

Cloudflareâ€™s Always Online feature is now integrated with the [Internet Archive](https://archive.org/) so that visitors can access a portion of your website even when your origin server is unreachable and a Cloudflare-cached version is unavailable. When your origin is unreachable, Always Online checks Cloudflareâ€™s cache for a stale or expired version of your website. If a version does not exist, Cloudflare goes to the Internet Archive to fetch and serve static portions of your website.

When you enable Always Online with Internet Archive integration, Cloudflare shares your hostname and popular URL paths with the archive so that the Internet Archiveâ€™s crawler stores the pages you want archived. When submitting targets to the crawler, Cloudflare identifies the most popular URLs found among GET requests that returned a 200 HTTP status code in the previous five hours.

Note that Cloudflare does not save a copy of every page of your website, and it cannot serve dynamic content while your origin is offline. If the requested page is not in the Internet Archive's Wayback Machine, the visitor sees the actual error page caused by the offline origin web server.

When the Internet Archive integration is enabled, Cloudflare tells the Internet Archive what pages to crawl and how often. The pages to crawl, as previously mentioned, are the most popular URLs that were successfully visited in the last five hours. The crawling intervals, to ensure stability of service, are limited by Cloudflare. Limits vary according to your Cloudflare plan.

## Availability

<FeatureTable id="cache.always_online" />

## Visitor Experience

When Always Online with Internet Archive integration is enabled, visitors see a banner at the top of the webpage explaining they are visiting an archived version of the website. Visitors can select the Refresh button to check whether the origin has recovered and fresh content is available.

When a visitor requests content for an offline website, Cloudflare returns an HTTP response status code in the range [520â€“527](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-520-web-server-returns-an-unknown-error), depending on the issue. These status codes indicate that the origin is unreachable.

When the Internet Archive integration is enabled, Cloudflare checks the archive and serves the most recently archived version of the page.

Visitors who interact with dynamic parts of a website, such as a shopping cart or comment box, will see an error page caused by the offline origin web server.

## Enable Always Online

Here is how to enable Always Online in the dashboard:

1. Log in to your Cloudflare account.
2. Choose the domain that will use Always Online with Internet Archive integration.
3. Select **Caching** > **Configuration**.
4. Under **Always Online**, set the toggle to **On**.

:::note[Note]


When turning on Always Online, you are also enabling the Internet Archive integration.


:::

Refer to [Always Online](/cache/troubleshooting/always-online/) for best practices, limitations, and FAQs.

---

# Cache keys

URL: https://developers.cloudflare.com/cache/how-to/cache-keys/

import { FeatureTable } from "~/components"

A Cache Key is an identifier that Cloudflare uses for a file in our cache, and the Cache Key Template defines the identifier for a given HTTP request.

A default cache key includes:

1. Full URL:
   * scheme - could be HTTP or HTTPS.
   * host - for example, `www.cloudflare.com`
   * URI with query string - for example, `/logo.jpg`
2. Origin header sent by client (for CORS support).
3. `x-http-method-override`, `x-http-method`, and `x-method-override` headers.
4. `x-forwarded-host`, `x-host`, `x-forwarded-scheme` (unless http or https), `x-original-url`, `x-rewrite-url`, and `forwarded` headers.

## Create custom cache keys

Custom cache keys let you precisely set the cacheability setting for any resource. They provide the benefit of more control, though they may reduce your cache hit rate and result in cache sharding:

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com), and select your account and domain.
2. Go to **Caching** > **Cache Rules**.
3. Select **Create rule**.
4. Under **When incoming requests match**, define the [rule expression](/ruleset-engine/rules-language/expressions/edit-expressions/#expression-builder).
5. Under **Then**, in the **Cache eligibility** section, select **Eligible for cache**.
6. Add the **Cache Key** setting to the rule and select the appropriate **Query String** setting.
7. You can also select settings for **Headers**, **Cookie**, **Host**, and **User**.
8. To save and deploy your rule, select **Deploy**. If you are not ready to deploy your rule, select **Save as Draft**.

## Cache Key Template

There are a couple of common reasons to change the Cache Key Template. You might change the Cache Key Template to:

* Fragment the cache so one URL is stored in multiple files. For example, to store different files based on a specific query string in the URL.
* Consolidate the cache so different HTTP requests are stored in the same file. For example, to remove the Origin header added to Cloudflare Cache Keys by default.

### Impact of SSL settings on Cache behavior

The `$scheme` refers to the protocol (HTTP or HTTPS) sent to your origin web server, not the protocol received from the visitor. Therefore, setting the Cloudflare [SSL option](/ssl/origin-configuration/ssl-modes/) influences caching decisions. For example, when using [Flexible SSL](/ssl/origin-configuration/ssl-modes/flexible/), Cloudflare only attempts to connect to your origin web server via HTTP. This means that Cloudflare serves the same cached resource for visitor requests via either HTTP or HTTPS, since Flexible SSL instructs Cloudflare to connect to an origin solely over HTTP.

It is important to understand how SSL setting changes affect the cache:

- Switching from **Off** to **Full**, **Full (strict)**, or **Strict** will change the origin scheme from HTTP to HTTPS. This results in a cache bust, meaning the cached content becomes invalid and needs to be re-fetched from the origin server.

- Transitioning from **Flexible** to **Full**, **Full (strict)**, or **Strict** changes the origin scheme from HTTP to HTTPS, causing a cache bust.

- Downgrading from **Full**, **Full (strict)**, or **Strict** to **Flexible** or **Off** changes the origin scheme from HTTPS to HTTP, resulting in a cache bust.

This behavior is important to consider when adjusting SSL settings, as any change in the origin scheme (HTTP to HTTPS or vice versa) triggers a cache reset.

### Cache Level: Ignore Query String

A [Cache Level](/cache/how-to/set-caching-levels/) of Ignore Query String creates a Cache Key that includes all the elements in the default cache key, except for the query string in the URI that is no longer included. For instance, a request for `http://example.com/file.jpg?something=123` and a request for `http://example.com/file.jpg?something=789` will have the same cache key, in this case.

## Cache Key Settings

The following fields control the Cache Key Template.

### Query String

The query string controls which URL query string parameters go into the Cache Key. You can `include` specific query string parameters or `exclude` them using the respective fields. When you include a query string parameter, the `value` of the query string parameter is used in the Cache Key.

#### Example

If you include the query string foo in a URL like `https://www.example.com/?foo=bar`, then bar appears in the Cache Key. Exactly one of `include` or `exclude` is expected.

#### Usage notes

* To include all query string parameters (the default behavior), use include: "\*"
* To ignore query strings, use exclude: "\*"
* To include most query string parameters but exclude a few, use the exclude field which assumes the other query string parameters are included.

### Headers

Headers control which headers go into the Cache Key. Similar to Query String, you can include specific headers or exclude default headers.

When you include a header, the header value is included in the Cache Key. For example, if an HTTP request contains an HTTP header like `X-Auth-API-key: 12345`, and you include the `X-Auth-API-Key header` in your Cache Key Template, then `12345` appears in the Cache Key.

In the **Check if header contains** section, you can add header names and their values to the cache key. For custom headers, values are optional, but for the following restricted headers, you must include one to three specific values:

  * `accept`
  * `accept-charset`
  * `accept-encoding`
  * `accept-datetime`
  * `accept-language`
  * `referer`
  * `user-agent`

To check for the presence of a header without including its actual value, use the **Check presence of** option.

Currently, you can only exclude the `Origin` header. The `Origin` header is always included unless explicitly excluded. Including the [Origin header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Origin) in the Cache Key is important to enforce [CORS](https://developer.mozilla.org/en-US/docs/Glossary/CORS).

Additionally, you cannot include the following headers:

* Headers that re-implement cache or proxy features
  * `connection`
  * `content-length`
  * `cache-control`
  * `if-match`
  * `if-modified-since`
  * `if-none-match`
  * `if-unmodified-since`
  * `range`
  * `upgrade`
* Headers that are covered by other Cache Key features
  * `cookie`
  * `host`
* Headers that are specific to Cloudflare and prefixed with `cf-`, for example, `cf-ray`
* Headers that are already included in the custom Cache Key template, for example, `origin`

### Host

Host determines which host header to include in the Cache Key.

* If `Use original host` (`resolved: false` in the API), Cloudflare includes the `Host` header in the HTTP request sent to the origin.
* If `Resolved host` (`resolved: true` in the API), Cloudflare includes the `Host` header that was resolved to get the `origin IP` for the request. The `Host` header may be different from the header actually sent if it has been changed with an [Origin Rule](/rules/origin-rules/features/#dns-record).

### Cookie

Like `query_string` or `header`, `cookie` controls which cookies appear in the Cache Key. You can either include the cookie value or check for the presence of a particular cookie.

#### Usage notes

You cannot include cookies specific to Cloudflare. Cloudflare cookies are prefixed with `__cf`, for example, `__cflb`

### User features

User feature fields add features about the end-user (client) into the Cache Key.

* `device_type` classifies a request as `mobile`, `desktop`, or `tablet` based on the User Agent
* `geo` includes the clientâ€™s country, derived from the IP address
* `lang` includes the first language code contained in the `Accept-Language` header sent by the client

## Availability

Cache keys options availability varies according to your plan.

<FeatureTable id="cache.cache_key" />

## Limitations

The Prefetch feature is not compatible with the custom cache keys. With Cache Rules, the custom cache key is used to cache all assets. However, Prefetch always uses the default cache key. This results in a key mismatch.

---

# Cache by status code

URL: https://developers.cloudflare.com/cache/how-to/configure-cache-status-code/

Customers can set cache time-to-live (TTL) based on the response status from the origin web server. Cache TTL refers to the duration of a resource in the Cloudflare network before being marked as `STALE` or discarded from cache. Status codes are returned by a resource's origin.

Setting cache TTL based on response status overrides the [default cache behavior (standard caching)](/cache/concepts/default-cache-behavior/) for static files and overrides cache instructions sent by the origin web server. To cache non-static assets, set a [Cache Level of Cache Everything using a Cache Rule](/cache/how-to/cache-rules/create-api/#example-requests). Setting `no-store` **Cache-Control** or a low TTL (using `max-age`/`s-maxage`) increases requests to origin web servers and decreases performance.

## Caching limits

The maximum caching limit for Free, Pro, and Business customers is 512 MB per file, and the maximum caching limit for Enterprise customers is 5 GB per file. If you need to raise the limits, contact your Customer Success Manager.

## Edge TTL

By default, Cloudflare caches certain HTTP response codes with the followingÂ Edge Cache TTLÂ when a `cache-control` directive or `expires` response header are not present.

| HTTP status code | Default TTL |
| ---------------- | ----------- |
| 200, 206, 301    | 120m        |
| 302, 303         | 20m         |
| 404, 410         | 3m          |

All other status codes are not cached by default.

## Set cache TTL by response status via the Cloudflare dashboard

To set cache TTL by response status, [create a Cache Rule](/cache/how-to/cache-rules/) for [**Cache TTL by status code**](/cache/how-to/cache-rules/settings/#edge-ttl).

## Set cache TTL by response status via the Cloudflare API

```bash title="Request"
curl --request PUT \
"https://api.cloudflare.com/client/v4/zones/{zone_id}/rulesets/{ruleset_id}" \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{
  "rules": [
    {
      "expression": "(http.host eq \"www.example.com\")",
      "description": "set cache TTL by response status",
      "action": "set_cache_settings",
      "action_parameters": {
        "cache": true,
        "edge_ttl": {
          "status_code_ttl": [
            {
              "status_code_range": {
                "to": 299
              },
              "value": 86400
            },
            {
              "status_code_range": {
                "from": 300,
                "to": 499
              },
              "value": 0  // no-cache
            },
            {
              "status_code_range": {
                "from": 500
              },
              "value": -1  // no-store
            }
          ],
          "mode": "respect_origin"
        }
      }
    }
  ]
}'
```

### Syntax

Provide a JSON object containing status codes and their corresponding TTLs. Each key-value pair in the cache TTL by status cache rule has the following syntax:

* `status_code`: An integer value such as 200 or 500. `status_code` matches the exact status code from the origin web server. Valid status codes are between 100-999.
* `status_code_range`: Integer values for `from` and `to`. `status_code_range` matches any status code from the origin web server within the specified range.
* `value`: An integer value that defines the duration an asset is valid in seconds or one of the following strings: `no-store` (equivalent to `-1`), `no-cache` (equivalent to `0`).

## Set cache TTL by response status via a Cloudflare Worker

The **cacheTtlByStatus** option is a version of the **cacheTtl** feature that designates a cache TTL for a requestâ€™s response status code (for example, `{ "200-299": 86400, 404: 1, "500-599": 0 }`).

## TTL handling for 304 and 200 status codes

1. If a TTL is not explicitly set for status code `304`, we automatically set it to match the TTL of status code `200` (if the user has defined one for `200`).

2. If a user explicitly sets a different TTL for `304` than for `200`, the following behavior will occur:

- When a `200` response is received, the asset is cached with the TTL specified for status `200`.
- Once the asset expires and we revalidate with the origin, if the origin returns a `304`, the cache TTL is updated to the value set for `304`.

For example, if a user specifies a TTL of one hour for status `200` and 0 seconds (cache and always revalidate) for status `304`, the asset will be cached for 1 hour. After it expires, we revalidate with the origin. If the origin returns a `304`, each subsequent request will trigger revalidation. If the origin continues to return `304`, this cycle will persist.

This behavior is likely undesirable unless the user has a specific use case. Therefore, users should ensure that the TTL for `304` matches the TTL for `200` unless they intentionally require this behavior.

---

# Cache configuration

URL: https://developers.cloudflare.com/cache/how-to/

import { DirectoryListing } from "~/components"

Learn how to complete different configuration options to customize your cache behavior.

<DirectoryListing />

---

# Caching levels

URL: https://developers.cloudflare.com/cache/how-to/set-caching-levels/

Caching levels determine how much of your websiteâ€™s static content Cloudflare should cache. Cloudflareâ€™s CDN caches static content according to the levels below.

* **No Query String**: Delivers resources from cache when there is no query string. Example URL: `example.com/pic.jpg`
* **Ignore Query String**: Delivers the same resource to everyone independent of the query string. Example URL: `example.com/pic.jpg?ignore=this-query-string`
* **Standard (Default)**: Delivers a different resource each time the query string changes. Example URL: `example.com/pic.jpg?with=query`

You can adjust the caching level from the dashboard under **Caching** > **Configuration** > **Caching level**.

:::note[Note]


Ignore Query String only disregards the query string for static file extensions. For example, Cloudflare serves the `style.css` resource to requests for either `style.css?this` or `style.css?that`.


:::

## API Caching level values

If you are using the API to change the cache level, the values will differ from those shown in the dashboard. Refer to the table below to see how the API values map to the values shown in the dashboard.

| Dashboard           | API        |
| ------------------- | ---------- |
| No Query String     | Basic      |
| Ignore Query String | Simplified |
| Standard (Default)  | Aggressive |

---

# Tiered Cache

URL: https://developers.cloudflare.com/cache/how-to/tiered-cache/

import { Details, FeatureTable } from "~/components"

Tiered Cache uses the size of Cloudflareâ€™s network to reduce requests to customer origins by dramatically increasing cache hit ratios. With data centers around the world, Cloudflare caches content very close to end users. However, if a piece of content is not in cache, the Cloudflare edge data centers must contact the origin server to receive the cacheable content.

Tiered Cache works by dividing Cloudflareâ€™s data centers into a hierarchy of lower-tiers and upper-tiers. If content is not cached in lower-tier data centers (generally the ones closest to a visitor), the lower-tier must ask an upper-tier to see if it has the content. If the upper-tier does not have the content, only the upper-tier can ask the origin for content. This practice improves bandwidth efficiency by limiting the number of data centers that can ask the origin for content, which reduces origin load and makes websites more cost-effective to operate.

Additionally, Tiered Cache concentrates connections to origin servers so they come from a small number of data centers rather than the full set of network locations. This results in fewer open connections using server resources.

To enable Tiered Cache, refer to [Enable Tiered Cache](/cache/how-to/tiered-cache/#enable-tiered-cache).

## Tiered Cache Topology

Cloudflare allows you to select your cache topology so that you have control over how your origin connects to Cloudflareâ€™s data centers. This will help ensure higher cache hit ratios, fewer origin connections, and a reduction of Internet latency. Below you can find details about the options we have available.

### Smart Tiered Cache

Smart Tiered Cache dynamically selects the single closest upper tier for each of your websiteâ€™s origins with no configuration required, using our in-house performance and routing data. Cloudflare collects latency data for each request to an origin, and uses the latency data to determine how well any upper-tier data center is connected with an origin. As a result, Cloudflare can select the data center with the lowest latency to be the upper-tier for an origin.

#### Load Balancing interaction

While Smart Tiered Cache selects one Upper Tier per origin, when using Load Balancing, Smart Tiered Cache will select the single best Upper Tier for the entire Load Balancing Pool.

#### Caveats

Smart Tiered Cache does not work when an origin is behind an [anycast](https://www.cloudflare.com/en-gb/learning/cdn/glossary/anycast-network/) or a regional unicast network because that will prevent us from knowing where the origin is located. As a result, we are unable to select the optimal upper tier and latency may be negatively impacted.

You need to be careful when updating your origin IPs/DNS records while Smart Tiered Cache is enabled. Depending on the changes made, it may cause the existing assigned upper tiers to change, resulting in an increased `MISS` rate as cache is refilled in the new upper tiers. If the origin is switched to a network behind anycast, it will significantly reduce the effectiveness of Smart Tiered Cache.

If you need to use anycast or regional unicast and want to use Smart Tiered cache, contact your account team.

### Generic Global Tiered Cache

Generic Global topology allows for all of Cloudflareâ€™s global data centers to serve as a network of upper-tiers. This topology may help reduce the long tail latencies for far-away visitors.

### Regional Tiered Cache

Regional Tiered Cache provides an additional layer of caching for Enterprise customers who have a global traffic footprint and want to serve content faster by avoiding network latency when there is a cache `MISS` in a lower-tier, resulting in an upper-tier fetch in a data center located far away.

Regional Cache instructs Cloudflare to check a regional hub data center near the lower tier before going to the upper tier that may be outside of the region. This can help improve performance for **Smart** and **Custom Tiered Cache** topologies with upper-tiers in one or two regions. Regional Tiered Cache is not beneficial for customers with many upper tiers in many regions like Generic Global Tiered Cache.

### Custom Tiered Cache

Custom Tiered cache allows you to work with Cloudflareâ€™s support team to set a custom topology that fits your specific needs, for instance you have close upper tiers or you have an unique traffic pattern. If you want a custom topology, please contact your CSM.

## Availability

<FeatureTable id="cache.tiered_cache" />

## Bandwidth Alliance

Enterprise customers can override Bandwidth Alliance configuration with Tiered Cache. For all other users, the Bandwidth Alliance takes precedence. Tiered Cache is still a valuable option to enable because the Bandwidth Alliance may not always be an available option, and in those instances, the Tiered Cache configuration will be used.

## Enable Tiered Cache

You can enable Tiered Cache in the dashboard or via API.

### Enable Tiered Cache in the dashboard

1. Log in to your [Cloudflare dashboard](https://dash.cloudflare.com) and select your domain.
2. Select **Caching** > **Tiered Cache**.
3. From **Tiered Cache**, toggle the button to **enabled**.
4. In **Tiered Cache Topology**, you can control how your origin connects to Cloudflareâ€™s data centers. You can select:
   * **Upper Tier Cache** - You have the option to choose between Smart or Generic Global Tiered Cache Topology.
   * **Middle Tier Cache** -  If you have selected Smart or Custom Tiered Cache Topology, you can now enable Regional Tiered Cache.
   * **Custom Tiered Cache** - Allows you to work with Cloudflareâ€™s support team to set a custom topology that fits your specific needs.
   * **Disable Tiered Cache**.

![Tiered Cache Topology dashboard](~/assets/images/cache/tiered_cache_topology.png)

### Enable Tiered Cache via API

To enable Tiered Cache via API use the following cURL example:

```bash
curl --request PATCH \
https://api.cloudflare.com/client/v4/zones/{zone_id}/argo/tiered_caching \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header 'Content-Type: application/json' \
--data '{ "value": "on" }'
```

You can also configure Tiered Cache Topology via API, for instance:


<Details header="Enable Smart Tiered Cache">

```bash
curl --request PATCH \
https://api.cloudflare.com/client/v4/zones/{zone_id}/cache/tiered_cache_smart_topology_enable \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{ "value": "on" }'
```


</Details>


<Details header="Enable Regional Tiered Cache">

```bash
curl --request PATCH \
https://api.cloudflare.com/client/v4/zones/{zone_id}/cache/regional_tiered_cache \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header 'Content-Type: application/json' \
--data '{ "value": "on" }'
```


</Details>

For more API examples and configuration options for Tiered Cache, refer to the [API documentation](/api/resources/argo/subresources/tiered_caching/methods/get/).

:::note


To confirm that Tiered Cache is working, make sure you have the value `CacheTieredFill` in your http\_requests logs, this will indicate that Tiered Cache was used to serve the request.


:::

---

# Interaction with Cloudflare products

URL: https://developers.cloudflare.com/cache/interaction-cloudflare-products/

import { DirectoryListing } from "~/components"

Review the following topics to learn more about how Cache interacts with other Cloudflare products.

<DirectoryListing />

---

# Control cache access with WAF and Snippets

URL: https://developers.cloudflare.com/cache/interaction-cloudflare-products/waf-snippets/

To limit access to the public bucket created for caching content, you can use Cloudflare's [WAF](/waf/custom-rules/use-cases/configure-token-authentication/). The WAF provides an additional security layer to filter requests and ensure that only authorized traffic reaches your bucket. 

The following diagram illustrates the flow of a user's request through WAF, Cache, and R2.

```mermaid
flowchart LR
accTitle: Connections with Cloudflare
A[User's request] --> B[WAF] --> C[Cache] --> D[R2]
```

<br/>

The WAF product uses token authentication to either sign or authenticate a request. You can then use this in either Workers or Snippets to control access.

## Presigned URLs

You can presign URLs similar to [S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html), enabling you to share direct access to your content with a with an associated timeout. This approach can be implemented using a combination of Snippets, Rules, or Cloudflare Workers.

For optimal performance, we recommend separating the creation and validation processes as follows:

- [Snippets](/rules/snippets/examples/signing-requests/) for HMAC creation
- [Rules](/ruleset-engine/rules-language/functions/#hmac-validation) for HMAC validation

In the Workers documentation, in the section [Signing requests](/workers/examples/signing-requests/), you can also find an example of how to verify a signed request using the HMAC.

---

# Customize cache behavior with Workers

URL: https://developers.cloudflare.com/cache/interaction-cloudflare-products/workers/

You can use [Workers](/workers/) to customize cache behavior on Cloudflare's CDN. Cloudflare Workers provide flexibility in handling assets and responses by running both before and after the cache. A Worker can be configured to run before a request reaches the cache, allowing for modifications to the request, and it can also be used to modify assets once they are returned from the cache.

The diagram below illustrates a common interaction flow between Workers and Cache.

![Workers and cache flow example flow diagram.](~/assets/images/cache/workers-cache-flow.png)

1. A User (a) Requests a URI, and this request is directed to a Worker. The Worker can then interact with the request, either requesting the content further upstream using (b) fetch() or sending a (f) Response back to the User.
2. If the content is cached, the Cache will send a (e) Response back to the Worker which can now interact with the response before sending a (f) Response back to the user.

Here are a few examples of how Workers can be used to customize cache behavior:

- **Modify Response**: Adjust or enhance content after it is retrieved from the cache, ensuring that responses are up-to-date or tailored to specific needs.

- **Signed URLs**: Generate URLs that are valid for a specific duration (for example, minutes, hours, days) to control access and enhance security.

- **Personalized Response**: Deliver personalized content based on user data while leveraging cached resources to reduce the load on the origin.

- **Reduce Latency**: Serve content from a location close to the user, decreasing load times and improving the user experience.

You can also use [Snippets](/rules/snippets/) as a free alternative for simple modifications and logic, bypassing the need for full Worker scripts. These lightweight scripts enable quick adjustments and optimizations, offering an efficient way to enhance your Cloudflare setup without the complexity and overhead of more extensive code deployments.

:::note
When using Workers and [Orange-to-Orange (O2O)](/cloudflare-for-platforms/cloudflare-for-saas/saas-customers/how-it-works/), some caveats and limitations may apply.
:::

## Cache features in Workers

- **fetch()**: Allows interaction with Cloudflare's Cache and Tiered Cache, providing control over how requests are handled. To optimize caching behavior, you can set TTLs, define custom cache keys, and configure cache headers directly within a fetch request. For more details on these configurations, refer to [Cache using fetch](/workers/examples/cache-using-fetch/).

- **Cache API**: Enables storing and retrieving responses from Cloudflare's cache, limited to the cache in the local data center and excluding content stored in the Tiered Cache. To use the Cache API to store responses in Cloudflare's cache, refer to [Using the Cache API](/workers/examples/cache-api/).

To understand more about how Cache and Workers interact refer to [Cache in Workers](/workers/reference/how-the-cache-works/).

---

# Enable cache in an R2 bucket

URL: https://developers.cloudflare.com/cache/interaction-cloudflare-products/r2/

To enable caching for a [Cloudflare R2](/r2/) bucket, make sure your bucket is public and accessible by the Cache. This can be done by creating a [Custom Domain](/r2/buckets/public-buckets/#custom-domains). Follow these steps to set up a Custom Domain for your bucket:

1. Go to **R2** and select your bucket.
2. On the bucket page, select **Settings**.
3. Under **Public access** > **Custom Domains**, select **Connect Domain**.
4. Enter the domain name you want to connect to and select **Continue**.
5. Review the new record that will be added to the DNS table and select **Connect Domain**.

This will generate a publicly available CNAME in the format `[name].domain.com`.

## Tiered Cache

By default Cloudflare will cache R2 content based on [cache rules](/cache/how-to/cache-rules/) at the Edge only.

Tiered cache can be enabled by configuring [Smart Tiered Cache](/cache/how-to/tiered-cache/#smart-tiered-cache) which will select an Upper Tier data center next to your R2 bucket for optimal performance.

## Additional considerations

- Apply access controls to your newly public bucket. Refer to [Control cache access with WAF and Snippets](/cache/interaction-cloudflare-products/waf-snippets/) for more information.
- Be aware of the [cacheable size limits](/cache/concepts/default-cache-behavior/#cacheable-size-limits) for files.

---

# Cache Analytics

URL: https://developers.cloudflare.com/cache/performance-review/cache-analytics/

import { FeatureTable } from "~/components"

Use Cache Analytics to improve site performance or reduce origin web server traffic. Cache Analytics helps determine if resources are missing from cache, expired, or ineligible for caching. Cache Analytics includes filter by hostname, list of top URLs that miss cache, and a query of up to three days of data.

## Availability

<FeatureTable id="cache.cache_analytics" />

## Access Cache Analytics

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account and domain.
2. Go to **Caching** > **Overview**.

## Requests vs Data Transfer

You can decide wheter to focus on **Requests** or **Data Transfer**:

- **Requests** (default view) help assess performance, as each cache miss slows down content delivery.
- **Data Transfer** is useful for cost analysis, since most hosting providers charge for every byte that leaves their network.

You can switch between these views while keeping other analytics filters applied.

For best practices related to Cache Analytics, refer to [Cache performance](/cache/performance-review/cache-performance/).

## Add filters

Cache Analytics also allows for flexible filtering of data. Create filters to focus on the traffic to optimize. Example filters include **Cache status**, **Host**, **Path**, or **Content type**.

To add filters, under **Cache Performance**, select **Add filter**. Select **Apply** when you are done.

## Review cache status

The **Requests summary** graph depicts how your traffic changes over time, such as in response to a high-traffic event or a recent configuration change. Note that the Requests summary content is based on a 10% sample of requests.

**Served by Cloudflare** indicates content served by Cloudflare that did not require contacting your origin web server. **Served by Origin** indicates traffic served from the origin web server.

For **Data Transfer**, **Revalidated** requests are considered **Served by Cloudflare**. However, revalidated requests count as **Served by Origin** within the **Requests** view. This analytics behavior reflects that Cloudflare must check the origin web server for revalidated cache requests before returning a result from cache.

**Cache status** graphs help explain why traffic is served from Cloudflare versus the origin web server. The graph shows analytics by content-type to portray how different components of your website perform:

For a breakdown of cache statuses and their descriptions, refer to [Cloudflare cache responses](/cache/concepts/cache-responses/).

## Review requests by source

Cache Analytics shows top metrics (Top-N) for several request components. Apply filters before reviewing Top-N metrics. For example, filtering to only view traffic with an Expired or Revalidated Cache status lets you review which URLs were primarily responsible for those statuses.

### Empty content types

Finding an **empty** content type when reviewing your analytics is common. This content type occurs when 301/302 redirects do not contain an HTTP response body. Additionally, most HTTP error codes, such as 403, do not return text/html and are therefore also reported as empty.

---

# Cache performance

URL: https://developers.cloudflare.com/cache/performance-review/cache-performance/

## Optimize cache ratios

Depending on the cache status you receive, you can make modifications to improve your cache ratio. To review the list of cache statuses, refer to [Cloudflare cache responses](/cache/concepts/cache-responses/).

- **Dynamic**: Default response for many file types including HTML. To cache additional content, refer to [Cache Rules](/cache/how-to/cache-rules/).
- **Revalidated**: To address an atypical quantity of revalidated content, consider [increasing your Edge Cache TTLs](/cache/how-to/cache-rules/settings/#edge-ttl).
- **Expired**: Consider [extending Edge Cache TTLs](/cache/how-to/cache-rules/settings/#edge-ttl)) for these resources via a Cache Rule or enable revalidation at your origin.
- **Miss**: Although tricky to optimize, there are a few potential remedies:
  - [Enable Argo Tiered Caching](/cache/how-to/tiered-cache/#enable-tiered-cache) to check cache in another Cloudflare data center before checking the origin web server.
  - [Create a custom cache key](/cache/how-to/cache-rules/examples/custom-cache-key/) for multiple URLs to match the same cached resource, for example by ignoring query string.

## Example reports for troubleshooting cache performance

Several examples of helpful insights into your site performance via Cache Analytics include:

- Not caching HTML.

  - Identify the issue: Select **Add filter** and select **Cache status equals Dynamic**.
  - Resolution: Set a Cloudflare Cache Rule to [cache dynamic content](/cache/how-to/cache-rules/examples/cache-everything/).

:::caution

This option caches all HTML regardless of the presence of dynamic content. If you use this approach to cache pages containing dynamic content, visitors may receive information not intended for them. To avoid caching dynamic content, you can add a condition to the rule's matching criteria to prevent it from matching that content. Some examples include:

- Checking for the presence of a cookie.
- Negative matching against known dynamic content file paths.
- Negative matching against dynamic content extensions (or lack of an extension).
  :::

* Short cache expiration TTL.

  - Identify the issue: Select **Add filter** and select **Cache status equals Revalidated**.
  - Resolution: [Increase Cloudflare's Edge Cache TTL via a Cache Rule](/cache/how-to/cache-rules/examples/edge-ttl/).

* Need to enable Tiered Cache or Custom Cache Key

  - Identify the issue: Select **Add filter** and select **Cache status equals Miss**.
  - Resolution: [Enable Argo Tiered Caching](/cache/how-to/tiered-cache/#enable-tiered-cache) or [create a custom cache key](/cache/how-to/cache-rules/examples/custom-cache-key/).

---

# Performance review

URL: https://developers.cloudflare.com/cache/performance-review/

import { DirectoryListing } from "~/components"

Review the following content to learn more about cache performance.

<DirectoryListing />

---

# Troubleshooting

URL: https://developers.cloudflare.com/cache/troubleshooting/

import { DirectoryListing } from "~/components"

The following topics are useful for troubleshooting Cache issues.

<DirectoryListing />

---

# Always Online

URL: https://developers.cloudflare.com/cache/troubleshooting/always-online/

Observe the following best practices when enabling Always Online with Internet Archive integration.

- **Allow requests from the Internet Archive IP addresses.** Origin servers receive requests from the Internet Archive IPs. Make sure you are not blocking requests from the Internet Archive IP range: `207.241.224.0/20` and `208.70.24.0/21`.
- **The Internet Archive does not consider your origin server's cache-control header.** When the Internet Archive is crawling sites, it will crawl sites regardless of their cache-control, since the Internet Archive does not cache assets, but archives them.
- **Consider potential conflicts with Cloudflare features that transform URIs.** Always Online with Internet Archive integration may cause issues with Cache Rules and other Cloudflare features that transform URIs due to the way the Internet Archive crawls pages to archive. Specifically, some redirects that take place at the edge may cause the Internet Archive's crawler not to archive the target URL. Before enabling Origin Cache Control, review [how Cloudflare caches resources by default](/cache/concepts/default-cache-behavior/) as well as any Cache Rules you have configured so that you can avoid these issues. If you experience problems, disable Always Online.
- **Do not block Known Bots or Verified Bots via a WAF custom rule.** If you block either of these bot lists, the Internet Archive will not be able to crawl.

Do not use Always Online with:

- API traffic.
- An [IP Access rule](/waf/tools/ip-access-rules/) or a [WAF custom rule](/waf/custom-rules/) that blocks the United States or
- Bypass Cache cache rules. Always Online ignores Bypass Cache cache rules and serves Always Online cached assets.

## Limitations

There are limitations with the Always Online functionality:

1. Always Online is not immediately active for sites recently added due to:
   - DNS record propagation, which can take 24-72 hours
   - Always Online has not initially crawled the website
2. Cloudflare cannot show private content behind logins or handle form submission (POSTs) if your origin web server is offline.

Always Online does not trigger for HTTP response codes such as [404](/support/troubleshooting/http-status-codes/4xx-client-error/#404-not-found), [503](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-503-service-temporarily-unavailable), or [500](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-500-internal-server-error) errors such as database connection errors or internal server errors.

## Frequently asked questions

1. How can I know if a page has been crawled?

   - You can go to the [Internet Archive](https://web.archive.org/) and search for the page URL to see if it has been crawled or not.
   - You can also check this via the [Internet Archive Availability API](https://archive.org/help/wayback_api.php).

2. Why were not pages x, y, and z crawled?

   - Since Cloudflare only requests to crawl the most popular pages on the site, it is possible that there will be missing pages. If you really want to archive a page, then you can visit the [Internet Archive](https://web.archive.org/save) save page and ask them to crawl a particular page.

3. What IP addresses do we need to allowlist to make sure crawling works?

   - IP Range: `207.241.224.0/20` and `208.70.24.0/21`. Note that this ip range belongs to Internet Archive and NOT Cloudflare, since it is the Internet Archive that does the crawling.

4. What user agent should the origin expect to see?
   - Currently the Internet Archive uses: `Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/605.1.15 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/605.1.15`.

---

# CSAM Scanning Tool

URL: https://developers.cloudflare.com/cache/reference/csam-scanning/

The Child Sexual Abuse Material (CSAM) Scanning Tool allows website owners to proactively identify and take action on CSAM located on their website. By enabling this tool, Cloudflare will compare content served for your website through the Cloudflare cache to known lists of CSAM. These lists are provided to Cloudflare by leading child safety advocacy groups such as the National Center for Missing and Exploited Children (NCMEC).

Remember, by enabling the Service, you agree to theÂ [Service-Specific Terms](https://www.cloudflare.com/service-specific-terms-application-services/#csam-scanning-tool-terms)Â for the CSAM Scanning Tool. You agree to use this tool solely for the purposes of preventing the spread of CSAM.

***

## Why would a URL be blocked?

Because knowingly distributing or viewing CSAM is illegal, the owner of the website has enabled Cloudflare's CSAM scanning tool to proactively identify and block images identified as CSAM located on their website.

***

## Configure the CSAM scanning tool

To enable the tool:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com/).
2. Select your account and zone.
3. Go to **Caching** > **Configuration**.
4. For **CSAM Scanning Tool**, select **Configure**.

You must provide an email address, which will be used to notify you in the event Cloudflare detects a positive match.

***

## What happens when a match is detected?

When a potential match is detected with the tool:

1. An email is sent to you once per day to inform you of any detections made in the past 24 hours. This email will include the file paths of any content that was matched.
2. If possible, a block is placed to prevent further serving of the matched content. If a block fails, we will indicate that the content has not been blocked in the email.

***

## What action should I take when a match is detected?

You are responsible for understanding and complying with any legal obligations you have as a website owner when made aware of any potential CSAM. Although legal obligations vary based on the provider and the jurisdiction, website owners often have obligations to report apparent CSAM, to remove content, and to preserve records. Some of those possible obligations are as follows:

- You likely have an obligation to report apparent CSAM to the appropriate authorities. You can file a report to NCMEC with additional information via NCMEC's CyberTip reporting form or find the preferred reporting portal for your jurisdiction via the INHOPE website.

<br/>

- You may need to preserve and securely store a copy of the content and related data in the case NCMEC or law enforcement reach out for additional details.
- You likely have an obligation to securely preserve certain information related to your report for at least 90 days in the case of an investigation. To ensure that access to the content is limited, take care not to store this information anywhere accessible to anyone but those within your organization responsible for legal requests.

<br/>

- You should remove the content and notify Cloudflare of the removal.
- Once any preservation obligations have been fulfilled, you should remove the content from your website. This is especially important if Cloudflare's notice to you indicates that our block was unsuccessful.

***

## How do I have a block removed from my website?

To disable a block, either because you have determined that the blocked content is not CSAM (a false positive) or because you have taken down the blocked content, view [Blocked Content in the Security Center](/security-center/blocked-content/) in the Cloudflare Dashboard and request reviews on the relevant blocks. A request to remove a block must be accompanied by a representation from you confirming that the blocked content is not CSAM or has been removed. 

These actions are available to users with the following roles:

- Admin
- Super Admin
- Trust & Safety

***

## Additional Resources

[CSAM Scanning Tool Supplemental Terms](https://www.cloudflare.com/supplemental-terms/)

[National Center for Missing and Exploited Children (NCMEC)](https://www.missingkids.org/)

[NCMEC CyberTipline](https://www.missingkids.org/gethelpnow/cybertipline)

[INHOPE](https://www.inhope.org/)

---

# Development Mode

URL: https://developers.cloudflare.com/cache/reference/development-mode/

Development Mode temporarily suspends Cloudflare's edge caching and [Polish](/images/polish/) features for three hours unless disabled beforehand. Development Mode allows customers to immediately observe changes to their cacheable content like images, CSS, or JavaScript.

:::note

To bypass cache for longer than three hours, use bypass cache in [Cache Rules](/cache/how-to/cache-rules/settings/#bypass-cache). 
:::

## Enable Development Mode

Development Mode temporarily bypasses Cloudflare's cache and does not purge cached files. To instantly purge your Cloudflare cache, refer to [purge cache](/cache/how-to/purge-cache/).

1. Log in to your Cloudflare account.
2. Select your domain.
3. Select **Caching**.
4. Toggle **Development Mode** to **On**.

---

# Using ETag Headers with Cloudflare

URL: https://developers.cloudflare.com/cache/reference/etag-headers/

ETag headers identify whether the version of a resource cached in the browser is the same as the resource at the web server.Â A visitor's browser stores ETags. When a visitor revisits a site, the browser compares each ETag to the one it stored. Matching values cause a `304 Not-Modified HTTP` response that indicates the cached resource version is current. Cloudflare supports both strong and weak ETags configured at your origin web server.

## Weak ETags

Weak ETag headers indicate a cached resource is semantically equivalent to the version on the web server but not necessarily byte-for-byte identical.

:::note


When using weak ETag headers, it is necessary to disable certain features such as [Email Obfuscation](/waf/tools/scrape-shield/email-address-obfuscation/) and [Automatic HTTPS Rewrites](/ssl/edge-certificates/additional-options/automatic-https-rewrites/) to prevent Cloudflare from removing the ETag headers set by your origin web server. For a comprehensive list of the features you need to disable, refer to the [Notes about end-to-end compression](/speed/optimization/content/compression/#notes-about-end-to-end-compression).


:::

## Strong ETags

Strong ETag headers ensure the resource in browser cache and on the web server are byte-for-byte identical. Use [Cache Rules](/cache/how-to/cache-rules/) to enable strong ETag headers.

### Behavior with Respect Strong ETags enabled

When you enable **Respect Strong ETags** in a cache rule, Cloudflare will use strong ETag header validation to ensure that resources in the Cloudflare cache and on the origin server are byte-for-byte identical.

However, in some situations Cloudflare will convert strong ETags to weak ETags. For example, given the following conditions:

* **Respect Strong ETags** is enabled
* [Brotli compression](/speed/optimization/content/compression/) is enabled
* The origin server's response includes an `etag: "foobar"` strong ETag header

The Cloudflare network will take the following actions, depending on the visitor's `accept-encoding` header and the compression used in the origin server's response:

<table-wrap>

| `accept-encoding`<br/>header from visitor | Compression used in origin server response | Cloudflare actions                                                                                       |
| ----------------------------------------- | ------------------------------------------ | -------------------------------------------------------------------------------------------------------- |
| `gzip, br`                                | GZIP                                       | Return GZIP-compressed response to visitor with strong ETag header: `etag: "foobar"`.                    |
| `gzip, br`                                | Brotli                                     | Return Brotli-compressed response to visitor with strong ETag header: `etag: "foobar"`.                  |
| `br`                                      | GZIP                                       | Decompress GZIP and return uncompressed response to visitor with weak ETag header: `etag: W/"foobar"`.   |
| `gzip`                                    | Brotli                                     | Decompress Brotli and return uncompressed response to visitor with weak ETag header: `etag: W/"foobar"`. |
| `gzip`                                    | (none)                                     | Return uncompressed response to visitor with strong ETag header: `etag: "foobar"`.                       |

</table-wrap>

Enabling **Respect Strong ETags** in Cloudflare automatically disables Rocket Loader, Email Obfuscation, Automatic HTTPS Rewrites, and Mirage.

### Behavior with Respect Strong ETags disabled

When **Respect Strong ETags** is disabled, Cloudflare will preserve strong ETag headers set by the origin web server if all the following conditions apply:

* The origin server sends a response compressed using GZIP or Brotli, or an uncompressed response.
* If the origin server sends a compressed response, the visitor accepts the same compression (GZIP, Brotli), according to the `accept-encoding` header.
* [Rocket Loader](/speed/optimization/content/rocket-loader/) and [Email Obfuscation](/waf/tools/scrape-shield/email-address-obfuscation/) features are disabled.

In all other situations, Cloudflare will either convert strong ETag headers to weak ETag headers or remove the strong ETag. For example, given the following conditions:

* **Respect Strong ETags** is disabled
* [Brotli compression](/speed/optimization/content/compression/) is enabled
* The origin server's response includes an `etag: "foobar"` strong ETag header

The Cloudflare network will take the following actions, depending on the visitor's `accept-encoding` header and the compression used in the origin server's response:

<table-wrap>

| `accept-encoding`<br/>header from visitor | Compression used in origin server response | Cloudflare actions                                                                                                                                |
| ----------------------------------------- | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| `gzip, br`                                | GZIP                                       | Decompress GZIP and return Brotli-compressed response to visitor (since Brotli compression is enabled) with weak ETag header: `etag: W/"foobar"`. |
| `gzip, br`                                | Brotli                                     | Return Brotli-compressed response to visitor with strong ETag header: `etag: "foobar"`.                                                           |
| `br`                                      | GZIP                                       | Decompress GZIP and return Brotli-compressed response to visitor with weak ETag header: `etag: W/"foobar"`.                                       |
| `gzip`                                    | Brotli                                     | Decompress Brotli and return GZIP-compressed response to visitor with weak ETag header: `etag: W/"foobar"`.                                       |
| `gzip`                                    | (none)                                     | Compress origin response using GZIP and return it to visitor with weak ETag header: `etag: W/"foobar"`.                                           |

</table-wrap>

## Important remarks

* You must set the value in a strong ETag header using double quotes (for example, `etag: "foobar"`). If you use an incorrect format, Cloudflare will remove the ETag header instead of converting it to a weak ETag.Â 

* If a resource is cacheable and there is a cache miss, Cloudflare does not send ETag headers to the origin server. This is because Cloudflare requires the full response body to fill its cache.

---

# Reference

URL: https://developers.cloudflare.com/cache/reference/

import { DirectoryListing } from "~/components"

Review the reference content to learn more about specific caching functionality.

<DirectoryListing />

---

# China Authoritative DNS

URL: https://developers.cloudflare.com/china-network/concepts/china-dns/

Cloudflare China Network provides a nearest-to-client nameservice by default. The DNS request is resolved on the data center nearest to the client out of China, and the HTTP request is served on the data center nearest to the client. The closest Cloudflare data center outside of China is adopted for clients outside of China, and the JD Cloud data center in China is adopted for clients in China.

## In-China Nameserver

Cloudflare is able to deploy DNS service in mainland China to improve the Time to First Byte (TTFB) performance. With this deployment, the DNS query will be resolved in mainland China instead of global DNS servers.

## When to use

Before you enable China Authoritative DNS, you should confirm that majority (over 90%) of the traffic is coming from China. After you enable China Authoritative DNS, all the global DNS requests will be routed to JD Cloud data centers in China instead of the nearest data centers.

## Comparison

The following table compares the default DNS offering with the In-China Nameserver option.

| DNS option          | Behavior                                      |
| ------------------- | --------------------------------------------- |
| Default             | Uses the DNS server closest to the end user.  |
| In-China Nameserver | Uses only DNS in China, operated by JD Cloud. |

## General setup

After you [enable the Cloudflare China Network service](/china-network/get-started/), do the following:

1.  Contact your Cloudflare sales team to enable the feature. Currently you cannot enable it in the Cloudflare dashboard.

    The current China Network supports both a [full setup](/dns/zone-setups/full-setup/) and a [partial setup](/dns/zone-setups/partial-setup/).

2.  Update your domain registrar with the assigned in-China nameservers.

    - For a full setup: These nameservers are displayed in the Cloudflare dashboard.
    - For a partial setup: Create a `CNAME` record pointing to `<host_name>.cdn.cloudflarecn.net`.

    For example, consider a zone named `samplezone.cn`. In this zone, you have two DNS records, `www` and `media`, pointing to two different origin servers. Your Authoritative DNS server needs to set the following DNS records:

    - CNAME `www.samplezone.cn` to `www.samplezone.cn.cdn.cloudflarecn.net`
    - CNAME `media.samplezone.cn` to `media.samplezone.cn.cdn.cloudflarecn.net`

3.  Test your configuration by checking if the domain resolves correctly.

For further assistance, contact your account team.

---

# Global Acceleration

URL: https://developers.cloudflare.com/china-network/concepts/global-acceleration/

:::note
Global Acceleration a service offering that can be an add-on to China Network and also extend Zero Trust services into China.
:::

Global Acceleration is a suite of connectivity, performance offerings and professional services designed to simplify your global assets' deployment in China. Global Acceleration is provided by our partners including CMI, CBC, and JD Cloud. Depending on what fits your needs best, Global Acceleration can support the following scenarios:

| Offer                                                 | Scenario                                                      |
| ----------------------------------------------------- | ------------------------------------------------------------- |
| [CDN Global Acceleration](#cdn-global-acceleration)   | Higher performance of China Network CDN with dynamic content. |
| [WARP Global Acceleration](#warp-global-acceleration) | WARP client used in China.                                    |
| [Magic WAN Connection](#mwan-global-acceleration)     | Magic WAN used in China.                                      |
| [ICP](#icp-services)                                  | ICP services.                                                 |
| [MLPS](#mlps-services)                                | MLPS certification services.                                  |
| [Travel SIM](#travel-sim)                             | Zero Trust clients in business traveling to China.            |

## CDN Global Acceleration

CDN Global Acceleration provides stable and reliable connections for dynamic content entering and exiting China, specifically beneficial for users' experience within the country. Additionally, this service can help facilitate seamless dynamic traffic management, ensuring stable connectivity across multiple global origins. It can support more advanced scenarios of Cloudflare features with great performance, such as accessing Cloudflare R2 and Cloudflare Fonts in China.

## WARP Global Acceleration

WARP Global Acceleration is a verified solution for enabling WARP client access within China, allowing remote employees to maintain secure and consistent connections.

## Magic WAN Global Acceleration

Magic WAN Global Acceleration is a verified solution for enabling Magic WAN access within China, allowing in-office employees to maintain secure and reliable connectivity.

## ICP services

Internet Content Provider (ICP) service simplifies the complexities of acquiring an ICP for domains to operate compliantly within China.

## MLPS services

The Multi-Level Protection Scheme (MLPS) service add-on streamlines the certification process to secure the MLPS L3 certification for your applications.

### Travel SIM

Travel SIM offers temporary, seamless WARP access for individual employees traveling to China, ensuring uninterrupted connectivity during their visit.

---

## General process

### 1. Validate prerequisites

Ensure that you have a Cloudflare [Enterprise plan](https://www.cloudflare.com/plans/enterprise/) and [China Network](/china-network/) if you want CDN Global Acceleration or other offers. Licenses of WARP and Magic WAN are required for WARP Connection or Magic WAN Global Acceleration.

### 2. Sign contract

Contact your Cloudflare account team. They will assist you with contracting with our local China partners.

### 3. Deploy Global Acceleration

Our local China partners will assist you to deploy Global Acceleration.

---

# Internet Content Provider (ICP)

URL: https://developers.cloudflare.com/china-network/concepts/icp/

Internet Content Provider (ICP) is a licensing regime instated by the Telecommunications Regulations of the People's Republic of China (ä¸­åŽäººæ°‘å…±å’Œå›½ç”µä¿¡æ¡ä¾‹), promulgated in September 2000.

Under ICP, all websites with their own domain name that operate inside China must obtain a license, whether hosted on a server in mainland China or provided to visitors from China via a CDN. Licenses are issued at the provincial level. You can use the Ministry of Industry and Information Technology (MIIT) website toÂ [check if a domain already has an ICP number](https://beian.miit.gov.cn/#/Integrated/recordQuery) (only available in Chinese).

All public websites in mainland China must have an ICP number [displayed on the website's home page](#display-your-icp-number). Websites with the same apex domain can share the same ICP number. China-based hosting providers are instructed to shut down any website (often without notice) without an ICP number.

## Types of ICP

To host web services in mainland China, you are legally required to acquire an **ICP filing** or an **ICP license** in China.

The type of ICP you must obtain depends on the type of website you are providing to customers in China:



|                    | ICP filing                                                                                                                                                                                                    | ICP license                                                                                                                                                                                          |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Definition         | An ICP filing, known in Chinese as â€œBeiâ€™An,â€ is the first level of ICP registration. An ICP filing enables the holder to host a website on a server or CDN in mainland China for informational purposes only. | An ICP license, known as â€œICP Zhengâ€ in Chinese, allows online platforms or third-party sellers selling goods and services to deploy their website on a hosting server or CDN within mainland China. |
| Website purpose    | Non-commercial and non-transactional purposes.                                                                                                                                                                | Commercial and transactional purposes.                                                                                                                                                               |
| Eligibility        | Representative office<br/>Wholly foreign-owned enterprise<br/>Joint venture<br/>Local company<br/>Individuals (personal website)                                                                              | Joint venture (foreign company with less than 50% ownership)<br/>Local company                                                                                                                       |
| Example format     | Beijing ICP preparation XXXXXXXX number                                                                                                                                                                       | Beijing ICP license XXXXXXXX number                                                                                                                                                                  |
| Other requirements | N/A                                                                                                                                                                                                           | Companies acquiring an ICP license must already have obtained an ICP filing.                                                                                                                         |
| Timeline           | 1-2 months                                                                                                                                                                                                    | 2-3 months                                                                                                                                                                                           |



If you wish to host a marketing-related website, you only need an ICP filing.

***

## Obtain an ICP number

Cloudflare recommends that you apply for an ICP license through your hosting or Cloud Services Provider. You will need to provide the necessary documents to your provider to register the ICP number on your behalf:



| For individuals                                                                                                                             | For commercial companies                                                  |
| ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- |
| â€“ ICP application form<br/>â€“ Copy of your personal ID<br/>â€“ Forms to authenticate website information<br/>â€“ Copy of your domain certificate | â€“ Copy of your business license<br/>â€“ Your organization code certificateÂ  |



After all required documents are submitted, it can take four to eight weeks to obtain an ICP number depending on the type of website and the province where the company is registered. Although there is no cost to register with the MIIT, your provider may charge you a fee.

After receiving the ICP number and the certificate, add it to your website's home page.Â 

## Display your ICP number

After you obtain an ICP number, you must display it in the footer of your website, like in the following example:

![An ICP number displayed in the footer of a website.](~/assets/images/china-network/icp-number-in-footer.png)

---

# Concepts

URL: https://developers.cloudflare.com/china-network/concepts/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Available products and features

URL: https://developers.cloudflare.com/china-network/reference/available-products/

The following products and features are available on the Cloudflare China Network operated by JD Cloud:

## Application services

| Product                                    | Feature                                                      | Description                                                                                                                                              |
| ------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| DNS                                        | [DNS resolution service](/china-network/concepts/china-dns/) | Authoritative DNS resolution inside mainland China.                                                                                                      |
| CDN                                        | [Caching](/cache/)                                           | Core cache features. Static cache only. Does not support Cache Reserve or Tiered Cache.                                                                  |
| CDN                                        | [Image transformations](/images/)                            | Optimize image format at the edge to fit a domainâ€™s layout.                                                                                              |
| [DDoS Protection](/ddos-protection/)       | Layer 7 (HTTP/HTTPS) DDoS protection                         | Layer 7 (application layer) protection against DDoS attacks such as HTTP flood attacks, WordPress Pingback attacks, HULK attacks, and LOIC attacks.      |
| Web Application Firewall (WAF)             | [WAF Managed Rules](/waf/managed-rules/)                     | Pre-configured OWASP rulesets and Cloudflare managed rulesets.                                                                                           |
| Web Application Firewall (WAF)             | [WAF custom rules](/waf/custom-rules/)                       | Custom WAF rules. Supports uploaded content scanning and managed challenges.                                                                             |
| Web Application Firewall (WAF)             | [Rate limiting rules](/waf/rate-limiting-rules/)             | Define rate limits for incoming requests matching an expression, and the action to take when those rate limits are reached.                              |
| [Page Shield](/page-shield/)               | Page Shield                                                  | Simplifies external script management by tracking loaded resources like scripts and providing alerts when it detects new resources or malicious scripts. |
| [Bot Management](/bots/)                   | Bot detection and mitigation                                 | Provides bot identification and protection for a domain. Only supports certain Machine Learning (ML) models.                                             |
| [Argo Smart Routing](/argo-smart-routing/) | Layer 7 smart routing                                        | Layer 7 (application layer) traffic smart-routed more efficiently to origin.                                                                             |
| [Rules](/rules/)                           | All rules products, except [Snippets](/rules/snippets/)      | Make adjustments to requests and responses, configure Cloudflare settings, and trigger specific actions for matching requests.                           |
| [Load Balancing](/load-balancing/)         | Full features                                                | Maximize application performance and availability.                                                                                                       |

## Developer services

| Product              | Feature                           | Description                                                                  |
| -------------------- | --------------------------------- | ---------------------------------------------------------------------------- |
| [Workers](/workers/) | Serverless computing              | A serverless execution environment running on the Cloudflare global network. |
| [Pages](/pages/)     | Serverless front-end applications | Deploy dynamic front-end applications in record time.                        |

## Network services

| Product             | Feature                                                                         | Description                                                                                                 |
| ------------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| Supported protocols | [IPv6](/network/ipv6-compatibility/)                                            | All data centers have IPv6 support by default.                                                              |
| Supported protocols | [SSL/TLS](/ssl/)                                                                | Customer Certificate, Dedicated Certificate, Universal Certificate, Custom, ACM (Dedicated), Universal SSL. |
| Supported protocols | [HTTP/3 (QUIC)](https://www.cloudflare.com/learning/performance/what-is-http3/) | The latest version of the HTTP protocol to optimize page loading performance.                               |
| Supported protocols | [WebSockets](/workers/runtime-apis/websockets/)                                 | Real-time communication with Cloudflare Workers serverless functions.                                       |

## Other services

| Product                  | Feature       | Description                                                      |
| ------------------------ | ------------- | ---------------------------------------------------------------- |
| [Analytics](/analytics/) | Web Analytics | Real-time traffic, security, and data monitoring.                |
| [Logs](/logs/)           | Instant Logs  | Live Tail your Cloudflare HTTP logs in the Cloudflare Dashboard. |

For more details or specific product features, contact your account team.

---

# Reference

URL: https://developers.cloudflare.com/china-network/reference/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Infrastructure

URL: https://developers.cloudflare.com/china-network/reference/infrastructure/

## China data centers

For up-to-date information, refer to the [Cloudflare China Network](https://www.cloudflare.com/china-network/) page.

### Network IP addresses

Cloudflare publishes a list of IP addresses for JD Cloud data centers, used by Cloudflare when connecting to the origin networks of customers to retrieve assets. These addresses are not the same IP addresses returned to website visitors as part of DNS resolution.

You can obtain the list of JD Cloud data center IP addresses via Cloudflare API. Use the [Cloudflare/JD Cloud IP Details](/api/resources/ips/methods/list/) operation with the `networks=jdcloud` query string parameter:

```sh title="Example request"
$ curl https://api.cloudflare.com/client/v4/ips?networks=jdcloud
```

IP addresses of JD Cloud data centers will be returned in the `jdcloud_cidrs` array:

```json title="Example response" {9,10,11}
{
  "result": {
    "ipv4_cidrs": [
      // (...)
    ],
    "ipv6_cidrs": [
      // (...)
    ],
    "jdcloud_cidrs": [
      // (...)
    ],
    "etag": "<ETAG>"
  },
  "success": true,
  "errors": [],
  "messages": []
}
```

Cloudflare will add new IP addresses to this list 30 days in advance before connecting from those IP addresses to an origin server. If you are using the China Network on JD Cloud, you should update your firewalls to reflect any IP address changes at least once every 30 days.

---

# About

URL: https://developers.cloudflare.com/client-ip-geolocation/about/

:::note


Client IP Geolocation is currently in closed Beta testing.


:::

Cloudflare Client IP Geolocation helps you understand where in the world a given client is located, even if their true client IP address is obscured by Cloudflare's proxy systems. We offer this service to ensure customers have high-quality experiences interacting with origins that rely on geolocation â€” such as mapping providers â€” and origin operators can deliver the right content to the right users. Ensuring origins know where clients are located avoids problems associated with traditional VPNs that obscure a consumerâ€™s geolocation and allow circumvention of geographic restrictions on content.

**Users cannot opt out of this geolocation support and Cloudflare will always make a best effort to convey geolocation.**

## Simple explanation

When Cloudflare connects to your origin over **IPv4**, we geolocate our VPN users to 1 of 1,000 egress locations (a `cell`). The list of cells is a super set of all the countries in the world and all US Designated Market Areas and includes multiple cells for large metropolitan areas. For all supported cell locations, download [our list](https://api.cloudflare.com/local-ip-ranges.csv).

When Cloudflare connects to your origin over **IPv6**, we geolocate our VPN users to one of thousands of locations distributed across the globe.

If a VPN user tries to use an interposed VPN or proxy system that attempts to hide their location, we share that information with downstream entities.

## Technical explanation

We map our egress IP addresses for our forward-proxy offerings to specific geolocations and continuously share this information with major geolocation database providers like *MaxMind*, *IP 2 Location*, and *Neustar*.

When Cloudflare receives an inbound request from a WARP or 1.1.1.1 user, we first geolocate the IP address that made the connection to Cloudflare. Then, we select an egress IP that maps to the client geolocation. We then discard Client IP information to preserve user privacy but return their true geographic location.

If the Client IP is a known proxy or VPN service without strong commitments to preserving client location, we return an unmapped/unknown location.

Because our cell list includes Designated Market Areas in the United States, you can use the returned geolocation information â€” or lack of it â€” to enforce content restrictions when delivering linear or on-demand video content subject to common licensing requirements.

## Example scenario

Jane is a Cloudflare WARP user in Glendive, Montana, USA. She is interested in the privacy benefits of using a VPN. Her client IP address is `108.59.112.0`, which is owned by [Mid-Rivers Telephone Cooperative](https://bgp.he.net/AS11961) and geolocates to 47.0984,-104.7275 (with a location accuracy of 50km using MaxMindâ€™s GeoIP service).

![Diagram of how Cloudflare reports accurate location information over VPN](~/assets/images/client-ip-geolocation/client-ip-geolocation-example.png)

Jane connects to Cloudflareâ€™s edge using a secure tunnel. Cloudflare operates an anycast network and dictates VPN ingress by anycast. Janeâ€™s VPN traffic lands in the Cloudflare Minneapolis data center, [930km from Glendive](http://www.gcmap.com/mapui?P=GDV-MSP).

A naive geolocation implementation would provide unacceptably inaccurate information. It would have Jane geolocated to Minneapolis, which is in a totally different state â€” Minnesota instead of Montana â€” and Designated Marketing Area â€” Glendive, MT, the [countryâ€™s smallest DMA](https://mediatracks.com/resources/nielsen-dma-rankings-2020/), instead of Minneapolis, MN â€” from her true location. From a content distribution perspective, this level of geolocation is **not acceptable**.

Instead, Cloudflare uses its [own geolocation mapping](#technical-explanation) to provide much more accurate information. Cloudflareâ€™s edge would geolocate Janeâ€™s client IP (`108.59.112.0`) to Glendive and select an egress IP address of `a.b.c.d` from its list of available egress IP addresses based on this geolocation result.

Jane attempts to watch live television via an over-the-top (OTT) video provider. The video providerâ€™s origin sees a connection from `a.b.c.d`. A geolocation lookup on `a.b.c.d` returns Glendale, MT, and Jane is able to access the linear video content because the OTT provider is able to geolocate the VPN traffic to the true client location.

---

# FAQ

URL: https://developers.cloudflare.com/client-ip-geolocation/faq/

:::note


Client IP Geolocation is currently in closed Beta testing.


:::

## Website operators

### What does this functionality mean for me as a website operator?

If you operate a website or ISP that needs to use IP address geolocation information for geographic content restriction, consider allowing IP addresses associated with a VPN.

You can now restrict content delivery to Cloudflare VPN users using the same client IP geolocation mechanisms used for non-VPN users.

### How does the above scenario change if I use Cloudflare to secure my infrastructure?

There is significant cross pollination between Cloudflare forward- and reverse-proxy services. When a user connects through Cloudflare proxies to origin infrastructure protected by Cloudflare security tools, our origin-facing tools automatically consume information from our user-facing systems about client geography, IP reputation, and other client metadata. This process happens in a privacy-preserving manner that reduces unnecessary collection of personally identifiable information while ensuring customers can maintain their desired security posture.

WAF custom rules specifying country- or region-level match criteria will match correctly on users passing through our VPN and forward-proxy systems with no action needed from you.

### In the example, what happens when Cloudflareâ€™s Minneapolis data center is removed from service for maintenance?

The [example scenario](/client-ip-geolocation/about/#example-scenario) still provides accurate geolocation data.

Geography-specific egress IPs are not tightly coupled to physical Cloudflare network locations. We continue using geography-specific egress IPs even if the geographically closest network location or locations are rerouted.

### What happens when a user nests or chains VPNs and connects to Cloudflare through a downstream proxy service?

Cloudflare will make best efforts to identify such circumstances and communicate this information upstream to origins. Client IPs will geolocate as `unknown` when the entity that made the initial connection to Cloudflare appears to have originated from an open-proxy service or we are unsure of the location of the user.

### I want greater geographic detail on egress locations. Can you provide it?

Yes! We can provide much finer granularity for origins reachable over IPv6. We encourage adoption of IPv6 for the good of the Internet, as well as for providing much finer detail on user locations to origin operators.

### What incentives does Cloudflare have to ensure location information is accurate?

Cloudflare wants those using our consumer VPN and corporate forward-proxy services to have as smooth an experience as possible. We want our users to have uninterrupted browsing experiences. At the same time, we also want to give origin operators the information they need to distribute the right content to the right users at the right times.

## Cloudflare VPN users

### What does this mean for me as a Cloudflare VPN user?

If you use [Cloudflare WARP](/warp-client/) or [1.1.1.1](/1.1.1.1/), geolocation improves your user experience. Because we communicate your geographic location accurately (but still in a non-identifiable way), you should have accurate, geography-specific experiences and uninterrupted access to the content you are licensed to consume in your local geography.

We also maintain all of our [privacy commitments](https://www.cloudflare.com/trust-hub/privacy-and-data-protection/) regarding your use of our consumer application and will keep your Internet browsing private and secure.

### What if I am a user and want to spoof my location?

Cloudflare does not permit or support the spoofing of location and will never offer such functionality in the future.

---

# Analytics

URL: https://developers.cloudflare.com/cloudflare-for-platforms/cloudflare-for-saas/hostname-analytics/

import { Render } from "~/components"

You can use custom hostname analytics for two general purposes: exploring how your customers use your product and sharing the benefits provided by Cloudflare with your customers.

These analytics include **Site Analytics**, **Bot Analytics**, **Cache Analytics**, **Security Events**, and [any other datasets](/analytics/graphql-api/features/data-sets/) with the `clientRequestHTTPHost` field.

:::note


The plan of your Cloudflare for SaaS application determines the analytics available for your custom hostnames.


:::

## Explore customer usage

Use custom hostname analytics to help your organization with billing and infrastructure decisions, answering questions like:

* "How many total requests is your service getting?"
* "Is one customer transferring significantly more data than the others?"
* "How many global customers do you have and where are they distributed?"

If you see one customer is using more data than another, you might increase their bill. If requests are increasing in a certain geographic region, you might want to increase the origin servers in that region.

To access custom hostname analytics, either [use the dashboard](/analytics/faq/about-analytics/) and filter by the `Host` field or [use the GraphQL API](/analytics/graphql-api/) and filter by the `clientRequestHTTPHost` field. For more details, refer to our tutorial on [Querying HTTP events by hostname with GraphQL](/analytics/graphql-api/tutorials/end-customer-analytics/).

## Share Cloudflare data with your customers

With custom hostname analytics, you can also share site information with your customers, including data about:

* How many pageviews their site is receiving.
* Whether their site has a large percentage of bot traffic.
* How fast their site is.

Build custom dashboards to share this information by specifying an individual custom hostname in `clientRequestHTTPHost` field of [any dataset](/analytics/graphql-api/features/data-sets/) that includes this field.

## Logpush

[Logpush](/logs/about/) sends metadata from Cloudflare products to your cloud storage destination or SIEM.

Using [filters](/logs/reference/filters/), you can send set sample rates (or not include logs altogether) based on filter criteria. This flexibility allows you to maintain selective logs for custom hostnames without massively increasing your log volume.

Filtering is available for [all Cloudflare datasets](/logs/reference/log-fields/zone/).

<Render file="filtering-limitations" product="logs" />

---

# Cloudflare for SaaS

URL: https://developers.cloudflare.com/cloudflare-for-platforms/cloudflare-for-saas/

import { LinkButton, Render } from "~/components";

<Render file="ssl-for-saas-definition" /> <br />

As a SaaS provider, you may want to support subdomains under your own zone in addition to letting your customers use their own domain names with your services. For example, a customer may want to use their vanity domain `app.customer.com` to point to an application hosted on your Cloudflare zone `service.saas.com`. Cloudflare for SaaS allows you to increase security, performance, and reliability of your customers' domains.

<Render file="non-contract-enablement" product="fundamentals" />

## Benefits

When you use Cloudflare for SaaS, it helps you to:

- Provide custom domain support.
- Keep your customers' traffic encrypted.
- Keep your customers online.
- Facilitate fast load times of your customers' domains.
- Gain insight through traffic analytics.

## Limitations

If your customers already have their applications on Cloudflare, they cannot control some Cloudflare features for hostnames managed by your Custom Hostnames configuration, including:

- Argo
- Early Hints
- Page Shield
- Spectrum
- Wildcard DNS

## How it works

As the SaaS provider, you can extend Cloudflare's products to customer-owned custom domains by adding them to your zone [as custom hostnames](/cloudflare-for-platforms/cloudflare-for-saas/domain-support/). Through a suite of easy-to-use products, Cloudflare for SaaS routes traffic from custom hostnames to an origin, set up on your domain. Cloudflare for SaaS is highly customizable. Three possible configurations are shown below.

### Standard Cloudflare for SaaS configuration:

Custom hostnames are routed to a default origin server called fallback origin. This configuration is available on all plans.

![Standard case](~/assets/images/cloudflare-for-platforms/use-cases/Standard.png)

### Cloudflare for SaaS with Apex Proxying:

This allows you to support apex domains even if your customers are using a DNS provider that does not allow a CNAME at the apex. This is available as an add-on for Enterprise plans. For more details, refer to [Apex Proxying](/cloudflare-for-platforms/cloudflare-for-saas/start/advanced-settings/apex-proxying/).

![Advanced case](~/assets/images/cloudflare-for-platforms/use-cases/Advanced.png)

### Cloudflare for SaaS with BYOIP:

This allows you to support apex domains even if your customers are using a DNS provider that does not allow a CNAME at the apex. Also, you can point to your own IPs if you want to bring an IP range to Cloudflare (instead of Cloudflare provided IPs). This is available as an add-on for Enterprise plans.

![Pro Case](~/assets/images/cloudflare-for-platforms/use-cases/Pro.png)

## Availability

Cloudflare for SaaS is bundled with non-Enterprise plans and available as an add-on for Enterprise plans. For more details, refer to [Plans](/cloudflare-for-platforms/cloudflare-for-saas/plans/).

## Next steps

<LinkButton
	variant="primary"
	href="/cloudflare-for-platforms/cloudflare-for-saas/start/getting-started/"
>
	Get started
</LinkButton>
<LinkButton
	variant="secondary"
	href="https://blog.cloudflare.com/introducing-ssl-for-saas/"
>
	Learn more
</LinkButton>

---

# Plans

URL: https://developers.cloudflare.com/cloudflare-for-platforms/cloudflare-for-saas/plans/

import { FeatureTable, Render } from "~/components"

<FeatureTable id="ssl.z_custom_hostnames" />

## Enterprise plan benefits

The Enterprise plan offers features that give SaaS providers flexibility when it comes to meeting their end customer's requirements. In addition to that, Enterprise customers are able to extend all of the benefits of the Enterprise plan to their customer's custom hostnames. This includes advanced Bot Mitigation, WAF rules, analytics, DDoS mitigation, and more.

In addition, large SaaS providers rely on Enterprise level support, multi-user accounts, SSO, and other benefits that are not provided in non-Enterprise plans.

<Render file="non-contract-enablement" product="fundamentals" />

---

# Demos

URL: https://developers.cloudflare.com/cloudflare-for-platforms/workers-for-platforms/demos/

import { ExternalResources, GlossaryTooltip } from "~/components"

Learn how you can use Workers for Platforms within your existing architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Workers for Platforms.

<ExternalResources type="apps" products={["Workers for Platforms"]} />

---

# Workers for Platforms

URL: https://developers.cloudflare.com/cloudflare-for-platforms/workers-for-platforms/

import { CardGrid, Description, Feature, LinkTitleCard, Plan, RelatedProduct, Stream } from "~/components"

<Description>


Deploy custom code on behalf of your users or let your users directly deploy their own code to your platform, managing infrastructure.


</Description>

<Plan type="paid" />

Workers for Platforms allows you to run your own code as a wrapper around your user's code. With Workers for Platforms, you can logically group your code separately from your users' code, create custom logic, and use additional APIs such as [script tags](/cloudflare-for-platforms/workers-for-platforms/configuration/tags/) for bulk operations.

Workers for Platforms is built on top of [Cloudflare Workers](/workers/). Workers for Platforms lets you surpass Cloudflare Workers' 500 scripts per account [limit](/cloudflare-for-platforms/workers-for-platforms/platform/limits/).

<br></br>

<Stream id="c8afb7a0a811f07db4b4ffaf56c277bc" title="Workers for Platforms Overview" thumbnail="8.6s" />

***

## Features

<Feature header="Get started" href="/cloudflare-for-platforms/workers-for-platforms/get-started/configuration/" cta="Get started">
Learn how to set up Workers for Platforms.
</Feature>

<Feature header="Workers for Platforms architecture" href="/cloudflare-for-platforms/workers-for-platforms/reference/how-workers-for-platforms-works/" cta="Learn more">
Learn about Workers for Platforms architecture.
</Feature>

***

## Related products

<RelatedProduct header="Workers" href="/workers/" product="workers">

Cloudflare Workers provides a serverless execution environment that allows you to create new applications or augment existing ones without configuring or maintaining infrastructure.


</RelatedProduct>

***

## More resources

<CardGrid>

<LinkTitleCard title="Limits" href="/cloudflare-for-platforms/workers-for-platforms/platform/limits/" icon="document">
Learn about limits that apply to your Workers for Platforms project.
</LinkTitleCard>

<LinkTitleCard title="Developer Discord" href="https://discord.cloudflare.com" icon="discord">
Connect with the Workers community on Discord to ask questions, show what you are building, and discuss the platform with other developers.
</LinkTitleCard>

<LinkTitleCard title="@CloudflareDev" href="https://x.com/cloudflaredev" icon="x.com">
Follow @CloudflareDev on Twitter to learn about product announcements, and what is new in Cloudflare Workers.
</LinkTitleCard>

</CardGrid>

---

# Terraform

URL: https://developers.cloudflare.com/cloudflare-one/api-terraform/access-with-terraform/

| Requirements                                                                                                                             |
| ---------------------------------------------------------------------------------------------------------------------------------------- |
| [Terraform](https://developer.hashicorp.com/terraform/tutorials/certification-associate-tutorials/install-cli) installed on your machine |
| The [Cloudflare provider](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs) properly configured                 |

[Terraform](https://developer.hashicorp.com/terraform/tutorials/certification-associate-tutorials/install-cli) is a tool for building, changing, and versioning infrastructure, and provides components and documentation for building [Cloudflare resources](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs). Listed below are examples to help you get started with building Access with Terraform. For a more generalized guide on configuring Cloudflare and Terraform, visit our [Getting Started with Terraform and Cloudflare](https://blog.cloudflare.com/getting-started-with-terraform-and-cloudflare-part-1/) blog post.

## Create an application with Terraform

1. Create an application.

   Here is an example configuration:

   ```tf
   variable "domain" {
     default = "example.com"
   }

   variable "zone_id" {
     default = <CLOUDFLARE_ZONE_ID>
   }

   resource "cloudflare_access_application" "cf_app" {
     zone_id          = var.zone_id
     name             = "My Example App"
     domain           = var.domain
     session_duration = "24h"
   }
   ```

2. Next, we need to export our environment variables and secrets:

   ```sh
   export CLOUDFLARE_EMAIL=<CLOUDFLARE_EMAIL>
   export CLOUDFLARE_API_KEY=<CLOUDFLARE_API_KEY>
   ```

3. Now we can run a `terraform plan` which will output any proposed changes. Make sure to review the plan carefully:

   ```sh
   terraform plan
   ```

   ```sh output
   Refreshing Terraform state in-memory prior to plan...
   The refreshed state will be used to calculate this plan, but will not be
   persisted to local or remote state storage.

   ------------------------------------------------------------------------

   An execution plan has been generated and is shown below.
   Resource actions are indicated with the following symbols:
   	+ create

   Terraform will perform the following actions:

   	# cloudflare_access_application.cf_app will be created
   	+ resource "cloudflare_access_application" "cf_app" {
   			+ aud              = (known after apply)
   			+ domain           = "example.com"
   			+ id               = (known after apply)
   			+ name             = "My Example App"
   			+ session_duration = "24h"
   			+ zone_id          = "1ce82492016e71df631bf4af9c02587f"
   		}

   Plan: 1 to add, 0 to change, 0 to destroy.

   ------------------------------------------------------------------------

   Note: You didn't specify an "-out" parameter to save this plan, so Terraform
   can't guarantee that exactly these actions will be performed if
   "terraform apply" is subsequently run.
   ```

4. Apply these changes using the `apply` command, once they look accurate and you're comfortable moving forward:

   ```sh
   terraform apply --auto-approve
   ```

   ```sh output
   cloudflare_access_application.cf_app: Creating...
   cloudflare_access_application.cf_app: Creation complete after 2s [id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx]

   Apply complete! Resources: 1 added, 0 changed, 0 destroyed.
   ```

## Create a policy

After you've created an application, you can start creating policies and attaching them to applications:

```tf
   resource "cloudflare_access_policy" "cf_policy" {
     application_id = cloudflare_access_application.cf_app.id
     zone_id        = var.zone_id
     name           = "Example Policy"
     precedence     = "1"
     decision       = "allow"

     include {
       email = ["test@example.com"]
     }
   }
```

To do so:

1. Run a `terraform plan`:

   ```sh
   terraform plan
   ```

   ```sh output

   Refreshing Terraform state in-memory prior to plan...
   The refreshed state will be used to calculate this plan, but will not be
   persisted to local or remote state storage.

   cloudflare_access_application.cf_app: Refreshing state... [id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx]

   ------------------------------------------------------------------------

   An execution plan has been generated and is shown below.
   Resource actions are indicated with the following symbols:
    + create

   Terraform will perform the following actions:

    # cloudflare_access_policy.cf_policy will be created
    + resource "cloudflare_access_policy" "cf_policy" {
        + application_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        + decision       = "allow"
        + id             = (known after apply)
        + name           = "My Example Policy"
        + precedence     = 1
        + zone_id        = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

        + include {
            + email = [
                + "test@example.com",
              ]
          }
      }

   Plan: 1 to add, 0 to change, 0 to destroy.

   ------------------------------------------------------------------------

   Note: You didn't specify an "-out" parameter to save this plan, so Terraform
   can't guarantee that exactly these actions will be performed if
   "terraform apply" is subsequently run.
   ```

2. Next, apply these changes using the `apply` command, once they look accurate and you're comfortable moving forward:

   ```sh
   terraform apply --auto-approve
   ```

### Configuring an identity provider

The example below shows how you can configure an identity provider and attach it to a policy:

```tf
resource "cloudflare_access_identity_provider" "github_oauth" {
  account_id = <CLOUDFLARE_ACCOUNT_ID>
  name       = "GitHub OAuth"
  type       = "github"
  config {
    client_id     = <GITHUB_CLIENT_ID>
    client_secret = <GITHUB_CLIENT_SECRET>
  }
}

resource "cloudflare_access_policy" "cf_policy" {
  application_id = cloudflare_access_application.cf_app.id
  zone_id        = var.zone_id
  name           = "My Example Policy"
  precedence     = "1"
  decision       = "allow"

  include {
    email = ["test@example.com"]
    github {
      name                 = "My GitHub Org"
      identity_provider_id = cloudflare_access_identity_provider.github_oauth.id
    }
  }
}
```

These are the basics to get up and running with Access and Terraform. Refer to our [API documentation](/api/) for other endpoints that can be managed via Terraform.

---

# API and Terraform

URL: https://developers.cloudflare.com/cloudflare-one/api-terraform/

import { DirectoryListing } from "~/components"

This section covers a few common use cases with the API and Terraform to manage Cloudflare Zero Trust. For more information, refer to our [API documentation](/api/) and [Terraform reference guide](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs).

<DirectoryListing />

## Set dashboard to read-only

Super Administrators can lock all settings as read-only in Zero Trust. Read-only mode ensures that all updates for the account are made through the API or Terraform.

To enable read-only mode:

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Settings** > **Account**.
2. Enable **API/Terraform read-only mode**.

All users, regardless of [user permissions](/cloudflare-one/roles-permissions/), will be prevented from making configuration changes through the UI.

---

# Scoped API tokens

URL: https://developers.cloudflare.com/cloudflare-one/api-terraform/scoped-api-tokens/

The administrators managing policies and groups in Cloudflare Access might be different from the users responsible for configuring WAF custom rules or other Cloudflare settings. Cloudflare Access supports [scoped API tokens](/fundamentals/api/get-started/create-token/) so that team members and automated systems can manage settings specific to Access without having permission to modify other configurations in Cloudflare.

## Creating a scoped API token

1. In the [Cloudflare dashboard](https://dash.cloudflare.com/), select the user icon > **My Profile**.

2. Select the **API Tokens** tab. The existing tokens will display.

   ![Existing API tokens listed in the API Tokens tab.](~/assets/images/cloudflare-one/api-terraform/create-token.png)

3. Select **Create Token**.

4. Select **Get started** next to **Create Custom Token**.

5. Select **Account** and **Access: Organizations, Identity Providers, and Groups** in the drop-downs under **Permissions**. You can configure the token to be Read or Write in the third drop-down.

   ![Dropdown displaying read and write options for API token customization.](~/assets/images/cloudflare-one/api-terraform/edit-token.png)

6. In the final section, the token can be applied to a single account or multiple if you are an administrator of multiple Cloudflare accounts.

7. Select **Continue to summary**. The next page will display the token details and instructions on how to use it.

## Review tokens

You can review tokens created in the **API Tokens** tab. In this view, you can roll, revoke, or edit issued tokens.

![A list of created API tokens.](~/assets/images/cloudflare-one/api-terraform/view-token.png)

---

# Access

URL: https://developers.cloudflare.com/cloudflare-one/changelog/access/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in src/content/changelogs/access.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# CASB

URL: https://developers.cloudflare.com/cloudflare-one/changelog/casb/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in src/content/changelogs/casb.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Browser Isolation

URL: https://developers.cloudflare.com/cloudflare-one/changelog/browser-isolation/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in src/content/changelogs/browser-isolation.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Data Loss Prevention

URL: https://developers.cloudflare.com/cloudflare-one/changelog/dlp/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in src/content/changelogs/dlp.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Digital Experience Monitoring

URL: https://developers.cloudflare.com/cloudflare-one/changelog/dex/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in src/content/changelogs/dex.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Email Security

URL: https://developers.cloudflare.com/cloudflare-one/changelog/email-security/

import { ProductReleaseNotes } from "~/components"

{/* <!-- Actual content lives in /src/content/release-notes/email-security.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Changelog

URL: https://developers.cloudflare.com/cloudflare-one/changelog/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in associated src/content/changelogs/{productName}.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Risk score

URL: https://developers.cloudflare.com/cloudflare-one/changelog/risk-score/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in src/content/changelogs/risk-score.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Cloudflare Tunnel

URL: https://developers.cloudflare.com/cloudflare-one/changelog/tunnel/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in src/content/changelogs/tunnel.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Gateway

URL: https://developers.cloudflare.com/cloudflare-one/changelog/gateway/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in src/content/changelogs/gateway.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Zero Trust WARP Client

URL: https://developers.cloudflare.com/cloudflare-one/changelog/warp/

import { ProductReleaseNotes } from "~/components"

{/* <!-- All changelog entries live in src/content/changelogs/warp.yaml. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# App Launcher

URL: https://developers.cloudflare.com/cloudflare-one/applications/app-launcher/

import { Render } from "~/components"

<Render file="access/app-launcher" />

## Tags

<Render file="access/tags" />

---

# Block page

URL: https://developers.cloudflare.com/cloudflare-one/applications/block-page/

import { Render } from "~/components"

<Render file="access/block-page" />

---

# Add bookmarks

URL: https://developers.cloudflare.com/cloudflare-one/applications/bookmarks/

import { Render } from "~/components"

<Render file="access/bookmarks" />

---

# Applications

URL: https://developers.cloudflare.com/cloudflare-one/applications/

import { DirectoryListing } from "~/components"

Cloudflare Zero Trust can secure self-hosted and SaaS applications with Zero Trust rules.

Learn how to secure your applications, and how to configure one dashboard for your users to reach all the applications you've secured behind Cloudflare Zero Trust:

<DirectoryListing />

Refer to our [reference architecture](/reference-architecture/architectures/sase/) for an understanding on how to architect a Zero Trust and SASE solution.

---

# Login page

URL: https://developers.cloudflare.com/cloudflare-one/applications/login-page/

import { Render } from "~/components"

You can customize the login page that is displayed to end users when they go to an Access application.

<Render file="access/login-page" />

---

# Connections

URL: https://developers.cloudflare.com/cloudflare-one/connections/

import { DirectoryListing } from "~/components"

Learn how you can connect your applications, devices, and networks to Cloudflare.

<DirectoryListing />

---

# Auto-move events

URL: https://developers.cloudflare.com/cloudflare-one/email-security/auto-moves/

Auto-move events are events where emails are automatically moved to different inboxes based on the disposition Email Security assigned to them.

Email Security shows you the total number of auto-moves and the source folder from which these moves are originating from.

To configure auto-move events:

1. Log in to [Zero Trust](https://one.dash.cloudflare.com/).
2. Select **Email security**.
3. Select **Settings**.
4. Select **Moves**.
5. Under **Auto-moves**, select **Configure**.
6. Assign actions based on malicious, spoof, suspicious, spam, and bulk dispositions. Select among:
   - **Soft delete - user recoverable**: Moves the message to the user's **Recoverable Items - Deleted** folder. Messages can be recovered by the user.
   - **Hard delete - admin recoverable**: Completely deletes messages from a user's inbox.
   - **Move to trash**: Moves messages to the trash or deleted items email folder.
   - **Move to junk**: Moves the message to the junk or spam folder.
   - **No action**: Messages stay in the origin folder.
7. Select **Post-delivery** moves:
   - **(Recommended) Post-delivery response**: Enabling this option allows Email Security to rescan delivered emails at multiple time intervals for previously unknown phishing sites or campaigns.
   - **(Recommended) Phish submission response**: Enabling this option allows Email Security to move emails that your users reported as phishing and Email Security determined to be malicious.
8. Select **Save**.

---

# Outbound Data Loss Prevention (DLP)

URL: https://developers.cloudflare.com/cloudflare-one/email-security/outbound-dlp/

:::note[Compatibility]
Outbound DLP is only compatible with Microsoft 365.
:::

Outbound Data Loss Prevention ensures the protection of sensitive information in outbound emails with [Cloudflare Data Loss Prevention (DLP)](/cloudflare-one/policies/data-loss-prevention/). Outbound Data Loss Prevention integrates with your inbox, and it proactively monitors your email to prevent unauthorized data leaks.

## Get started

To begin using outbound DLP, install the Cloudflare add-in in Microsoft 365:

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Email Security** > **Outbound DLP**.
2. In **Protect sensitive data in outbound emails**, select **Get started**.
3. Select **Download add-in** to download the Cloudflare add-in.
4. Configure Microsoft 365 to use the Cloudflare add-in:
   1. In the [Microsoft 365 Apps admin center](https://config.office.com/), go to **Microsoft 365 Admin Center** > **Settings** > **Integrated Apps**.
   2. Select **Upload custom apps**. For the application type, choose _Office Add-in_.
   3. Select **Upload manifest file (.xml) from device**.
   4. Upload the Cloudflare add-in file.
   5. Verify and complete the wizard.
5. Confirm the Cloudflare add-in was configured in Microsoft 365.

After configuring the Cloudflare add-in in Microsoft 365, you can select **Add a policy** to create an [outbound DLP policy](#create-an-outbound-policy).

:::note
The Cloudflare add-in can take up to 24 hours to propagate after install.
:::

## Create an outbound policy

An outbound policy allows you to control outbound email flow.

To create an outbound DLP policy:

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Email Security** > **Outbound DLP**.
2. Select **Add a policy**.
3. Name your policy.
4. Build an expression to match specific email traffic. For example, you can create a policy that blocks outbound emails containing identifying numbers:

   | Selector            | Operator | Value                                                     | Logic | Action |
   | ------------------- | -------- | --------------------------------------------------------- | ----- | ------ |
   | Recipient email     | not in   | `example.com`                                             | And   | Block  |
   | Matched DLP profile | in       | _Social Security, Insurance, Tax, and Identifier Numbers_ |       |        |

5. (Optional) Choose whether to use the default block message or a custom message.
6. Select **Create policy**.

After creating your policy, you can modify or reorder your policies in **Email Security** > **Outbound DLP**.

### Selectors

| Selector            | Description                                                                                                                |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| Recipient email     | The intended recipient of an outbound email.                                                                               |
| Email sender        | The user in your organization sending an email.                                                                            |
| Matched DLP profile | The [DLP profile](/cloudflare-one/policies/data-loss-prevention/dlp-profiles/) that content of an email matches upon scan. |

## DLP Assist add-in

The Data Loss Prevention (DLP) Assist add-in allows Microsoft O365 users to deploy a DLP solution for free using Cloudflare's Email Security.

To set up DLP Assist add-in:

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Email Security** > **Outbound DLP**.
2. Select **View Microsoft add-in instructions** > Select **Download add-in**. This downloads a `.xml` file necessary to install the add-in on the client side.
3. Set up the add-in in Microsoft 365:
    - Log in to the [Microsoft admin panel](https://security.microsoft.com/homepage) and go to **Microsoft 365 Admin Center** > **Settings** > **Integrated Apps**.
    - Choose **Upload custom apps** and select **Office Add-in** for the application type.
    - Select **Upload manifest file (.xml) from device**.
    - Upload the Cloudflare add-in file you downloaded in step three. Then, verify and complete the wizard. It can take up to 24 hours for an add-in to propagate.

The add-in works by inserting headers into the [EML](https://en.wikipedia.org/wiki/EML) on the client side before the message is sent out. 

To block, encrypt, or send approval, you can configure rules within Microsoft Purview DLP:

1. Go to [Microsoft Purview](https://purview.microsoft.com/datalossprevention/overview?tid=11648e1c-3d60-40e2-bf07-f8d481e48e2d).
2. Select **Policies** > **Create policy**.
3. Do not choose any templates or custom policy. Select **Next**.
4. Choose a name and description for the policy: You can choose any name. However, this guide will use `Cloudflare Assist Block`.
5. Select **Next** on **Admin Units**:
    - Choose to only apply to **Exchange Email**.
    - Choose **Create or customize advanced DLP Rules**.
6. Select **Create rule**:
    - Create a policy name.
    - Add the following conditions:
        - **Header contains words or phrases**: `Key: cf_outbound_dlp with Value: BLOCK`
        - Select **AND**.
        - **Content is shared from Microsoft 365**: Select **with people from outside my organization**.
7. Under **Actions**, the admin can choose what to do with the message. You can use the **Restrict access or encrypt the content in Microsoft 365 locations** to block the message or encrypt it.
8. Under **User notifications**, turn on notifications. Admins can also edit the message if they want to. You can also configure if the admin wants to receive a notification under **Incident reports** >  **Use this severity level in admin alerts and reports**.
9. Select **Save**.
10. Select **Turn the Policy On Immediately**.

## Set up DLP profiles

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Email Security** > **Outbound DLP**. 
2. Select **Add a policy**:
    - **Name your policy**.
    - **Build an expression**.
    - **(Optional) Configure message type**: Create a custom message to be be displayed when a violation occurs.
3. Select **Save**.

---

# Email Security

URL: https://developers.cloudflare.com/cloudflare-one/email-security/

import { Description } from "~/components"

:::caution[Important]
Refer to [Area 1](/email-security/) if you are looking for the Area 1 documentation.
:::

<Description>
Secure your email inbox with API-first Email Security.
</Description>

Email Security secures your email inbox with API-first email security. Email Security integrates with your existing email provider and blocks phishing attacks, malware, [Business Email Compromise](https://www.cloudflare.com/en-gb/learning/email-security/business-email-compromise-bec/) attacks, and vendor email fraud.

Email Security allows you to:

- Configure [allow policies](/cloudflare-one/email-security/detection-settings/allow-policies/), [blocked senders](/cloudflare-one/email-security/detection-settings/blocked-senders/), [trusted domains](/cloudflare-one/email-security/detection-settings/trusted-domains/), and [additional detections](/cloudflare-one/email-security/detection-settings/additional-detections/).
- [Automatically move messages](/cloudflare-one/email-security/auto-moves/) to specific folders based on a [certain disposition](/cloudflare-one/email-security/reference/dispositions-and-attributes/).
- [Manage directories](/cloudflare-one/email-security/directories/).
- [Monitor your inbox](/cloudflare-one/email-security/email-monitoring/), perform a thorough [search of your email](/cloudflare-one/email-security/email-monitoring/search-email/), and download a [disposition report](/cloudflare-one/email-security/email-monitoring/download-disposition-report/).

---

# Phish submissions

URL: https://developers.cloudflare.com/cloudflare-one/email-security/phish-submissions/

import { GlossaryTooltip, Render } from "~/components"

As part of your continuous email security posture, administrators and security analysts need to submit missed <GlossaryTooltip term="phishing">phishing</GlossaryTooltip> samples to Email Security, so Cloudflare can process them and take necessary action.

Submitting missed phish samples to Cloudflare is of paramount importance and necessary for continuous protection. Submitting missed phish samples helps Cloudflare improve our machine learning (ML) models, and alerts us of new attack vectors before they become prevalent.

There are three routes you can use to report an email as a phish:

- Via Investigation, by [reclassifying an email](/cloudflare-one/email-security/phish-submissions/#reclassify-an-email).
- Via [PhishNet O365](/cloudflare-one/email-security/phish-submissions/#phishnet-o365) or [PhishNet for Google Workspace](/cloudflare-one/email-security/phish-submissions/#phishnet-for-google-workspace), depending on your email provider.
- Via [Submission addresses](/cloudflare-one/email-security/phish-submissions/#submission-addresses).

## Reclassify an email

1. Log in to [Zero Trust](https://one.dash.cloudflare.com/).
2. Select **Email Security** > **Investigation**.
3. On the **Investigation** page, under **Your matching messages**, select the message you want to reclassify. Select the three dots, then select **Reclassify**. By selecting **Reclassify**, you are requesting a new disposition for the message.
4. Select the new disposition, then select **Save**.

## PhishNet O365

PhishNet is an add-in button that helps users to submit directly to Email Security <GlossaryTooltip term="phishing">phish</GlossaryTooltip> samples missed by Email Security's detection. 

To set up PhishNet O365:

1. Log in to the [Microsoft admin panel](https://admin.microsoft.com/). Go to **Microsoft 365 admin center** > **Settings** > **Integrated Apps**.
2. Select **Upload custom apps**.
3. Choose **Provide link to manifest file** and paste the following URL:
    ```txt
    https://phishnet-o365.area1cloudflare-webapps.workers.dev?clientId=ODcxNDA0MjMyNDM3NTA4NjQwNDk1Mzc3MDIxNzE0OTcxNTg0Njk5NDEyOTE2NDU5ODQyNjU5NzYzNjYyNDQ3NjEwMzIxODEyMDk1NQ
    ```
4. Verify and complete the wizard.

## PhishNet for Google Workspace

To set up PhishNet with Google Workspace you need admin access to your Google Workspace account.

### Set up PhishNet for Google Workspace

1. Log in to [Google Workspace Marketplace apps](https://workspace.google.com/marketplace/app/cloudflare_phishnet/11369379045) using this direct link and an administrator account.
2. Select **Admin install** to install Cloudflare PhishNet. Read the warning, and select **Continue**.
3. You will be redirected to the **Allow data access** page, where you can choose to install Cloudflare PhishNet for **Everyone at your organization**, or **Certain groups or organizational units**. If you choose the latter option, you will have to select the users in the next step.
4. After choosing the groups you want to install PhishNet for, agree with Google's terms of service, and select **Finish**. 
5. Cloudflare PhishNet has been installed. Select **DONE**.

You have now successfully installed Cloudflare PhishNet.

### Submit phish with PhishNet

1. In your Gmail web client, open the message you would like to flag as either spam or phish.
2. Select the PhishNet logo on the side panel.
3. Under **Select Submission Type**, select **Spam** or **Phish**.
4. Select **Submit Report**.

## Submission addresses

To view the destination addresses of user and team submissions:

1. Log in to [Zero Trust](https://one.dash.cloudflare.com/).
2. Select **Email Security**.
3. Select **Settings**.
4. Go to **Phish submission** > **Submission addresses** > **View**.

The dashboard will display **User submission addresses** and **Team submission addresses**.

---

# Identity FAQ

URL: https://developers.cloudflare.com/cloudflare-one/faq/authentication-faq/

[â® Back to FAQ](/cloudflare-one/faq/)

## Can Access work with multiple identity providers at the same time?

Yes. Your team can simultaneously use multiple providers, reducing friction when working with partners or contractors. Get started by adding your preferred identity providers as login methods in Zero Trust. Then, when securing a new application behind Access, you'll be able to choose which providers you want your users to log in with to reach that application.

## What if the identity provider my team uses is not listed?

You can add your preferred identity providers to Cloudflare Access even if you do not see them listed in Zero Trust, as long as these providers support SAML 2.0 or [OpenID Connect (OIDC)](/cloudflare-one/identity/idp-integration/generic-oidc/).

## How do end users log out of an application protected by Access?

Access provides a URL that will end a user's current session.

To force log out of an Access application, go to:

`<your-application-domain>/cdn-cgi/access/logout`

To log out of an App Launcher session, go to:

`<your-team-name>.cloudflareaccess.com/cdn-cgi/access/logout`

For more information, refer to our [session management page](/cloudflare-one/identity/users/session-management/#log-out-as-a-user).

---

# PhishGuard

URL: https://developers.cloudflare.com/cloudflare-one/email-security/phish-guard/

PhishGuard is a managed email security service that provides resources for end-to-end phish and targeted attack management and response. With PhishGuard, you can preemptively block [phishing attacks](https://www.cloudflare.com/en-gb/learning/access-management/phishing-attack/), [malware](https://www.cloudflare.com/en-gb/learning/ddos/glossary/malware/), [Business Email Compromise (BEC)](https://www.cloudflare.com/en-gb/learning/email-security/business-email-compromise-bec/), and vendor email fraud.

To use PhishGuard:

1. Log in to [Zero Trust](https://one.dash.cloudflare.com/).
2. Select **Email security**.
3. Select **PhishGuard**.

The dashboard will display the following metrics:

- ROI Calculator
- Insider threat defense
- Email threat hunting
- Actions
- API Status
- Managed email security operations
- Reports

## ROI Calculator

Use the ROI Calculator to compare triage durations and hourly rates to calculate PhishGuard's return on investment.

The ROI Calculator displays:

- Total aggregated saved number in USD dollars.
- Triage duration: The amount of time in minutes spent triaging the message.
- Hourly rate.

## Insider threat defense

An [insider threat](https://www.cloudflare.com/en-gb/learning/access-management/what-is-an-insider-threat/) is a risk to an organization's security stemming from someone associated with the organization.

Insider threat defense on the dashboard displays **Insider leads** and **Insider reports generated**. **Insider leads** displays the number of emails identified as potential insider threat email. **Insider reports generated** displays the number of reports created based on insider leads.

## Email threat hunting

Email threat hunting displays previously unknown phishing attacks.

Email threat hunting displays **Threat leads generated** and **Total reposts generated**.

## Actions

**Actions** allows you to review the most common actions taken by the PhishGuard team, such as escalations, threat hunts, and moves.

## API Status

API Status allows you to monitor and configure the current status of API message auto-moves and directory integrations.

Select **Message moves** to [configure auto-moves](/cloudflare-one/email-security/auto-moves/). Select **Directory integration** to [configure directories](/cloudflare-one/email-security/directories/).

## Managed email security operations

Managed email security operations allows you to review the results of phish submissions reviewed by the PhishGuard team.

It displays the following:

- Total [phish submissions](/cloudflare-one/email-security/phish-submissions/)
- Tracked incidents
- Median time to resolve
- Resolved track incidents

## Reports

Under Reports, you can review reports of threats discovered and resolved by the PhishGuard team.

If you select the three dots, you can:

- **View report details**: Report Details gives you the following information about each report:
  - **Overview**: An Overview of the report. This includes date and time of the report, type of attack performed, and more.
  - **Target and victimology**: Company targeted.
  - **Details**: Displays information such as delivery disposition, current disposition, ES Alert ID, Message-ID, Timestamp, Subject, and Attempted Fraudulent Amount.
  - **Indicators of compromise (IOC)**: [Indicators of compromise (IOC)](https://www.cloudflare.com/en-gb/learning/security/what-are-indicators-of-compromise/) are information about a specific security breach that can help security teams determine if an attack has taken place. 
- Preview email.
- [Move email](/cloudflare-one/email-security/auto-moves/).

---

# Tunnels FAQ

URL: https://developers.cloudflare.com/cloudflare-one/faq/cloudflare-tunnels-faq/

import { Render } from "~/components";

[â® Back to FAQ](/cloudflare-one/faq/)

## â€‹Can I create a Tunnel for an apex domain?

Yes. With [Named Tunnels](https://blog.cloudflare.com/argo-tunnels-that-live-forever/) you can create a CNAME at the apex that points to the named tunnel.

## â€‹Does Cloudflare Tunnel support Websockets?

Yes. Cloudflare Tunnel has full support for Websockets.

## â€‹Does Cloudflare Tunnel support gRPC?

Yes. <Render file="tunnel/grpc-support" />

## How can Tunnel be used with Partial DNS (CNAME Setup)?

Cloudflare offers two modes of setup: [Full Setup](/dns/zone-setups/full-setup/), in which the domain uses Cloudflare DNS nameservers, and [Partial Setup](/dns/zone-setups/partial-setup/) (also known as CNAME setup) in which the domain uses non-Cloudflare DNS servers.

The best experience with Cloudflare Tunnel is using Full Setup because Cloudflare manages DNS for the domain and can automatically configure DNS records for newly started Tunnels.

You can still use Tunnel with Partial Setup. You will need to create a new DNS record with your current DNS provider for each new hostname connected through Cloudflare Tunnel. The DNS record should be of type CNAME or ALIAS if it is on the root of the domain. The name of the record should be the subdomain it corresponds to (e.g. `example.com` or `tunnel.example.com`) and the value of the record should be `subdomain.domain.tld.cdn.cloudflare.net`. (e.g. `example.com.cdn.cloudflare.net` or `tunnel.example.com.cdn.cloudflare.net`)

## How can origin servers be secured when using Tunnel?

Tunnel can expose web applications to the Internet that sit behind a NAT or firewall. Thus, you can keep your web server otherwise completely locked down. To double check that your origin web server is not responding to requests outside Cloudflare while Tunnel is running you can run netcat in the command line:

```sh
netcat -zv [your-server's-ip-address] 80
netcat -zv [your-server's-ip-address] 443
```

If your server is still responding on those ports, you will see:

```txt
[ip-address] 80 (http) open
```

If your server is correctly locked down, you will see:

```txt
[ip-address] 443 (https): Connection refused
```

## What records are created for routing to a Named Tunnel's hostname?

Named Tunnels can be routed via DNS records, in which case we use CNAME records to point to the `<UUID>.cfargotunnel.com`; Or as Load Balancing endpoints, which also point to `<UUID>.cfargotunnel.com`.

## Does Cloudflare Tunnel send visitor IPs to my origin?

No. When using Cloudflare Tunnel, all requests to the origin are made internally between `cloudflared` and the origin.

To log external visitor IPs, you will need to [configure an alternative method](/support/troubleshooting/restoring-visitor-ips/restoring-original-visitor-ips/).

## Why does the name 'warp' and 'argo' appear in some legacy materials?

Cloudflare Tunnel was previously named Warp during the beta phase. As Warp was added to the Argo product family, we changed the name to Argo Tunnel to match. Once we no longer required users to purchase Argo to create Tunnels, we renamed Argo Tunnel to Cloudflare Tunnel.

For more information about migrating from Argo Tunnel, refer to [Migrate legacy tunnels](/cloudflare-one/connections/connect-networks/do-more-with-tunnels/migrate-legacy-tunnels/).

## Is it possible to restore a deleted tunnel?

No. You cannot undo a tunnel deletion. If the tunnel was locally-managed, its [`config.yaml` file](/cloudflare-one/connections/connect-networks/get-started/tunnel-useful-terms/#configuration-file) will still be present and you can create a new tunnel with the same configuration. If the tunnel was remotely-managed, both the tunnel and its configuration are permanently deleted.

## How do I contact support?

Before contacting the Cloudflare support team:

1. Take note of any specific error messages and/or problematic behaviors.

2. Make sure that `cloudflared` is updated to the [latest version](https://github.com/cloudflare/cloudflared).

3. Gather any relevant error/access logs from your server.

4. (Locally-managed tunnels only) Set [`--loglevel`](/cloudflare-one/connections/connect-networks/configure-tunnels/cloudflared-parameters/run-parameters/#loglevel) to `debug`, so the Cloudflare support team can get more info from the `cloudflared.log` file.

5. Include your [Cloudflare Tunnel diagnostic logs](/cloudflare-one/connections/connect-networks/troubleshoot-tunnels/diag-logs/) (`cloudflared-diag-YYYY-MM-DDThh-mm-ss.zip`).

---

# Devices FAQ

URL: https://developers.cloudflare.com/cloudflare-one/faq/devices-faq/

[â® Back to FAQ](/cloudflare-one/faq/)

## Why does my Windows device appear to switch from Wi-Fi to Ethernet when I enable WARP?

As the WARP client has replaced WinDivert with WinTun architecture, all Windows machines using WinTun will show as being connected using a virtual adapter. Windows, by default, shows virtual adapter connections with a wired Ethernet connection icon, even if the device is connected over wireless. This is by design and should have no impact on connectivity.

## Why is my device not connecting to a closer Cloudflare data center?

As our [Network Map](https://www.cloudflare.com/en-gb/network/) shows, we have locations all over the globe. However, in the Advanced Connection stats of our application, you may notice that the data center (colo) you are connecting to isn't necessarily the one physically closest to your location. This can be due to a number of reasons:

* Sometimes your nearest colo may be undergoing maintenance or having problems. Check the [Cloudflare Status page](https://www.cloudflarestatus.com/) for system status.
* Your Internet provider may choose to route traffic along an alternate path for reasons such as cost savings, reliability, or other infrastructure concerns.

## Why is my public IP address sometimes visible?

Cloudflare WARP Client in WARP mode was meant to ensure all your traffic is kept private between you and the origin (the site you are connecting to), but not from the origin itself. In a number of cases, if the origin site you are communicating with can't determine who you are and where you're from, they can't serve locale relevant content to you.
Sites inside Cloudflare network are able to see this information. If a site is showing you your IP address, chances are they are in our network. Most sites outside our network (orange clouded sites) however are unable to see this information and instead see the nearest egress colo to their server. We are working to see if in the future we can't find a way to more easily share this information with a limited number of gray clouded sites where it is relevant to both parties.

## Why has my throughput dropped while using WARP?

Cloudflare WARP is in part powered by 1.1.1.1. When visiting sites or going to a new location on the Internet, you should see blazing fast DNS lookups. However, WARP is built to trade some throughput for enhanced privacy, because it encrypts all traffic both to and from your device. While this isn't noticeable at most mobile speeds, on desktop systems in countries where high speed broadband is available, you may notice a drop. We think the tradeoff is worth it though and continue to work on improving performance all over the system.

## Why is my device not connecting to a public Wi-Fi?

The Wi-Fi network may have a captive portal that is blocking WARP from establishing a secure connection. In order to access the portal, and therefore the Internet, you will need to temporarily turn off WARP. After you login to the captive portal through your browser, you can turn WARP back on to access corporate resources.

For more information, refer to [Captive portal detection](/cloudflare-one/connections/connect-devices/warp/configure-warp/warp-settings/captive-portals/).

## Why is my device not connecting to the Internet?

A third-party service or ISP may be blocking WARP, or Zero Trust settings may be misconfigured. For a list of common issues and steps to resolve, refer to our [troubleshooting guide](/cloudflare-one/connections/connect-devices/warp/troubleshooting/common-issues/).

## Why is my device not connecting to the corporate Wi-Fi?

An [OS firewall rule](/cloudflare-one/connections/connect-devices/warp/configure-warp/route-traffic/warp-architecture/#system-firewall) on the device may be blocking the EAP/Radius server that allows users to join the Wi-Fi network. If your corporate Wi-Fi uses a Radius server for network authentication, add the Radius server to your [Split Tunnel](/cloudflare-one/connections/connect-devices/warp/configure-warp/route-traffic/split-tunnels/) Exclude list.

## Why is my device not connecting to my private network?

If your private network is [exposed via Cloudflare Tunnel](/cloudflare-one/connections/connect-networks/private-net/cloudflared/):

* Verify that the WARP client is [properly configured](/cloudflare-one/connections/connect-networks/private-net/cloudflared/#device-configuration) on the device.
* Verify that the user is allowed through by your Access and Gateway policies.
* Verify that the [local LAN settings](/cloudflare-one/connections/connect-networks/private-net/cloudflared/#router-configuration) for the device do not overlap with the CIDR range of your private network.

When contacting Cloudflare support, ensure that you include [WARP debug logs](/cloudflare-one/connections/connect-devices/warp/troubleshooting/warp-logs/) for your device. These logs will help Cloudflare support understand the overall architecture of your machine and networks.

---

# General

URL: https://developers.cloudflare.com/cloudflare-one/faq/general-faq/

[â® Back to FAQ](/cloudflare-one/faq/)

## What is the difference between Cloudflare Gateway and 1.1.1.1?

1.1.1.1 does not block any DNS query. When a browser requests for example.com, 1.1.1.1 simply looks up the answer either in cache or by performing a full recursive DNS query.

Cloudflare Gateway's DNS resolver introduces security into this flow. Instead of allowing all DNS queries, Gateway first checks the hostname being queried against the intelligence Cloudflare has about threats on the Internet. If that query matches a known threat, or is requesting a blocked domain configured by an administrator as part of a Gateway policy, Gateway stops it before the site could load for the user - and potentially execute code or phish that team member.

## Is multi-factor authentication supported?

Access is subjected to the MFA policies set in your identity provider. For example, users attempting to log in to an Access protected app might log in through Okta. Okta would enforce an MFA check before sending the valid authentication confirmation back to Cloudflare Access.

Access does not have an independent or out-of-band MFA feature.

## Which browsers are supported?

These browsers are supported:

- Internet Explorer 11
- Edge (current release, last release)
- Firefox (current release, last release)
- Chrome (current release, last release)
- Safari (current release, last release)

## What data localization services are supported?

Cloudflare Zero Trust can be used with the Data Localization Suite to ensure that traffic is only inspected in the regions you choose. For more information refer to [Use Zero Trust with Data Localization Suite](/data-localization/how-to/zero-trust/).

---

# Getting started with Cloudflare Zero Trust FAQ

URL: https://developers.cloudflare.com/cloudflare-one/faq/getting-started-faq/

[â® Back to FAQ](/cloudflare-one/faq/)

## How do I sign up for Cloudflare Zero Trust?

You can sign up today at [this link](https://dash.cloudflare.com/sign-up/teams). Follow the onboarding steps, choose a team name and a payment plan, and start protecting your network in just a few minutes.

## What is a team domain/team name?

Your team domain is a unique subdomain assigned to your Cloudflare account, for example, `<your-team-name>.cloudflareaccess.com`. [Setting up a team domain](/cloudflare-one/setup/#create-a-zero-trust-organization) is an essential step in your Zero Trust configuration. This is where your users will find the apps you have secured behind Cloudflare Zero Trust â€” displayed in the [App Launcher](/cloudflare-one/applications/app-launcher/) â€” and will be able to make login requests to them. The customizable portion of your team domain is called **team name**. You can view your team name and team domain in Zero Trust under **Settings** > **Custom Pages**.

| team name        | team domain                             |
| ---------------- | --------------------------------------- |
| `your-team-name` | `<your-team-name>.cloudflareaccess.com` |

You can change your team name at any time, unless you have the Cloudflare dashboard SSO feature enabled on your account. Cloudflare dashboard SSO does not currently support team name changes.

:::caution[Warning]

If you change your team name, you need to update your organization's identity providers (IdPs) and the WARP client to reflect the new team name in order to avoid any mismatch errors.
:::

### Why is my old team name is still showing up on the Login page and App Launcher?

After changing your team name, you will need to check your Block page, Login page, and App Launcher settings to make sure the new team name is reflected.

To verify that your team name change is successfully rendering on the Block page, Login page and App Launcher:

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Settings** > **Custom Pages**.
2. Find the **Block page** and **Login page** > select **Customize** next to the page you would like to review first.
3. Review that the value in **Your Organization's name** matches your new team name.
4. If the desired name is not already displayed, change the value to your desired team name and select **Save**.
5. Check both pages (**Block page** and **Login page**) to set **Your Organization's name** as your desired team name.

The App Launcher will display the same team name set on the Login page, so you do not need to update the **Your Organization's name** field in the App Launcher page.

## How do I change my subscription plan?

To make changes to your subscription, visit the Billing section under Account in [Zero Trust](https://one.dash.cloudflare.com/). You can change or cancel your subscription at any time. Just remember - if you downgrade your plan during a billing cycle, your downgraded pricing will apply in the next billing cycle. If you upgrade during a billing cycle, you will be billed for the upgraded plan at the moment you select it.

## How are active seats measured?

Cloudflare Zero Trust subscriptions consist of seats that users in your account consume. When users authenticate to an application or enroll their agent into WARP, they count against one of your active seats. Seats can be added, removed, or revoked at **Settings** > **Account** > **Plan**. If all seats are currently consumed, you must first remove users before decreasing your purchased seat count.

### Removing users

User seats can be removed for Access and Gateway at **My Team** > **Users**. Removing a user will have consequences both on Access and on Gateway:

- **Access**: All active sessions for that user will be invalidated. A user will be able to log back into an application unless you create an [Access policy](/cloudflare-one/policies/access/) to block future logins from that user.

- **Gateway**: All active devices for that user will be logged out of your Zero Trust organization, which stops all filtering and routing via the WARP client. A user will be able to re-enroll their device unless you create a [device enrollment policy](/cloudflare-one/connections/connect-devices/warp/deployment/device-enrollment/) to block them.

:::caution

The Remove action will remove a user's seat, but it will not permanently revoke their ability to authenticate. To permanently disable a user's ability to authenticate, you must modify the policies that allow them to reach a given application or enroll a device in WARP.

:::

### Revoking users

The Revoke action will terminate active sessions and log out active devices, but will not remove the user's consumption of an active seat.

## How do I know if my network is protected behind Cloudflare Zero Trust?

You can visit the [Zero Trust help page](https://help.teams.cloudflare.com). This page will give you an overview of your network details, as well as an overview of the categories that are being blocked and/or allowed.

---

# FAQ

URL: https://developers.cloudflare.com/cloudflare-one/faq/

import { LinkButton } from "~/components";

Review answers to the most commonly asked questions on Cloudflare Zero Trust, as well as a troubleshooting section to help you solve common issues and errors you may come across.

If you cannot find the answer you are looking for, go to our [community page](https://community.cloudflare.com/) and post your question there.

---

## Getting started with Cloudflare Zero Trust

For extra guidance on experiencing Cloudflare Zero Trust for the first time.

<LinkButton variant="primary" href="/cloudflare-one/faq/getting-started-faq/">
	Getting started â¯
</LinkButton>

## General

For general questions on Cloudflare Zero Trust and how it works.

<LinkButton variant="primary" href="/cloudflare-one/faq/general-faq/">
	General â¯
</LinkButton>

## Identity

For questions on identity providers and accessing applications behind Cloudflare Zero Trust.

<LinkButton variant="primary" href="/cloudflare-one/faq/authentication-faq/">
	Identity â¯
</LinkButton>

## Policies

For questions on how policies work, and how to create and test them.

<LinkButton variant="primary" href="/cloudflare-one/faq/policies-faq/">
	Policies â¯
</LinkButton>

## Devices

For questions on device connectivity and the WARP client.

<LinkButton variant="primary" href="/cloudflare-one/faq/devices-faq/">
	Devices â¯
</LinkButton>

## Tunnels

For questions on connecting applications with Tunnels.

<LinkButton
	variant="primary"
	href="/cloudflare-one/faq/cloudflare-tunnels-faq/"
>
	Tunnels â¯
</LinkButton>

## Troubleshooting

Got an unexpected error? Check if it is covered in our troubleshooting section.

<LinkButton variant="primary" href="/cloudflare-one/faq/troubleshooting/">
	Troubleshooting â¯
</LinkButton>

---

# Policies FAQ

URL: https://developers.cloudflare.com/cloudflare-one/faq/policies-faq/

[â® Back to FAQ](/cloudflare-one/faq/)

## What is the order of policy enforcement?

Gateway and Access policies generally trigger from top to bottom based on their position in the policy table in the UI. Exceptions include Bypass and Service Auth policies, which Access evaluates first. Similarly, for Gateway HTTP policies, Do Not Inspect and Isolate policies take precedence over all Allow or Block policies. To learn more about order of enforcement, refer to our documentation for [Access policies](/cloudflare-one/policies/access/#order-of-execution) and [Gateway policies](/cloudflare-one/policies/gateway/order-of-enforcement/).

## **How can I bypass the L7 firewall for a website?**

Cloudflare Gateway uses the hostname in the HTTP `CONNECT` header to identify the destination of the request. Administrators who wish to bypass a site must create a [Do Not Inspect](/cloudflare-one/policies/gateway/http-policies/#do-not-inspect) policy in order to prevent HTTP inspection from occurring on both encrypted and plaintext traffic.

Bypassing the L7 firewall results in no HTTP traffic inspection, and logging is disabled for that HTTP session.

## Can I secure applications with a second-level subdomain URL?

Yes. Ensure that your SSL certificates cover the first- and second-level subdomain. Most certificates only cover the first-level subdomain and not the second. This is true for most Cloudflare certificates. To cover a second-level subdomain with a CF certificate, create an [advanced certificate](/ssl/edge-certificates/advanced-certificate-manager/manage-certificates/).

Wildcard-based policies in Cloudflare Access only cover the level where they are applied. Add the wildcard policy to the left-most subdomain to be covered.

## How do isolation policies work together with HTTP policies?

Isolation policies, like all HTTP policies, are evaluated in stages. When a user makes a request which evaluates an Isolation policy, the request will be rerouted to an isolated browser and re-evaluated for HTTP policies. This makes it possible for an isolated browser to remotely render a block page, or have malicious content within the isolated browser blocked by HTTP policies.

## Why is API or CLI traffic not isolated?

Isolation policies are applied to requests that include `Accept: text/html*`. This allows Browser Isolation policies to co-exist with API and command line requests.

## Can Access enforce policies on a specific nonstandard port?

No. Cloudflare Access cannot enforce a policy that would contain a port appended to the URL. However, you can use Cloudflare Tunnel to point traffic to non-standard ports. For example, if Jira is available at port `8443` on your origin, you can proxy traffic to that port via Cloudflare Tunnel.

## Why can I still reach domains blocked by a Gateway policy?

If the domain is blocked by a DNS, network, or HTTP policy, it may be because:

- **Your policy is still being updated.** After you edit or create a policy, Cloudflare updates the new setting across all of our data centers around the world. It takes about 60 seconds for the change to propagate.

If the domain is only blocked by a DNS policy, it may be because:

- **Your device is using another DNS resolver.** If you have other DNS resolvers in your DNS settings, your device could be using IP addresses for resolvers that are not part of Gateway. As a result, the domain you are trying to block is still accessible from your device. Make sure to remove all other IP addresses from your DNS settings and only include Gateway's DNS resolver IP addresses.

- **Your policy is not assigned to a DNS location.** If your policy is not assigned to a DNS location and you send a DNS query from that location, Gateway will not apply that policy. Assign a policy to a DNS location to make sure the desired policy is applied when you send a DNS query from that location.

- **Your DoH endpoint is not a Gateway DNS location**. Browsers can be configured to use any DoH endpoint. If you chose to configure DoH directly in your browser, make sure that the DoH endpoint is a Gateway DNS location.

If the domain is only blocked by a network policy, it may be because:

- **Your browser is reusing an existing connection**. Network policies only apply when a connection is opened. If a browser is connected to a domain to be blocked by a network policy, Gateway will not block requests until the connection is closed. To block the domain, close any related tabs or restart your browser.

## When does Access return a Forbidden status page versus a login page?

Access returns a Forbidden page with status codes `401`/`403` when it determines there is no way a user can pass a [policy](/cloudflare-one/policies/access/). If Cloudflare can make a full policy determination that a user will not be able to log in, Access will return a Forbidden page instead of a [login page](/cloudflare-one/applications/login-page/).

For example, your application has a policy that requires a user to be in a [specific geolocation](/cloudflare-one/policies/access/#allow) to log in.

As admin, you could define this geolocation policy by using [Include](/cloudflare-one/policies/access/#include) rules, meaning the user could log in to the application from Country A or Country B.

Or you could define this geolocation policy using a [Require](/cloudflare-one/policies/access/#require) rule, meaning the user must be in Country A to log in.

If a user from country C attempts to access the application, in both the Include and Require scenarios, the user will receive the Forbidden page. This is because Country C was not defined in either scenario. Therefore, Cloudflare has determined that this user cannot meet policy requirements and will receive the Forbidden status page.

---

# Troubleshooting

URL: https://developers.cloudflare.com/cloudflare-one/faq/troubleshooting/

import { GlossaryTooltip, Render } from "~/components";

[â® Back to FAQ](/cloudflare-one/faq/)

## I tried to register the WARP client with my Zero Trust domain but received the following error messages: `Authentication Expired` and `Registration error. Please try again later`.

When a user logs into an organization, WARP will open a web page so the user can sign in via Cloudflare Access. Access then generates a JSON Web Token (JWT) that is passed from the web page to the WARP client to authenticate the device. This JWT has a timestamp indicating the exact time it was created, as well as a timestamp indicating it will expire 50 seconds into the future.

This error message means that when the JWT is finally passed to the WARP client, it has already expired. One of two things can be happening:

1. (Most likely): Your computer system clock is not properly synced using Network Time Protocol (NTP). Visit [https://time.is](https://time.is) on the affected machine to validate your clock is properly synchronized within 20 seconds of the actual time.

2. You are waiting more than one minute to open Cloudflare WARP from the time Cloudflare Access prompts you. Open the WARP client as soon as you get the prompt.

## I see a website is blocked, and it shouldn't be.

If you believe a domain has been incorrectly blocked, you can use [this form](https://radar.cloudflare.com/categorization-feedback/) to get the URL reviewed.

## I see an error saying `No Access-Control-Allow-Origin header is present on the requested resource`.

Cloudflare Access requires that the credentials: `same-origin parameter` be added to JavaScript when using the Fetch API (to include cookies). AJAX requests fail without this parameter present. For more information, refer to our documentation about [CORS settings](/cloudflare-one/identity/authorization-cookie/cors/).

## I see untrusted certificate warnings for every page and I am unable to browse the Internet.

Advanced security features including HTTPS traffic inspection require users to install and trust the Cloudflare root certificate on their machine or device. If you are installing certificates manually on all of your devices, these steps will need to be performed on each new device that is to be subject to HTTP Filtering.
To install the Cloudflare root certificate, follow [this guide](/cloudflare-one/connections/connect-devices/user-side-certificates/).

## I see error 526 when browsing to a website.

Gateway presents an **HTTP Response Code: 526** error page in the following cases:

- **An untrusted certificate is presented from the origin to Gateway.** Gateway will consider a certificate is untrusted if any of these conditions are true:

  - The server certificate issuer is unknown or is not trusted by the service.
  - The server certificate is revoked and fails a CRL check.
  - There is at least one expired certificate in the certificate chain for the server certificate.
  - The common name on the certificate does not match the URL you are trying to reach.
  - The common name on the certificate contains invalid characters (such as underscores). Gateway uses [BoringSSL](https://csrc.nist.gov/projects/cryptographic-module-validation-program/validated-modules/search?SearchMode=Basic&Vendor=Google&CertificateStatus=Active&ValidationYear=0) to validate certificates. Chrome's [validation logic](https://chromium.googlesource.com/chromium/src/+/refs/heads/main/net/cert/x509_certificate.cc#429) allows non-RFC 1305 compliant certificates, which is why the website may load when you turn off WARP.

- **The connection from Gateway to the origin is insecure.** Gateway does not trust origins which:

  - Only offer insecure cipher suites (such as RC4, RC4-MD5, or 3DES). You can use the [SSL Server Test tool](https://www.ssllabs.com/ssltest/index.html) to check which ciphers are supported by the origin.
  - Do not support [FIPS-compliant ciphers](/cloudflare-one/policies/gateway/http-policies/tls-decryption/#cipher-suites) (if you have enabled [FIPS compliance mode](/cloudflare-one/policies/gateway/http-policies/tls-decryption/#fips-compliance)). In order to load the page, you can either disable FIPS mode or create a Do Not Inspect policy for this host (which has the effect of disabling FIPS compliance for this origin).
  - Redirect all HTTPS requests to HTTP.

If none of the above scenarios apply, contact Cloudflare support with the following information:

- Operating System (Windows 10, macOS 10.x, iOS 14.x)
- Web browser (Chrome, Firefox, Safari, Edge)
- URL of the request
- Screenshot or copy/paste of the content from the error page

For more troubleshooting information, refer to [Support](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-526-invalid-ssl-certificate).

## I see an error in the Gateway Overview page, and no analytics are displayed.

![An error displayed in the Gateway Overview page instead of analytics.](~/assets/images/cloudflare-one/faq/gateway-dash-overview-empty.png)

You may not see analytics on the Overview page for the following reasons:

- **You are not sending DNS queries to Gateway**. Verify that the destination IP addresses you are sending DNS queries to are correct. You can check the destination IP addresses for your DNS location by going to **Gateway** > **DNS locations** and then expanding the location.
- **You are using other DNS resolvers**. If you have other DNS resolvers in your DNS settings, your device could be using IP addresses for resolvers that are not part of Gateway. Make sure to remove all other IP addresses from your DNS settings and only include Gateway's DNS resolver IP addresses.
- **The source IPv4 address for your DNS location is incorrect**. If you are using IPv4, check the source IPv4 address that you entered for the DNS location matches with the network's source IPv4 address.
- **Analytics is not available yet**. It takes some time to generate the analytics for Cloudflare Gateway. If you are not seeing anything even after 5 minutes, file a support ticket.

## I see a "No Browsers Available" alert.

If you encounter this error, [file feedback](/cloudflare-one/policies/browser-isolation/known-limitations/) via the WARP client and we will investigate.

## I see a "Maximum Sessions Reached" alert.

This can occur if your device is attempting to establish a connection to more than two remote browser instances.
A browser isolation session is a connection from your local browser to a remote browser. Tabs and windows within the same browser share a single remote browser session. In practice, this generally means that you can open both Chrome and Firefox to use browser isolation concurrently, but attempting to open a third browser such as Opera will cause this alert to appear. To release a browser session, close all tabs/windows in your local browser. The remote browser session will be automatically terminated within 15 minutes.

## I see `SAML Verify: Invalid SAML response, SAML Verify: No certificate selected to verify` when testing a SAML identity provider.

This error occurs when the identity provider has not included the signing public key in the SAML response. While not required by the SAML 2.0 specification, Cloudflare Access always checks that the public key provided matches the **Signing certificate** uploaded to Zero Trust. For the integration to work, you will need to configure your identity provider to add the public key.

## I see `Error 0: Bad Request. Please create a ca for application.` when attempting to connect to SSH with a short-lived certificate.

This error will appear if a certificate has not been generated for the Access application users are attempting to connect to. For more information on how to generate a certificate for the application on the Access Service Auth SSH page, refer to [these instructions](/cloudflare-one/applications/non-http/short-lived-certificates-legacy/).

## Mobile applications warn of an invalid certificate, even though I installed a Cloudflare certificate on my system.

These mobile applications may use <GlossaryTooltip term="certificate pinning">certificate pinning</GlossaryTooltip> Cloudflare Gateway dynamically generates a certificate for all encrypted connections in order to inspect the content of HTTP traffic. This certificate will not match the expected certificate by applications that use certificate pinning.
To allow these applications to function normally, administrators can configure bypass rules to exempt traffic to hosts associated with the application from being intercepted and inspected.

## Firefox shows a network protocol violation when I use the WARP client.

If you see this warning, you may have to disable DNS over HTTPS setting in Firefox. If you need help doing that, see [these instructions](https://support.mozilla.org/en-US/kb/firefox-dns-over-https#w_manually-enabling-and-disabling-dns-over-https).

## Chrome shows `NET::ERR_CERT_AUTHORITY_INVALID` when I use the WARP client.

Advanced security features including HTTPS traffic inspection require you to deploy a [root certificate](/cloudflare-one/connections/connect-devices/user-side-certificates/) on the device. If [**Install CA to system certificate store**](/cloudflare-one/connections/connect-devices/user-side-certificates/automated-deployment/) is enabled, the WARP client will automatically install a new root certificate whenever you install or update WARP.

Certain web browsers (such as Chrome and Microsoft Edge) load and cache root certificates when they start. Therefore, if you install a root certificate while the browser is already running, the browser may not detect the new certificate. To resolve the error, restart the browser.

## I see `Access api error auth_domain_cannot_be_updated_dash_sso`.

This error appears if you try to change your [team domain](/cloudflare-one/faq/getting-started-faq/#whats-a-team-domainteam-name) while the [Cloudflare dashboard SSO](/cloudflare-one/applications/configure-apps/dash-sso-apps/) feature is enabled on your account.
Cloudflare dashboard SSO does not currently support team domain changes. Contact your account team for more details.

## WARP on Linux shows `DNS connectivity check failed`.

This error means that the `systemd-resolved` service on Linux is not allowing WARP to resolve DNS requests. You can identify this issue in the [`daemon.log`](/cloudflare-one/connections/connect-devices/warp/troubleshooting/warp-logs/#warp-diag-logs:~:text=the%20WARP%20client.-,daemon.log,-Detailed%20log%20of) file of the `warp diag` logs, where the error message appears as `ERROR main_loop: warp::warp::connectivity_check: DNS connectivity check failed to resolve host="warp-svc."`.

To solve the issue:

1. Add the following line to `/etc/systemd/resolved.conf`:

```txt
ResolveUnicastSingleLabel=yes
```

2. Make sure that no other DNS servers are configured in `/etc/systemd/resolved.conf`. For example, if the file contains `DNS=X.Y.Z.Q`, comment out the line.

3. Restart the service:

```sh
sudo systemctl restart systemd-resolved.service
```

## Windows incorrectly shows `No Internet access` when WARP is enabled.

[NCSI](https://learn.microsoft.com/en-us/windows-server/networking/ncsi/ncsi-overview) is a Windows feature for determining network quality and connectivity. When WARP is enabled, NCSI checks can sometimes fail and cause a cosmetic UI error where the user believes they have no Internet even though the device still has full connectivity. Some apps (Outlook, JumpCloud) may refuse to connect because Windows is reporting there is no Internet connectivity.

To resolve the issue, you will need to edit two Windows registry keys:

1. Configure NCSI to detect WARP's [local DNS proxy](/cloudflare-one/connections/connect-devices/warp/configure-warp/route-traffic/warp-architecture/#dns-traffic).

   ```txt
   HKEY_LOCAL_MACHINE\SOFTWARE\POLICIES\MICROSOFT\Windows\NetworkConnectivityStatusIndicator
   Type: DWORD
   Value: UseGlobalDNS
   Data: 1
   ```

2. Configure NCSI to use active probing mode, as WARP may be obscuring the number of hops expected by the [passive probe](https://learn.microsoft.com/en-us/windows-server/networking/ncsi/ncsi-frequently-asked-questions#how-does-passive-probing-determine-connectivity).

   ```txt
   HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\NlaSvc\Parameters\Internet
   Type: DWORD
   Value: EnableActiveProbing
   Data: 1
   ```

If you continue to have issues with Microsoft 365 applications, consider enabling [**Directly route Microsoft 365 traffic**](/cloudflare-one/connections/connect-devices/warp/configure-warp/warp-settings/#directly-route-microsoft-365-traffic).

## I see `WebGL Rendering Error`.

Cloudflare Browser Isolation leverages Network Vector Rendering (NVR) technology. This allows us to deliver a secure, performant remote computing experience without the bandwidth limitations of traditional solutions. While we expect most websites to work perfectly, some browser features and web technologies such as WebGL (Web Graphics Library) are unsupported.

WebGL is a JavaScript API for rendering high-performance interactive 2D and 3D graphics within any compatible web browser without the use of plug-ins. Support for WebGL is present in all modern browsers. However, the user's device must also have access to the underlying [hardware](https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API#browser_compatibility) that supports these features.

When running remote browser isolation in a virtualized environment, the user's device may not have access to the required system resources. To resolve the error, you can configure your browser to render vector graphics entirely through software, without using the hardware acceleration provided by a GPU.

To enable software rasterization:

1. Go to `chrome://flags/#override-software-rendering-list`.
2. Set **Override software rendering list** to _Enabled_.
3. Select **Relaunch** to apply the change.

## I cannot send emails on port `25`.

By default, the WARP client blocks outgoing SMTP traffic on port `25` to prevent users from abusing our service to send spam. Modern email service providers use port `587` or `465` to encrypt emails over a TLS/SSL connection. For more information, refer to [What SMTP port should be used?](https://www.cloudflare.com/learning/email-security/smtp-port-25-587/).

If you need to unblock port `25`, contact your account team.

## I see `502 Bad Gateway` when browsing to a website.

This issue can occur when communicating with an origin that partially supports HTTP/2. In these scenarios, the connection from Gateway to the website starts using HTTP/2 but requests a downgrade to HTTP/1.1 for some requests. For example, servers such as [Microsoft Internet Information Services (IIS)](https://learn.microsoft.com/iis/get-started/whats-new-in-iis-10/http2-on-iis#when-is-http2-not-supported) do not support authentication over HTTP/2. When errors occur, the website may send back a `RST_STREAM` frame with the error code `HTTP_1_1_REQUIRED`, which indicates that the browser should retry the request over HTTP/1.1. Gateway translates any received upstream `RST_STREAM` frames to a pseudo socket close, so this appears as a `502 Bad Gateway` exception page. The browser will not indicate why it failed.

Gateway does not support this downgrade mechanism. When receiving the `HTTP_1_1_REQUIRED` error code, Gateway will not reissue requests over HTTP/1.1. To make the connection from Gateway to the website successfully, you will need to disable HTTP/2 at the origin.

## I see `This site can't provide a secure connection.`

If you see an error with the title `This site can't provide a secure connection` and a subtitle of `<hostname> uses an unsupported protocol`, you must [order an Advanced Certificate](/ssl/edge-certificates/advanced-certificate-manager/manage-certificates/#create-a-certificate).

If you added a [multi-level subdomain](/cloudflare-one/connections/connect-networks/get-started/create-remote-tunnel/#2a-connect-an-application) (more than one level of subdomain), you must [order an Advanced Certificate for the hostname](/cloudflare-one/connections/connect-networks/get-started/create-remote-tunnel/#2a-connect-an-application) as Cloudflare's Universal certificate will not cover the public hostname by default.

## As of February 2, 2025, my end-user device's browser is returning a `Your connection is not private` warning.

### Why am I getting this error?

The default global Cloudflare root certificate expired on 2025-02-02 at 16:05 UTC. If you installed the default Cloudflare certificate before 2024-10-17, you must [generate a new certificate](/cloudflare-one/connections/connect-devices/user-side-certificates/#generate-a-cloudflare-root-certificate) and activate it for your Zero Trust organization to avoid inspection errors. If you did not generate a new certificate before February 2, 2025, you will encounter browser warnings like `Your connection is not private`.

Starting with WARP client version 2024.12.554.0 and later, the WARP client will automatically install Cloudflare certificates in an end-user device's certificate store as soon as the Cloudflare certificates appear as **Available** in the Cloudflare dashboard.

For WARP client versions prior to 2024.12.554.0, certificates had to be marked as **In-Use** in the Cloudflare dashboard before the WARP client could push the Cloudflare certificates to an end-user device's certificate store.

### What do I need to do?

Before deploying a new certificate, [update WARP](/cloudflare-one/connections/connect-devices/warp/download-warp/update-warp/#how-to-update-warp) to version 2024.12.554.0 or newer.

For WARP client versions before and after 2024.12.554.0, certificate propagation will only occur when the WARP client is responsible for automatically installing the certificate on the client device. To enable the WARP client to propogate certificates:

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Settings** > **WARP Client**.
2. Turn on **Install CA to system certificate store**.

If **Install CA to system certificate store** is turned off, you must [manually install the certificate](/cloudflare-one/connections/connect-devices/user-side-certificates/manual-deployment/), use an [MDM solution](/cloudflare-one/connections/connect-devices/user-side-certificates/manual-deployment/#mobile-device-management-mdm-software) to distribute the Cloudflare certificate to your fleet of devices, or not use the Cloudflare certificate because you do not want to have TLS decryption enabled. [TLS decryption](/cloudflare-one/policies/gateway/http-policies/tls-decryption/) must be enabled to enforce Gateway HTTP policies for HTTPS traffic.

After enabling certificate propagation, you must update your certificate:

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Settings** > **Resources**, then select **Manage** next to **Cloudflare certificates**.
2. Select **Generate certificate**.
3. Select the expiration date for this new certificate (five years is the default, but this can be adjusted) and select **Generate certificate**.
4. The new certificate will be marked **Inactive** at first. Select the **three dots** to the right of the certificate, then select **Activate** to activate the certificate.

For WARP versions on or above 2024.12.554.0, selecting **Activate** will download the new certificate to end-user devices.

Certificate propagation to end-user devices can take up to 24 hours, but can be expedited by resetting the encryption keys.

To reset the encryption keys:

1. Open the WARP GUI on your device.
2. Select the gear icon on the top right > **Preferences**.
3. Select **Connection**, then select **Reset Encryption Keys**.

macOS Big Sur and newer releases do not allow WARP to automatically trust the certificate. You must either [manually trust the certificate](/cloudflare-one/connections/connect-devices/user-side-certificates/automated-deployment/#macos) as the user or [use a MDM to trust the certificate](/cloudflare-one/connections/connect-devices/user-side-certificates/manual-deployment/#mobile-device-management-mdm-software).

After confirming that the certificate is installed and trusted on the end-user device, mark the certificate as **In-Use**. To mark the certificate as **In-Use**:

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Settings** > **Resources**, then select **Manage** next to **Cloudflare certificates**.
2. Select a certificate.
3. In the detailed menu under **Basic Information**, select **Confirm and turn on certificate**.
4. Once turned on, the new certificate will now show as **In-Use** in Zero Trust. **In-Use** indicates that the certificate is being used for inspection.

It is recommended to have end users disconnect and reconnect WARP to expedite this change being reflected on their local machine. To verify the new certificate is being used correctly:

1. Connect to WARP.
2. Visit an HTTPS site.
3. Verify that no certificate error is enountered.

Additionally, you can check the certificate used within your browser by viewing the certificate (steps vary by browser, but you can generally do this check by selecting the lock icon next to the URL) and verifying the Organizational Unit (OU) does not reference `ECC Certificate Authority`.

The new certificate will be valid until the configured expiration date.

### I followed all the instructions but I am still having problems with my certificate.

If the new certificate is not activating on the end-user device or you are getting a `Certificate is missing` warning even though the certificate is marked **In-Use**. Refer to the following troubleshooting options:

1. Rotate the keys used by WARP to force activate the new certificate by running:

   ```sh
   warp-cli tunnel rotate-keys
   ```

2. [Upgrade](/cloudflare-one/connections/connect-devices/warp/download-warp/update-warp/#how-to-update-warp) to WARP version 2024.12.554.0.

   Some customers who are on versions earlier than 2024.11.309.0 have experienced inconsistencies with certificate installation and may need to upgrade.

3. Turn off TLS Decryption.

If no measure is working quickly and you are encountering browser warnings that are blocking work, [turning off TLS decryption](/cloudflare-one/policies/gateway/http-policies/tls-decryption/#turn-on-tls-decryption) will prevent HTTP policies from being enforced and will ensure websites resolve until the certificate can be deployed to more user devices.

Turning off TLS decryption should be a temporary measure. TLS decryption should be turned on if you need to enforce HTTP policies and log traffic for HTTPS traffic.

## I am getting an `Error 401: deleted_client - The OAuth Client was deleted` authorization error.

<Render file="access/error-401" product="cloudflare-one" />

## I entered an override code for WARP that was supposed to be valid for 3 hours but the override code expired faster than I expected.

[Admin override](/cloudflare-one/connections/connect-devices/warp/configure-warp/warp-settings/#admin-override) codes are time-sensitive and adhere to fixed-hour time blocks. Override codes can be reused until the end of their timeout. An override code's timeout begins in the hour the override code was generated in. Refer to the following scenarios.

### Scenario one: Admin generates an override code at 9:00 AM with a timeout of one hour.

If admin generates an override code with a timeout of one hour at **9:00 AM** and the user inputs the override code in their device at **9:59 AM**, the user will be able to toggle WARP on and off until **10:59 AM** (a one hour duration.)

However, if the user attempts to enter the override code at **10:00 AM**, the override code will not work. The override code will not work because the override code was generated at **9:00 AM** and its one hour validity was counted as used in the 9:00 AM to 10:00 AM hour.

### Scenario two: Admin generates an override code at 9:30 AM with timeout of three hours.

If admin generates an override code with a timeout of three hours at **9:30 AM** and the user inputs the override code in their device at **9:59 AM**, the user will be able to toggle WARP on and off until **12:59 PM** (a three hour duration.)

However, if the user attempts to enter the override code at **10:00 AM**, the override code will only be valid until **12:00 PM** (a two hour duration). The override code was generated at **9:30 AM** and one hour of its total three hour validity was counted as used in the 9:00 AM to 10:00 AM hour.

### Scenariot three: Admin generates an override code at 12:30 PM with a timeout of 24 hours.

If admin generates an override code with a timeout of 24 hours at **12:30 PM** and the user inputs the override code in their device at **12:59 PM** the same day, the user will be able to toggle WARP on and off until **12:59 PM** the next day (a 24 hour duration.)

However, if the user attempts to enter the override code at **1:00 PM** the same day, the override code will only be valid until **11:00 AM** the next day (a 23 hour duration). The override code was generated at **12:30 PM** and one hour of its total 24 hour validity was counted as used in the 12:00 PM to 1:00 PM hour.

If the user attempts to enter the override code at **11:59 AM** the next day, the override code will only be valid until **12:59 PM** (a one hour duration). The override code was generated at **12:00 PM** and 23 hours of its total 24 hour validity were counted as used from 12:00 PM to 11:00 AM the next day (a 23 hour duration).

## I disabled WARP using an override code but WARP turned on by itself before my override code expired.

If you are using an [Admin override](/cloudflare-one/connections/connect-devices/warp/configure-warp/warp-settings/#admin-override) code with [Auto connect](/cloudflare-one/connections/connect-devices/warp/configure-warp/warp-settings/#auto-connect) also enabled, WARP will turn on automatically according to the Timeout set for **Auto connect**. Using an override code to override the WARP lock switch will not disable Auto connect. As best practice, review your Auto connect settings before sending the override code to the user.

To prevent WARP from auto connecting while using an admin override code, disable Auto connect or set a longer **Timeout** for **Auto connect**. Note the changes you make to Auto connect while the end user is using the admin override code if you need to revert these changes later.

## I am getting the error `Failed to fetch user/group information from the identity provider`.

This error is returned when proper API permissions are not set up in the identity provider. When Cloudflare attempts to fetch user/group information from the identity provider and proper API permissions have not been configured, the `Failed to fetch user/group information from the identity provider` error will appear. Review the [SSO integration](/cloudflare-one/identity/idp-integration/) guide for your identity provider to ensure your application has the appropriate API permissions.

For example, [Microsoft Entra](/cloudflare-one/identity/idp-integration/entra-id/#2-configure-api-permissions-in-entra-id) and [Okta](</cloudflare-one/identity/idp-integration/okta/#:~:text=(Optional)%20Create%20an%20Okta%20API%20token%20and%20enter%20it%20in%20Zero%20Trust%20(the%20token%20can%20be%20read%2Donly).%20This%20will%20prevent%20your%20Okta%20groups%20from%20failing%20if%20you%20have%20more%20than%20100%20groups>) have required permissions stated in their integration guides.

You can also examine logs in your identity provider to identify any denied requests related to API access.

---

# Identity

URL: https://developers.cloudflare.com/cloudflare-one/identity/

import { DirectoryListing } from "~/components"

Cloudflare Zero Trust integrates with your organization's identity provider to apply Zero Trust and Secure Web Gateway policies. If you work with partners, contractors, or other organizations, you can integrate multiple identity providers simultaneously.

As an alternative to configuring an identity provider, Cloudflare Zero Trust can send a [one-time PIN (OTP)](/cloudflare-one/identity/one-time-pin/) to approved email addresses. No configuration needed â€” simply add a user's email address to an [Access policy](/cloudflare-one/policies/access/) and to the group that allows your team to reach the application.

You can simultaneously configure an OTP and an identity provider to allow users to use their own authentication method.

Additionally, Cloudflare Zero Trust can integrate with [endpoint protection providers](/cloudflare-one/identity/devices/) to check requests for device posture. This allows you to configure security policies that rely on additional signals from endpoint security providers to allow or deny connections to your applications.

<DirectoryListing />

---

# One-time PIN login

URL: https://developers.cloudflare.com/cloudflare-one/identity/one-time-pin/

import { Tabs, TabItem, Render } from '~/components';

Cloudflare Access can send a one-time PIN (OTP) to approved email addresses as an alternative to integrating an identity provider. You can simultaneously configure OTP login and the identity provider of your choice to allow users to select their own authentication method.

For example, if your team uses Okta but you are collaborating with someone outside your organization, you can use OTP to grant access to guests.

<Render file="access/one-time-pin-warning" />

## Set up OTP

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Settings** > **Authentication**.
2. Under **Login methods**, select **Add new**.
3. Select **One-time PIN**.

</TabItem> <TabItem label="API">

1. [Create an API token](/fundamentals/api/get-started/create-token/) with the following permissions:
		| Type    | Item             | Permission |
		| ------- | ---------------- | ---------- |
		| Account | Access: Organizations, Identity Providers, and Groups   | Edit    |

2. Make a `POST` request to the [Identity Providers](/api/resources/zero_trust/subresources/identity_providers/methods/create/) endpoint:

		```sh
		curl https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/access/identity_providers \
		--header "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
		--data '{
			"name": "One-time PIN login",
			"type": "onetimepin",
			"config": {}
		}'
		```

</TabItem> <TabItem label="Terraform">

:::note[Provider versions]
The following example requires Cloudflare provider version `>=4.40.0`.
:::

1. Add the following permission to your [`cloudflare_api_token`](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/api_token):
	- `Access: Organizations, Identity Providers, and Groups Write`

2. Configure the [`cloudflare_zero_trust_access_identity_provider`](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/zero_trust_access_identity_provider) resource:

		```tf
		resource "cloudflare_zero_trust_access_identity_provider" "onetimepin_login" {
			account_id = var.cloudflare_account_id
			name       = "One-time PIN login"
			type       = "onetimepin"
		}
		```
</TabItem> </Tabs>

:::tip
If your organization uses a third-party email scanning service (for example, Mimecast or Barracuda), add `noreply@notify.cloudflare.com` to the email scanning allowlist.
:::

To grant a user access to an application, simply add their email address to an [Access policy](/cloudflare-one/policies/access/policy-management/#create-a-policy).

## Log in with OTP

To log in to Access using the one-time PIN:

1. Go to the application protected by Access.
2. On the Access login page, enter your email address and select **Send me a code**.
   ![Enter email to sign in with OTP.](~/assets/images/cloudflare-one/identity/otp/otp1.png)
3. If the email is allowed by an Access policy, you will receive a PIN in your inbox. This secure PIN expires 10 minutes after the initial request.

:::note

By design, blocked users will not receive an email. The login page will always say **A code has been emailed to you**, regardless of whether or not an email was sent.
:::

4. Paste the PIN into the Access login page and select **Sign in**.
   ![Enter PIN to sign in.](~/assets/images/cloudflare-one/identity/otp/otp2.png)

   - If the code was valid, you will be redirected to the application.
   - If the code was invalid, you will see **That account does not have access.**

:::note

Access only logs an authentication attempt after the user enters a code. If the user enters their email but never submits a code, the event will not appear in your [audit logs](/cloudflare-one/insights/logs/audit-logs/#authentication-logs).
:::

---

# Service tokens

URL: https://developers.cloudflare.com/cloudflare-one/identity/service-tokens/

import { AvailableNotifications, Render } from "~/components";

You can provide automated systems with service tokens to authenticate against your Zero Trust policies. Cloudflare Access will generate service tokens that consist of a Client ID and a Client Secret. Automated systems or applications can then use these values to reach an application protected by Access.

This section covers how to create, renew, and revoke a service token.

## Create a service token

<Render file="access/create-service-token" />

You can now configure your Access applications and [device enrollment permissions](/cloudflare-one/connections/connect-devices/warp/deployment/device-enrollment/#check-for-service-token) to accept this service token. Make sure to set the policy action to [**Service Auth**](/cloudflare-one/policies/access/#service-auth); otherwise, Access will prompt for an identity provider login.

## Connect your service to Access

### Initial request

To authenticate to an Access application using your service token, add the following to the headers of any HTTP request:

`CF-Access-Client-Id: <CLIENT_ID>`

`CF-Access-Client-Secret: <CLIENT_SECRET>`

For example,

```sh
curl -H "CF-Access-Client-Id: <CLIENT_ID>" -H "CF-Access-Client-Secret: <CLIENT_SECRET>" https://app.example.com
```

If the service token is valid, Access generates a JWT scoped to the application in the form of a [`CF_Authorization` cookie](/cloudflare-one/identity/authorization-cookie/). You can use this cookie to authenticate [subsequent requests](#subsequent-requests) to the application.

### Subsequent requests

After you have [authenticated to the application](#initial-request) using the service token, add the resulting `CF_Authorization` cookie to the headers of all subsequent requests:

```sh
curl -H "cookie: CF_Authorization=<CF_AUTHORIZATION_COOKIE>" https://app.example.com
```

If you prefer to use a raw header, send the value as `cf-access-token`:

```sh
curl -H "cf-access-token: <CF_AUTHORIZATION_COOKIE>" https://app.example.com
```

All requests with this cookie will succeed until the JWT expires.

:::note

If your Access application only has Service Auth policies, you must send the service token on every subsequent request. You can only use the JWT if the application has at least one Allow policy.
:::

## Renew service tokens

Service tokens expire according to the token duration you selected when you created the token.

To renew the service token:

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Access** > **Service auth** > **Service Tokens**.
2. Locate the token you want to renew.
3. To extend the token's lifetime by one year, select **Refresh**.
4. To extend the token's lifetime by more than a year:
   1. Select **Edit**.
   2. Choose a new **Service Token Duration**.
   3. Select **Save**. The expiration date will be extended by the selected amount of time.

## Revoke service tokens

If you need to revoke access before the token expires, simply delete the token.

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Access** > **Service auth** > **Service Tokens**.
2. **Delete** the token you need to revoke.

Services that rely on a deleted service token can no longer reach your application.

:::note

When editing an Access application, selecting **Revoke existing tokens** revokes existing sessions but does not prevent the user from starting a new session. As long as the Client ID and Client Secret are still valid, they can be exchanged for a new token on the next request. To revoke access, you must delete the service token.
:::

## Set a token expiration alert

An alert can be configured to notify a week before a service token expires to allow an administrator to invoke a token refresh.

<AvailableNotifications product="Cloudflare Access" />

To configure a service token expiration alert:

1. In the [Cloudflare dashboard](https://dash.cloudflare.com), go to the **Notifications** tab.
2. Select **Add**.
3. Select _Expiring Access Service Token_.
4. Enter a name for your alert and an optional description.
5. (Optional) Add other recipients for the notification email.
6. Select **Save**.

Your alert has been set and is now visible in the **Notifications** tab of the Cloudflare dashboard.

---

# Implementation guides

URL: https://developers.cloudflare.com/cloudflare-one/implementation-guides/

import { CardGrid, LinkTitleCard } from "~/components";

Implementation guides cover deployment steps and best practices for specific Cloudflare One use cases.

<CardGrid>

<LinkTitleCard
	title="Secure your Internet Traffic and SaaS apps"
	href="/learning-paths/secure-internet-traffic/concepts/"
	icon="seti:lock"
>
	Provide your users and networks with a secure, performant, and flexible path
	to the Internet.
</LinkTitleCard>

<LinkTitleCard
	title="Replace your VPN"
	href="/learning-paths/replace-vpn/concepts/"
	icon="seti:db"
>
	Give users secure, auditable network and application access.
</LinkTitleCard>

<LinkTitleCard
	title="Deploy Zero Trust Web Access"
	href="/learning-paths/zero-trust-web-access/concepts/"
	icon="laptop"
>
	Secure access to internal web applications without a device client.
</LinkTitleCard>

<LinkTitleCard
	title="Secure Microsoft 365 email with Email Security"
	href="/learning-paths/secure-o365-email/concepts/"
	icon="email"
>
	Use Cloudflare's Email Security to protect your Microsoft 365 email inbox from
	phishing and malware attacks.
</LinkTitleCard>

</CardGrid>

---

# Policies

URL: https://developers.cloudflare.com/cloudflare-one/policies/

import { GlossaryTooltip } from "~/components";

With Cloudflare Zero Trust, you can create:

- [**Secure Web Gateway**](/cloudflare-one/policies/gateway/) policies to inspect outbound traffic to the Internet with <GlossaryTooltip term="Cloudflare Gateway">Cloudflare Gateway</GlossaryTooltip>.
- [**Access**](/cloudflare-one/policies/access/) policies to secure inbound traffic to your applications with <GlossaryTooltip term="Cloudflare Access">Cloudflare Access</GlossaryTooltip>.
- [**Browser Isolation**](/cloudflare-one/policies/browser-isolation/) policies to protect your organization's devices from threats on the Internet and prevent data loss by loading requests in an isolated browser.
- [**Data Loss Prevention**](/cloudflare-one/policies/data-loss-prevention/) policies to detect and secure your organization's sensitive data in web traffic and SaaS applications.

---

# Insights

URL: https://developers.cloudflare.com/cloudflare-one/insights/

import { DirectoryListing } from "~/components";

Cloudflare Zero Trust gives you comprehensive and in-depth visibility into your network. Whether you need data on network usage, on security threats blocked by Cloudflare Zero Trust, or on how many users have logged in to your applications this month, Zero Trust provides you with the right tools for the job.

<DirectoryListing />

---

# Risk score

URL: https://developers.cloudflare.com/cloudflare-one/insights/risk-score/

:::note

Only available on Enterprise plans.
:::

Zero Trust risk scoring detects user activity and behaviors that could introduce risk to your organization's systems and data. Risk scores add user and entity behavior analytics (UEBA) to the Zero Trust platform.

## User risk scoring

Cloudflare Zero Trust assigns a risk score of Low, Medium, or High based on detections of users' activities, posture, and settings. A user's score is equal to the highest-level risk behavior they trigger.

### View a user's risk score

To view a user's risk score in [Zero Trust](https://one.dash.cloudflare.com/), go to **Risk score** > **User risk scoring**. Select a user's name to view their instances of risk behaviors, if any. You can select an instance of a risk behavior to view the log associated with the detection.

Users that have had their risk score [cleared](#clear-a-users-risk-score) will not appear in the table unless they trigger another risk behavior.

### Clear a user's risk score

If required, you can reset risk scores for specific users. Once reset, users will not appear in the associated risk table until they trigger another risk behavior.

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Risk score** > **User risk scoring**.
2. Select the user you want to clear the risk score for.
3. In **User risk overview**, select **Reset user risk**.
4. Select **Confirm**.

### Send risk score to Okta

In addition to controls in Zero Trust, Okta users can send risk scores to Okta to apply SSO-level policies.

First, configure Zero Trust to send user risk scores to Okta.

1. Set up the [Okta SSO integration](/cloudflare-one/identity/idp-integration/okta/).
2. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Settings** > **Authentication**.
3. In **Login methods**, locate your Okta integration and select **Edit**.
4. Turn on **Send risk score to Okta**.
5. Select **Save**.
6. Upon saving, Zero Trust will display the well-known URL for your organization. Copy the value.

Next, configure Okta to receive your risk scores.

1. On your Okta admin dashboard, go to **Security** > **Device Integrations**.
2. Go to **Receive shared signals**, then select **Create stream**.
3. Name your integration. In **Set up integration with**, choose _Well-known URL_.
4. In **Well-known URL**, enter the well-known URL value provided by Zero Trust.
5. Select **Create**.

For more information on configuring user risk score within Okta, refer to the [Okta documentation](https://help.okta.com/oie/en-us/content/topics/itp/overview.htm).

While the Okta integration is turned on, Zero Trust will send any user risk score updates to Okta, including score increases and resets. Score update events will appear in your [Access audit logs](/cloudflare-one/insights/logs/audit-logs/).

## Predefined risk behaviors

By default, all predefined behaviors are disabled. When a behavior is enabled, Zero Trust will continuously evaluate all users within the organization for the behavior. You can [change the risk level](#change-risk-behavior-risk-levels) for predefined behaviors if the default assignment does not suit your environment.

| Risk behaviors                         | Requirements                                                                                                | Description                                                                                                                                                                                                            |
| -------------------------------------- | ----------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Impossible travel                      | [A configured Access application](/cloudflare-one/applications/)                                            | User has a successful login from two different locations that they could not have traveled between in that period of time. Matches will appear in your [Access audit logs](/cloudflare-one/insights/logs/audit-logs/). |
| High number of DLP policies triggered  | [A configured DLP profile](/cloudflare-one/policies/data-loss-prevention/dlp-profiles/)                     | User has created a high number of DLP policy matches within a narrow frame of time. Matches will appear in your [Gateway activity logs](/cloudflare-one/insights/logs/gateway-logs/).                                  |
| SentinelOne threat detected on machine | [SentinelOne service provider integration](/cloudflare-one/identity/devices/service-providers/sentinelone/) | SentinelOne returns one or more configured [device posture attributes](/cloudflare-one/identity/devices/service-providers/sentinelone/#device-posture-attributes) for a user.                                          |

## Manage risk behaviors

To toggle risk behaviors, go to **Risk score** > **Risk behaviors**.

### Enable risk behaviors

When a specific behavior is enabled, Zero Trust will continuously monitor all users within the organization for any instances of that behavior.

If a user engages in an enabled risk behavior, their risk level is re-evaluated. Zero Trust will update their risk score to the highest value between the current risk level and the risk level of the behavior they triggered.

### Disable risk behaviors

When a risk behavior is disabled, monitoring for future activity will cease. Previously detected risk behaviors will remain in the logs and associated with a user.

### Change risk behavior risk levels

You can change the risk level for a behavior at any time.

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Risk score** > **Risk behaviors**.
2. Select the risk behavior you want to modify.
3. In the drop-down menu, choose your desired risk level.
4. Select **Save**.

---

# Create custom headers for Cloudflare Access-protected origins with Workers

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/access-workers/

This tutorial covers how to use a [Cloudflare Worker](/workers/) to add custom HTTP headers to traffic, and how to send those custom headers to your origin services protected by [Cloudflare Access](/cloudflare-one/policies/access/).

Some applications and networking implementations require specific custom headers to be passed to the origin, which can be difficult to implement for traffic moving through a Zero Trust proxy. You can configure a Worker to send the [user authorization headers](/cloudflare-one/identity/authorization-cookie/) required by Access.

---

## Before you begin

- Secure your origin server with Cloudflare Access

## Before you begin

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account. Go to **Workers & Pages**.

2. If this is your first Worker, select **Create Worker**. Otherwise, select **Create application**, then select **Create Worker**.

3. Enter an identifiable name for the Worker, then select **Deploy**.

4. Select **Edit code**.

5. Input the following Worker:

```js title="Worker with custom HTTP headers"
export default {
  async fetch(request, env, ctx): Promise<Response> {
    const { headers } = request;
    const cfaccessemail = headers.get("cf-access-authenticated-user-email");

    const requestWithID = new Request(request);
    requestWithID.headers.set('company-user-id', cfaccessemail);

    return fetch(requestWithID);
  },
} satisfies ExportedHandler<Env>;
```

6. Select **Save and deploy**.

Your Worker is now ready to send custom headers to your Access-protected origin services.

## Apply the Worker to your hostname

1. Select the Worker you created, then go to **Triggers**.
2. In **Routes**, select **Add route**.
3. Enter the hostname and zone for your origin, then select **Add route**.

The Worker will now insert a custom header into requests that match the defined route. For example:

```http title="Example custom header" {4,5}
"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Encoding": "gzip",
    "Accept-Language": "en-US,en;q=0.9",
    "Cf-Access-Authenticated-User-Email": "user@example.com",
    "Company-User-Id": "user@example.com",
    "Connection": "keep-alive"
```

---

# Connect through Cloudflare Access using a CLI

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/cli/

Cloudflare's `cloudflared` command-line tool allows you to interact with endpoints protected by Cloudflare Access. You can use `cloudflared` to interact with a protected application's API.

These instructions are not meant for configuring a service to run against an API. The token in this example is tailored to user identity and intended only for an end user interacting with an API via a command-line tool.

**This walkthrough covers how to:**

- Connect to resources secured by Cloudflare Access from a CLI

**Time to complete:**

30 minutes

---

## Authenticate a session from the command line

Once you have installed `cloudflared`, you can use it to retrieve a Cloudflare Access [application token](/cloudflare-one/identity/authorization-cookie/application-token/). This walkthrough uses the domain `example.com` as a stand-in for a protected API.

1. To generate a token, run the following command:

   ```sh
   cloudflared access login https://example.com
   ```

   With this command, `cloudflared` launches a browser window containing the same Access login page found when attempting to access a web application.

2. Select your identity provider and log in.

If the browser window does not launch, you can use the unique URL that is automatically printed to the command line.

1. Once you have successfully authenticated, the browser returns the token to `cloudflared` in a cryptographic transfer and stores it.

The token is valid for the [session duration](/cloudflare-one/identity/users/session-management/) configured by the Access administrator.

## Access your API

Once you have retrieved a token, you can access the protected API. The `cloudflared` command-line tool includes a wrapper for transferring data via `curl`, which uses URL syntax (for more, see the [curl](https://github.com/curl/curl) GitHub project). The wrapper injects the token into the `curl` request as a query argument named _token_. You can invoke the wrapper as follows:

```sh
cloudflared access curl http://example.com
```

It is possible also to use the `put` command with `cloudflared` for any Unix tool to include the token in the request.

Read on for other available commands.

## Available commands

### login

The `login` command initiates the login flow for an application behind Access.

```sh
cloudflared access login http://example.com
```

### curl

The `curl` command invokes the client wrapper and includes the token in the request automatically.

```sh
cloudflared access curl http://example.com
```

### token

The `token` command retrieves the token scoped to that specific application for use in other command-line tools.

```sh
cloudflared access token -app=http://example.com
```

## Using the token as an environment variable

It is possible to save the token as an environment variable for convenience and concision in scripts that access a protected application.

Set up a token as an environment variable as follows:

1. Run the following command to export the token to the shell environment:

   ```sh
   export TOKEN=$(cloudflared access token -app=http://example.com)
   ```

2. Confirm the token was saved with the following:

   ```sh
   echo $TOKEN
   ```

Once you have exported the token to your environment, use the variable with the Cloudflare Access request header in the script to access a protected endpoint, as in the following example:

```sh
curl -H "cf-access-token: $TOKEN" https://example.com/rest/api/2/item/foo-123
```

---

# Access a web application via its private hostname without WARP

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/clientless-access-private-dns/

import { Render } from "~/components";

With Cloudflare Browser Isolation and resolver policies, users can connect to private web-based applications via their private hostnames without needing to install the WARP client. By the end of this tutorial, users who pass your Gateway DNS and network policies will be able to access your private application at `https://<your-team-name>.cloudflareaccess.com/browser/https://internalrecord.com`.

## Before you begin

Make sure you have:

- [Cloudflare Browser Isolation](/cloudflare-one/policies/browser-isolation/) enabled on your account
- [Resolver policies](/cloudflare-one/policies/gateway/resolver-policies/) enabled on your account
- An HTTP or HTTPS application that users access through a browser

## Create a Cloudflare Tunnel

First, install `cloudflared` on a server in your private network:

<Render file="tunnel/create-tunnel" product="cloudflare-one" />

## Add private network routes

1. In the **Private Networks** tab, add the following IP addresses:

   - Private IP/CIDR of your application server (for example, `10.128.0.175/32`)
   - Private IP/CIDR of your DNS server

2. Select **Save tunnel**.

The application and DNS server are now connected to Cloudflare.

## Enable Clientless Web Isolation

<Render file="clientless-browser-isolation" />

3. For **Permissions**, select **Manage**.

4. Select **Add a rule**.

5. Create an expression that defines who can open the Clientless Web Isolation browser. For example,

   | Rule action | Rule type | Selector         | Value          | Action           |
   | ----------- | --------- | ---------------- | -------------- | ---------------- |
   | Allow       | Include   | Emails ending in | `@example.com` | Select **Save**. |

To test, open a browser and go to `https://<team-name>.cloudflareaccess.com/browser/https://<private-IP-of-application>`.

## Create a Gateway resolver policy

1. Go to **Gateway** > **Resolver policies**.

2. Select **Add a policy**.

3. Create an expression to match against the private [domain](/cloudflare-one/policies/gateway/resolver-policies/#domain) or [hostname](/cloudflare-one/policies/gateway/resolver-policies/#host) of the application:

   | Selector | Operator | Value                |
   | -------- | -------- | -------------------- |
   | Domain   | in       | `internalrecord.com` |

4. In **Select DNS resolver**, select _Configure custom DNS resolvers_.

5. Enter the private IP address of your DNS server.

6. In the dropdown menu, select _`<IP-address> - Private`_.

7. (Optional) Enter a custom port.

8. Select **Create policy**.

To test, open a browser and go to `https://<team-name>.cloudflareaccess.com/browser/https://internalrecord.com`.

## Create a Gateway network policy (recommended)

1. Go to **Gateway** > **Firewall policies** > **Network**.

2. Add a [network policy](/cloudflare-one/policies/gateway/network-policies/) that targets the private IP address of your application. You can optionally include any ports or protocols relevant for application access. For example,

   | Selector         | Operator      | Value           | Logic | Action |
   | ---------------- | ------------- | --------------- | ----- | ------ |
   | Destination IP   | in            | `10.128.0.175`  | And   | Allow  |
   | Destination Port | in            | `80`            | Or    |        |
   | User Email       | matches regex | `.*example.com` |       |        |

:::note

Device posture checks are not supported because they require the WARP client.

:::

For best practices on securing private applications, refer to [Build secure access policies](/learning-paths/replace-vpn/build-policies/).

## Connect as a user

Users can now access the application at the following URL:

`https://<team-name>.cloudflareaccess.com/browser/https://internalrecord.com`

The application will load in an isolated browser. You can optionally [configure remote browser controls](/cloudflare-one/policies/browser-isolation/isolation-policies/#policy-settings) such as disabling copy/paste, printing, or keyboard input.

---

# Use Microsoft Entra ID Conditional Access policies in Cloudflare Access

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/entra-id-conditional-access/

With [Conditional Access](https://learn.microsoft.com/entra/identity/conditional-access/overview) in Microsoft Entra ID (formerly Azure Active Directory), administrators can enforce policies on applications and users directly in Entra ID. Conditional Access has a set of checks that are specialized to Windows and are often preferred by organizations with Windows power users.

## Before you begin

Make sure you have:

- Global admin rights to Microsoft Entra ID account
- Configured users in the Microsoft Entra ID account

## Set up an identity provider for your application

Refer to [our IdP setup instructions](/cloudflare-one/identity/idp-integration/entra-id/#set-up-entra-id-as-an-identity-provider) for Entra ID.

## Add API permission in Entra ID

Once the base IdP integration is tested and working, grant permission for Cloudflare to read Conditional Access policies from Entra ID.

1. In Microsoft Entra ID, go to **App registrations**.

2. Select the application you created for the IdP integration.

3. Go to **API permissions** and select **Add a permission**.

4. Select **Microsoft Graph**.

5. Select **Application permissions** and add `Policy.Read.ConditionalAccess`.

6. Select **Grant admin consent**.

## Configure Conditional Access in Entra ID

1. In Microsoft Entra ID, go to **Enterprise applications** > **Conditional Access**.
2. Go to **Authentication Contexts**.
3. [Create an authentication context](https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-conditional-access-cloud-apps#authentication-context) to reference in your Cloudflare Access policies. Give the authentication context a descriptive name (for example, `Require compliant devices`).
4. Next, go to **Policies**.
5. [Create a new Conditional Access policy](https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-conditional-access-policies) or select an existing policy.
6. Assign the conditional access policy to an authentication context:
   1. In the policy builder, select **Target resources**.
   2. In the **Select what this policy applies to** dropdown, select _Authentication context_.
   3. Select the authentication context that will use this policy.
   4. Save the policy.

## Sync Conditional Access with Zero Trust

To import your Conditional Access policies into Cloudflare Access:

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Settings** > **Authentication**.
2. Find your Microsoft Entra ID integration and select **Edit**.
3. Enable **Azure AD Policy Sync**.
4. Select **Save**.

## Create an Access application

To enforce your Conditional Access policies on a Cloudflare Access application:

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Access** > **Applications**.

2. Select **Add an application**.

3. Select **Self-hosted**.

4. Enter any name for the application.

5. Select **Add public hostname** and enter the target URL of the protected application.

6. Select **Create new policy** and build an [Access policy](/cloudflare-one/policies/access/) using the _Azure AD - Auth context_ selector. For example:

   | Action | Rule type | Selector                | Value                       |
   | ------ | --------- | ----------------------- | --------------------------- |
   | Allow  | Include   | Emails ending in        | `@example.com`              |
   |        | Require   | Azure AD - Auth context | `Require compliant devices` |

7. Add this policy to your application configuration.

8. For **Identity providers**, select your Microsoft Entra ID integration.

9. Follow the remaining [self-hosted application creation steps](/cloudflare-one/applications/configure-apps/self-hosted-public-app/) to publish the application.

Users will only be allowed access if they pass the Microsoft Entra ID Conditional Access policies associated with this authentication context.

---

# Isolate risky Entra ID users

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/entra-id-risky-users/

import { WranglerConfig } from "~/components";


Microsoft Entra ID (formerly Azure Active Directory) calculates a user's [risk level](https://learn.microsoft.com/entra/id-protection/howto-identity-protection-investigate-risk) based on the probability that their account has been compromised. With Cloudflare Zero Trust, you can synchronize the Entra ID risky users list with Cloudflare Access and apply more stringent Zero Trust policies to users at higher risk.

This tutorial demonstrates how to automatically redirect users to a remote browser when they are deemed risky by Entra ID.

**Time to complete:**

1 hour

## Prerequisites

- Microsoft Entra ID Premium P2 license
- [Cloudflare Browser Isolation](/cloudflare-one/policies/browser-isolation/) add-on
- [Gateway HTTP filtering](/cloudflare-one/policies/gateway/initial-setup/http/) enabled on your devices
- [npm](https://docs.npmjs.com/getting-started) installation
- [Node.js](https://nodejs.org/en/) installation

## 1. Set up Entra ID as an identity provider

Refer to [our IdP setup instructions](/cloudflare-one/identity/idp-integration/entra-id/#set-up-entra-id-as-an-identity-provider) for Entra ID.

:::note

- When you configure the IdP in Zero Trust, be sure to select **Enable group membership change reauthentication**.
- Save the **Application (client) ID**, **Directory (tenant) ID**, and **Client secret** as you will need them again in a later step.
  :::

## 2. Add Entra ID API permissions

Once the base IdP integration is tested and working, enable additional permissions that will allow a script to create and update risky user groups in Entra ID:

1. In Microsoft Entra ID, go to **App registrations**.

2. Select the application you created for the IdP integration.

3. Go to **API permissions** and select **Add a permission**.

4. Select **Microsoft Graph**.

5. Select **Application permissions** and add the following [permissions](https://learn.microsoft.com/en-us/graph/permissions-reference):

   - `IdentityRiskyUser.ReadAll`
   - `Directory.ReadWriteAll`
   - `Group.Create`
   - `Group.ReadAll`
   - `GroupMember.ReadAll`
   - `GroupMember.ReadWriteAll`

6. Select **Grant admin consent**.

You will see the list of enabled permissions.

![API permissions in Entra ID](~/assets/images/cloudflare-one/identity/azure/risky-users-permissions.png)

## 3. Add risky users to Entra ID group

Next, configure an automated script that will populate an Entra ID security group with risky users.

To get started quickly, deploy our example Cloudflare Workers script by following the step-by-step instructions below. Alternatively, you can implement the script using [Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview) or any other tool.

1. Open a terminal and clone our example project.

   ```sh
   npm create cloudflare@latest risky-users -- --template https://github.com/cloudflare/msft-risky-user-ad-sync
   ```

2. Go to the project directory.

   ```sh
   cd risky-users
   ```

3. Modify the [Wrangler configuration file](/workers/wrangler/configuration/) to include the following values:

   - `<ACCOUNT_ID>`: your Cloudflare [account ID](/fundamentals/setup/find-account-and-zone-ids/).
   - `<TENANT_ID>`: your Entra ID **Directory (tenant) ID**, obtained when [setting up Entra ID as an identity provider](#1-set-up-entra-id-as-an-identity-provider).
   - `<CLIENT_ID>`: your Entra ID **Application (client) ID**, obtained when [setting up Entra ID as an identity provider](#1-set-up-entra-id-as-an-identity-provider).


   <WranglerConfig>

   ```toml
   name = "risky-users"
   compatibility_date = "2023-01-04"
   main = "src/index.js"
   workers_dev = false

   account_id = "<ACCOUNT-ID>"

   [vars]
   AZURE_AD_TENANT_ID = "<TENANT-ID>"
   AZURE_AD_CLIENT_ID = "<CLIENT-ID>"

   [triggers]
   crons = ["* * * * *"]
   ```

   </WranglerConfig>

:::note

The [Cron Trigger](/workers/configuration/cron-triggers/) in this example schedules the script to run every minute. Learn more about [supported cron expressions](/workers/configuration/cron-triggers/#supported-cron-expressions).
:::

4. Deploy the Worker to Cloudflare's global network.

   ```sh
   npx wrangler deploy
   ```

5. Create a secret variable named `AZURE_AD_CLIENT_SECRET`.

   ```sh
   wrangler secret put AZURE_AD_CLIENT_SECRET
   ```

   You will be prompted to input the secret's value. Enter the **Client secret** obtained when [setting up Microsoft Entra ID as an identity provider](#1-set-up-azure-ad-as-an-identity-provider).

The Worker script will begin executing once per minute. To view realtime logs, run the following command and wait for the script to execute:

```sh
wrangler tail --format pretty
```

After the initial run, the auto-generated groups will appear in the Entra ID dashboard.

![Risky user groups in the Entra ID dashboard](~/assets/images/cloudflare-one/identity/azure/risky-users-groups.png)

## 4. Synchronize risky user groups

Next, synchronize Entra ID risky user groups with Cloudflare Access:

1. [Enable SCIM synchronization](/cloudflare-one/identity/idp-integration/entra-id/#synchronize-users-and-groups).

2. In Entra ID, assign the following groups to your SCIM enterprise application:
   - `IdentityProtection-RiskyUser-RiskLevel-high`
   - `IdentityProtection-RiskyUser-RiskLevel-medium`
   - `IdentityProtection-RiskyUser-RiskLevel-low`

Cloudflare Access will now synchronize changes in group membership with Entra ID. You can verify the synchronization status on the SCIM application's **Provisioning** page.

## 5. Create a browser isolation policy

Finally, create a [Gateway HTTP policy](/cloudflare-one/policies/gateway/http-policies/) to isolate traffic for risky user groups.

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Gateway** > **Firewall policies** > **HTTP**.

2. Select **Add a policy**.

3. Build an [Isolate policy](/cloudflare-one/policies/browser-isolation/isolation-policies/) that contains a _User Group Names_ rule. For example, the following policy serves `app1.example.com` and `app2.example.com` in a remote browser for all members flagged as high risk:

   | Selector         | Operator | Value                                         | Logic | Action  |
   | ---------------- | -------- | --------------------------------------------- | ----- | ------- |
   | Domain           | in       | `app1.example.com`, `app2.example.com`        | And   | Isolate |
   | User Group Names | in       | `IdentityProtection-RiskyUser-RiskLevel-high` |       |         |

To test the policy, refer to the Microsoft documentation for [simulating risky detections](https://learn.microsoft.com/entra/id-protection/howto-identity-protection-simulate-risk).

---

# Send SSO attributes to Access-protected origins with Workers

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/extend-sso-with-workers/

import { Render, GlossaryTooltip, PackageManagers, WranglerConfig } from "~/components"

This tutorial will walk you through extending the single-sign-on (SSO) capabilities of [Cloudflare Access](/cloudflare-one/policies/access/) with our serverless computing platform, [Cloudflare Workers](/workers/). Specifically, this guide will demonstrate how to modify requests sent to your secured origin to include additional information from the Cloudflare Access authentication event.

**Time to complete:** 45 minutes

## Authentication flow

[Cloudflare Access](/cloudflare-one/policies/access/) is an authentication proxy in charge of validating a user's identity before they connect to your application. As shown in the diagram below, Access inserts a [JWT](/cloudflare-one/identity/authorization-cookie/application-token/) into the request, which can then be [verified](/cloudflare-one/identity/authorization-cookie/validating-json/#validate-jwts) by the origin server.

![Standard authentication flow for a request to an Access application](~/assets/images/cloudflare-one/applications/access-standard-flow.png)

You can extend this functionality by using a Cloudflare Worker to insert additional HTTP headers into the request. In this example, we will add the [device posture attributes](/cloudflare-one/identity/devices/#enforce-device-posture) `firewall_activated` and `disk_encrypted`, but you can include any attributes that Cloudflare Access collects from the authentication event.

![Extended authentication flow uses a Worker to pass additional request headers to the origin](~/assets/images/cloudflare-one/applications/access-extended-flow-serverless.png)

## Benefits

This approach allows you to:

* **Enhance security:** By incorporating additional information from the authentication event, you can implement more robust security measures. For example, you can use device posture data to enforce access based on device compliance.
* **Improve user experience:** You can personalize the user experience by tailoring content or functionality based on user attributes. For example, you can display different content based on the user's role or location.
* **Simplify development:** By using Cloudflare Workers, you can easily extend your Cloudflare Access configuration without modifying your origin application code.


## Before you begin

- Add a [self-hosted application](/cloudflare-one/applications/configure-apps/self-hosted-public-app/) to Cloudflare Access.
- Enable the [Disk encryption](/cloudflare-one/identity/devices/warp-client-checks/disk-encryption/) and [Firewall](/cloudflare-one/identity/devices/warp-client-checks/firewall/) device posture checks.
- Install [Wrangler](/workers/wrangler/install-and-update/) on your local machine.

## 1. Create the Worker

1. Create a new Workers project:

		<PackageManagers
			type="create"
			pkg="cloudflare@latest"
			args={"device-posture-worker"}
		/>

		<Render
			file="c3-post-run-steps"
			product="workers"
			params={{
				category: "hello-world",
				type: "Hello World Worker",
				lang: "JavaScript",
			}}
		/>

2. Change to the project directory:

    ```sh
    $ cd device-posture-worker
    ```

3. Copy-paste the following code into `src/index.js`. Be sure to replace `<your-team-name>` with your Zero Trust <GlossaryTooltip term="team name">team name</GlossaryTooltip>.

    ```js title="index.js"

    import { parse } from "cookie";
    export default {
      async fetch(request, env, ctx) {
        // The name of the cookie
        const COOKIE_NAME = "CF_Authorization";
      const CF_GET_IDENTITY = "https://<your-team-name>.cloudflareaccess.com/cdn-cgi/access/get-identity";
        const cookie = parse(request.headers.get("Cookie") || "");
        if (cookie[COOKIE_NAME] != null) {
        try {
        let id = await (await fetch(CF_GET_IDENTITY, request)).json()
        let diskEncryptionStatus = false;
        let firewallStatus = false;

        for (const checkId in id.devicePosture) {
          const check = id.devicePosture[checkId];
          if (check.type === "disk_encryption") {
            console.log(check.type)
            diskEncryptionStatus = check.success;
          }
          if (check.type === "firewall") {
            console.log(check.type)
            firewallStatus = check.success;
            break;
          }
        }
        //clone request (immutable otherwise) and insert posture values in new header set
        let newRequest = await new Request(request)
        newRequest.headers.set("Cf-Access-Firewall-Activated", firewallStatus)
        newRequest.headers.set("Cf-Access-Disk-Encrypted", firewallStatus)

        //sent modified request to origin
        return await fetch(newRequest)

        } catch (e) {
        console.log(e)
        return await fetch(request)
        }
        }
        return await fetch(request)
      },
    };

    ```

## 2. View the user's identity

The script in `index.js` uses the [`get-identity`](/cloudflare-one/identity/authorization-cookie/application-token/#user-identity) endpoint to fetch a user's complete identity from a Cloudflare Access authentication event. To view a list of available data fields, log in to your Access application and append `/cdn-cgi/access/get-identity` to the URL. For example, if `www.example.com` is behind Access, go to `https://www.example.com/cdn-cgi/access/get-identity`.

Below is an example of a user identity that includes the `disk_encryption` and `firewall` posture checks. The Worker inserts the posture check results into the request headers **Cf-Access-Firewall-Activated** and **Cf-Access-Disk-Encrypted**.

```json title="Example user identity" {33,52}
{
  "id": "P51Tuu01fWHMBjIBvrCK1lK-eUDWs2aQMv03WDqT5oY",
  "name": "John Doe",
  "email": "john.doe@cloudflare.com",
  "amr": [
    "pwd"
  ],
  "oidc_fields": {
    "principalName": "XXXXXX_cloudflare.com#EXT#@XXXXXXcloudflare.onmicrosoft.com"
  },
  "groups": [
    {
      "id": "fdaedb59-e9be-4ab7-8001-3e069da54185",
      "name": "XXXXX"
    }
  ],
  "idp": {
    "id": "b9f4d68e-dac1-48b0-b728-ae05a5f0d4b2",
    "type": "azureAD"
  },
  "geo": {
    "country": "FR"
  },
  "user_uuid": "ce40d564-c72f-475f-a9b8-f395f19ad986",
  "account_id": "121287a0c6e6260ec930655e6b39a3a8",
  "iat": 1724056537,
  "devicePosture": {
    "f6f9391e-6776-4878-9c60-0cc807dc7dc8": {
      "id": "f6f9391e-6776-4878-9c60-0cc807dc7dc8",
      "schedule": "5m",
      "timestamp": "2024-08-19T08:31:59.274Z",
      "description": "",
      "type": "disk_encryption",
      "check": {
        "drives": {
          "C": {
            "encrypted": true
          }
        }
      },
      "success": false,
      "rule_name": "Disk Encryption - Windows",
      "input": {
        "requireAll": true,
        "checkDisks": []
    },
    "a0a8e83d-be75-4aa6-bfa0-5791da6e9186": {
      "id": "a0a8e83d-be75-4aa6-bfa0-5791da6e9186",
      "schedule": "5m",
      "timestamp": "2024-08-19T08:31:59.274Z",
      "description": "",
      "type": "firewall",
      "check": {
        "firewall": false
      },
      "success": false,
      "rule_name": "Local Firewall Check - Windows",
      "input": {
        "enabled": true
      }
    }
    ...
  }
```

## 3. Route the Worker to your application

In the [Wrangler configuration file](/workers/wrangler/configuration/), [set up a route](/workers/configuration/routing/routes/) that maps the Worker to your Access application domain:

<WranglerConfig>

```toml
route = { pattern= "app.example.com/*", zone_name="example.com"}
```

</WranglerConfig>

## 4. Deploy the Worker

```sh
npx wrangler deploy
```

The Worker will now insert the **Cf-Access-Firewall-Activated** and **Cf-Access-Disk-Encrypted** headers into requests that pass your application's Access policies.

```json title="Example request headers" {7,8}
{
  "headers": {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Encoding": "gzip",
    "Accept-Language": "en-US,en;q=0.9,fr-FR;q=0.8,fr;q=0.7,en-GB;q=0.6",
    "Cf-Access-Authenticated-User-Email": "John.Doe@cloudflare.com",
    "Cf-Access-Disk-Encrypted": "false",
    "Cf-Access-Firewall-Activated": "false",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
  }
}
```

You can verify that these headers are received by the origin server.

---

# Validate the Access token with FastAPI

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/fastapi/

This tutorial covers how to validate that the [Access JWT](/cloudflare-one/identity/authorization-cookie/validating-json/) is on requests made to FastAPI apps.

**Time to complete:** 15 minutes

## Prerequisites

* A [self-hosted Access application](/cloudflare-one/applications/configure-apps/self-hosted-public-app/) for your FastAPI app
* The [AUD tag](/cloudflare-one/identity/authorization-cookie/validating-json/#get-your-aud-tag) for your Access application

## 1. Create a validation function

1. In your FastAPI project, create a new file called `cloudflare.py` that contains the following code:

```python
from fastapi import Request, HTTPException

# The Application Audience (AUD) tag for your application
POLICY_AUD = "XXXXX"

# Your CF Access team domain
TEAM_DOMAIN = "https://<your-team-name>.cloudflareaccess.com"
CERTS_URL = "{}/cdn-cgi/access/certs".format(TEAM_DOMAIN)

async def validate_cloudflare(request: Request):
    """
    Validate that the request is authenticated by Cloudflare Access.
    """
    if verify_token(request) != True:
        raise HTTPException(status_code=400, detail="Not authenticated properly!")


def _get_public_keys():
    """
    Returns:
        List of RSA public keys usable by PyJWT.
    """
    r = requests.get(CERTS_URL)
    public_keys = []
    jwk_set = r.json()
    for key_dict in jwk_set["keys"]:
        public_key = jwt.algorithms.RSAAlgorithm.from_jwk(json.dumps(key_dict))
        public_keys.append(public_key)
    return public_keys


def verify_token(request):
    """
    Verify the token in the request.
    """
    token = ""

    if "CF_Authorization" in request.cookies:
        token = request.cookies["CF_Authorization"]
    else:
        raise HTTPException(status_code=400, detail="missing required cf authorization token")

    keys = _get_public_keys()

    # Loop through the keys since we can't pass the key set to the decoder
    valid_token = False
    for key in keys:
        try:
            # decode returns the claims that has the email when needed
            jwt.decode(token, key=key, audience=POLICY_AUD, algorithms=["RS256"])
            valid_token = True
            break
        except:
            raise HTTPException(status_code=400, detail="Error decoding token")
    if not valid_token:
        raise HTTPException(status_code=400, detail="Invalid token")

    return True

```

## 2. Use the validation function in your app

You can now add the validation function as a dependency in your FastAPI app. One way to do this is by creating an [`APIRouter` instance](https://fastapi.tiangolo.com/tutorial/bigger-applications/#another-module-with-apirouter). The following example executes the validation function on each request made to paths that start with `/admin`:

```python
from fastapi import APIRouter, Depends, HTTPException
from cloudflare import validate_cloudflare

router = APIRouter(
    prefix="/admin",
    tags=["admin"],
    dependencies=[Depends(validate_cloudflare)]
    responses={404: {"description": "Not found"}},
)

@router.get("/")
async def root():
    return {"message": "Hello World"}
```

---

# Zero Trust GitLab SSH & HTTP

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/gitlab/

You can use Cloudflare Access to add Zero Trust rules to a self-hosted instance of GitLab. Combined with Cloudflare Tunnel, users can connect through HTTP and SSH and authenticate with your team's identity provider.

**This walkthrough covers how to:**

- Deploy an instance of GitLab
- Lock down all inbound connections to that instance and use Cloudflare Tunnel to set outbound connections to Cloudflare
- Build policies with Cloudflare Access to control who can reach GitLab
- Connect over HTTP and SSH through Cloudflare

**Time to complete:**

1 hour

---

## Deploying GitLab

This section walks through deploying GitLab in DigitalOcean. If you have already deployed GitLab, you can skip this section.

Create a Droplet that has 16 GB of RAM and 6 CPUs. This should make it possible to support 500 users, based on [GitLab's resource recommendations](https://docs.gitlab.com/ee/install/requirements.html).

![Create Droplet](~/assets/images/cloudflare-one/zero-trust-security/gitlab/create-droplet.png)

GitLab will provide an external IP that is exposed to the Internet (for now). You will need to connect to the deployed server using this external IP for the initial configuration. You can secure connections to the IP by [adding SSH keys](https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2) to your DigitalOcean account.

This example uses a macOS machine to configure the Droplet. Copy the IP address assigned to the machine from DigitalOcean.

![Machine IP](~/assets/images/cloudflare-one/zero-trust-security/gitlab/show-ip.png)

Open Terminal and run the following command, replacing the IP address with the IP assigned by DigitalOcean.

```sh
ssh root@134.209.124.123
```

Next, install GitLab. This example uses the [Ubuntu package](https://about.gitlab.com/install/#ubuntu) and the steps in the GitLab documentation, with a few exceptions called out below.

Run the following commands to begin.

```sh
sudo apt-get update

sudo apt-get install -y curl openssh-server ca-certificates
curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.deb.sh | sudo bash
```

The commands above download the GitLab software to this machine. You must now install it. This is the first place this tutorial will diverge from the operations in the GitLab documentation. The next step in the GitLab-provided tutorial sets an external hostname. Instead, you can just install the software.

```sh
sudo apt-get install gitlab-ee
```

After a minute or so, GitLab will be installed.

![Install GitLab](~/assets/images/cloudflare-one/zero-trust-security/gitlab/install-gitlab.png)

However, the application is not running yet. You can check to see what ports are listening to confirm by using `ss`.

```sh
sudo ss -lntup
```

The result should be only the services currently active on the machine:

{/* <!-- ![Just Services](/images/cloudflare-one/zero-trust-security/gitlab/just-services.png) --> */}

```bash
sudo ss -lntup
```

```bash output
Netid   State    Recv-Q   Send-Q     Local Address:Port     Peer Address:Port   Process
udp     UNCONN   0        0                      *:9094                *:*
tcp     LISTEN   0        128              0.0.0.0:22            0.0.0.0:*       users:(("sshd",pid=29,fd=3))
tcp     LISTEN   0        128                 [::]:22               [::]:*       users:(("sshd",pid=29,fd=4))
```

To start GitLab, run the software's reconfigure command.

```sh
sudo gitlab-ctl reconfigure
```

GitLab will launch its component services. Once complete, confirm that GitLab is running and listening on both ports 22 and 80.

![GitLab Services](~/assets/images/cloudflare-one/zero-trust-security/gitlab/gitlab-services.png)

```bash
sudo ss -lntup
```

```bash output
Netid   State    Recv-Q   Send-Q     Local Address:Port     Peer Address:Port   Process
udp     UNCONN   0        0                      *:9094                *:*
tcp     LISTEN   0        4096           127.0.0.1:9236          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:8150          0.0.0.0:*
tcp     LISTEN   0        128              0.0.0.0:22            0.0.0.0:*       users:(("sshd",pid=29,fd=3))
tcp     LISTEN   0        4096           127.0.0.1:8151          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:3000          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:8153          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:8154          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:8155          0.0.0.0:*
tcp     LISTEN   0        511              0.0.0.0:8060          0.0.0.0:*       users:(("nginx",pid=324,fd=8))
tcp     LISTEN   0        4096           127.0.0.1:9121          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:9090          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:9187          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:9093          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:9229          0.0.0.0:*
tcp     LISTEN   0        1024           127.0.0.1:8080          0.0.0.0:*
tcp     LISTEN   0        511              0.0.0.0:80            0.0.0.0:*       users:(("nginx",pid=324,fd=7))
tcp     LISTEN   0        4096           127.0.0.1:9168          0.0.0.0:*
tcp     LISTEN   0        4096           127.0.0.1:8082          0.0.0.0:*
tcp     LISTEN   0        128                 [::]:22               [::]:*       users:(("sshd",pid=29,fd=4))
tcp     LISTEN   0        4096                   *:9094                *:*
```

Users connect to GitLab over SSH (port 22 here) and HTTP for the web app (port 80). In the next step, you will make it possible for users to try both through Cloudflare Access. I'll leave this running and head over to the Cloudflare dashboard.

## Securing GitLab with Zero Trust rules

### Building Zero Trust policies

You can use Cloudflare Access to build Zero Trust rules to determine who can connect to both the web application of GitLab (HTTP) and who can connect over SSH.

When a user makes a request to a site protected by Access, that request hits Cloudflare's network first. Access can then check if the user is allowed to reach the application. When integrated with Cloudflare Tunnel, the Zero Trust architecture looks like this:

![GitLab Services](~/assets/images/cloudflare-one/zero-trust-security/gitlab/teams-diagram.png)

To determine who can reach the application, Cloudflare Access relies on integration with identity providers like Okta, Microsoft Entra ID, or Google to issue the identity cards that get checked at the door. While a VPN allows users free range on a private network unless someone builds an active rule to stop them, Access enforces that identity check on every request (and at any granularity configured).

For GitLab, start by building two policies. Users will connect to GitLab in a couple of methods: in the web app and over SSH. Create policies to secure a subdomain for each. First, the web app.

Before you build the rule, you'll need to follow [these instructions](/cloudflare-one/setup/) to set up Cloudflare Access in your account.

Once enabled, go to the **Applications** page in Zero Trust. Select **Add an application**.

Choose self-hosted from the options presented.

![Self Hosted](~/assets/images/cloudflare-one/zero-trust-security/gitlab/policy.png)

In the policy builder, you will be prompted to add a subdomain that will represent the resource. This must be a subdomain of a domain in your Cloudflare account. You will need separate subdomains for the web application and SSH flows.

This example uses `gitlab.widgetcorp.tech` for the web application and `gitlab-ssh.widgetcorp.tech` for SSH connectivity.

While on this page, you can decide which identity providers will be allowed to authenticate. By default, all configured providers are allowed. Select **Next** to build rules to determine who can reach the application.

You can then add rules to determine who can reach the site.

Select **Next** and **Next** again on the **Setup** page - this example does not require advanced CORS configuration. Repeat these steps for the second application, `gitlab-ssh.widgetcorp.tech`.

## Cloudflare Tunnel

Cloudflare Tunnel creates a secure, outbound-only, connection between this machine and Cloudflare's network. With an outbound-only model, you can prevent any direct access to this machine and lock down any externally exposed points of ingress. And with that, no open firewall ports.

Cloudflare Tunnel is made possible through a lightweight daemon from Cloudflare called `cloudflared`. Download and install `cloudflared` on the DigitalOcean machine by following the instructions listed on the [Downloads](/cloudflare-one/connections/connect-networks/downloads/) page.

Once installed, authenticate the instance of `cloudflared` with the following command.

```sh
cloudflared login
```

The command will print a URL that you must visit to login with your Cloudflare account.

Choose a website that you have added into your account.

Once you select one of the sites in your account, Cloudflare will download a certificate file to authenticate this instance of `cloudflared`. You can now use `cloudflared` to control Cloudflare Tunnel connections in your Cloudflare account.

![Download Cert](~/assets/images/cloudflare-one/secure-origin-connections/share-new-site/cert-download.png)

### Connecting to Cloudflare

You can now connect GitLab to Cloudflare using Cloudflare Tunnel.

1. Create a new Tunnel by running the following command.

```sh
cloudflared tunnel create gitlab
```

`cloudflared` will generate a unique ID for this Tunnel, for example `6ff42ae2-765d-4adf-8112-31c55c1551ef`. You can use this Tunnel both for SSH and HTTP traffic.

2. You will need to configure Cloudflare Tunnel to proxy traffic to both destinations. The configuration below will take traffic bound for the DNS record that will be created for the web app and the DNS record to represent SSH traffic to the right port.

You use the text editor of your choice to edit the configuration file. The example relies on `Vi`.

```sh
vim ~/.cloudflared/config.yml
```

3. Configure the Tunnel to serve traffic.

```yml
tunnel: 6ff42ae2-765d-4adf-8112-31c55c1551ef
credentials-file: /root/.cloudflared/6ff42ae2-765d-4adf-8112-31c55c1551ef.json

ingress:
  - hostname: gitlab.widgetcorp.tech
    service: http://localhost:80
  - hostname: gitlab-ssh.widgetcorp.tech
    service: ssh://localhost:22
  # Catch-all rule, which just responds with 404 if traffic doesn't match any of
  # the earlier rules
  - service: http_status:404
```

![Self Hosted](~/assets/images/cloudflare-one/zero-trust-security/gitlab/config-file.png)

4. You can test that the configuration file is set correctly with the following command:

```sh
cloudflared tunnel ingress validate
```

`cloudflared` should indicate the Tunnel is okay. You can now begin running the Tunnel.

```sh
cloudflared tunnel run
```

![Tunnel Run](~/assets/images/cloudflare-one/zero-trust-security/gitlab/tunnel-run.png)

:::note

This command should be run as a `systemd` service for long-term use; if it terminates, GitLab will be unavailable.

:::

### Configure DNS records

You can now create DNS records for GitLab in the Cloudflare dashboard. Remember, you will still need two records - one for the web application and one for SSH traffic.

In the **DNS** tab, choose the website where you built your [Access policies](/cloudflare-one/policies/access/). Select **Add record** and select `CNAME` from type. In the **Name** field, input `gitlab`. In the **Target** field, input the ID of the Tunnel created followed by `cfargotunnel.com`. In this example, that value is:

```txt
6ff42ae2-765d-4adf-8112-31c55c1551ef.cfargotunnel.com
```

Select **Save**. Repeat the process again by creating a second `CNAME` record, with the same **Target**, but input `gitlab-ssh` for the **Name**. Both records should then appear, pointing to the same Tunnel. The ingress rules defined in the configuration file above will direct traffic to the appropriate port.

![View DNS](~/assets/images/cloudflare-one/zero-trust-security/gitlab/view-dns.png)

### Connecting to the web application

You can now test the end-to-end configuration for the web application. Visit the subdomain created for the web application. Cloudflare Access will prompt you to authenticate. Login with your provider.

Once authenticated, you should see the GitLab web application.

![GitLab Web](~/assets/images/cloudflare-one/zero-trust-security/gitlab/gitlab-web.png)

Register your own account and create a Blank project to test SSH in the next step.

![Blank Project](~/assets/images/cloudflare-one/zero-trust-security/gitlab/blank-project.png)

GitLab will create a new project and repository.

:::note

To pull or push code, you must also add an SSH key to your profile in GitLab.

:::

### Configuring SSH

To push and pull code over SSH, you will need to install `cloudflared` on the client machine as well. This example uses a macOS laptop. On macOS, you can install `cloudflared` with the following command.

```sh
brew install cloudflared
```

While you need to install `cloudflared`, you do not need to wrap your SSH commands in any unique way. Instead, you will need to make a one-time change to your SSH configuration file.

```sh
vim /Users/samrhea/.ssh/config
```

Input the following values; replacing `gitlab-ssh.widgetcorp.tech` with the hostname you created.

```txt
Host gitlab-ssh.widgetcorp.tech
  ProxyCommand /usr/local/bin/cloudflared access ssh --hostname %h
```

You can now test the SSH flow by attempting to clone the project created earlier.

```sh
git clone git@gitlab-ssh.widgetcorp.tech:samrhea/demo
```

`cloudflared` will prompt you to login with my identity provider and, once successful, issue a token to your device to allow you to authenticate.

![GitLab Clone](~/assets/images/cloudflare-one/zero-trust-security/gitlab/git-clone.png)

### Lock down exposed ports

You can now configure your DigitalOcean firewall with a single rule, block any inbound traffic, to prevent direct access.

![Set Rules](~/assets/images/cloudflare-one/zero-trust-security/gitlab/disable-ingress.png)

Cloudflare Tunnel will continue to run outbound-only connections and I can avoid this machine getting caught up in a crypto mining operation, or something worse.

## View logs

You can also view logs of the events that are allowed and blocked. Open the `Access` page of the `Logs` section in Zero Trust.

---

# Tutorials

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/

import { ListTutorials } from "~/components"

<ListTutorials />

---

# Monitor Cloudflare Tunnel with Grafana

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/grafana/

[Grafana](https://grafana.com/) is a dashboard tool that visualizes data stored in other databases. You can use Grafana to convert your [tunnel metrics](/cloudflare-one/connections/connect-networks/monitor-tunnels/metrics/) into actionable insights.

It is not possible to push metrics directly from `cloudflared` to Grafana. Instead, `cloudflared` runs a [Prometheus](https://prometheus.io) metrics endpoint, which a Prometheus server periodically scrapes. Grafana then uses Prometheus as a data source to present metrics to the administrator.

```mermaid
flowchart LR

  subgraph 192.168.1.1
  A[cloudflared]-->B[Metrics endpoint]
  end

  B--->C
  subgraph 192.168.1.2
  C[Prometheus server]-->D[Grafana dashboard]
  end
```

This tutorial covers how to create the metrics endpoint, set up the Prometheus server, and view the data in Grafana.

## Before you begin

- You will need a Cloudflare Tunnel. To create a tunnel, refer to our [getting started guide](/cloudflare-one/connections/connect-networks/get-started/).

## Create the metrics endpoint

If your tunnel was created via the CLI, run the following command on the `cloudflared` server (`192.168.1.1`):

```sh
cloudflared tunnel --metrics 192.168.1.1:60123 run my-tunnel
```

If your tunnel was created via the dashboard, the [--metrics](/cloudflare-one/connections/connect-networks/configure-tunnels/cloudflared-parameters/run-parameters/#metrics) flag must be added to your `cloudflared` system service configuration. Refer to [Add tunnel run parameters](/cloudflare-one/connections/connect-networks/configure-tunnels/cloudflared-parameters/#update-tunnel-run-parameters) for instructions on how to do this.

## Set up Prometheus

On the Prometheus and Grafana server (`192.168.1.2`):

1. [Download](https://prometheus.io/download/) Prometheus.

2. Extract Prometheus:

   ```sh
   tar xvfz prometheus-*.tar.gz
   cd prometheus-*
   ```

3. Open `prometheus.yml` in a text editor and add the `cloudflared` job to the end of the file:

   ```yml null {31-33}
   # my global config
   global:
     scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
     evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
     # scrape_timeout is set to the global default (10s).

   # Alertmanager configuration
   alerting:
     alertmanagers:
       - static_configs:
           - targets:
             # - alertmanager:9093

   # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
   rule_files:
     # - "first_rules.yml"
     # - "second_rules.yml"

   # A scrape configuration containing exactly one endpoint to scrape:
   # Here it's Prometheus itself.
   scrape_configs:
     # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
     - job_name: "prometheus"

       # metrics_path defaults to '/metrics'
       # scheme defaults to 'http'.

       static_configs:
         - targets: ["localhost:9090"] ## Address of Prometheus dashboard

     - job_name: "cloudflared"
       static_configs:
         - targets: ["198.168.1.1:60123"] ## cloudflared server IP and the --metrics port configured for the tunnel
   ```

4. Start Prometheus:

   ```sh
   ./prometheus --config.file="prometheus.yml"
   ```

   You can optionally configure Prometheus to run as a service so that it does not need to be manually started if the machine reboots.

5. Open a browser and go to `http://localhost:9090/`. You should be able to access the Prometheus dashboard.

6. To verify that Prometheus is fetching tunnel metrics, enter `cloudflared_tunnel_total_requests` into the expression console and select **Execute**.

   ![Prometheus dashboard showing tunnel metrics data](~/assets/images/cloudflare-one/secure-origin-connections/monitor-tunnels/Prometheus-dashboard.png)

Refer to [Available metrics](/cloudflare-one/connections/connect-networks/monitor-tunnels/metrics/#available-metrics) to check what other metrics are available.

## Connect Grafana to Prometheus

1. [Download](https://grafana.com/grafana/download) and install Grafana.

2. Start Grafana as a system service:

   ```sh
   sudo systemctl daemon-reload
   sudo systemctl start grafana-server
   ```

3. Verify that Grafana is running:

   ```sh
   sudo systemctl status grafana-server
   ```

4. Open a browser and go to `http://localhost:3000/`. The default HTTP port that Grafana listens to is `3000` unless you have configured a different port.

5. On the sign-in page, enter your Grafana credentials.

   To test without an account, you can enter `admin` for both the username and password and skip the password change step.

6. In Grafana, go to **Connections** > **Data sources**.

7. Select **Add a new data source** and select **Prometheus**.

8. In the **Prometheus server URL** field, enter the IP address and port of your Prometheus dashboard (`http://localhost:9090`).

9. Select **Save & test**.

## Build Grafana dashboard

1. In Grafana, go to **Dashboards** > **New** > **New dashboard**.
2. Select **Add visualization**.
3. Select **Prometheus**.
4. In the metrics field, enter `cloudflared_tunnel_total_requests` and select **Run queries**. You will see a graph showing the number of requests as a function of time.

![Grafana dashboard showing a tunnel metrics graph](~/assets/images/cloudflare-one/secure-origin-connections/monitor-tunnels/Grafana-dashboard.png)

You can add operations to the queries to modify what is displayed. For example, you could show all tunnel requests over a recent period of time, such as a day, rather than all tunnel requests since metrics began reporting.

---

# Integrate Microsoft MCAS with Cloudflare Zero Trust

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/integrate-microsoft-mcas-teams/

Many security teams rely on Microsoft MCAS (Microsoft Cloud App Security), Microsoft's CASB solution, to identify and block threats on the Internet, as well as allow or block access to cloud applications. This tutorial covers how to integrate MCAS with Cloudflare Zero Trust, and create Gateway HTTP policies to ensure visibility and control over data.

Microsoft provides an MCAS API endpoint to allow queries to see which applications have been marked as blocked or allowed. With an MCAS API call, you can manage a URL category that contains the blocked URLs returned by the API query, and use the output to create a Hostname List that can be used by Gateway HTTP policies to block them.

**Time to complete:**

20 minutes

## Basic configuration

In your Microsoft account, you first need to create an API token and URL endpoint to use to query the URLs blocked by MCAS.
Follow the guide for [Managing API tokens for Microsoft Cloud App Security](https://docs.microsoft.com/en-us/cloud-app-security/api-authentication) to generate a new API token and a custom API URL for the API endpoint.

## Using the API to query banned applications

Once you have the API token and API URL, use curl to get the list of banned applications from Microsoft MCAS:

```sh
curl -v "https://<MCAS API URL>/api/discovery_block_scripts/?format=120&type=banned" -H "Authorization: Token <API token>"
```

This will return a list of banned hostnames. In this case, Angie's List is the banned application.

![Banned hostnames](~/assets/images/cloudflare-one/microsoft-mcas/mcas-domains.png)

### Processing the output

As you can see, the banned hostnames are preceded by a `.`. To use this output for a Zero Trust List, we need to do some text processing.

1. Run the curl API call and direct the output to a file, in this case `mcas.txt`:

   ```sh
   curl -v "https://<MCAS API URL>/api/discovery_block_scripts/?format=120&type=banned" -H "Authorization: Token <API token>" > mcas.txt
   ```

2. Remove the leading `.`, for example by running `sed` from the CLI:

   ```sh
   sed -i 's/^.//' mcas.txt
   ```

3. This will give you the list of hostnames without leading `.`.

4. Replace the file's `.txt` extension with `.csv`. The file can now be imported into Cloudflare Zero Trust as a Hostname list.

## Using the API to query allowed applications

If you would like to get a list of all of the MCAS allowed applications, you can use the same API query, but instead of using `type=banned`, use `type=allowed`. This will return a much larger list.

```sh
curl -v "https://<MCAS API URL>/api/discovery_block_scripts/?format=120&type=allowed" -H "Authorization: Token <API token>"
```

## Adding a hostname list in Zero Trust

1. In Zero Trust, go to **My Team** > **Lists**
2. Select **Upload CSV**. Even though the hostname list is not really in CSV format, it will work with no issues.
3. Add a name for the list, specify "Hostnames" as the list type, and give it a description.
4. Drag and drop your MCAS output file created via the API call, or you can select **Select a file**.
5. Select **Create**. You will see the list of hostnames that have been added to the list.
6. Save the list.

Your list is now ready to be referenced by Gateway HTTP policies.

## Creating an HTTP policy

1. Go to **Gateway** > **Firewall policies**. Select **HTTP**.
2. Select **Add a policy**.
3. Create the following policy.

   | Selector | Operator | Value                | Action |
   | -------- | -------- | -------------------- | ------ |
   | Host     | in list  | \<NEW_HOSTNAME_LIST> | Block  |

Now when trying to visit one of the MCAS defined sites, the user will be blocked.

![Access Restricted](~/assets/images/cloudflare-one/microsoft-mcas/mcas-block-page.png)

---

# Connect through Cloudflare Access using kubectl

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/kubectl/

You can connect to machines over `kubectl` using Cloudflare's Zero Trust platform.

**This walkthrough covers how to:**

- Build a policy in Cloudflare Access to secure the machine
- Connect a machine to Cloudflare's network using kubectl
- Connect from a client machine

**Before you start**

- [Add a website to Cloudflare](/fundamentals/setup/manage-domains/add-site/)

**Time to complete:**

30 minutes

---

## Create a Zero Trust policy

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Access** > **Applications**.
2. Select **Add an application**.
3. Select **Self-hosted**.
4. Enter a name for your Access application.
5. Select **Add public hostname** and input a subdomain. This will be the hostname where your application will be available to users.
6. [Create a new policy](/cloudflare-one/policies/access/policy-management/) to control who can reach the application, or select existing policies.
7. Follow the remaining [self-hosted application creation steps](/cloudflare-one/applications/configure-apps/self-hosted-public-app/) to publish the application.

## Install `cloudflared`

Cloudflare Tunnel creates a secure, outbound-only connection between this machine and Cloudflare's network. With an outbound-only model, you can prevent any direct access to this machine and lock down any externally exposed points of ingress. And with that, no open firewall ports.

Cloudflare Tunnel is made possible through a lightweight daemon from Cloudflare called `cloudflared`. Download and install `cloudflared` on the DigitalOcean machine by following the instructions listed on the [Downloads](/cloudflare-one/connections/connect-networks/downloads/) page.

## Authenticate `cloudflared`

Run the following command to authenticate cloudflared into your Cloudflare account.

```sh
cloudflared tunnel login
```

`cloudflared` will open a browser window and prompt you to log in to your Cloudflare account. If you are working on a machine that does not have a browser, or a browser window does not launch, you can copy the URL from the command-line output and visit the URL in a browser on any machine.

Choose any hostname presented in the list. Cloudflare will issue a certificate scoped to your account. You do not need to pick the specific hostname where you will serve the Tunnel.

## Create a Tunnel

Next, create a tunnel with the command below.

```sh
cloudflared tunnel create <NAME>
```

Replacing `<NAME>` with a name for the Tunnel. This name can be any value. A single Tunnel can also serve traffic for multiple hostnames to multiple services in your environment, including a mix of connection types like SSH and HTTP.

The command will output an ID for the Tunnel and generate an associated credentials file. At any time you can list the Tunnels in your account with the following command.

```sh
cloudflared tunnel list
```

## Configure the Tunnel

You can now [configure the tunnel](/cloudflare-one/connections/connect-networks/do-more-with-tunnels/local-management/create-local-tunnel/#4-create-a-configuration-file) to serve traffic.

Create a `YAML` file that `cloudflared` can reach. By default, `cloudflared` will look for the file in the same folder where `cloudflared` has been installed.

```sh
vim ~/.cloudflared/config.yml
```

Next, configure the Tunnel, replacing the example ID below with the ID of the Tunnel created above. Additionally, replace the hostname in this example with the hostname of the application configured with Cloudflare Access.

```yaml
tunnel: 6ff42ae2-765d-4adf-8112-31c55c1551ef
credentials-file: /root/.cloudflared/6ff42ae2-765d-4adf-8112-31c55c1551ef.json

ingress:
  - hostname: azure.widgetcorp.tech
    service: tcp://kubernetes.docker.internal:6443
    originRequest:
      proxyType: socks
  - service: http_status:404
  # Catch-all rule, which responds with 404 if traffic doesn't match any of
  # the earlier rules
```

## Route to the Tunnel

You can now create a DNS record that will route traffic to this Tunnel. Multiple DNS records can point to a single Tunnel and will send traffic to the configured service as long as the hostname is defined with an [ingress rule](/cloudflare-one/connections/connect-networks/do-more-with-tunnels/local-management/configuration-file/#file-structure-for-public-hostnames).

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account. Select your domain and go to **DNS**.

2. Select **Add record**. Choose `CNAME` as the record type. For **Name**, choose the hostname where you want to create a Tunnel. This should match the hostname of the Access policy.

3. For **Target**, input the ID of your Tunnel followed by `.cfargotunnel.com`. For example:

```txt
  6ff42ae2-765d-4adf-8112-31c55c1551ef.cfargotunnel.com
```

4. Select **Save**.

## Run the Tunnel

You can now run the Tunnel to connect the target service to Cloudflare. Use the following command to run the Tunnel, replacing `<NAME>` with the name created for your Tunnel.

```sh
cloudflared tunnel run <NAME>
```

We recommend that you run `cloudflared` [as a service](/cloudflare-one/connections/connect-networks/do-more-with-tunnels/local-management/as-a-service/) that is configured to launch on start.

## Connect from a client machine

You can now connect from a client machine using `cloudflared`.

This example uses a macOS laptop. On macOS, you can install `cloudflared` with the following command using Homebrew.

```sh
brew install cloudflared
```

Run the following command to create a connection from the device to Cloudflare. Any available port can be specified.

```sh
cloudflared access tcp --hostname azure.widgetcorp.tech --url 127.0.0.1:1234
```

With this service running, you can run a `kubectl` command and `cloudflared` will launch a browser window and prompt the user to authenticate with your SSO provider. Once authenticated, `cloudflared` will expose the connection to the client machine at the local URL specified in the command.

`kubeconfig` does not support proxy command configurations at this time, though the community has submitted plans to do so. In the interim, users can alias the cluster's API server to save time.

```sh
alias kubeone="env HTTPS_PROXY=socks5://127.0.0.1:1234 kubectl"
```

---

# Protect access to Microsoft 365 with dedicated egress IPs

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/m365-dedicated-egress-ips/

:::note

Only available on Zero Trust Enterprise plans.

:::

This tutorial covers how to secure access to your Microsoft 365 applications with Cloudflare Gateway dedicated egress IPs.

You can map a named location in Microsoft Entra ID to a location associated with your dedicated egress IPs. Traffic will egress from Cloudflare with these IP addresses. If users attempt to access your Microsoft applications without these IPs, Entra ID will block access.

## Before you begin

Make sure you have:

- In Cloudflare, a Zero Trust Enterprise plan with [dedicated egress IPs](/cloudflare-one/policies/gateway/egress-policies/dedicated-egress-ips/)
- In Microsoft 365, an organization managed with [Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/identity/)

## Create an egress policy in Cloudflare Gateway

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Gateway** > **Egress policies**.

2. Select **Add a policy**.

3. Name your policy, then add conditions to check users are configured in Microsoft Entra ID. For example, you can check for [identity conditions](/cloudflare-one/policies/gateway/identity-selectors/):

   | Selector         | Operator | Value                                         |
   | ---------------- | -------- | --------------------------------------------- |
   | User Group Names | in       | `Sales and Marketing`, `Retail`, `U.S. Sales` |

   Additionally, you can check for [device posture conditions](/cloudflare-one/identity/devices/):

   | Selector                    | Operator | Value                                             | Logic |
   | --------------------------- | -------- | ------------------------------------------------- | ----- |
   | Passed Device Posture Check | is       | `CrowdStrike Overall ZTA score (Crowdstrike s2s)` | And   |
   | Passed Device Posture Check | is       | `AppCheckMac - Required Software (Application)`   |       |

4. Enable **Use dedicated Cloudflare egress IPs**. Select your desired IPv4 and IPv6 addresses. For example:

   | Primary IPv4 address | IPv6 address    |
   | -------------------- | --------------- |
   | `203.0.113.0`        | `2001:db8::/32` |

## Create a named IP range location in Microsoft Entra ID

1. Log in to the [Microsoft Azure portal](https://aka.ms/azureportal).
2. In the sidebar, select **Microsoft Entra ID**.
3. Go to **Security** > **Named locations**.
4. Select **IP ranges location**.
5. Name your location, then add the IP addresses used in your Cloudflare dedicated egress IP policy.
6. Select **Upload**.

This named location corresponds with the locations of your dedicated egress IPs.

## Create a conditional access policy in Microsoft Entra ID

1. In **Protect**, go to **Conditional Access**.
2. Select **Create new policy**.
3. Configure which Entra ID users you want to limit access for, and which traffic, applications, or actions you want to protect.
4. In **Conditions**, select **Locations**. Enable **Configure**.
5. In **Include**, select _Any location_. In **Exclude**, select the named location you created.
6. In **Access controls**, go to **Grant**. Enable _Block access_.

Your policy will block access for your selected users from any location except those using your dedicated egress IPs.

## Test your policies

1. Using [WARP](/cloudflare-one/connections/connect-devices/warp/), sign in to your Zero Trust organization with a user's account.
2. Go to any Microsoft 365 app within your organization. Entra ID should allow access.
3. Disconnect WARP from your Zero Trust organization. Entra ID should block access to any Microsoft 365 applications.

---

# Use cloudflared to expose a Kubernetes app to the Internet

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/many-cfd-one-tunnel/

You can use [Cloudflare Tunnel](/cloudflare-one/connections/connect-networks/) to connect applications and servers to Cloudflare's network. Tunnel relies on a piece of software, [cloudflared](https://github.com/cloudflare/cloudflared), to create those connections.

The same Tunnel can be run from multiple instances of `cloudflared`, giving you the ability to run many `cloudflared` replicas to scale your system when incoming traffic changes.

In this tutorial, we will walk through running an application as a Kubernetes [Service](https://kubernetes.io/docs/concepts/services-networking/service/), and then running `cloudflared` in a separate [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/).

This architecture allows `cloudflared` instances to proxy Internet traffic into whichever Kubernetes Service it was configured to.

**This tutorial covers how to:**

- Deploy `cloudflared` in a replica model
- Proxy traffic into a Kubernetes service with Tunnel

**Time to complete: 45 minutes**

## Install `cloudflared`

Start by [downloading and installing](/cloudflare-one/connections/connect-networks/downloads/) the lightweight Cloudflare Tunnel daemon, `cloudflared`. Reference our installation guide for instructions on how to install `cloudflared` on your operating system.

## Login to Cloudflare

Once installed, you can use the `tunnel login` command in `cloudflared` to obtain a certificate.

```sh
cloudflared tunnel login
```

## Create your Tunnel

In the example below, simply change `<example-tunnel>` to the name you wish to assign to your Tunnel.

```sh
cloudflared tunnel create example-tunnel
```

```sh output
Tunnel credentials written to /Users/cf000197/.cloudflared/ef824aef-7557-4b41-a398-4684585177ad.json. cloudflared chose this file based on where your origin certificate was found. Keep this file secret. To revoke these credentials, delete the tunnel.

Created tunnel example-tunnel with id ef824aef-7557-4b41-a398-4684585177ad
```

## Upload the Tunnel credentials file to Kubernetes

Next, you will upload the generated Tunnel credential file as a secret to your Kubernetes cluster. You will also need to provide the filepath that the Tunnel credentials file was created under. You can find that path in the output of `cloudflared tunnel create <example-tunnel>` above.

```sh
kubectl create secret generic tunnel-credentials \
--from-file=credentials.json=/Users/cf000197/.cloudflared/ef824aef-7557-4b41-a398-4684585177ad.json
```

## Associate your Tunnel with a DNS record

1. Go to the Cloudflare dashboard.
2. Go to the DNS tab.
3. Now create a CNAME targeting `.cfargotunnel.com`. In this example, the tunnel ID is ef824aef-7557-4b41-a398-4684585177ad, so create a CNAME record specifically targeting `ef824aef-7557-4b41-a398-4684585177ad.cfargotunnel.com`.

You can also create multiple CNAME records targeting the same Tunnel, if desired.

Alternatively, you can perform this step from the command line by running `cloudflared tunnel route dns <tunnel> <hostname>`. For example, `cloudflared tunnel route dns example-tunnel tunnel.example.com`. You can use a similar method to route traffic to `cloudflared` from a [Cloudflare Load Balancer](https://www.cloudflare.com/load-balancing/), see [docs](/cloudflare-one/connections/connect-networks/routing-to-tunnel/lb/) for details.

## Deploy `cloudflared`

Now, we'll deploy `cloudflared` by applying its [manifest](https://github.com/cloudflare/argo-tunnel-examples/blob/master/named-tunnel-k8s/cloudflared.yaml). This will start a [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) for running `cloudflared` and a [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) with `cloudflared`'s config. When Cloudflare receives traffic for the DNS or Load Balancing hostname you configured in the previous step, it will send that traffic to the `cloudflared` instances running in this deployment. Then, those `cloudflared` instances will proxy the request to your [application's Service](https://github.com/cloudflare/argo-tunnel-examples/blob/master/named-tunnel-k8s/app.yaml).

```sh
kubectl apply -f cloudflared.yaml
```

```sh output
deployment.apps/cloudflared created
configmap/cloudflared configured
```

## Examine status of your pod

```
    $ kubectl get pods
    NAME                                  READY   STATUS    RESTARTS   AGE
    cloudflared-57746f77fd-frc99          1/1     Running   0          12m
    cloudflared-57746f77fd-xht8n          1/1     Running   0          12m
    httpbin-deployment-67f749774f-42tqj   1/1     Running   0          20h
    $ kubectl logs $(kubectl get pod -l app=cloudflared -o jsonpath="{.items[0].metadata.name}")
    2021-05-04T17:39:49Z INF Starting tunnel tunnelID=ef824aef-7557-4b41-a398-4684585177ad
    2021-05-04T17:39:49Z INF Version
    2021-05-04T17:39:49Z INF GOOS: linux, GOVersion: go1.15.7, GoArch: amd64
    2021-05-04T17:39:49Z INF Settings: map[config:/etc/cloudflared/config/config.yaml cred-file:/etc/cloudflared/creds/credentials.json credentials-file:/etc/cloudflared/creds/credentials.json metrics:0.0.0.0:2000 no-autoupdate:true]
    2021-05-04T17:39:49Z INF Generated Connector ID: 4c5dc5d3-8e10-480e-ac74-e385e591553e
    2021-05-04T17:39:49Z INF Initial protocol h2mux
    2021-05-04T17:39:49Z INF Starting metrics server on [::]:2000/metrics
    2021-05-04T17:39:49Z INF Connection 1daced2f-466c-4610-8ba6-7642a8ddec68 registered connIndex=0 location=MCI
    2021-05-04T17:39:50Z INF Connection 1a5276bc-3313-4bb7-a677-d93deccab24f registered connIndex=1 location=DFW
    2021-05-04T17:39:51Z INF Connection aa7adacc-e855-4b11-bf41-e113419b7ef4 registered connIndex=2 location=MCI
    2021-05-04T17:39:51Z INF Connection a8055c76-2a90-4be5-8dc9-ebaa5c58fb5f registered connIndex=3 location=DFW
```

## Visit your hostname

At this point, you'll see the httpbin welcome page.

In this tutorial, we've covered how the same Tunnel can be run in many `cloudflared` processes. You can also use this knowledge to support elastic scaling, graceful `cloudflared` restarts, and rolling upgrades in the future.

We love to hear your feedback! Join the discussion in our [community](https://community.cloudflare.com/).

---

# Migrate to Named Tunnels with Load Balancer

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/migrate-lb-tunnel/

import { Example, GlossaryTooltip } from "~/components";

Cloudflare Tunnel is available in two deployment modes: "Legacy" Tunnel and "Named" Tunnel. [Named Tunnel](https://blog.cloudflare.com/argo-tunnels-that-live-forever/) mode improves maintainability and stability by distinguishing between routing and configuration.

Unlike Legacy mode, Named Tunnels give users the ability to manage routing in the Cloudflare dashboard and to run `cloudflared` once for [multiple services](https://blog.cloudflare.com/many-services-one-cloudflared/).

If you are using Legacy Tunnel today you can migrate to Named Tunnel deployment in less than 10 minutes.

**This tutorial covers how to:**

- Migrate a Legacy Tunnel deployment to Named Tunnel model
- Use Cloudflare Load Balancer to perform a zero downtime migration

**Time to complete:**

10 minutes

See additional documentation for working with [Kubernetes](/cloudflare-one/connections/connect-networks/deployment-guides/kubernetes/).

---

## Legacy Tunnel with Cloudflare Load Balancer

This tutorial starts by documenting the steps to create a Legacy Tunnel with Cloudflare Load Balancer so that those can be compared to the migration steps. If you would prefer to start the migration now, skip to [Create a Named Tunnel](#create-a-named-tunnel).

In both modes, the first step is to create a load balancer and endpoint pool.

1. Go to **Traffic** > **Load Balancing**.

2. Select **Create Load Balancer**.

3. On the **Hostname** page:
   - Enter a **Hostname**, which is the DNS name at which the load balancer is available. For more details on record priority, refer to [DNS records for load balancing](/load-balancing/load-balancers/dns-records/).
   - Toggle the orange cloud icon to update the [proxy mode](/load-balancing/understand-basics/proxy-modes/), which affects how traffic is routed and which IP addresses are advertised.
   - If you want [session-based load balancing](/load-balancing/understand-basics/session-affinity/), toggle the **Session Affinity** switch.

Next, [create an endpoint pool](/load-balancing/pools/create-pool/#create-a-pool) for the load balancer. A pool is a group of <GlossaryTooltip term="endpoint">endpoints</GlossaryTooltip> or origins, whether Cloudflare Tunnel connections or traditional IP addresses, used by the load balancer.

In Legacy mode, adding a new instance of `cloudflared` into a Load Balancer pool must be done from the command line tool itself. The `cloudflared` agent will start and create 4 separate connections, enrolling each of these into a load balancer pool.

```sh
cloudflared tunnel --hostname app.widgetcorp.tech --url http://localhost:8000 --lb-pool lisbon-data-center
```

<Example>
For this example, the load balancer `app.widgetcorp.tech` would have a pool `lisbon-data-center` with endpoints similar to the following:

| Endpoint Name   | Endpoint Address   | Weight |
| --------------- | ------------------ | ------ |
| `0-51cc...d004` | `tunnel:snf...aad` | 1      |
| `1-51cc...d004` | `tunnel:10x...x1b` | 1      |
| `2-51cc...d004` | `tunnel:yeq...y15` | 1      |
| `3-51cc...d004` | `tunnel:vn6...7p6` | 1      |

</Example>

However, the Legacy Tunnel mode has some downsides, including:

- You cannot manage these connections from the Cloudflare dashboard.
- When `cloudflared` restarts, it will attempt to register these connections as new connections which can lead to service disruption.

The Named Tunnel, documented below, model provides easier management and greater stability.

## Create a Named Tunnel

To migrate to the Named Tunnel model, first [download and authenticate](/cloudflare-one/connections/connect-networks/) `cloudflared`. Install the agent within your environment in a location that can reach the service you plan to connect to Cloudflare.

To begin, create a Named Tunnel with the following command.

```sh
cloudflared tunnel create lisbon-app
```

This command will create a Tunnel object in your Cloudflare account that is represented by this instance of `cloudflared`. You can point DNS records or LB records to this connection when you run the Tunnel.

## Create a configuration file

Next, configure your Tunnel. The example below consists of a web service that is available at port 8000. The ingress rule will send traffic that `cloudflared` receives for the specified hostname to that port. You can also connect [multiple services](/cloudflare-one/connections/connect-networks/do-more-with-tunnels/local-management/configuration-file/#file-structure-for-public-hostnames) with a single instance of `cloudflared`.

In the configuration file, you must specify the location of the credentials file generated previously when you created the Tunnel.

![Example of configuration file that specifies the location of the credentials file](~/assets/images/cloudflare-one/secure-origin-connections/migrate-lb-tunnel/tunnel-config.png)

Save the configuration file.

## Run the Tunnel

You can now run the Tunnel. Running the Tunnel will connect `cloudflared` to Cloudflare's edge in a connection only available to your account. You can use the Tunnel ID value to treat that connection like the IP address of an <GlossaryTooltip term="endpoint">endpoint</GlossaryTooltip> or origin, without the risk of someone reaching it directly and bypassing Cloudflare.

Run the following command, replacing `lisbon-app` with the name of your Tunnel.

```sh
cloudflared tunnel run lisbon-app
```

## Migrate the Load Balancer configuration

You can now begin migrating your Load Balancer deployment to use the new Named Tunnel. Create a new pool in the load balancer. Add a new endpoint to the list.

In the **Endpoint Address** field, input the ID of the tunnel followed by `cfargotunnel.com`.

<Example>

In this example, the endpoint address value would be `6b9b8f72-b655-46fb-b008-a45366e26b48.cfargotunnel.com`.

| Endpoint Name            | Endpoint Address                                        | Weight |
| ------------------------ | ------------------------------------------------------- | ------ |
| `lisbon-data-center-one` | `6b9b8f72-b655-46fb-b008-a45366e26b48.cfargotunnel.com` | 1      |

</Example>

Wait 1 minute while the new origin is recognized as healthy by Cloudflare Load Balancer. Once healthy, you can begin to disable the Legacy Argo Tunnel endpoints from the legacy Load Balancer pool.

---

# MongoDB SSH

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/mongodb-tunnel/

import { Details } from "~/components";

You can build Zero Trust rules to secure connections to MongoDB deployments using Cloudflare Access and Cloudflare Tunnel. Cloudflare Tunnel requires a lightweight daemon, `cloudflared`, running alongisde the deployment and as on the client side.

In this tutorial, a client running `cloudflared` connects over SSH to a MongoDB deployment running on Kubernetes. The deployment example is structured to connect [Compass](https://www.mongodb.com/products/compass) to the MongoDB instance. The MongoDB Kubernetes deployment runs both the MongoDB database service and `cloudflared` as a ingress service that operates like a jump host.

**This tutorial covers how to:**

- Create a Cloudflare Access rule to secure a MongoDB deployment
- Configure a StatefulSet and service definition for the deployment
- Configure an Cloudflare Tunnel connection to Cloudflare's edge
- Create an SSH configuration file for the client

**Time to complete:**

50 minutes

---

## Configure Cloudflare Access

You can build a rule in Cloudflare Access to control who can connect to your MongoDB deployment. Cloudflare Access rules are built around a hostname; even though this deployment will be accessible over SSH, the resource will be represented in Cloudflare as a hostname. For example, if you have the website `app.com` in your Cloudflare account, you can build a rule to secure `mongodb.app.com`.

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Access** > **Applications**.

2. Select **Add an application**.

3. Select **Self-hosted**.

4. Enter any name for the application.

5. Select **Add public hostname** and enter the subdomain where users will connect to your deployment (for example, `mongodb.app.com`).

6. Add [Access policies](/cloudflare-one/policies/access/) to control who can reach the deployment. You can build a policy that allows anyone in your organization to connect or you can build more granular policies based on signals like identity provider groups, [multifactor method](/cloudflare-one/tutorials/okta-u2f/), or [country](/cloudflare-one/policies/access/groups/).

7. Follow the remaining [self-hosted application creation steps](/cloudflare-one/applications/configure-apps/self-hosted-public-app/) to publish the application.

## Configure the Kubernetes deployment

To be accessible over SSH, the Kubernetes deployment should manage both the MongoDB standalone service and an SSH proxy service. The configuration below will deploy 1 replica of the database service, available at port 27017, as well as an SSH proxy available at port 22.

<Details header=" StatefulSet Configuration">

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb-standalone
  namespace: mongodb
spec:
  serviceName: database
  replicas: 1
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
        selector: mongodb-standalone
    spec:
      containers:
        - name: mongodb-standalone
          image: mongo
          command: ["mongod"]
          args: ["--config=/config/mongod.conf"]
          ports:
            - containerPort: 27017
              protocol: TCP
              name: mongod
          volumeMounts:
            - name: mongodb-conf
              mountPath: /config
              readOnly: true
            - name: mongodb-data
              mountPath: /data/db
            - name: tls
              mountPath: /etc/tls
            - name: mongodb-socket
              mountPath: /socket
        - name: ssh-proxy
          image: ubuntu:20.04
          command: ["/scripts/entrypoint.sh"]
          ports:
            - containerPort: 22
              protocol: TCP
              name: ssh-port
          volumeMounts:
            - name: mongodb-socket
              mountPath: /socket
            - name: scripts
              mountPath: /scripts
              readOnly: true
            - name: ssh-authorized-keys
              mountPath: /config/ssh
              readOnly: true
          resources:
            requests:
              cpu: 20m
              memory: 32Mi
      volumes:
        - name: mongodb-socket
          emptyDir: {}
        - name: mongodb-conf
          configMap:
            name: mongodb-standalone
            items:
              - key: mongod.conf
                path: mongod.conf
        - name: tls
          secret:
            secretName: tls
        - name: mongodb-data
          persistentVolumeClaim:
            claimName: mongodb-standalone
        - name: scripts
          configMap:
            name: scripts
            items:
              - key: entrypoint.sh
                path: entrypoint.sh
                mode: 0744
        - name: ssh-authorized-keys
          configMap:
            name: ssh-proxy-config
            items:
              - key: authorized_keys
                path: authorized_keys
                mode: 0400
```

</Details>

The corresponding service definition should also specify the ports and target ports for the containers (in this case, the database service and the SSH proxy service).

<Details header="Service Definition">

```yaml
apiVersion: v1
kind: Service
metadata:
  name: database
  namespace: mongodb
  labels:
    app: database
spec:
  clusterIP: None
  selector:
    app: database
  ports:
    - protocol: TCP
      port: 27017
      targetPort: 27017
---
apiVersion: v1
kind: Service
metadata:
  name: ssh-proxy
  namespace: mongodb
  labels:
    app: database
spec:
  selector:
    app: database
  ports:
    - protocol: TCP
      port: 22
      targetPort: 22
```

</Details>

The MongoDB pod and the SSH jump host will share a Unix socket over an empty directory volume. The `entrypoint.sh` file run by the jump host, example below, will start an OpenSSH server.

```bash
#!/bin/sh
export TZ=America/Chicago
ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
apt-get update -y && apt-get install -y openssh-server
mkdir /root/.ssh
cp /config/ssh/authorized_keys /root/.ssh/authorized_keys
chmod 400 /root/.ssh/authorized_keys
service ssh start
while true;
do sleep 30;
done;
```

## Configure Cloudflare Tunnel

Next, you can use `cloudflared` to connect to Cloudflare's Edge using Cloudflare Tunnel. Start by [downloading and installing](/cloudflare-one/connections/connect-networks/do-more-with-tunnels/local-management/create-local-tunnel/) the Cloudflare Tunnel daemon, `cloudflared`.

Once installed, run the following command to authenticate the instance of `cloudflared` into your Cloudflare account.

```sh
cloudflared login
```

The command will launch a browser window and prompt you to login with your Cloudflare account. Choose a website that you have added into your account.

Once you select one of the sites in your account, Cloudflare will download a certificate file, called `cert.pem` to authenticate this instance of `cloudflared`. The `cert.pem` file uses a certificate to authenticate your instance of `cloudflared` and includes an API key for your account to perform actions like DNS record changes.

You can now use `cloudflared` to control Cloudflare Tunnel connections in your Cloudflare account.

![Download Certificate](~/assets/images/cloudflare-one/secure-origin-connections/share-new-site/cert-download.png)

### Create a Tunnel

You can now [create a Tunnel](/cloudflare-one/connections/connect-networks/do-more-with-tunnels/local-management/create-local-tunnel/) that will connect `cloudflared` to Cloudflare's edge. You'll configure the details of that Tunnel in the next step.

Run the following command to create a Tunnel. You can replace `mongodb` with any name that you choose. This command requires the `cert.pem` file.

`cloudflared tunnel create mongodb`

Cloudflare will create the Tunnel with that name and generate an ID and credentials file for that Tunnel.

![New Tunnel](~/assets/images/cloudflare-one/secure-origin-connections/share-new-site/create.png)

### Delete the `cert.pem` file

The credentials file is separate from the `cert.pem` file. Unlike the `cert.pem` file, the credentials file consists of a token that authenticates only the Named Tunnel you just created. Formatted as `JSON`, the file cannot make changes to your Cloudflare account or create additional Tunnels.

If you are done creating Tunnels, you can delete the `cert.pem` file, leave only the credentials file, and continue to manage DNS records directly in the Cloudflare dashboard or API. For additional information on the different functions of the two files, refer to the list of [useful terms](/cloudflare-one/connections/connect-networks/get-started/tunnel-useful-terms/#certpem).

Store the `JSON` file as a Kubernetes secret.

### Configure Cloudflare Tunnel

The previous setps used `cloudflared` to generate a credentials file for your Cloudflare account. When run as a service alongside the MongoDB Kubernetes deployment you will need to use a Docker image of `cloudflared`. Cloudflare makes an [official image available](https://hub.docker.com/r/cloudflare/cloudflared) in DockerHub.

The configuration below will run a single replica of `cloudflared` as an ingress point alongside the MongoDB and SSH proxy services. `cloudflared` will proxy traffic to the SSH proxy service. The `cloudflared` instance will run as its own deployment in a different namespace and, if network policy allows, ingress to any service in the Kubernetes node.

<Details header="`cloudflared` Configuration">

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dashboard-tunnel
  namespace: argotunnel
  labels:
    app: dashboard-tunnel
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dashboard-tunnel
  template:
    metadata:
      labels:
        app: dashboard-tunnel
    spec:
      containers:
        - name: dashboard-tunnel
          # Image from https://hub.docker.com/r/cloudflare/cloudflared
          image: cloudflare/cloudflared:2020.11.11
          command: ["cloudflared", "tunnel"]
          args: ["--config", "/etc/tunnel/config.yaml", "run"]
          ports:
            - containerPort: 5000
          livenessProbe:
            tcpSocket:
              port: 5000
            initialDelaySeconds: 60
            periodSeconds: 60
          volumeMounts:
            - name: dashboard-tunnel-config
              mountPath: /etc/tunnel
            - name: tunnel-credentials
              mountPath: /etc/credentials
      volumes:
        - name: dashboard-tunnel-config
          configMap:
            name: dashboard-tunnel-config
        - name: tunnel-credentials
          secret:
            secretName: tunnel-credentials
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dashboard-tunnel-config
  namespace: argotunnel
data:
  config.yaml: |
    tunnel: 9a00ef26-4997-4de2-83db-631efc74245c
    credentials-file: /etc/credentials/k8s-dashboard.json
    metrics: :5000
    protocol: http2
    no-autoupdate: true
    ingress:
    - hostname: mongodb.widgetcorp.tech
      originRequest:
        bastionMode: true
    - service: http_status:404
```

</Details>

## Connect from a client

Once deployed, you can run `cloudflared` on the client side to connect to the MongoDB deployment. Add the following lines to your SSH configuration file, replacing the examples with your hostname and details. The `--destination` value should match the URL of the SSH Proxy service configured previously.

```bash
Host mongodb
  ProxyCommand /usr/local/bin/cloudflared access ssh --hostname mongodb.widgetcorp.tech --destination ssh-proxy.mongodb.svc.cluster.local:22
  LocalForward 27000 /socket/mongodb-27017.sock
  User root
  IdentityFile /Users/username/.ssh/id_rsa
```

This is a one-time step. When you next attempt to make an SSH connection to the deployment, `cloudflared` will launch a browser window and prompt you to authenticate. Once authenticated, you will be connected if you have a valid session. Once the tunnel is established, all requests to `localhost:27000` on your machine will be forwarded to `/socket/mongodb-27017.sock` on the SSH proxy container.

You can then set MongoDB Compass to connect to `localhost:27000`.

---

# Access and secure a MySQL database using Cloudflare Tunnel and network policies

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/mysql-network-policy/

import { Render } from "~/components";

Using Cloudflare Tunnel's private networks, users can connect to arbitrary non-browser based TCP/UDP applications, like databases. You can set up network policies that implement zero trust controls to define who and what can access those applications using the WARP client.

By the end of this tutorial, users that pass network policies will be able to access a remote MySQL database available through a Cloudflare Tunnel on TCP port 3306.

## Before you begin

Make sure you have:

- A MySQL database listening for remote connections and configured with users that can connect remotely
- (Optional)[Resolver policies](/cloudflare-one/policies/gateway/resolver-policies/) enabled on your account

## Create a Cloudflare Tunnel

Install `cloudflared` on a server in your private network. This server should have connectivity to the MySQL database.

<Render file="tunnel/create-tunnel" product="cloudflare-one" />

## Add private network routes

1. In the **Private Networks** tab, add the following IP addresses:

- Private IP/CIDR of your MySQL server (for example, `10.128.0.175/32`)
- (Optional) Private IP/CIDR of your internal DNS server

2. Select **Save tunnel**.

The application and (optional) DNS server are now connected to Cloudflare.

## Create a Gateway network policy

1. Go to **Gateway** > **Firewall policies** > **Network**.
2. Add a [network policy](/cloudflare-one/policies/gateway/network-policies/) that targets the private IP address and the port of the MySQL database (port 3306 by default). The following example allows access to the database to the users that enrolled into WARP using an `@example.com` email address. The network policies can also take into consideration [device posture checks](/cloudflare-one/identity/devices/).

| Selector         | Operator      | Value           | Logic | Action |
| ---------------- | ------------- | --------------- | ----- | ------ |
| Destination IP   | in            | `10.128.0.175`  | And   | Allow  |
| Destination Port | in            | `3306`          | And   |        |
| User Email       | matches regex | `.*example.com` |       |        |

In addition to the Allow rule above, Cloudflare recommends adding a [catch-all block policy](/learning-paths/replace-vpn/build-policies/) to the bottom of your network policy list to enforce a default-deny model.

Allowed WARP users can now connect to the MySQL server at `10.128.0.175` using the MySQL client of their choice.

## (Optional) Create a Gateway resolver policy

To allow users to access the MySQL database using an internal hostname instead of the private IP address, configure a Gateway resolver policy.

1. Go to **Gateway** > **Resolver policies**.

2. Select **Add a policy**.

3. Create an expression to match against the private [domain](/cloudflare-one/policies/gateway/resolver-policies/#domain) or [hostname](/cloudflare-one/policies/gateway/resolver-policies/#host) of the application, like in the following example:

   | Selector | Operator | Value                |
   | -------- | -------- | -------------------- |
   | Domain   | in       | `internalrecord.com` |

4. In **Select DNS resolver**, select _Configure custom DNS resolvers_.

5. Enter the private IP address of your DNS server.

6. In the dropdown menu, select _`<IP-address> - Private`_.

7. (Optional) Enter a custom port.

8. Select **Create policy**.

If your internal DNS server has an `A` record for the MySQL database, users can connect to the server using this record. For example, assuming a BIND server that includes the entry:

`mysql IN  A  10.128.0.175`

Allowed WARP users can connect to the MySQL database at `mysql.internalrecord.com` using the MySQL client of their choice.

---

# Require U2F with Okta

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/okta-u2f/

Many identity providers, like Okta, support multiple multifactor authentication (MFA) options simultaneously. For example, Okta will allow you to login with your password and a temporary code generated in an app or a U2F hard key like a Yubikey.

Some second factor methods are more resistant to phishing. U2F options require you to have access to a physical device, also known as a hardware key. Without that key, a user cannot impersonate you even if they have your password. You can build rules in Cloudflare Access to require that users authenticate with a hardware key - even if your provider supports multiple options. When users login with a less secure option, like an app-based code, Access will block them.

**This tutorial covers how to:**

* Integrate Cloudflare Access with Okta
* Configure Okta for U2F enrollment
* Build an [Access policy](/cloudflare-one/policies/access/) that require users login with a hardware key
* Specify that policy to apply to certain Access applications

The first two sections of this tutorial link to guides to set up Cloudflare Access and integrate Okta. If you already use Cloudflare Access with Okta, you can skip ahead to the fourth section.

**Time to complete:**

20 minutes

***

## Configure Cloudflare Access

Before you begin, you'll need to follow [these instructions](/cloudflare-one/setup/) to set up Cloudflare Access in your account. The hardware key feature is available on any plan, including the free plan.

## Integrate Okta

Follow [these instructions](/cloudflare-one/identity/idp-integration/okta/) to integrate Okta with your Cloudflare Access account. Once integrated, Access will be able to apply rules using identity, group membership, and multifactor method from Okta.

## Configure Okta for U2F

An Okta administrator in your organization must first [enable U2F support](https://help.okta.com/en/prod/Content/Topics/Security/MFA.htm) in your Okta account **and** [configure users](https://help.okta.com/en/prod/Content/Topics/Security/healthinsight/required-factors.htm) to be prompted for it. This is a global setting; if your account has already configured U2F, you do not need to do anything unique to use it with Cloudflare Access.

## Test U2F in Access

You can begin building U2F policies by testing your Okta integration.

In [Zero Trust](https://one.dash.cloudflare.com/), go to the **Settings** > **Authentication**. Next, choose the row for Okta and select **Test**.

Cloudflare Access will prompt you to login with your Okta account. For the purposes of the test, use a second factor option like an app-based code. Okta will return `amr` values to Cloudflare Access - these are standard indicators of multifactor methods shared between identity control systems.

The `mfa` value is sent by Okta to tell Cloudflare Access that you used a multifactor authentication option. The `pwd` value indicates you used a password. In this example, the `otp` value is sent because the user authenticatd with an app-based code.

You can test with a hardkey by logging out of Okta and returning to the list of providers in Access. Select **Test** again, but this time use your hardware key as a second factor. Cloudflare Access will now see Okta share `hwk` in the `amr` fields.

![Test MFA](~/assets/images/cloudflare-one/zero-trust-security/require-yubikey/with-hwk.png)

## Build a Zero Trust policy to require U2F

You can use this information to build a rule in Access. Go to the `Applications` list in the Cloudflare Access section of the dashboard. Choose an application that you have already built or create a new one. This example adds the requirement to an existing application.

Select **Edit** to edit the existing `Allow` rule.

Add a `Require` rule and select `Authentication Method` from the list. Choose `hwk` as the required `Authentication Method`. Select **Save rule**.

![Require Rule](~/assets/images/cloudflare-one/zero-trust-security/require-yubikey/require-hwk.png)

Optional: you can also configure Cloudflare Access to only show users Okta for this application if you have multiple other providers integrated. In the `Authentication` Tab, choose `Okta` as the only option to show users.

## Testing the rule

You can now test the rule. Visit the application and attempt to login using an app-based code or method other than a hardware security key. Access will block the attempt.

![Blocked](~/assets/images/cloudflare-one/zero-trust-security/require-yubikey/blocked-user.png)

If you sign out of Okta, and reattempt with a hardware key, Access will then allow the connection.

---

# Use Cloudflare R2 as a Zero Trust log destination

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/r2-logs/

:::note


Only available on Zero Trust Enterprise plans.


:::

This tutorial covers how to build a [Cloudflare R2 bucket](/r2/buckets/) to store logs, and how to connect the bucket to the Zero Trust [Logpush service](/cloudflare-one/insights/logs/logpush/) to store logs persistently and export them into other tools.


## Before you begin


* Ensure Cloudflare R2 and the Zero Trust Logpush integration are included in your plan. For more information, contact your account team.


## Create a Cloudflare R2 bucket


1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to **R2** > **Overview**. Select **Create bucket**.
3. Enter an identifiable name for the bucket, then select **Create bucket**.


## Create an R2 API token

1. Return to **R2**, then select **Manage R2 API tokens**.
2. Select **Create API token**.
3. In **Permissions**, select **Object Read & Write**.
4. In **Specify bucket(s)**, choose *Apply to specific buckets only*. Select the bucket you created.
5. Configure other token settings to your preferences.
6. Select **Create API Token**.
7. Copy the **Access Key ID**, **Secret Access Key**, and endpoint URL values. You will not be able to access these values again.
8. Select **Finish**.


## Connect a Zero Trust Logpush job


1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Logs** > **Logpush**.
2. Select **Connect a service**.
3. Choose which data sets and fields you want to send to your bucket. Select **Next**.
4. Select **S3 Compatible**.
5. In **S3 Compatible Bucket Path**, enter the name of your bucket.
6. In **Bucket region**, enter `auto`.
7. Enter the values for **Access Key ID**, **Secret Access Key**, and **Endpoint URL** in their corresponding fields.
8. Select **Push**. If prompted, you do not need to prove ownership with a token challenge.

The Logpush job will send the selected Zero Trust logs to your R2 bucket.

---

# Protect access to Amazon S3 buckets with Cloudflare Zero Trust

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/s3-buckets/

This tutorial demonstrates how to secure access to Amazon S3 buckets with Cloudflare Zero Trust so that data in these buckets is not publicly exposed on the Internet. You can combine Cloudflare Access and AWS VPC endpoints. Enterprise may also use Cloudflare Gateway egress policies with dedicated egress IPs.

## Method 1: Via Cloudflare Access and VPC endpoints

```mermaid
flowchart TB
    cf1[/Agentless and WARP </br>Zero Trust users/]--Access policy-->cf2{{Cloudflare}}
    cf2--Cloudflare Tunnel-->vpc1

    subgraph VPC
    vpc1[EC2 VM]-->vpc2[VPC endpoint]
    end
    vpc2-->s3_1

    subgraph S3 service
    s3_1([S3 bucket])
    end

    i1[/Users outside </br> Zero Trust/]-. "S3 access denied" .->s3_1
```

### Prerequisites

- S3 bucket to be protected by Cloudflare Zero Trust
- AWS VPC with one EC2 virtual machine (VM) hosting the [Cloudflare Tunnel daemon](/cloudflare-one/connections/connect-networks/)
- S3 bucket and AWS VPC configured in the same [AWS region](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html)

### 1. Create a VPC endpoint in AWS

1. In the [AWS dashboard](https://aws.amazon.com/console/), go to **Services** > **Networking & Content Delivery** > **VPC**.
2. Under **Virtual private cloud**, go to **Endpoints**.
3. Select **Create endpoint** and name the endpoint.
4. Choose _AWS services_ as the service category.
5. In **Services**, search and select the S3 service in the same region of the VPC. For example, for the AWS region **Europe (London) - eu-west-2**, the corresponding S3 service is named `com.amazonaws.eu-west-2.s3` with a type of Gateway.
6. In **VPC**, select your VPC that contains the EC2 VM hosting the Cloudflare tunnel daemon.
7. In **Route tables**, select the route table associated with the VPC.
8. In **Policy**, choose _Full access_.
9. Select **Create endpoint**.

After you create the VPC endpoint, a new entry in the VPC route table with the target being your VPC endpoint. The entry will have the format `vpce-xxxxxxxxxxxxxxxxx`.

### 2. Set up a bucket policy for VPC access

1. Go to **Services** > **Storage** > **S3**.
2. In Amazon S3, go to **Buckets** > **\<your-S3-bucket>** > **Permissions**.
3. Disable **Block all public access**.
4. In **Bucket policy**, add the following policy:

```json
{
	"Version": "2012-10-17",
	"Id": "VPCe",
	"Statement": [
		{
			"Sid": "VPCe",
			"Effect": "Allow",
			"Principal": "*",
			"Action": "s3:*",
			"Resource": [
				"arn:aws:s3:::<your-S3-bucket01>",
				"arn:aws:s3:::<your-S3-bucket01>/*"
			],
			"Condition": {
				"StringEquals": {
					"aws:SourceVpce": "<your-vpc-endpoint>"
				}
			}
		}
	]
}
```

Your bucket policy will allow your VPC to access your S3 bucket.

### 3. Enable static website hosting for the S3 bucket

1. Return to Amazon S3, then go to **Buckets** > **\<your-S3-bucket01>** > **Properties**.
2. In **Static website hosting**, select **Edit**.
3. Enable **Static website hosting**.
4. Specify the Index and Error documents for the S3 bucket.
5. Select **Save changes**.

A bucket website endpoint will be available at `http://<your-S3-bucket01>.s3-website.<aws-region>.amazonaws.com`. Because of the bucket policy, this website endpoint will only be accessible from the VPC with the VPC endpoint configured.

### 4. Add a new public hostname to the Cloudflare Tunnel

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Access** > **Tunnels**
2. Select your Tunnel, then select **Configure**.
3. Go to **Public Hostname**, then select **Add a public hostname**.
4. Enter a subdomain your organization will use to access the S3 bucket. For example, `s3-bucket.<your-domain>.com`.
5. Under **Service**, choose _HTTP_ for **Type**. In **URL**, enter `<your-S3-bucket01>.s3-website.<aws-region>.amazonaws.com`.
6. In **Additional application settings** > **HTTP Settings**, input the **HTTP Host Header** as `<your-S3-bucket01>.s3-website.<aws-region>.amazonaws.com`.
7. Select **Save hostname**.

Your Cloudflare Tunnel will terminate at the AWS VPC using your public hostname.

### 5. Restrict S3 access with an Access policy

1. Go to **Access** > **Applications**.
2. Select **Add an application**.
3. Select **Self-hosted**.
4. Enter a name for the application.
5. Select **Add public hostname** and enter the public hostname used by your Tunnel. For example, `s3-bucket.<your-domain>.com`.
6. Add [Access policies](/cloudflare-one/policies/access/) to determine which users and applications may access your bucket. You can optionally create a [service token](/cloudflare-one/identity/service-tokens/) policy to automatically authenticate access to your S3 bucket.
7. Follow the remaining [self-hosted application creation steps](/cloudflare-one/applications/configure-apps/self-hosted-public-app/) to publish the application.

Users and applications that successfully authenticate via Cloudflare Access can access your S3 bucket at `https://s3-bucket.<your-domain>.com`.

## Method 2: Via Cloudflare Gateway egress policies

:::note

This method is only available on Enterprise plans.
:::

```mermaid
flowchart TB
    cf1[/WARP users/]--Egress policy-->cf2{{Cloudflare}}
    cf2--Egress with dedicated IP-->i1[Internet]
    i1-->s3_1

    subgraph S3 Service
    s3_1([S3 bucket])
    end

    i2[/Users outside </br> Zero Trust/]-. "IPs denied" .->s3_1
```

### Prerequisites

- Cloudflare Zero Trust account with [dedicated egress IPs](/cloudflare-one/policies/gateway/egress-policies/dedicated-egress-ips/)
- S3 bucket to be protected by Cloudflare Zero Trust

### 1. Set up a bucket policy to restrict access to a specific IP address

1. In the [AWS dashboard](https://aws.amazon.com/console/), go to **Services** > **Storage** > **S3**.
2. Go to **Buckets** > **\<your-S3-bucket02>** > **Permissions**.
3. Disable **Block all public access**.
4. In **Bucket policy**, add the following policy:

```json
{
	"Version": "2012-10-17",
	"Id": "SourceIP",
	"Statement": [
		{
			"Sid": "SourceIP",
			"Effect": "Allow",
			"Principal": "*",
			"Action": "s3:*",
			"Resource": [
				"arn:aws:s3:::<your-S3-bucket02>",
				"arn:aws:s3:::<your-S3-bucket02>/*"
			],
			"Condition": {
				"IpAddress": {
					"aws:SourceIp": "<your-dedicated-ip>/32"
				}
			}
		}
	]
}
```

### 2. Enable static website hosting for the S3 bucket

1. Return to your bucket, then go to **Properties**.
2. In **Static website hosting**, select **Edit**.
3. Enable **Static website hosting**.
4. Specify the Index and Error documents for the S3 bucket.
5. Select **Save changes**.

A bucket website endpoint will be available at `http://<your-S3-bucket02>.s3-website.<aws-region>.amazonaws.com`. Because of the bucket policy, the website endpoint will only be accessible to traffic sourced from the dedicated egress IP specified.

### 3. Setup a dedicated egress IP policy

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Gateway** > **Egress policies**. Select **Add a policy**.
2. Create a policy that specifies which proxied traffic Gateway should assign a [dedicated egress IP](/cloudflare-one/policies/gateway/egress-policies/dedicated-egress-ips/) to. For more information, refer to [Egress policies](/cloudflare-one/policies/gateway/egress-policies/).
3. In **Select an egress IP**, choose _Use dedicated Cloudflare egress IPs_. Select the dedicated egress IP defined in your bucket policy.
4. Select **Create policy**.

Traffic proxied by Gateway and assigned your specified egress IP can access your S3 bucket at `http://<your-S3-bucket02>.s3-website.<aws-region>.amazonaws.com`.

---

# Use Cloudflare Tunnels with Kubernetes client-go plugin

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/tunnel-kubectl/

# Use Cloudflare Tunnels with Kubernetes client-go credential plugins

This tutorial explains how to use Cloudflare Tunnels with Kubernetes client-go credential plugins for authentication. By following these steps, you can securely access your Kubernetes cluster through a Cloudflare Tunnel using the `kubectl` command-line tool.

## Prerequisites

- A Cloudflare account
- The Cloudflare Tunnel client (`cloudflared`) installed on your machine
- Access to a Kubernetes cluster
- `kubectl` installed on your machine

## 1. Set up a Cloudflare Tunnel

1. Authenticate `cloudflared` with your Cloudflare account:

   ```sh
   cloudflared tunnel login
   ```

2. Create a new tunnel:

   ```sh
   cloudflared tunnel create k8s-tunnel
   ```

3. Configure your tunnel by creating a configuration file named `config.yml`:

   ```yaml
   tunnel: <TUNNEL_ID>
   credentials-file: /path/to/credentials.json
   ingress:
     - hostname: k8s.example.com
       service: tcp://kubernetes.default.svc.cluster.local:443
     - service: http_status:404
   ```

   Replace `<TUNNEL_ID>` with your tunnel ID and adjust the hostname as needed.

4. Start the tunnel:

   ```sh
   cloudflared tunnel run k8s-tunnel
   ```

## 2. Configure the Kubernetes API server

Ensure your Kubernetes API server is configured to accept authentication from Cloudflare Tunnels. This may involve setting up an authentication webhook or configuring the API server to trust the Cloudflare Tunnel's client certificates.

## 3. Set up client-go credential plugin

1. Create a script named `cloudflare-k8s-auth.sh` with the following content:

   ```bash
   #!/bin/bash
   
   echo '{
     "apiVersion": "client.authentication.k8s.io/v1beta1",
     "kind": "ExecCredential",
     "status": {
       "token": "'"$(cloudflared access token -app=https://k8s.example.com)"'"
     }
   }'
   ```

   Make the script executable:

   ```sh
   chmod +x cloudflare-k8s-auth.sh
   ```

2. Update your `~/.kube/config` file to use the credential plugin:

   ```yaml
   apiVersion: v1
   kind: Config
   clusters:
   - cluster:
       server: https://k8s.example.com
     name: cloudflare-k8s
   users:
   - name: cloudflare-user
     user:
       exec:
         apiVersion: client.authentication.k8s.io/v1beta1
         command: /path/to/cloudflare-k8s-auth.sh
         interactiveMode: Never
   contexts:
   - context:
       cluster: cloudflare-k8s
       user: cloudflare-user
     name: cloudflare-k8s-context
   current-context: cloudflare-k8s-context
   ```

## 4. Use kubectl with Cloudflare Tunnel

Now you can use `kubectl` commands as usual. The client-go credential plugin will automatically handle authentication through the Cloudflare Tunnel:

```sh
kubectl get pods
```

## Troubleshooting

If you encounter issues:

- Ensure `cloudflared` is running and the tunnel is active
- Check that your `~/.kube/config` file is correctly configured
- Verify that the Kubernetes API server is properly set up to accept authentication from Cloudflare Tunnels
- Review the Cloudflare Tunnel logs for any error messages

For more information, refer to the [Cloudflare Tunnels documentation](https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/) and the [Kubernetes client-go credential plugins documentation](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins).

---

# Use virtual networks to change user egress IPs

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/user-selectable-egress-ips/

import { Details, TabItem, Tabs } from "~/components";

:::note

Only available on Enterprise plans.

:::

This tutorial gives administrators an easy way to allow their users to change their egress IP address between any of your assigned dedicated egress IP addresses. Your users can choose which egress IP to use by switching virtual networks directly from in the WARP client.

Changing egress IPs can be useful in quality assurance (QA) and other similar scenarios in which users both use their local egress location and either switch to or simulate other remote locations.

## Before you begin

Make sure you have:

- [Deployed the WARP client](/cloudflare-one/connections/connect-devices/warp/deployment/) on your users' devices.
- [Configured tunnels](/cloudflare-one/connections/connect-networks/private-net/cloudflared/) to connect your private network to Cloudflare. This tutorial assumes you have:
  - Created two tunnels [through the dashboard](/cloudflare-one/connections/connect-networks/get-started/create-remote-tunnel/).
  - Routed `10.0.0.0/8` through one tunnel.
  - Routed `192.168.88.0/24` through the other tunnel.
- Received multiple [dedicated egress IP addresses](/cloudflare-one/policies/gateway/egress-policies/dedicated-egress-ips/).

## Create a virtual network for each egress route

First, create [virtual networks](/cloudflare-one/connections/connect-networks/private-net/cloudflared/tunnel-virtual-networks/) corresponding to your dedicated egress IPs.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Settings** > **WARP Client**.
2. In **Network locations**, go to **Virtual networks** and select **Manage**.
3. Select **Create virtual network**.
4. Name your virtual network. We recommend using a name related to the location of the corresponding dedicated egress IP. For example, if your users will egress from the Americas, you can name the virtual network `vnet-AMER`.
5. Select **Save**.
6. Repeat Steps 3-5 for each dedicated egress IP you want users to switch between. For example, you can create another virtual network called `vnet-EMEA` for egress from Europe, the Middle East, and Africa.

</TabItem>

<TabItem label="API">

1. Create a [virtual network](/cloudflare-one/connections/connect-networks/private-net/cloudflared/tunnel-virtual-networks/) corresponding to one of your dedicated egress IPs. We recommend using a name related to the location of the corresponding dedicated egress IP. For example, if your users will egress from the Americas, you can name the virtual network `vnet-AMER`.

   ```bash
   curl https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/teamnet/virtual_networks \
   --header "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
   --header "Content-Type: application/json" \
   --data '{
     "comment": "Virtual network to egress from the Americas",
     "is_default": false,
     "name": "vnet-AMER"
   }'
   ```

   For more information, refer to [Create a virtual network](/api/resources/zero_trust/subresources/networks/subresources/virtual_networks/methods/create/).

2. Repeat Step 1 for each dedicated egress IP you want users to switch between. For example, you can create another virtual network called `vnet-EMEA` for egress from Europe, the Middle East, and Africa.

</TabItem> </Tabs>

## Assign each virtual network to each tunnel

After creating your virtual networks, route your private network CIDRs over each virtual network. This ensures that users can reach all services on your network regardless of which egress IP they use.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Go to **Networks** > **Tunnels**.
2. Select your tunnel routing `10.0.0.0/8`, then select **Configure**.
3. Go to **Private Network**. Select the `10.0.0.0/8` route.
4. In **Additional settings**, choose your first virtual network. For example, `vnet-AMER`.
5. Select **Save private network**.
6. To route `10.0.0.0/8` over another virtual network, select **Add a private network**.
7. In **CIDR**, enter `10.0.0.0/8`. In **Additional settings**, choose your second virtual network. For example, `vnet-EMEA`.
8. Select **Save private network**.
9. Repeat Steps 6-8 for each virtual network you created.
10. Return to **Networks** > **Tunnels**. Repeat Steps 2-9 for each private network tunnel route.

</TabItem>

<TabItem label="API">

1. Assign your first virtual network to your private network route. For example, assign `vnet-AMER` to your tunnel that routes `10.0.0.0/8`:

   ```bash
   curl --request PATCH \
   https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/teamnet/routes/$ROUTE_ID \
   --header "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
   --header "Content-Type: application/json" \
   --data '{
     "network": "10.0.0.0/8",
     "tunnel_id": <TUNNEL_UUID>,
     "virtual_network_id": <VNET_AMER_UUID>
   }'
   ```

   For more information, refer to [Update a tunnel route](/api/resources/zero_trust/subresources/networks/subresources/routes/methods/edit/).

2. Repeat this process for each virtual network you created. For example:

   ```bash
   curl --request PATCH \
   https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/teamnet/routes/$ROUTE_ID \
   --header "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
   --header "Content-Type: application/json" \
   --data '{
     "network": "10.0.0.0/8",
     "tunnel_id": <TUNNEL_UUID>,
     "virtual_network_id": <VNET_EMEA_UUID>
   }'
   ```

3. Repeat Steps 1-2 for each private network tunnel route.

</TabItem> </Tabs>

Each tunnel connected to your private network should have each of your virtual networks assigned to it. For example, if you have tunnels routing `10.0.0.0/8` and `192.168.88.0/24`, both tunnels should have the `vnet-AMER` and `vnet-EMEA` virtual networks assigned.

| Tunnel       | CIDR              | Virtual network |
| ------------ | ----------------- | --------------- |
| **Tunnel 1** | `10.0.0.0/8`      | `vnet-AMER`     |
|              | `10.0.0.0/8`      | `vnet-EMEA`     |
| **Tunnel 2** | `192.168.88.0/24` | `vnet-AMER`     |
|              | `192.168.88.0/24` | `vnet-EMEA`     |

## Create virtual network egress policies

Next, assign your dedicated egress IPs to each virtual network using Gateway egress policies.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Gateway** > **Egress policies**.

2. Select **Add a policy**.

3. Name your policy. We recommend including the country or region traffic will egress from.

4. Add the virtual network with the _Virtual Network_ selector. For example:

   | Selector        | Operator | Value       |
   | --------------- | -------- | ----------- |
   | Virtual Network | is       | _vnet-AMER_ |

5. In **Select an egress IP**, choose **Use dedicated Cloudflare egress IPs**. Choose the dedicated IPv4 and IPv6 addresses you want traffic to egress with.

6. Select **Create policy**.

7. Repeat Steps 1-6 to create a separate egress policy for each virtual network you created.

</TabItem>

<TabItem label="API">

1. Add a Gateway egress policy that matches the corresponding virtual network. For example:

   ```bash
   curl https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/gateway/rules \
   --header "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
   --header "Content-Type: application/json" \
   --data '{
     "action": "egress",
     "description": "Egress via North America by connecting to vnet-AMER",
     "enabled": true,
     "filters": [
     "egress"
     ],
     "name": "Egress AMER vnet",
     "precedence": 0,
     "traffic": "net.vnet_id == <VNET_AMER_UUID>",
     "rule_settings": {
       "egress": {
         "ipv4": <DEDICATED_IPV4_ADDRESS>,
         "ipv4_fallback": <SECONDARY_DEDICATED_IPV6_ADDRESS>,
         "ipv6": <DEDICATED_IPV6_ADDRESS>
         }
     }
   }'
   ```

   For more information, refer to [Create a Zero Trust Gateway rule](/api/resources/zero_trust/subresources/gateway/subresources/rules/methods/create/).

2. Repeat Step 1 to create an egress policy for each virtual network you created.

</TabItem> </Tabs>

Each policy you create should correspond to a different primary dedicated egress IP.

## Test virtual network egress

<Details header="Windows, macOS, and Linux">

1. On your user's device, log in to your Zero Trust organization in the WARP client.

2. In a terminal, run the following command to check the default egress IP address.

   ```sh
   curl ifconfig.me -4
   ```

   The command should output your organization's default egress IP.

3. In the WARP client, select the gear icon > **Virtual Networks**. Choose a virtual network you created.

4. Check the egress IP address by running `curl ifconfig.me -4` again. The command should output the IP address specified in your egress policy.

</Details>

<Details header="iOS and Android">

1. On your user's device, log in to your Zero Trust organization in the Cloudflare One Agent app.
2. In a browser, go to [ifconfig.me](https://ifconfig.me/). Your organization's default egress IP should appear in **IP Address**.
3. In Cloudflare One Agent, go to **Advanced** > **Connection options** > **Virtual networks**. Choose a virtual network you created.
4. Check the egress IP address by reloading the browser page from Step 1. The IP address specified in your egress policy should appear in **IP Address**.

</Details>

While your users are connected to a virtual network, their traffic will route via the dedicated egress IP specified. You can repeat these steps to test that each virtual network is egressing from the correct IP.

---

# Render a VNC client in browser

URL: https://developers.cloudflare.com/cloudflare-one/tutorials/vnc-client-in-browser/

Cloudflare can render a Virtual Network Computer (VNC) terminal in your browser without any client software or configuration required.

Administrators can use Cloudflare Tunnel to connect a VNC host to Cloudflare's network. Using Cloudflare Access, you can apply Zero Trust policies to determine who can access your VNC server. Cloudflare's network will then enforce the Zero Trust policies and, when a user is allowed, render the client in the browser.

**This walkthrough covers how to:**

- Install and run a Cloudflare Tunnel on a Linux virtual machine
- Install and configure VNC on a Linux virtual machine
- Build a Zero Trust policy to determine who can reach the host
- Render the VNC server in your browser

**Time to complete:**

10 minutes

:::note

There are a number of VNC versions, deployments, and instances. This tutorial focuses on configuring a Tight VNC server on an Azure hosted Linux virtual machine (VM). For help with other configurations, post your questions in our [community](https://community.cloudflare.com/t/feedback-for-browser-vnc/280619/3).

:::

## Before you start

1. [Add a website to Cloudflare.](/fundamentals/setup/manage-domains/add-site/)
2. [Enable Cloudflare Zero Trust on your account.](/cloudflare-one/setup/)
3. [Connect your identity provider to Cloudflare Zero Trust.](/cloudflare-one/identity/idp-integration/)

---

## Configure VNC on your virtual machine

This section covers how to install a VNC server with TightVNC and the GNOME desktop environment. If you already have a VNC server installed, you can skip this step.

1. Open a terminal window for your VM.

2. To install the VNC software, run the following commands:

   ```sh
   sudo apt-get update

   sudo apt-get install gnome-core gnome-panel ubuntu-gnome-desktop tightvncserver
   ```

3. Once installed, you can create the VNC server instance with the following command:

   ```sh
   sudo tightvncserver
   ```

4. Select a password for the VNC server. This password will be used during login for your browser VNC server.

5. Run the following command, which will take you to your VNC server configuration directory.

   ```sh
   cd .vnc
   ```

6. Open your `xstartup` file.

   ```sh
   vim xstartup
   ```

7. Update the file to the following configuration (this is for demonstration purposes â€” browser-based VNC will work with most configurations):

   ```txt
   xsetroot -solid grey
   x-terminal-emulator -geometry 80x24+10+10 -ls -title "$VNCDESKTOP Desktop" &
   #x-window-manager &

   # Fix to make GNOME work
   export XKL_XMODMAP_DISABLE=1
   /etc/X11/Xsession

   #gnome-session &
   gnome-panel &
   nautilus &
   ```

8. To create your VNC server, run the following command:

   ```sh
   vncserver
   ```

At this point, you have a VNC server ready to test with browser-based VNC. We recommend performing a brief test with an existing VNC browser to verify any missing packages or configuration changes that might need to be made before continuing. Once your VNC server appears as desired, continue with your setup.

## Configure Cloudflare Tunnel on your machine

1. Follow [these instructions](/cloudflare-one/connections/connect-networks/downloads/) to install `cloudflared`.

2. Authenticate `cloudflared` with the command:

   ```sh
   cloudflared tunnel login
   ```

3. Create a Tunnel with the command:

   ```sh
   cloudflared tunnel create <NAME>
   ```

4. Create a Tunnel configuration file with the command:

   ```sh
   vim config.yml
   ```

5. Add the following configuration to your configuration file.

   ```txt
   tunnel: <NAME>
   ingress:
   - hostname: vnc.kennyatx.com
     service: tcp://localhost:5901
   - service: http_status:404
   ```

   As you do that, replace `<HOSTNAME>` with the domain you wish to use to expose your VNC server in the browser. Also, replace `5901` with the port your VNC server is running on. To get a list of ports, run `sudo ss -lnpt` and look for `VNC` to get the value that should be specified in your configuration file.

6. [Route your Tunnel](/cloudflare-one/connections/connect-networks/routing-to-tunnel/dns/) to your website.

7. Run your Tunnel:

   ```sh
   cloudflared tunnel --config path/config.yaml run <NAME>
   ```

8. Follow [this guide](/cloudflare-one/connections/connect-networks/configure-tunnels/tunnel-with-firewall/) to open outbound connections for Cloudflare Tunnel if you have a firewall enabled.

At this point you have a running VNC server and a Cloudflare Tunnel on your machine ready to accept inbound VNC requests.

## Create a Zero Trust VNC application

The last step is to create a Zero Trust application to run your VNC server in the Browser.

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Access** > **Applications**.

2. Select **Add an application**.

3. Select **Self-hosted**.

4. Enter any name for the application.

5. Select **Add public hostname** and set the domain to which you would like to expose the VNC server.

6. In **Access policies**, add an Allow or Block policy. For example policies, refer to the [Access policies documentation](/cloudflare-one/policies/access/#allow).

:::note

Service Auth and Bypass policies are not supported for browser-based VNC applications.
:::

7. In **Advanced settings**, set **Browser rendering** to _VNC_.

Users will see a login screen with your configured identity providers. After successful authentication, they may be prompted to enter the VNC server's password.

You can define granular access controls across each individual VNC instance.

---

# Client API

URL: https://developers.cloudflare.com/constellation/platform/client-api/

The Constellation client API allows developers to interact with the inference engine using the models configured for each project. Inference is the process of running data inputs on a machine-learning model and generating an output, or otherwise known as a prediction.

Before you use the Constellation client API, you need to:

* Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up).
* Enable Constellation by logging into the Cloudflare dashboard > **Workers & Pages** > **Constellation**.
* Create a Constellation project and configure the binding.
* Import the `@cloudflare/constellation` library in your code:

```javascript
import { Tensor, run } from "@cloudflare/constellation";
```

## Tensor class

Tensors are essentially multidimensional numerical arrays used to represent any kind of data, like a piece of text, an image, or a time series. TensorFlow popularized the use of [Tensors](https://www.tensorflow.org/guide/tensor) in machine learning (hence the name). Other frameworks and runtimes have since followed the same concept.

Constellation also uses Tensors for model input.

Tensors have a data type, a shape, the data, and a name.

```typescript
enum TensorType {
    Bool = "bool",
    Float16 = "float16",
    Float32 = "float32",
    Int8 = "int8",
    Int16 = "int16",
    Int32 = "int32",
    Int64 = "int64",
}

type TensorOpts = {
  shape?: number[],
  name?: string
}

declare class Tensor<TensorType> {
  constructor(
    type: T,
    value: any | any[],
    opts: TensorOpts = {}
  )
}
```

### Create new Tensor

```typescript
new Tensor(
  type:TensorType,
  value:any | any[],
  options?:TensorOpts
  )
```

#### type

Defines the type of data represented in the Tensor. Options are:

* TensorType.Bool
* TensorType.Float16
* TensorType.Float32
* TensorType.Int8
* TensorType.Int16
* TensorType.Int32
* TensorType.Int64

#### value

This is the tensor's data. Example tensor values can include:

* scalar: 4
* vector: \[1, 2, 3]
* two-axes 3x2 matrix: \[\[1,2], \[2,4], \[5,6]]
* three-axes 3x2 matrix \[ \[\[1, 2], \[3, 4]], \[\[5, 6], \[7, 8]], \[\[9, 10], \[11, 12]] ]

#### options

You can pass options to your tensor:

##### shape

Tensors store multidimensional data. The shape of the data can be a scalar, a vector, a 2D matrix or multiple-axes matrixes. Some examples:

* \[] - scalar data
* \[3] - vector with 3 elements
* \[3, 2] - two-axes 3x2 matrix
* \[3, 2, 2] - three-axis 2x2 matrix

Refer to the [TensorFlow documentation](https://www.tensorflow.org/guide/tensor) for more information about shapes.

If you don't pass the shape, then we try to infer it from the value object. If we can't, we thrown an error.

##### name

Naming a tensor is optional, it can be a useful key for mapping operations when building the tensor inputs.

### Tensor examples

#### A scalar

```javascript
  new Tensor(TensorType.Int16, 123);
```

#### Arrays

```javascript
  new Tensor(TensorType.Int32, [1, 23]);
  new Tensor(TensorType.Int32, [ [1, 2], [3, 4], ], { shape: [2, 2] });
  new Tensor(TensorType.Int32, [1, 23], { shape: [1] });
```

#### Named

```javascript
  new Tensor(TensorType.Int32, 1, { name: "foo" });
```

### Tensor properties

You can read the tensor's properties after it has been created:

```javascript
const tensor = new Tensor(TensorType.Int32, [ [1, 2], [3, 4], ], { shape: [2, 2], name: "test" });

console.log ( tensor.type );
// TensorType.Int32

console.log ( tensor.shape );
// [2, 2]

console.log ( tensor.name );
// test

console.log ( tensor.value );
//  [ [1, 2], [3, 4], ]
```

### Tensor methods

#### async tensor.toJSON()

Serializes the tensor to a JSON object:

```javascript
const tensor = new Tensor(TensorType.Int32, [ [1, 2], [3, 4], ], { shape: [2, 2], name: "test" });

tensor.toJSON();

{
  type: TensorType.Int32,
  name: "test",
  shape:  [2, 2],
  value: [ [1, 2], [3, 4], ]
}
```

#### async tensor.fromJSON()

Serializes a JSON object to a tensor:

```javascript
const tensor = Tensor.fromJSON(
  {
    type: TensorType.Int32,
    name: "test",
    shape:  [2, 2],
    value: [ [1, 2], [3, 4], ]
  }
);
```

## InferenceSession class

Constellation requires an inference session before you can run a task. A session is locked to a specific project, defined in your binding, and the project model.

You can, and should, if possible, run multiple tasks under the same inference session. Reusing the same session, means that we instantiate the runtime and load the model to memory once.

```typescript
export class InferenceSession {
    constructor(binding: any, modelId: string, options: SessionOptions = {})
}

export type InferenceSession = {
  binding: any;
  model: string;
  options: SessionOptions;
};
```

### InferenceSession methods

#### new InferenceSession()

To create a new session:

```javascript
import { InferenceSession } from "@cloudflare/constellation";

const session = new InferenceSession(
  env.PROJECT,
  "0ae7bd14-a0df-4610-aa85-1928656d6e9e"
);
```

* **env.PROJECT** is the project binding defined in your Wrangler configuration.
* **0ae7bd14...** is the model ID inside the project. Use Wrangler to list the models and their IDs in a project.

#### async session.run()

Runs a task in the created inference session. Takes a list of tensors as the input.

```javascript
import { Tensor, InferenceSession, TensorType } from "@cloudflare/constellation";

const session = new InferenceSession(
  env.PROJECT,
  "0ae7bd14-a0df-4610-aa85-1998656d6e9e"
);

const tensorInputArray = [ new Tensor(TensorType.Int32, 1), new Tensor(TensorType.Int32, 2), new Tensor(TensorType.Int32, 3) ];

const out = await session.run(tensorInputArray);

```

You can also use an object and name your tensors.

```javascript
const tensorInputNamed = {
  "tensor1": new Tensor(TensorType.Int32, 1),
  "tensor2": new Tensor(TensorType.Int32, 2),
  "tensor3": new Tensor(TensorType.Int32, 3)
};

out = await session.run(tensorInputNamed);
```

This is the same as using the name option when you create a tensor.

```javascript
{ "tensor1": new Tensor(TensorType.Int32, 1) } == [ new Tensor(TensorType.Int32, 1, { name: "tensor1" } ];
```

---

# Platform

URL: https://developers.cloudflare.com/constellation/platform/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Import and export data

URL: https://developers.cloudflare.com/d1/best-practices/import-export-data/

D1 allows you to import existing SQLite tables and their data directly, enabling you to migrate existing data into D1 quickly and easily. This can be useful when migrating applications to use Workers and D1, or when you want to prototype a schema locally before importing it to your D1 database(s).

D1 also allows you to export a database. This can be useful for [local development](/d1/best-practices/local-development/) or testing.

## Import an existing database

To import an existing SQLite database into D1, you must have:

1. The Cloudflare [Wrangler CLI installed](/workers/wrangler/install-and-update/).
2. A database to use as the target.
3. An existing SQLite (version 3.0+) database file to import.

:::note

You cannot import a raw SQLite database (`.sqlite3` files) directly. Refer to [how to convert an existing SQLite file](#convert-sqlite-database-files) first.

:::

For example, consider the following `users_export.sql` schema & values, which includes a `CREATE TABLE IF NOT EXISTS` statement:

```sql
CREATE TABLE IF NOT EXISTS users (
	id VARCHAR(50),
	full_name VARCHAR(50),
	created_on DATE
);
INSERT INTO users (id, full_name, created_on) VALUES ('01GREFXCN9519NRVXWTPG0V0BF', 'Catlaina Harbar', '2022-08-20 05:39:52');
INSERT INTO users (id, full_name, created_on) VALUES ('01GREFXCNBYBGX2GC6ZGY9FMP4', 'Hube Bilverstone', '2022-12-15 21:56:13');
INSERT INTO users (id, full_name, created_on) VALUES ('01GREFXCNCWAJWRQWC2863MYW4', 'Christin Moss', '2022-07-28 04:13:37');
INSERT INTO users (id, full_name, created_on) VALUES ('01GREFXCNDGQNBQAJG1AP0TYXZ', 'Vlad Koche', '2022-11-29 17:40:57');
INSERT INTO users (id, full_name, created_on) VALUES ('01GREFXCNF67KV7FPPSEJVJMEW', 'Riane Zamora', '2022-12-24 06:49:04');
```

With your `users_export.sql` file in the current working directory, you can pass the `--file=users_export.sql` flag to `d1 execute` to execute (import) our table schema and values:

```sh
npx wrangler d1 execute example-db --remote --file=users_export.sql
```

To confirm your table was imported correctly and is queryable, execute a `SELECT` statement to fetch all the tables from your D1 database:

```sh
npx wrangler d1 execute example-db --remote --command "SELECT name FROM sqlite_schema WHERE type='table' ORDER BY name;"
```

```sh output
...
ðŸŒ€ To execute on your local development database, remove the --remote flag from your wrangler command.
ðŸš£ Executed 1 commands in 0.3165ms
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ name   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ _cf_KV â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ users  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

:::note

The `_cf_KV` table is a reserved table used by D1's underlying storage system. It cannot be queried and does not incur read/write operations charges against your account.

:::

From here, you can now query our new table from our Worker [using the D1 Workers Binding API](/d1/worker-api/).

:::caution[Known limitations]

For imports, `wrangler d1 execute --file` is limited to 5GiB files, the same as the [R2 upload limit](/r2/platform/limits/). For imports larger than 5GiB, we recommend splitting the data into multiple files.
:::

### Convert SQLite database files

:::note

In order to convert a raw SQLite3 database dump (a `.sqlite3` file) you will need the [sqlite command-line tool](https://sqlite.org/cli.html) installed on your system.

:::

If you have an existing SQLite database from another system, you can import its tables into a D1 database. Using the `sqlite` command-line tool, you can convert an `.sqlite3` file into a series of SQL statements that can be imported (executed) against a D1 database.

For example, if you have a raw SQLite dump called `db_dump.sqlite3`, run the following `sqlite` command to convert it:

```sh
sqlite3 db_dump.sqlite3 .dump > db.sql
```

Once you have run the above command, you will need to edit the output SQL file to be compatible with D1:

1. Remove `BEGIN TRANSACTION` and `COMMIT;` from the file
2. Remove the following table creation statement (if present):
   ```sql
   CREATE TABLE _cf_KV (
    	key TEXT PRIMARY KEY,
    	value BLOB
   ) WITHOUT ROWID;
   ```

You can then follow the steps to [import an existing database](#import-an-existing-database) into D1 by using the `.sql` file you generated from the database dump as the input to `wrangler d1 execute`.

## Export an existing D1 database

In addition to importing existing SQLite databases, you might want to export a D1 database for local development or testing. You can export a D1 database to a `.sql` file using [wrangler d1 export](/workers/wrangler/commands/#d1-export) and then execute (import) with `d1 execute --file`.

To export full D1 database schema and data:

```sh
npx wrangler d1 export <database_name> --remote --output=./database.sql
```

To export single table schema and data:

```sh
npx wrangler d1 export <database_name> --remote --table=<table_name> --output=./table.sql
```

To export only D1 database schema:

```sh
npx wrangler d1 export <database_name> --remote --output=./schema.sql --no-data
```

To export only D1 table schema:

```sh
npx wrangler d1 export <database_name> --remote --table=<table_name> --output=./schema.sql --no-data
```

To export only D1 database data:

```sh
npx wrangler d1 export <database_name> --remote --output=./data.sql --no-schema
```

To export only D1 table data:

```sh
npx wrangler d1 export <database_name> --remote --table=<table_name> --output=./data.sql --no-schema
```

### Known limitations

- Export is not supported for virtual tables, including databases with virtual tables. D1 supports virtual tables for full-text search using SQLite's [FTS5 module](https://www.sqlite.org/fts5.html). As a workaround, delete any virtual tables, export, and then recreate virtual tables.
- A running export will block other database requests.

## Troubleshooting

If you receive an error when trying to import an existing schema and/or dataset into D1:

- Ensure you are importing data in SQL format (typically with a `.sql` file extension). Refer to [how to convert SQLite files](#convert-sqlite-database-files) if you have a `.sqlite3` database dump.
- Make sure the schema is [SQLite3](https://www.sqlite.org/docs.html) compatible. You cannot import data from a MySQL or PostgreSQL database into D1, as the types and SQL syntax are not directly compatible.
- If you have foreign key relationships between tables, ensure you are importing the tables in the right order. You cannot refer to a table that does not yet exist.
- If you receive a `"cannot start a transaction within a transaction"` error, make sure you have removed `BEGIN TRANSACTION` and `COMMIT` from your dumped SQL statements.

### Resolve `Statement too long` error

If you encounter a `Statement too long` error when trying to import a large SQL file into D1, it means that one of the SQL statements in your file exceeds the maximum allowed length.

To resolve this issue, convert the single large `INSERT` statement into multiple smaller `INSERT` statements. For example, instead of inserting 1,000 rows in one statement, split it into four groups of 250 rows, as illustrated in the code below.

Before:

```sql
INSERT INTO users (id, full_name, created_on)
VALUES
  ('1', 'Jacquelin Elara', '2022-08-20 05:39:52'),
  ('2', 'Hubert Simmons', '2022-12-15 21:56:13'),
  ...
  ('1000', 'Boris Pewter', '2022-12-24 07:59:54');
```

After:

```sql
INSERT INTO users (id, full_name, created_on)
VALUES
  ('1', 'Jacquelin Elara', '2022-08-20 05:39:52'),
  ...
  ('100', 'Eddy Orelo', '2022-12-15 22:16:15');
...
INSERT INTO users (id, full_name, created_on)
VALUES
  ('901', 'Roran Eroi', '2022-08-20 05:39:52'),
  ...
  ('1000', 'Boris Pewter', '2022-12-15 22:16:15');
```

## Foreign key constraints

When importing data, you may need to temporarily disable [foreign key constraints](/d1/sql-api/foreign-keys/). To do so, call `PRAGMA defer_foreign_keys = true` before making changes that would violate foreign keys.

Refer to the [foreign key documentation](/d1/sql-api/foreign-keys/) to learn more about how to work with foreign keys and D1.

## Next Steps

- Read the SQLite [`CREATE TABLE`](https://www.sqlite.org/lang_createtable.html) documentation.
- Learn how to [use the D1 Workers Binding API](/d1/worker-api/) from within a Worker.
- Understand how [database migrations work](/d1/reference/migrations/) with D1.

---

# Best practices

URL: https://developers.cloudflare.com/d1/best-practices/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Local development

URL: https://developers.cloudflare.com/d1/best-practices/local-development/

import { WranglerConfig } from "~/components";

D1 has fully-featured support for local development, running the same version of D1 as Cloudflare runs globally. Local development uses [Wrangler](/workers/wrangler/install-and-update/), the command-line interface for Workers, to manage local development sessions and state.

## Start a local development session

:::note

This guide assumes you are using [Wrangler v3.0](https://blog.cloudflare.com/wrangler3/) or later.

Users new to D1 and/or Cloudflare Workers should visit the [D1 tutorial](/d1/get-started/) to install `wrangler` and deploy their first database.

:::

Local development sessions create a standalone, local-only environment that mirrors the production environment D1 runs in so that you can test your Worker and D1 _before_ you deploy to production.

An existing [D1 binding](/workers/wrangler/configuration/#d1-databases) of `DB` would be available to your Worker when running locally.

To start a local development session:

1. Confirm you are using wrangler v3.0+.

   ```sh
   wrangler --version
   ```

   ```sh output
   â›…ï¸ wrangler 3.0.0
   ```

2. Start a local development session

   ```sh
   wrangler dev
   ```

   ```sh output
   ------------------
   wrangler dev now uses local mode by default, powered by ðŸ”¥ Miniflare and ðŸ‘· workerd.
   To run an edge preview session for your Worker, use wrangler dev --remote
   Your worker has access to the following bindings:
   - D1 Databases:
   	- DB: test-db (c020574a-5623-407b-be0c-cd192bab9545)
   âŽ” Starting local server...

   [mf:inf] Ready on http://127.0.0.1:8787/
   [b] open a browser, [d] open Devtools, [l] turn off local mode, [c] clear console, [x] to exit
   ```

In this example, the Worker has access to local-only D1 database. The corresponding D1 binding in your [Wrangler configuration file](/workers/wrangler/configuration/) would resemble the following:

<WranglerConfig>

```toml
[[d1_databases]]
binding = "DB"
database_name = "test-db"
database_id = "c020574a-5623-407b-be0c-cd192bab9545"
```

</WranglerConfig>

Note that `wrangler dev` separates local and production (remote) data. A local session does not have access to your production data by default. To access your production (remote) database, pass the `--remote` flag when calling `wrangler dev`. Any changes you make when running in `--remote` mode cannot be undone.

Refer to the [`wrangler dev` documentation](/workers/wrangler/commands/#dev) to learn more about how to configure a local development session.

## Develop locally with Pages

You can only develop against a _local_ D1 database when using [Cloudflare Pages](/pages/) by creating a minimal [Wrangler configuration file](/workers/wrangler/configuration/) in the root of your Pages project. This can be useful when creating schemas, seeding data or otherwise managing a D1 database directly, without adding to your application logic.

:::caution[Local development for remote databases]

It is currently not possible to develop against a _remote_ D1 database when using [Cloudflare Pages](/pages/).
:::

Your [Wrangler configuration file](/workers/wrangler/configuration/) should resemble the following:

<WranglerConfig>

```toml
# If you are only using Pages + D1, you only need the below in your Wrangler config file to interact with D1 locally.
[[d1_databases]]
binding = "DB" # Should match preview_database_id
database_name = "YOUR_DATABASE_NAME"
database_id = "the-id-of-your-D1-database-goes-here" # wrangler d1 info YOUR_DATABASE_NAME
preview_database_id = "DB" # Required for Pages local development
```

</WranglerConfig>

You can then execute queries and/or run migrations against a local database as part of your local development process by passing the `--local` flag to wrangler:

```bash
wrangler d1 execute YOUR_DATABASE_NAME \
  --local --command "CREATE TABLE IF NOT EXISTS users ( user_id INTEGER PRIMARY KEY, email_address TEXT, created_at INTEGER, deleted INTEGER, settings TEXT);"
```

The preceding command would execute queries the **local only** version of your D1 database. Without the `--local` flag, the commands are executed against the remote version of your D1 database running on Cloudflare's network.

## Persist data

:::note

By default, in Wrangler v3 and above, data is persisted across each run of `wrangler dev`. If your local development and testing requires or assumes an empty database, you should start with a `DROP TABLE <tablename>` statement to delete existing tables before using `CREATE TABLE` to re-create them.

:::

Use `wrangler dev --persist-to=/path/to/file` to persist data to a specific location. This can be useful when working in a team (allowing you to share) the same copy, when deploying via CI/CD (to ensure the same starting state), or as a way to keep data when migrating across machines.

Users of wrangler `2.x` must use the `--persist` flag: previous versions of wrangler did not persist data by default.

## Test programmatically

### Miniflare

[Miniflare](https://miniflare.dev/) allows you to simulate a Workers and resources like D1 using the same underlying runtime and code as used in production.

You can use Miniflare's [support for D1](https://miniflare.dev/storage/d1) to create D1 databases you can use for testing:



<WranglerConfig>

```toml
[[d1_databases]]
binding = "DB"
database_name = "test-db"
database_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
```

</WranglerConfig>

```js
const mf = new Miniflare({
	d1Databases: {
		DB: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
	},
});
```

You can then use the `getD1Database()` method to retrieve the simulated database and run queries against it as if it were your real production D1 database:

```js
const db = await mf.getD1Database("DB");

const stmt = db.prepare("SELECT name, age FROM users LIMIT 3");
const { results } = await stmt.all();

console.log(results);
```

### `unstable_dev`

Wrangler exposes an [`unstable_dev()`](/workers/wrangler/api/) that allows you to run a local HTTP server for testing Workers and D1. Run [migrations](/d1/reference/migrations/) against a local database by setting a `preview_database_id` in your Wrangler configuration.

Given the below Wrangler configuration:

<WranglerConfig>

```toml
[[ d1_databases ]]
binding = "DB" # i.e. if you set this to "DB", it will be available in your Worker at `env.DB`
database_name = "your-database" # the name of your D1 database, set when created
database_id = "<UUID>" # The unique ID of your D1 database, returned when you create your database or run `
preview_database_id = "local-test-db" # A user-defined ID for your local test database.
```

</WranglerConfig>

Migrations can be run locally as part of your CI/CD setup by passing the `--local` flag to `wrangler`:

```sh
wrangler d1 migrations apply your-database --local
```

### Usage example

The following example shows how to use Wrangler's `unstable_dev()` API to:

- Run migrations against your local test database, as defined by `preview_database_id`.
- Make a request to an endpoint defined in your Worker. This example uses `/api/users/?limit=2`.
- Validate the returned results match, including the `Response.status` and the JSON our API returns.

```ts
import { unstable_dev } from "wrangler";
import type { UnstableDevWorker } from "wrangler";

describe("Test D1 Worker endpoint", () => {
	let worker: UnstableDevWorker;

	beforeAll(async () => {
		// Optional: Run any migrations to set up your `--local` database
		// By default, this will default to the preview_database_id
		execSync(`NO_D1_WARNING=true wrangler d1 migrations apply db --local`);

		worker = await unstable_dev("src/index.ts", {
			experimental: { disableExperimentalWarning: true },
		});
	});

	afterAll(async () => {
		await worker.stop();
	});

	it("should return an array of users", async () => {
		// Our expected results
		const expectedResults = `{"results": [{"user_id": 1234, "email": "foo@example.com"},{"user_id": 6789, "email": "bar@example.com"}]}`;
		// Pass an optional URL to fetch to trigger any routing within your Worker
		const resp = await worker.fetch("/api/users/?limit=2");
		if (resp) {
			// https://jestjs.io/docs/expect#tobevalue
			expect(resp.status).toBe(200);
			const data = await resp.json();
			// https://jestjs.io/docs/expect#tomatchobjectobject
			expect(data).toMatchObject(expectedResults);
		}
	});
});
```

Review the [`unstable_dev()`](/workers/wrangler/api/#usage) documentation for more details on how to use the API within your tests.

## Related resources

- Use [`wrangler dev`](/workers/wrangler/commands/#dev) to run your Worker and D1 locally and debug issues before deploying.
- Learn [how to debug D1](/d1/observability/debug-d1/).
- Understand how to [access logs](/workers/observability/logs/) generated from your Worker and D1.

---

# Query a database

URL: https://developers.cloudflare.com/d1/best-practices/query-d1/

D1 is compatible with most SQLite's SQL convention since it leverages SQLite's query engine. You can use SQL commands to query D1.

There are a number of ways you can interact with a D1 database:

1. Using [D1 Workers Binding API](/d1/worker-api/) in your code.
2. Using [D1 REST API](/api/resources/d1/subresources/database/methods/create/).
3. Using [D1 Wrangler commands](/d1/wrangler-commands/).

## Use SQL to query D1

D1 understands SQLite semantics, which allows you to query a database using SQL statements via Workers BindingAPI or REST API (including Wrangler commands). Refer to [D1 SQL API](/d1/sql-api/sql-statements/) to learn more about supported SQL statements.

### Use foreign key relationships

When using SQL with D1, you may wish to define and enforce foreign key constraints across tables in a database. Foreign key constraints allow you to enforce relationships across tables, or prevent you from deleting rows that reference rows in other tables. An example of a foreign key relationship is shown below.

```sql
CREATE TABLE users (
    user_id INTEGER PRIMARY KEY,
    email_address TEXT,
    name TEXT,
    metadata TEXT
)

CREATE TABLE orders (
    order_id INTEGER PRIMARY KEY,
    status INTEGER,
    item_desc TEXT,
    shipped_date INTEGER,
    user_who_ordered INTEGER,
    FOREIGN KEY(user_who_ordered) REFERENCES users(user_id)
)
```

Refer to [Define foreign keys](/d1/sql-api/foreign-keys/) for more information.

### Query JSON

D1 allows you to query and parse JSON data stored within a database. For example, you can extract a value inside a JSON object.

Given the following JSON object (`type:blob`) in a column named `sensor_reading`, you can extract values from it directly.

```json
{
    "measurement": {
        "temp_f": "77.4",
        "aqi": [21, 42, 58],
        "o3": [18, 500],
        "wind_mph": "13",
        "location": "US-NY"
    }
}
```
```sql
-- Extract the temperature value
SELECT json_extract(sensor_reading, '$.measurement.temp_f')-- returns "77.4" as TEXT
```

Refer to [Query JSON](/d1/sql-api/query-json/) to learn more about querying JSON objects.

## Query D1 with Workers Binding API

Workers Binding API primarily interacts with the data plane, and allows you to query your D1 database from your Worker.

This requires you to:

1. Bind your D1 database to your Worker.
2. Prepare a statement.
3. Run the statement.

```js title="index.js"
export default {
    async fetch(request, env) {
        const {pathname} = new URL(request.url);
        const companyName1 = `Bs Beverages`;
        const companyName2 = `Around the Horn`;
        const stmt = env.DB.prepare(`SELECT * FROM Customers WHERE CompanyName = ?`);

        if (pathname === `/RUN`) {
            const returnValue = await stmt.bind(companyName1).run();
            return Response.json(returnValue);
        }

        return new Response(
            `Welcome to the D1 API Playground!
						\nChange the URL to test the various methods inside your index.js file.`,
        );
    },
};
```

Refer to [Workers Binding API](/d1/worker-api/) for more information.

## Query D1 with REST API

REST API primarily interacts with the control plane, and allows you to create/manage your D1 database.

Refer to [D1 REST API](/api/resources/d1/subresources/database/methods/create/) for D1 REST API documentation.

## Query D1 with Wrangler commands

You can use Wrangler commands to query a D1 database. Note that Wrangler commands use REST APIs to perform its operations.

```sh
npx wrangler d1 execute prod-d1-tutorial --command="SELECT * FROM Customers"
```
```sh output
ðŸŒ€ Mapping SQL input into an array of statements
ðŸŒ€ Executing on local database production-db-backend (<DATABASE_ID>) from .wrangler/state/v3/d1:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CustomerId â”‚ CompanyName         â”‚ ContactName       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1          â”‚ Alfreds Futterkiste â”‚ Maria Anders      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4          â”‚ Around the Horn     â”‚ Thomas Hardy      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 11         â”‚ Bs Beverages        â”‚ Victoria Ashworth â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 13         â”‚ Bs Beverages        â”‚ Random Name       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Remote development

URL: https://developers.cloudflare.com/d1/best-practices/remote-development/

D1 supports remote development using the [dashboard playground](/workers/playground/#use-the-playground). The dashboard playground uses a browser version of Visual Studio Code, allowing you to rapidly iterate on your Worker entirely in your browser.

## 1. Bind a D1 database to a Worker

:::note


This guide assumes you have previously created a Worker, and a D1 database.

Users new to D1 and/or Cloudflare Workers should read the [D1 tutorial](/d1/get-started/) to install `wrangler` and deploy their first database.


:::

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to [**Workers & Pages** > **Overview**](https://dash.cloudflare.com/?to=/:account/workers-and-pages).
3. Select an existing Worker.
4. Select the **Settings** tab.
5. Select the **Variables** sub-tab.
6. Scroll down to the **D1 Database Bindings** heading.
7. Enter a variable name, such as `DB`, and select the D1 database you wish to access from this Worker.
8. Select **Save and deploy**.

## 2. Start a remote development session

1. On the Worker's page on the Cloudflare dashboard, select **Edit Code** at the top of the page.
2. Your Worker now has access to D1.

Use the following Worker script to verify that the Worker has access to the bound D1 database:

```js
export default {
  async fetch(request, env, ctx) {
    const res = await env.DB.prepare("SELECT 1;").all();
    return new Response(JSON.stringify(res, null, 2));
  },
};
```

## Related resources

* Learn [how to debug D1](/d1/observability/debug-d1/).
* Understand how to [access logs](/workers/observability/logs/) generated from your Worker and D1.

---

# Use indexes

URL: https://developers.cloudflare.com/d1/best-practices/use-indexes/

import { GlossaryTooltip } from "~/components";

Indexes enable D1 to improve query performance over the indexed columns for common (popular) queries by reducing the amount of data (number of rows) the database has to scan when running a query.

## When is an index useful?

Indexes are useful:

* When you want to improve the read performance over columns that are regularly used in predicates - for example, a `WHERE email_address = ?` or `WHERE user_id = 'a793b483-df87-43a8-a057-e5286d3537c5'` - email addresses, usernames, user IDs and/or dates are good choices for columns to index in typical web applications or services.
* For enforcing uniqueness constraints on a column or columns - for example, an email address or user ID via the `CREATE UNIQUE INDEX`.
* In cases where you query over multiple columns together - `(customer_id, transaction_date)`.

Indexes are automatically updated when the table and column(s) they reference are inserted, updated or deleted. You do not need to manually update an index after you write to the table it references.

## Create an index

:::note

Tables that use the default primary key (an `INTEGER` based `ROWID`), or that define their own `INTEGER PRIMARY KEY`, do not need to create an index for that column.
:::

To create an index on a D1 table, use the `CREATE INDEX` SQL command and specify the table and column(s) to create the index over.

For example, given the following `orders` table, you may want to create an index on `customer_id`. Nearly all of your queries against that table filter on `customer_id`, and you would see a performance improvement by creating an index for it.

```sql
CREATE TABLE IF NOT EXISTS orders (
    order_id INTEGER PRIMARY KEY,
    customer_id STRING NOT NULL, -- for example, a unique ID aba0e360-1e04-41b3-91a0-1f2263e1e0fb
    order_date STRING NOT NULL,
    status INTEGER NOT NULL,
    last_updated_date STRING NOT NULL
)
```

To create the index on the `customer_id` column, execute the below statement against your database:

:::note

A common naming format for indexes is `idx_TABLE_NAME_COLUMN_NAMES`, so that you can identify the table and column(s) your indexes are for when managing your database.
:::

```sql
CREATE INDEX IF NOT EXISTS idx_orders_customer_id ON orders(customer_id)
```

Queries that reference the `customer_id` column will now benefit from the index:

```sql
-- Uses the index: the indexed column is referenced by the query.
SELECT * FROM orders WHERE customer_id = ?

-- Does not use the index: customer_id is not in the query.
SELECT * FROM orders WHERE order_date = '2023-05-01'
```

In more complex cases, you can confirm whether an index was used by D1 by [analyzing a query](#test-an-index) directly.

### Run `PRAGMA optimize`

After creating an index, run the `PRAGMA optimize` command to improve your database performance.

`PRAGMA optimize` runs `ANALYZE` command on each table in the database, which collects statistics on the tables and indices. These statistics allows the <GlossaryTooltip term="query planner">query planner</GlossaryTooltip> to generate the most efficient query plan when executing the user query.

For more information, refer to [`PRAGMA optimize`](/d1/sql-api/sql-statements/#pragma-optimize).

## List indexes

List the indexes on a database, as well as the SQL definition, by querying the `sqlite_schema` system table:

```sql
SELECT name, type, sql FROM sqlite_schema WHERE type IN ('index');
```

This will return output resembling the below:

```txt
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ name                             â”‚ type  â”‚ sql                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ idx_users_id                     â”‚ index â”‚ CREATE INDEX idx_users_id ON users(id) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Note that you cannot modify this table, or an existing index. To modify an index, [delete it first](#remove-indexes) and [create a new index](#create-an-index) with the updated definition.

## Test an index

Validate that an index was used for a query by prepending a query with [`EXPLAIN QUERY PLAN`](https://www.sqlite.org/eqp.html). This will output a query plan for the succeeding statement, including which (if any) indexes were used.

For example, if you assume the `users` table has an `email_address TEXT` column and you created an index `CREATE UNIQUE INDEX idx_email_address ON users(email_address)`, any query with a predicate on `email_address` should use your index.

```sql
EXPLAIN QUERY PLAN SELECT * FROM users WHERE email_address = 'foo@example.com';
QUERY PLAN
`--SEARCH users USING INDEX idx_email_address (email_address=?)
```

Review the `USING INDEX <INDEX_NAME>` output from the query planner, confirming the index was used.

This is also a fairly common use-case for an index. Finding a user based on their email address is often a very common query type for login (authentication) systems.

Using an index can reduce the number of rows read by a query. Use the `meta` object to estimate your usage. Refer to ["Can I use an index to reduce the number of rows read by a query?"](/d1/platform/pricing/#can-i-use-an-index-to-reduce-the-number-of-rows-read-by-a-query) and ["How can I estimate my (eventual) bill?"](/d1/platform/pricing/#how-can-i-estimate-my-eventual-bill).

## Multi-column indexes

For a multi-column index (an index that specifies multiple columns), queries will only use the index if they specify either *all* of the columns, or a subset of the columns provided all columns to the "left" are also within the query.

Given an index of `CREATE INDEX idx_customer_id_transaction_date ON transactions(customer_id, transaction_date)`, the following table shows when the index is used (or not):

| Query                                                                                       | Index Used?                                                                                        |
| ------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| `SELECT * FROM transactions WHERE customer_id = '1234' AND transaction_date = '2023-03-25'` | Yes: specifies both columns in the index.                                                          |
| `SELECT * FROM transactions WHERE transaction_date = '2023-03-28'`                          | No: only specifies `transaction_date`, and does not include other leftmost columns from the index. |
| `SELECT * FROM transactions WHERE customer_id = '56789'`                                    | Yes: specifies `customer_id`, which is the leftmost column in the index.                           |

Notes:

* If you created an index over three columns instead â€” `customer_id`, `transaction_date` and `shipping_status` â€” a query that uses both `customer_id` and `transaction_date` would use the index, as you are including all columns "to the left".
* With the same index, a query that uses only `transaction_date` and `shipping_status` would *not* use the index, as you have not used `customer_id` (the leftmost column) in the query.

## Partial indexes

Partial indexes are indexes over a subset of rows in a table. Partial indexes are defined by the use of a `WHERE` clause when creating the index. A partial index can be useful to omit certain rows, such as those where values are `NULL` or where rows with a specific value are present across queries.

* A concrete example of a partial index would be on a table with a `order_status INTEGER` column, where `6` might represent `"order complete"` in your application code.
* This would allow queries against orders that are yet to be fulfilled, shipped or are in-progress, which are likely to be some of the most common users (users checking their order status).
* Partial indexes also keep the index from growing unbounded over time. The index does not need to keep a row for every completed order, and completed orders are likely to be queried far fewer times than in-progress orders.

A partial index that filters out completed orders from the index would resemble the following:

```sql
CREATE INDEX idx_order_status_not_complete ON orders(order_status) WHERE order_status != 6
```

Partial indexes can be faster at read time (less rows in the index) and at write time (fewer writes to the index) than full indexes. You can also combine a partial index with a [multi-column index](#multi-column-indexes).

## Remove indexes

Use `DROP INDEX` to remove an index. Dropped indexes cannot be restored.

## Considerations

Take note of the following considerations when creating indexes:

* Indexes are not always a free performance boost. You should create indexes only on columns that reflect your most-queried columns. Indexes themselves need to be maintained. When you write to an indexed column, the database needs to write to the table and the index. The performance benefit of an index and reduction in rows read will, in nearly all cases, offset this additional write.
* You cannot create indexes that reference other tables or use non-deterministic functions, since the index would not be stable.
* Indexes cannot be updated. To add or remove a column from an index, [remove](#remove-indexes) the index and then [create a new index](#create-an-index) with the new columns.
* Indexes contribute to the overall storage required by your database: an index is effectively a table itself.

---

# Environments

URL: https://developers.cloudflare.com/d1/configuration/environments/

import { WranglerConfig } from "~/components";

[Environments](/workers/wrangler/environments/) are different contexts that your code runs in. Cloudflare Developer Platform allows you to create and manage different environments. Through environments, you can deploy the same project to multiple places under multiple names.

To specify different D1 databases for different environments, use the following syntax in your Wrangler file:

<WranglerConfig>

```toml
# This is a staging environment
[env.staging]
d1_databases = [
    { binding = "<BINDING_NAME_1>", database_name = "<DATABASE_NAME_1>", database_id = "<UUID1>" },
]

# This is a production environment
[env.production]
d1_databases = [
    { binding = "<BINDING_NAME_2>", database_name = "<DATABASE_NAME_2>", database_id = "<UUID2>" },
]
```

</WranglerConfig>

In the code above, the `staging` environment is using a different database (`DATABASE_NAME_1`) than the `production` environment (`DATABASE_NAME_2`).

## Anatomy of Wrangler file

If you need to specify different D1 databases for different environments, your [Wrangler configuration file](/workers/wrangler/configuration/) may contain bindings that resemble the following:

<WranglerConfig>

```toml
[[production.d1_databases]]
binding = "DB"
database_name = "DATABASE_NAME"
database_id = "DATABASE_ID"
```

</WranglerConfig>

In the above configuration:

- `[[production.d1_databases]]` creates an object `production` with a property `d1_databases`, where `d1_databases` is an array of objects, since you can create multiple D1 bindings in case you have more than one database.
- Any property below the line in the form `<key> = <value>` is a property of an object within the `d1_databases` array.

Therefore, the above binding is equivalent to:

```json
{
  "production": {
    "d1_databases": [
      {
        "binding": "DB",
        "database_name": "DATABASE_NAME",
        "database_id": "DATABASE_ID"
      }
    ]
  }
}
```

### Example



<WranglerConfig>

```toml
[[env.staging.d1_databases]]
binding = "BINDING_NAME_1"
database_name = "DATABASE_NAME_1"
database_id = "UUID_1"

[[env.production.d1_databases]]
binding = "BINDING_NAME_2"
database_name = "DATABASE_NAME_2"
database_id = "UUID_2"

```

</WranglerConfig>

The above is equivalent to the following structure in JSON:

```json
{
  "env": {
    "production": {
      "d1_databases": [
        {
          "binding": "BINDING_NAME_2",
          "database_id": "UUID_2",
          "database_name": "DATABASE_NAME_2"
        }
      ]
    },
    "staging": {
      "d1_databases": [
        {
          "binding": "BINDING_NAME_1",
          "database_id": "UUID_1",
          "database_name": "DATABASE_NAME_1"
        }
      ]
    }
  }
}
```

---

# Data location

URL: https://developers.cloudflare.com/d1/configuration/data-location/

Learn how the location of data stored in D1 is determined, including where the leader is placed and how you optimize that location based on your needs.

## Automatic (recommended)

By default, D1 will automatically create your database in a location close to where you issued the request to create a database. In most cases this allows D1 to choose the optimal location for your database on your behalf.

## Provide a location hint

Location hint is an optional parameter you can provide to indicate your desired geographical location for your database.

You may want to explicitly provide a location hint in cases where the majority of your writes to a specific database come from a different location than where you are creating the database from. location hints can be useful when:

- Working in a distributed team.
- Creating databases specific to users in specific locations.
- Using continuous deployment (CD) or Infrastructure as Code (IaC) systems to programmatically create your databases.

Provide a location hint when creating a D1 database when:

- Using [`wrangler d1`](/workers/wrangler/commands/#d1) to create a database.
- Creating a database [via the Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/workers/d1).

:::caution
Providing a location hint does not guarantee that D1 runs in your preferred location. Instead, it will run in the nearest possible location (by latency) to your preference.
:::

### Use Wrangler

:::note

To install Wrangler, the command-line interface for D1 and Workers, refer to [Install and Update Wrangler](/workers/wrangler/install-and-update/).

:::

To provide a location hint when creating a new database, pass the `--location` flag with a valid location hint:

```sh
wrangler d1 create new-database --location=weur
```

### Use the dashboard

To provide a location hint when creating a database via the dashboard:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to [**Workers & Pages** > **D1**](https://dash.cloudflare.com/?to=/:account/workers/d1).
3. Select **Create database**.
4. Provide a database name and an optional **Location**.
5. Select **Create** to create your database.

## Available location hints

D1 supports the following location hints:

| Hint | Hint description      |
| ---- | --------------------- |
| wnam | Western North America |
| enam | Eastern North America |
| weur | Western Europe        |
| eeur | Eastern Europe        |
| apac | Asia-Pacific          |
| oc   | Oceania               |

:::caution
D1 location hints are not currently supported for South America (`sam`), Africa (`afr`), and the Middle East (`me`). D1 databases do not run in these locations.
:::

---

# Configuration

URL: https://developers.cloudflare.com/d1/configuration/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Query D1 from Hono

URL: https://developers.cloudflare.com/d1/examples/d1-and-hono/

import { TabItem, Tabs } from "~/components";

Hono is a fast web framework for building API-first applications, and it includes first-class support for both [Workers](/workers/) and [Pages](/pages/).

When using Workers:

- Ensure you have configured your [Wrangler configuration file](/d1/get-started/#3-bind-your-worker-to-your-d1-database) to bind your D1 database to your Worker.
- You can access your D1 databases via Hono's [`Context`](https://hono.dev/api/context) parameter: [bindings](https://hono.dev/getting-started/cloudflare-workers#bindings) are exposed on `context.env`. If you configured a [binding](/pages/functions/bindings/#d1-databases) named `DB`, then you would access [D1 Workers Binding API](/d1/worker-api/prepared-statements/) methods via `c.env.DB`.
- Refer to the Hono documentation for [Cloudflare Workers](https://hono.dev/getting-started/cloudflare-workers).

If you are using [Pages Functions](/pages/functions/):

1. Bind a D1 database to your [Pages Function](/pages/functions/bindings/#d1-databases).
2. Pass the `--d1 BINDING_NAME=DATABASE_ID` flag to `wrangler dev` when developing locally. `BINDING_NAME` should match what call in your code, and `DATABASE_ID` should match the `database_id` defined in your Wrangler configuration file: for example, `--d1 DB=xxxx-xxxx-xxxx-xxxx-xxxx`.
3. Refer to the Hono guide for [Cloudflare Pages](https://hono.dev/getting-started/cloudflare-pages).

The following examples show how to access a D1 database bound to `DB` from both a Workers script and a Pages Function:

<Tabs> <TabItem label="workers">

```ts
import { Hono } from "hono";

// This ensures c.env.DB is correctly typed
type Bindings = {
	DB: D1Database;
};

const app = new Hono<{ Bindings: Bindings }>();

// Accessing D1 is via the c.env.YOUR_BINDING property
app.get("/query/users/:id", async (c) => {
	const userId = c.req.param("id");
	try {
		let { results } = await c.env.DB.prepare(
			"SELECT * FROM users WHERE user_id = ?",
		)
			.bind(userId)
			.all();
		return c.json(results);
	} catch (e) {
		return c.json({ err: e.message }, 500);
	}
});

// Export our Hono app: Hono automatically exports a
// Workers 'fetch' handler for you
export default app;
```

</TabItem> <TabItem label="pages">

```ts
import { Hono } from "hono";
import { handle } from "hono/cloudflare-pages";

const app = new Hono().basePath("/api");

// Accessing D1 is via the c.env.YOUR_BINDING property
app.get("/query/users/:id", async (c) => {
	const userId = c.req.param("id");
	try {
		let { results } = await c.env.DB.prepare(
			"SELECT * FROM users WHERE user_id = ?",
		)
			.bind(userId)
			.all();
		return c.json(results);
	} catch (e) {
		return c.json({ err: e.message }, 500);
	}
});

// Export the Hono instance as a Pages onRequest function
export const onRequest = handle(app);
```

</TabItem> </Tabs>

---

# Query D1 from SvelteKit

URL: https://developers.cloudflare.com/d1/examples/d1-and-sveltekit/

import { TabItem, Tabs } from "~/components";

[SvelteKit](https://kit.svelte.dev/) is a full-stack framework that combines the Svelte front-end framework with Vite for server-side capabilities and rendering. You can query D1 from SvelteKit by configuring a [server endpoint](https://kit.svelte.dev/docs/routing#server) with a binding to your D1 database(s).

To set up a new SvelteKit site on Cloudflare Pages that can query D1:

1. **Refer to [the SvelteKit guide](/pages/framework-guides/deploy-a-svelte-kit-site/) and Svelte's [Cloudflare adapter](https://kit.svelte.dev/docs/adapter-cloudflare)**.
2. Install the Cloudflare adapter within your SvelteKit project: `npm i -D @sveltejs/adapter-cloudflare`.
3. Bind a D1 database [to your Pages Function](/pages/functions/bindings/#d1-databases).
4. Pass the `--d1 BINDING_NAME=DATABASE_ID` flag to `wrangler dev` when developing locally. `BINDING_NAME` should match what call in your code, and `DATABASE_ID` should match the `database_id` defined in your [Wrangler configuration file](/workers/wrangler/configuration/): for example, `--d1 DB=xxxx-xxxx-xxxx-xxxx-xxxx`.

The following example shows you how to create a server endpoint configured to query D1.

- Bindings are available on the `platform` parameter passed to each endpoint, via `platform.env.BINDING_NAME`.
- With SvelteKit's [file-based routing](https://kit.svelte.dev/docs/routing), the server endpoint defined in `src/routes/api/users/+server.ts` is available at `/api/users` within your SvelteKit app.

The example also shows you how to configure both your app-wide types within `src/app.d.ts` to recognize your `D1Database` binding, import the `@sveltejs/adapter-cloudflare` adapter into `svelte.config.js`, and configure it to apply to all of your routes.

<Tabs> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import type { RequestHandler } from "@sveltejs/kit";

/** @type {import('@sveltejs/kit').RequestHandler} */
export async function GET({ request, platform }) {
	let result = await platform.env.DB.prepare(
		"SELECT * FROM users LIMIT 5",
	).run();
	return new Response(JSON.stringify(result));
}
```

```ts
// See https://kit.svelte.dev/docs/types#app
// for information about these interfaces
declare global {
	namespace App {
		// interface Error {}
		// interface Locals {}
		// interface PageData {}
		interface Platform {
			env: {
				DB: D1Database;
			};
			context: {
				waitUntil(promise: Promise<any>): void;
			};
			caches: CacheStorage & { default: Cache };
		}
	}
}

export {};
```

```js
import adapter from "@sveltejs/adapter-cloudflare";

export default {
	kit: {
		adapter: adapter({
			// See below for an explanation of these options
			routes: {
				include: ["/*"],
				exclude: ["<all>"],
			},
		}),
	},
};
```

</TabItem> </Tabs>

---

# Query D1 from Remix

URL: https://developers.cloudflare.com/d1/examples/d1-and-remix/

import { TabItem, Tabs } from "~/components";

Remix is a full-stack web framework that operates on both client and server. You can query your D1 database(s) from Remix using Remix's [data loading](https://remix.run/docs/en/main/guides/data-loading) API with the [`useLoaderData`](https://remix.run/docs/en/main/hooks/use-loader-data) hook.

To set up a new Remix site on Cloudflare Pages that can query D1:

1. **Refer to [the Remix guide](/pages/framework-guides/deploy-a-remix-site/)**.
2. Bind a D1 database to your [Pages Function](/pages/functions/bindings/#d1-databases).
3. Pass the `--d1 BINDING_NAME=DATABASE_ID` flag to `wrangler dev` when developing locally. `BINDING_NAME` should match what call in your code, and `DATABASE_ID` should match the `database_id` defined in your [Wrangler configuration file](/workers/wrangler/configuration/): for example, `--d1 DB=xxxx-xxxx-xxxx-xxxx-xxxx`.

The following example shows you how to define a Remix [`loader`](https://remix.run/docs/en/main/route/loader) that has a binding to a D1 database.

- Bindings are passed through on the `context.env` parameter passed to a `LoaderFunction`.
- If you configured a [binding](/pages/functions/bindings/#d1-databases) named `DB`, then you would access [D1 Workers Binding API](/d1/worker-api/prepared-statements/) methods via `context.env.DB`.

<Tabs> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import type { LoaderFunction } from "@remix-run/cloudflare";
import { json } from "@remix-run/cloudflare";
import { useLoaderData } from "@remix-run/react";

interface Env {
  DB: D1Database;
}

export const loader: LoaderFunction = async ({ context, params }) => {
  let env = context.cloudflare.env as Env;

  let { results } = await env.DB.prepare("SELECT * FROM users LIMIT 5").all();
  return json(results);
};

export default function Index() {
  const results = useLoaderData<typeof loader>();
  return (
    <div style={{ fontFamily: "system-ui, sans-serif", lineHeight: "1.8" }}>
      <h1>Welcome to Remix</h1>
      <div>
        A value from D1:
        <pre>{JSON.stringify(results)}</pre>
      </div>
    </div>
  );
}
```

</TabItem> </Tabs>

---

# Query D1 from Python Workers

URL: https://developers.cloudflare.com/d1/examples/query-d1-from-python-workers/

import { WranglerConfig } from "~/components";

The Cloudflare Workers platform supports [multiple languages](/workers/languages/), including TypeScript, JavaScript, Rust and Python. This guide shows you how to query a D1 database from [Python](/workers/languages/python/) and deploy your application globally.

:::note

Support for Python in Cloudflare Workers is in beta. Review the [documentation on Python support](/workers/languages/python/) to understand how Python works within the Workers platform.

:::

## Prerequisites

Before getting started, you should:

1. Review the [D1 tutorial](/d1/get-started/) for TypeScript and JavaScript to learn how to **create a D1 database and configure a Workers project**.
2. Refer to the [Python language guide](/workers/languages/python/) to understand how Python support works on the Workers platform.
3. Have basic familiarity with the Python language.

If you are new to Cloudflare Workers, refer to the [Get started guide](/workers/get-started/guide/) first before continuing with this example.

## Query from Python

This example assumes you have an existing D1 database. To allow your Python Worker to query your database, you first need to create a [binding](/workers/runtime-apis/bindings/) between your Worker and your D1 database and define this in your [Wrangler configuration file](/workers/wrangler/configuration/).

You will need the `database_name` and `database_id` for a D1 database. You can use the `wrangler` CLI to create a new database or fetch the ID for an existing database as follows:

```sh title="Create a database"
npx wrangler d1 create my-first-db
```

```sh title="Retrieve a database ID"
npx wrangler d1 info some-existing-db
```

```sh output
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚                   â”‚ c89db32e-83f4-4e62-8cd7-7c8f97659029 â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ name              â”‚ db-enam                              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ created_at        â”‚ 2023-06-12T16:52:03.071Z             â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. Configure bindings

In your Wrangler file, create a new `[[d1_databases]]` configuration block and set `database_name` and `database_id` to the name and id (respectively) of the D1 database you want to query:

<WranglerConfig>

```toml
name = "python-and-d1"
main = "src/entry.py"
compatibility_flags = ["python_workers"] # Required for Python Workers
compatibility_date = "2024-03-29"

[[d1_databases]]
binding = "DB" # This will be how you refer to your database in your Worker
database_name = "YOUR_DATABASE_NAME"
database_id = "YOUR_DATABASE_ID"
```

</WranglerConfig>

The value of `binding` is how you will refer to your database from within your Worker. If you change this, you must change this in your Worker script as well.

### 2. Create your Python Worker

To create a Python Worker, create an empty file at `src/entry.py`, matching the value of `main` in your Wrangler file with the contents below:

```python
from js import Response

async def on_fetch(request, env):
    # Do anything else you'd like on request here!

    # Query D1 - we'll list all tables in our database in this example
    results = await env.DB.prepare("PRAGMA table_list").all()
    # Return a JSON response
    return Response.json(results)

```

The value of `binding` in your Wrangler file exactly must match the name of the variable in your Python code. This example refers to the database via a `DB` binding, and query this binding via `await env.DB.prepare(...)`.

You can then deploy your Python Worker directly:

```sh
npx wrangler deploy
```

```sh output
# Example output
#
# Your worker has access to the following bindings:
# - D1 Databases:
#   - DB: db-enam (c89db32e-83f4-4e62-8cd7-7c8f97659029)
# Total Upload: 0.18 KiB / gzip: 0.17 KiB
# Uploaded python-and-d1 (4.93 sec)
# Published python-and-d1 (0.51 sec)
#   https://python-and-d1.YOUR_SUBDOMAIN.workers.dev
# Current Deployment ID: 80b72e19-da82-4465-83a2-c12fb11ccc72
```

Your Worker will be available at `https://python-and-d1.YOUR_SUBDOMAIN.workers.dev`.

If you receive an error deploying:

- Make sure you have configured your [Wrangler configuration file](/workers/wrangler/configuration/) with the `database_id` and `database_name` of a valid D1 database.
- Ensure `compatibility_flags = ["python_workers"]` is set in your [Wrangler configuration file](/workers/wrangler/configuration/), which is required for Python.
- Review the [list of error codes](/workers/observability/errors/), and ensure your code does not throw an uncaught exception.

## Next steps

- Refer to [Workers Python documentation](/workers/languages/python/) to learn more about how to use Python in Workers.
- Review the [D1 Workers Binding API](/d1/worker-api/) and how to query D1 databases.
- Learn [how to import data](/d1/best-practices/import-export-data/) to your D1 database.

---

# Examples

URL: https://developers.cloudflare.com/d1/examples/

import { GlossaryTooltip, ListExamples } from "~/components";

Explore the following <GlossaryTooltip term="code example">examples</GlossaryTooltip> for D1.

<ListExamples directory="d1/examples/" />

---

# Audit Logs

URL: https://developers.cloudflare.com/d1/observability/audit-logs/

[Audit logs](/fundamentals/setup/account/account-security/review-audit-logs/) provide a comprehensive summary of changes made within your Cloudflare account, including those made to D1 databases. This functionality is available on all plan types, free of charge, and is always enabled.

## Viewing audit logs

To view audit logs for your D1 databases:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/?account=audit-log) and select your account.
2. Go to **Manage Account** > **Audit Log**.

For more information on how to access and use audit logs, refer to [Review audit logs](/fundamentals/setup/account/account-security/review-audit-logs/).

## Logged operations

The following configuration actions are logged:

<table>
	<tbody>
		<th colspan="5" rowspan="1" style="width:220px">
			Operation
		</th>
		<th colspan="5" rowspan="1">
			Description
		</th>
		<tr>
			<td colspan="5" rowspan="1">
				CreateDatabase
			</td>
			<td colspan="5" rowspan="1">
				Creation of a new database.
			</td>
		</tr>
		<tr>
			<td colspan="5" rowspan="1">
				DeleteDatabase
			</td>
			<td colspan="5" rowspan="1">
				Deletion of an existing database.
			</td>
		</tr>
		<tr>
			<td colspan="5" rowspan="1">
				<a href="/d1/reference/time-travel">TimeTravel</a>
			</td>
			<td colspan="5" rowspan="1">
				Restoration of a past database version.
			</td>
		</tr>
	</tbody>
</table>

## Example log entry

Below is an example of an audit log entry showing the creation of a new database:

```json
{
	"action": { "info": "CreateDatabase", "result": true, "type": "create" },
	"actor": {
		"email": "<ACTOR_EMAIL>",
		"id": "b1ab1021a61b1b12612a51b128baa172",
		"ip": "1b11:a1b2:12b1:12a::11a:1b",
		"type": "user"
	},
	"id": "a123b12a-ab11-1212-ab1a-a1aa11a11abb",
	"interface": "API",
	"metadata": {},
	"newValue": "",
	"newValueJson": { "database_name": "my-db" },
	"oldValue": "",
	"oldValueJson": {},
	"owner": { "id": "211b1a74121aa32a19121a88a712aa12" },
	"resource": {
		"id": "11a21122-1a11-12bb-11ab-1aa2aa1ab12a",
		"type": "d1.database"
	},
	"when": "2024-08-09T04:53:55.752Z"
}
```

---

# Billing

URL: https://developers.cloudflare.com/d1/observability/billing/

D1 exposes analytics to track billing metrics (rows read, rows written, and total storage) across all databases in your account.

The metrics displayed in the [Cloudflare dashboard](https://dash.cloudflare.com/) are sourced from Cloudflare's [GraphQL Analytics API](/analytics/graphql-api/). You can access the metrics [programmatically](/d1/observability/metrics-analytics/#query-via-the-graphql-api) via GraphQL or HTTP client.

## View metrics in the dashboard

Total account billable usage analytics for D1 are available in the Cloudflare dashboard. To view current and past metrics for an account:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to **Manage Account** > **Billing**.
3. Select the **Billable Usage** tab.

From here you can view charts of your account's D1 usage on a daily or month-to-date timeframe.

Note that billable usage history is stored for a maximum of 30 days.

## Billing Notifications

Usage-based billing notifications are available within the [Cloudflare dashboard](https://dash.cloudflare.com) for users looking to monitor their total account usage.

Notifications on the following metrics are available:
- Rows Read
- Rows Written

---

# Debug D1

URL: https://developers.cloudflare.com/d1/observability/debug-d1/

D1 allows you to capture exceptions and log errors returned when querying a database. To debug D1, you will use the same tools available when [debugging Workers](/workers/observability/).

## Handle errors

The D1 [Workers Binding API](/d1/worker-api/) returns detailed error messages within an `Error` object.

To ensure you are capturing the full error message, log or return `e.message` as follows:

```ts
try {
    await db.exec("INSERTZ INTO my_table (name, employees) VALUES ()");
} catch (e: any) {
    console.error({
        message: e.message
    });
}
/*
{
  "message": "D1_EXEC_ERROR: Error in line 1: INSERTZ INTO my_table (name, employees) VALUES (): sql error: near \"INSERTZ\": syntax error in INSERTZ INTO my_table (name, employees) VALUES () at offset 0"
}
*/
```

### Errors

The [`stmt.`](/d1/worker-api/prepared-statements/) and [`db.`](/d1/worker-api/d1-database/) methods throw an [Error object](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Error) whenever an error occurs.

:::note
Prior to [`wrangler` 3.1.1](https://github.com/cloudflare/workers-sdk/releases/tag/wrangler%403.1.1), D1 JavaScript errors used the [cause property](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Error/cause) for detailed error messages.

To inspect these errors when using older versions of `wrangler`, you should log `error?.cause?.message`.
:::

To capture exceptions, log the `Error.message` value. For example, the code below has a query with an invalid keyword - `INSERTZ` instead of `INSERT`:

```js
try {
    // This is an intentional misspelling
    await db.exec("INSERTZ INTO my_table (name, employees) VALUES ()");
} catch (e: any) {
    console.error({
        message: e.message
    });
}
```

The code above throws the following error message:

```json
{
	"message": "D1_EXEC_ERROR: Error in line 1: INSERTZ INTO my_table (name, employees) VALUES (): sql error: near \"INSERTZ\": syntax error in INSERTZ INTO my_table (name, employees) VALUES () at offset 0"
}
```

### Error list

D1 returns the following error constants, in addition to the extended (detailed) error message:

| Message              | Cause                                                                                                                                                            |
| -------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `D1_ERROR`           | Generic error.                                                                                                                                                   |
| `D1_TYPE_ERROR`      | Returned when there is a mismatch in the type between a column and a value. A common cause is supplying an `undefined` variable (unsupported) instead of `null`. |
| `D1_COLUMN_NOTFOUND` | Column not found.                                                                                                                                                |
| `D1_DUMP_ERROR`      | Database dump error.                                                                                                                                             |
| `D1_EXEC_ERROR`      | Exec error in line x: y error.                                                                                                                                   |


## View logs

View a stream of live logs from your Worker by using [`wrangler tail`](/workers/observability/logs/real-time-logs#view-logs-using-wrangler-tail) or via the [Cloudflare dashboard](/workers/observability/logs/real-time-logs#view-logs-from-the-dashboard).

## Report issues

* To report bugs or request features, go to the [Cloudflare Community Forums](https://community.cloudflare.com/c/developers/d1/85).
* To give feedback, go to the [D1 Discord channel](https://discord.com/invite/cloudflaredev).
* If you are having issues with Wrangler, report issues in the [Wrangler GitHub repository](https://github.com/cloudflare/workers-sdk/issues/new/choose).

You should include as much of the following in any bug report:

* The ID of your database. Use `wrangler d1 list` to match a database name to its ID.
* The query (or queries) you ran when you encountered an issue. Ensure you redact any personally identifying information (PII).
* The Worker code that makes the query, including any calls to `bind()` using the [Workers Binding API](/d1/worker-api/).
* The full error text, including the content of [`error.cause.message`](#handle-errors).

## Related resources

* Learn [how to debug Workers](/workers/observability/).
* Understand how to [access logs](/workers/observability/logs/) generated from your Worker and D1.
* Use [`wrangler dev`](/workers/wrangler/commands/#dev) to run your Worker and D1 locally and [debug issues before deploying](/workers/local-development/).

---

# Observability

URL: https://developers.cloudflare.com/d1/observability/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Metrics and analytics

URL: https://developers.cloudflare.com/d1/observability/metrics-analytics/

import { Details } from "~/components";

D1 exposes database analytics that allow you to inspect query volume, query latency, and storage size across all and/or each database in your account.

The metrics displayed in the [Cloudflare dashboard](https://dash.cloudflare.com/) charts are queried from Cloudflareâ€™s [GraphQL Analytics API](/analytics/graphql-api/). You can access the metrics [programmatically](#query-via-the-graphql-api) via GraphQL or HTTP client.

## Metrics

D1 currently exports the below metrics:

| Metric                 | GraphQL Field Name        | Description                                                                                                                           |
| ---------------------- | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| Read Queries (qps)     | `readQueries`             | The number of read queries issued against a database. This is the raw number of read queries, and is not used for billing.            |
| Write Queries (qps)    | `writeQueries`            | The number of write queries issued against a database. This is the raw number of write queries, and is not used for billing.          |
| Rows read (count)      | `rowsRead`                | The number of rows read (scanned) across your queries. See [Pricing](/d1/platform/pricing/) for more details on how rows are counted. |
| Rows written (count)   | `rowsWritten`             | The number of rows written across your queries.                                                                                       |
| Query Response (bytes) | `queryBatchResponseBytes` | The total response size of the serialized query response, including any/all column names, rows and metadata. Reported in bytes.       |
| Query Latency (ms)     | `queryBatchTimeMs`        | The total query response time, including response serialization, on the server-side. Reported in milliseconds.                        |
| Storage (Bytes)        | `databaseSizeBytes`       | Maximum size of a database. Reported in bytes.                                                                                        |

Metrics can be queried (and are retained) for the past 31 days.

### Row counts

D1 returns the number of rows read, rows written (or both) in response to each individual query via [the Workers Binding API](/d1/worker-api/return-object/).

Row counts are a precise count of how many rows were read (scanned) or written by that query.
Inspect row counts to understand the performance and cost of a given query, including whether you can reduce the rows read [using indexes](/d1/best-practices/use-indexes/). Use query counts to understand the total volume of traffic against your databases and to discern which databases are actively in-use.

Refer to the [Pricing documentation](/d1/platform/pricing/) for more details on how rows are counted.

## View metrics in the dashboard

Per-database analytics for D1 are available in the Cloudflare dashboard. To view current and historical metrics for a database:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to [**Workers & Pages** > **D1**](https://dash.cloudflare.com/?to=/:account/workers/d1).
3. Select an existing database.
4. Select the **Metrics** tab.

You can optionally select a time window to query. This defaults to the last 24 hours.

## Query via the GraphQL API

You can programmatically query analytics for your D1 databases via the [GraphQL Analytics API](/analytics/graphql-api/). This API queries the same datasets as the Cloudflare dashboard, and supports GraphQL [introspection](/analytics/graphql-api/features/discovery/introspection/).

D1's GraphQL datasets require an `accountTag` filter with your Cloudflare account ID and include:

- `d1AnalyticsAdaptiveGroups`
- `d1StorageAdaptiveGroups`
- `d1QueriesAdaptiveGroups`

### Examples

To query the sum of `readQueries`, `writeQueries` for a given `$databaseId`, grouping by `databaseId` and `date`:

```graphql
query {
	viewer {
		accounts(filter: { accountTag: $accountId }) {
			d1AnalyticsAdaptiveGroups(
				limit: 10000
				filter: {
					date_geq: $startDate
					date_leq: $endDate
					databaseId: $databaseId
				}
				orderBy: [date_DESC]
			) {
				sum {
					readQueries
					writeQueries
				}
				dimensions {
					date
					databaseId
				}
			}
		}
	}
}
```

To query both the average `queryBatchTimeMs` and the 90th percentile `queryBatchTimeMs` per database:

```graphql
query {
	viewer {
		accounts(filter: { accountTag: $accountId }) {
			d1AnalyticsAdaptiveGroups(
				limit: 10000
				filter: {
					date_geq: $startDate
					date_leq: $endDate
					databaseId: $databaseId
				}
				orderBy: [date_DESC]
			) {
				quantiles {
					queryBatchTimeMsP90
				}
				dimensions {
					date
					databaseId
				}
			}
		}
	}
}
```

To query your account-wide `readQueries` and `writeQueries`:

```graphql
query {
	viewer {
		accounts(filter: { accountTag: $accountId }) {
			d1AnalyticsAdaptiveGroups(
				limit: 10000
				filter: {
					date_geq: $startDate
					date_leq: $endDate
					databaseId: $databaseId
				}
			) {
				sum {
					readQueries
					writeQueries
				}
			}
		}
	}
}
```

## Query `insights`

D1 provides metrics that let you understand and debug query performance. You can access these via GraphQL's `d1QueriesAdaptiveGroups` or `wrangler d1 insights` command.

D1 captures your query strings to make it easier to analyze metrics across query executions. [Bound parameters](/d1/worker-api/prepared-statements/#guidance) are not captured to remove any sensitive information.

:::note

`wrangler d1 insights` is an experimental Wrangler command. Its options and output may change.

Run `wrangler d1 insights --help` to view current options.

:::

| Option             | Description                                                                                                      |
| ------------------ | ---------------------------------------------------------------------------------------------------------------- |
| `--timePeriod`     | Fetch data from now to the provided time period (default: `1d`).                                                 |
| `--sort-type`      | The operation you want to sort insights by. Select between `sum` and `avg` (default: `sum`).                     |
| `--sort-by`        | The field you want to sort insights by. Select between `time`, `reads`, `writes`, and `count` (default: `time`). |
| `--sort-direction` | The sort direction. Select between `ASC` and `DESC` (default: `DESC`).                                           |
| `--json`           | A boolean value to specify whether to return the result as clean JSON (default: `false`).                        |
| `--limit`          | The maximum number of queries to be fetched.                                                                     |

<Details header="To find top 3 queries by execution count:">

```sh
npx wrangler d1 insights <database_name> --sort-type=sum --sort-by=count --limit=3
```
```sh output
 â›…ï¸ wrangler 3.95.0
-------------------

-------------------
ðŸš§ `wrangler d1 insights` is an experimental command.
ðŸš§ Flags for this command, their descriptions, and output may change between wrangler versions.
-------------------

[
  {
    "query": "SELECT tbl_name as name,\n                   (SELECT ncol FROM pragma_table_list(tbl_name)) as num_columns\n            FROM sqlite_master\n            WHERE TYPE = \"table\"\n              AND tbl_name NOT LIKE \"sqlite_%\"\n              AND tbl_name NOT LIKE \"d1_%\"\n              AND tbl_name NOT LIKE \"_cf_%\"\n            ORDER BY tbl_name ASC;",
    "avgRowsRead": 2,
    "totalRowsRead": 4,
    "avgRowsWritten": 0,
    "totalRowsWritten": 0,
    "avgDurationMs": 0.49505,
    "totalDurationMs": 0.9901,
    "numberOfTimesRun": 2,
    "queryEfficiency": 0
  },
  {
    "query": "SELECT * FROM Customers",
    "avgRowsRead": 4,
    "totalRowsRead": 4,
    "avgRowsWritten": 0,
    "totalRowsWritten": 0,
    "avgDurationMs": 0.1873,
    "totalDurationMs": 0.1873,
    "numberOfTimesRun": 1,
    "queryEfficiency": 1
  },
  {
    "query": "SELECT * From Customers",
    "avgRowsRead": 0,
    "totalRowsRead": 0,
    "avgRowsWritten": 0,
    "totalRowsWritten": 0,
    "avgDurationMs": 1.0225,
    "totalDurationMs": 1.0225,
    "numberOfTimesRun": 1,
    "queryEfficiency": 0
  }
]
```
</Details>

<Details header="To find top 3 queries by average execution time:">

```sh
npx wrangler d1 insights <database_name> --sort-type=avg --sort-by=time --limit=3
```
```sh output
â›…ï¸ wrangler 3.95.0
-------------------

-------------------
ðŸš§ `wrangler d1 insights` is an experimental command.
ðŸš§ Flags for this command, their descriptions, and output may change between wrangler versions.
-------------------

[
  {
    "query": "SELECT * From Customers",
    "avgRowsRead": 0,
    "totalRowsRead": 0,
    "avgRowsWritten": 0,
    "totalRowsWritten": 0,
    "avgDurationMs": 1.0225,
    "totalDurationMs": 1.0225,
    "numberOfTimesRun": 1,
    "queryEfficiency": 0
  },
  {
    "query": "SELECT tbl_name as name,\n                   (SELECT ncol FROM pragma_table_list(tbl_name)) as num_columns\n            FROM sqlite_master\n            WHERE TYPE = \"table\"\n              AND tbl_name NOT LIKE \"sqlite_%\"\n              AND tbl_name NOT LIKE \"d1_%\"\n              AND tbl_name NOT LIKE \"_cf_%\"\n            ORDER BY tbl_name ASC;",
    "avgRowsRead": 2,
    "totalRowsRead": 4,
    "avgRowsWritten": 0,
    "totalRowsWritten": 0,
    "avgDurationMs": 0.49505,
    "totalDurationMs": 0.9901,
    "numberOfTimesRun": 2,
    "queryEfficiency": 0
  },
  {
    "query": "SELECT * FROM Customers",
    "avgRowsRead": 4,
    "totalRowsRead": 4,
    "avgRowsWritten": 0,
    "totalRowsWritten": 0,
    "avgDurationMs": 0.1873,
    "totalDurationMs": 0.1873,
    "numberOfTimesRun": 1,
    "queryEfficiency": 1
  }
]
```
</Details>

<Details header="To find top 10 queries by rows written in last 7 days:">

```sh
npx wrangler d1 insights <database_name> --sort-type=sum --sort-by=writes --limit=10 --timePeriod=7d
```
```sh output
â›…ï¸ wrangler 3.95.0
-------------------

-------------------
ðŸš§ `wrangler d1 insights` is an experimental command.
ðŸš§ Flags for this command, their descriptions, and output may change between wrangler versions.
-------------------

[
  {
    "query": "SELECT * FROM Customers",
    "avgRowsRead": 4,
    "totalRowsRead": 4,
    "avgRowsWritten": 0,
    "totalRowsWritten": 0,
    "avgDurationMs": 0.1873,
    "totalDurationMs": 0.1873,
    "numberOfTimesRun": 1,
    "queryEfficiency": 1
  },
  {
    "query": "SELECT * From Customers",
    "avgRowsRead": 0,
    "totalRowsRead": 0,
    "avgRowsWritten": 0,
    "totalRowsWritten": 0,
    "avgDurationMs": 1.0225,
    "totalDurationMs": 1.0225,
    "numberOfTimesRun": 1,
    "queryEfficiency": 0
  },
  {
    "query": "SELECT tbl_name as name,\n                   (SELECT ncol FROM pragma_table_list(tbl_name)) as num_columns\n            FROM sqlite_master\n            WHERE TYPE = \"table\"\n              AND tbl_name NOT LIKE \"sqlite_%\"\n              AND tbl_name NOT LIKE \"d1_%\"\n              AND tbl_name NOT LIKE \"_cf_%\"\n            ORDER BY tbl_name ASC;",
    "avgRowsRead": 2,
    "totalRowsRead": 4,
    "avgRowsWritten": 0,
    "totalRowsWritten": 0,
    "avgDurationMs": 0.49505,
    "totalDurationMs": 0.9901,
    "numberOfTimesRun": 2,
    "queryEfficiency": 0
  }
]
```
</Details>

:::note
The quantity `queryEfficiency` measures how efficient your query was. It is calculated as: the number of rows returned divided by the number of rows read.

Generally, you should try to get `queryEfficiency` as close to `1` as possible. Refer to [Use indexes](/d1/best-practices/use-indexes/) for more information on efficient querying.
:::

---

# Changelog

URL: https://developers.cloudflare.com/d1/platform/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/d1.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Platform

URL: https://developers.cloudflare.com/d1/platform/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Limits

URL: https://developers.cloudflare.com/d1/platform/limits/

import { Render } from "~/components";

| Feature                                                                                                             | Limit                                             |
| ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
| Databases                                                                                                           | 50,000 (Workers Paid)[^1] / 10 (Free)                 |
| Maximum database size                                                                                               | 10 GB (Workers Paid) / 500 MB (Free)              |
| Maximum storage per account                                                                                         | 250 GB (Workers Paid)[^1] / 5 GB (Free)           |
| [Time Travel](/d1/reference/time-travel/) duration (point-in-time recovery)                                         | 30 days (Workers Paid) / 7 days (Free)            |
| Maximum Time Travel restore operations                                                                              | 10 restores per 10 minute (per database)          |
| Queries per Worker invocation (read [subrequest limits](/workers/platform/limits/#how-many-subrequests-can-i-make)) | 50 (Bundled) / 1000 (Unbound)                     |
| Maximum number of columns per table                                                                                 | 100                                               |
| Maximum number of rows per table                                                                                    | Unlimited (excluding per-database storage limits) |
| Maximum string, `BLOB` or table row size                                                                            | 2,000,000 bytes (2 MB)                            |
| Maximum SQL statement length                                                                                        | 100,000 bytes (100 KB)                            |
| Maximum bound parameters per query                                                                                  | 100                                               |
| Maximum arguments per SQL function                                                                                  | 32                                                |
| Maximum characters (bytes) in a `LIKE` or `GLOB` pattern                                                            | 50 bytes                                          |
| Maximum bindings per Workers script                                                                                 | Approximately 5,000 [^2]                          |
| Maximum SQL query duration                                                                                          | 30 seconds [^3]                                   |
| Maximum file import (`d1 execute`) size                                                                             | 5 GB [^4]                                        |

:::note[Batch limits]
Limits for individual queries (listed above) apply to each individual statement contained within a batch statement. For example, the maximum SQL statement length of 100 KB applies to each statement inside a `db.batch()`.
:::

[^1]: The maximum storage per account can be increased by request on Workers Paid and Enterprise plans. See the guidance on limit increases on this page to request an increase.

[^2]: A single Worker script can have up to 1 MB of script metadata. A binding is defined as a binding to a resource, such as a D1 database, KV namespace, environmental variable or secret. Each resource binding is approximately 150-bytes, however environmental variables and secrets are controlled by the size of the value you provide. Excluding environmental variables, you can bind up to \~5,000 D1 databases to a single Worker script.

[^3]: Requests to Cloudflare API must resolve in 30 seconds. Therefore, this duration limit also applies to the entire batch call.

[^4]: The imported file is uploaded to R2. See [R2 upload limit](/r2/platform/limits).

Cloudflare also offers other storage solutions such as [Workers KV](/kv/api/), [Durable Objects](/durable-objects/), and [R2](/r2/get-started/). Each product has different advantages and limits. Refer to [Choose a data or storage product](/workers/platform/storage-options/) to review which storage option is right for your use case.

<Render file="limits_increase" product="workers" />

## Frequently Asked Questions

Frequently asked questions related to D1 limits:

### How much work can a D1 database do?

D1 is designed for horizontal scale out across multiple, smaller (10 GB) databases, such as per-user, per-tenant or per-entity databases. D1 allows you to build applications with thousands of databases at no extra cost for isolating with multiple databases, as the pricing is based only on query and storage costs.

- Each D1 database can store up to 10 GB of data, and you can create up to thousands of separate D1 databases. This allows you to split a single monolithic database into multiple, smaller databases, thereby isolating application data by user, customer, or tenant.
- SQL queries over a smaller working data set can be more efficient and performant while improving data isolation.

:::caution
Note that the 10 GB limit of a D1 database cannot be further increased.
:::

---

# Pricing

URL: https://developers.cloudflare.com/d1/platform/pricing/

import { Render } from "~/components";

D1 bills based on:

- **Usage**: Queries you run against D1 will count as rows read, rows written, or both (for transactions or batches).
- **Scale-to-zero**: You are not billed for hours or capacity units. If you are not running queries against your database, you are not billed for compute.
- **Storage**: You are only billed for storage above the included [limits](/d1/platform/limits/) of your plan.

## Billing metrics

<Render file="d1-pricing" product="workers" />

## Frequently Asked Questions

Frequently asked questions related to D1 pricing:

### Will D1 always have a Free plan?

Yes, the [Workers Free plan](/workers/platform/pricing/#workers) will always include the ability to prototype and experiment with D1 for free.

### What happens if I exceed the daily limits on reads and writes, or the total storage limit, on the Free plan?

When your account hits the daily read and/or write limits, you will not be able to run queries against D1. D1 API will return errors to your client indicating that your daily limits have been exceeded. Once you have reached your included storage limit, you will need to delete unused databases or clean up stale data before you can insert new data, create or alter tables or create indexes and triggers.

Upgrading to the Workers Paid plan will remove these limits, typically within minutes.

### What happens if I exceed the monthly included reads, writes and/or storage on the paid tier?

You will be billed for the additional reads, writes and storage according to [D1's pricing metrics](#billing-metrics).

### How can I estimate my (eventual) bill?

Every query returns a `meta` object that contains a total count of the rows read (`rows_read`) and rows written (`rows_written`) by that query. For example, a query that performs a full table scan (for instance, `SELECT * FROM users`) from a table with 5000 rows would return a `rows_read` value of `5000`:

```json
"meta": {
  "duration": 0.20472300052642825,
  "size_after": 45137920,
  "rows_read": 5000,
  "rows_written": 0
}
```

These are also included in the D1 [Cloudflare dashboard](https://dash.cloudflare.com) and the [analytics API](/d1/observability/metrics-analytics/), allowing you to attribute read and write volumes to specific databases, time periods, or both.

### Does D1 charge for data transfer / egress?

No.

### Does D1 charge additional for additional compute?

D1 itself does not charge for additional compute. Workers querying D1 and computing results: for example, serializing results into JSON and/or running queries, are billed per [Workers pricing](/workers/platform/pricing/#workers), in addition to your D1 specific usage.

### Do queries I run from the dashboard or Wrangler (the CLI) count as billable usage?

Yes, any queries you run against your database, including inserting (`INSERT`) existing data into a new database, table scans (`SELECT * FROM table`), or creating indexes count as either reads or writes.

### Can I use an index to reduce the number of rows read by a query?

Yes, you can use an index to reduce the number of rows read by a query. [Creating indexes](/d1/best-practices/use-indexes/) for your most queried tables and filtered columns reduces how much data is scanned and improves query performance at the same time. If you have a read-heavy workload (most common), this can be particularly advantageous. Writing to columns referenced in an index will add at least one (1) additional row written to account for updating the index, but this is typically offset by the reduction in rows read due to the benefits of an index.

### Does a freshly created database, and/or an empty table with no rows, contribute to my storage?

Yes, although minimal. An empty table consumes at least a few kilobytes, based on the number of columns (table width) in the table. An empty database consumes approximately 12 KB of storage.

---

# Alpha database migration guide

URL: https://developers.cloudflare.com/d1/platform/alpha-migration/

:::caution

D1 alpha databases stopped accepting live SQL queries on August 22, 2024.

:::

D1's open beta launched in October 2023, and newly created databases use a different underlying architecture that is significantly more reliable and performant, with increased database sizes, improved query throughput, and reduced latency.

This guide will instruct you to recreate alpha D1 databases on our production-ready system.

## Prerequisites

1. You have the [`wrangler` command-line tool](/workers/wrangler/install-and-update/) installed
2. You are using `wrangler` version `3.33.0` or later (released March 2024) as earlier versions do not have the [`--remote` flag](/d1/platform/changelog/#2024-03-12) required as part of this guide
3. An 'alpha' D1 database. All databases created before July 27th, 2023 ([release notes](/d1/platform/changelog/#2024-03-12)) use the alpha storage backend, which is no longer supported and was not recommended for production.

## 1. Verify that a database is alpha

```sh
npx wrangler d1 info <database_name>
```

If the database is alpha, the output of the command will include `version` set to `alpha`:

```
...
â”‚ version           â”‚ alpha                                 â”‚
...
```

## 2. Create a manual backup

```sh
npx wrangler d1 backup create <alpha_database_name>
```

## 3. Download the manual backup

The command below will download the manual backup of the alpha database as `.sqlite3` file:

```sh
npx wrangler d1 backup download <alpha_database_name> <backup_id> # See available backups with wrangler d1 backup list <database_name>
```

## 4. Convert the manual backup into SQL statements

The command below will convert the manual backup of the alpha database from the downloaded `.sqlite3` file into SQL statements which can then be imported into the new database:

```sh
sqlite3 db_dump.sqlite3 .dump > db.sql
```

Once you have run the above command, you will need to edit the output SQL file to be compatible with D1:

1. Remove `BEGIN TRANSACTION` and `COMMIT;` from the file.
2. Remove the following table creation statement:

   ```sql
   CREATE TABLE _cf_KV (
    	key TEXT PRIMARY KEY,
    	value BLOB
   ) WITHOUT ROWID;
   ```

## 5. Create a new D1 database

All new D1 databases use the updated architecture by default.

Run the following command to create a new database:

```sh
npx wrangler d1 create <new_database_name>
```

## 6. Run SQL statements against the new D1 database

```sh
npx wrangler d1 execute <new_database_name> --remote --file=./db.sql
```

## 7. Delete your alpha database

To delete your previous alpha database, run:

```sh
npx wrangler d1 delete <alpha_database_name>
```

---

# Backups (Legacy)

URL: https://developers.cloudflare.com/d1/reference/backups/

D1 has built-in support for creating and restoring backups of your databases, including support for scheduled automatic backups and manual backup management.

:::caution[Time Travel]

The snapshot based backups described in this documentation are deprecated, and limited to the original alpha databases.

Databases using D1's [production storage subsystem](https://blog.cloudflare.com/d1-turning-it-up-to-11/) can use Time Travel. Time Travel replaces the [snapshot-based backups](/d1/reference/backups/) used for legacy alpha databases.

To understand which storage subsystem your database uses, run `wrangler d1 info YOUR_DATABASE` and inspect the `version` field in the output. Databases with `version: production` support the new Time Travel API. Databases with `version: alpha` only support the older, snapshot-based backup API.

:::

## Automatic backups

D1 automatically backs up your databases every hour on your behalf, and [retains backups for 24 hours](/d1/platform/limits/). Backups will block access to the DB while they are running. In most cases this should only be a second or two, and any requests that arrive during the backup will be queued.

To view and manage these backups, including any manual backups you have made, you can use the `d1 backup list <DATABASE_NAME>` command to list each backup.

For example, to list all of the backups of a D1 database named `existing-db`:

```sh
wrangler d1 backup list existing-db
```

```sh output

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ created_at   â”‚ id                                   â”‚ num_tables â”‚ size    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1 hour ago   â”‚ 54a23309-db00-4c5c-92b1-c977633b937c â”‚ 1          â”‚ 95.3 kB â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ <...>        â”‚ <...>                                â”‚ <...>      â”‚ <...>   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2 months ago â”‚ 8433a91e-86d0-41a3-b1a3-333b080bca16 â”‚ 1          â”‚ 65.5 kB â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜%
```

The `id` of each backup allows you to download or restore a specific backup.

## Manually back up a database

Creating a manual backup of your database before making large schema changes, manually inserting or deleting data, or otherwise modifying a database you are actively using is a good practice to get into. D1 allows you to make a backup of a database at any time, and stores the backup on your behalf. You should also consider [using migrations](/d1/reference/migrations/) to simplify changes to an existing database.

To back up a D1 database, you must have:

1. The Cloudflare [Wrangler CLI installed](/workers/wrangler/install-and-update/)
2. An existing D1 database you want to back up.

For example, to create a manual backup of a D1 database named `example-db`, call `d1 backup create`.

```sh
wrangler d1 backup create example-db
```

```sh output
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ created_at                  â”‚ id                                   â”‚ num_tables â”‚ size    â”‚ state â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2023-02-04T15:49:36.113753Z â”‚ 123a81a2-ab91-4c2e-8ebc-64d69633faf1 â”‚ 1          â”‚ 65.5 kB â”‚ done  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
```

Larger databases, especially those that are several megabytes (MB) in size with many tables, may take a few seconds to backup. The `state` column in the output will let you know when the backup is done.

## Downloading a backup locally

To download a backup locally, call `wrangler d1 backup download <DATABASE_NAME> <BACKUP_ID>`. Use `wrangler d1 backup list <DATABASE_NAME>` to list the available backups, including their IDs, for a given D1 database.

For example, to download a specific backup for a database named `example-db`:

```sh
wrangler d1 backup download example-db 123a81a2-ab91-4c2e-8ebc-64d69633faf1
```

```sh output

ðŸŒ€ Downloading backup 123a81a2-ab91-4c2e-8ebc-64d69633faf1 from 'example-db'
ðŸŒ€ Saving to /Users/you/projects/example-db.123a81a2.sqlite3
ðŸŒ€ Done!
```

The database backup will be download to the current working directory in native SQLite3 format. To import a local database, read [the documentation on importing data](/d1/best-practices/import-export-data/) to D1.

## Restoring a backup

:::caution

Restoring a backup will overwrite the existing version of your D1 database in-place. We recommend you make a manual backup before you restore a database, so that you have a backup to revert to if you accidentally restore the wrong backup or break your application.

:::

Restoring a backup will overwrite the current running version of a database with the backup. Database tables (and their data) that do not exist in the backup will no longer exist in the current version of the database, and queries that rely on them will fail.

To restore a previous backup of a D1 database named `existing-db`, pass the ID of that backup to `d1 backup restore`:

```sh
wrangler d1 backup restore existing-db  6cceaf8c-ceab-4351-ac85-7f9e606973e3
```

```sh output
Restoring existing-db from backup 6cceaf8c-ceab-4351-ac85-7f9e606973e3....
Done!
```

Any queries against the database will immediately query the current (restored) version once the restore has completed.

---

# Community projects

URL: https://developers.cloudflare.com/d1/reference/community-projects/

Members of the Cloudflare developer community and broader developer ecosystem have built and/or contributed tooling â€” including ORMs (Object Relational Mapper) libraries, query builders, and CLI tools â€” that build on top of D1.

:::note


Community projects are not maintained by the Cloudflare D1 team. They are managed and updated by the project authors.


:::

## Projects

### Sutando ORM

Sutando is an ORM designed for Node.js. With Sutando, each table in a database has a corresponding model that handles CRUD (Create, Read, Update, Delete) operations.

- [GitHub](https://github.com/sutandojs/sutando)
- [D1 with Sutando ORM Example](https://github.com/sutandojs/sutando-examples/tree/main/typescript/rest-hono-cf-d1)

### knex-cloudflare-d1

knex-cloudflare-d1 is the Cloudflare D1 dialect for Knex.js. Note that this is not an official dialect provided by Knex.js.

- [GitHub](https://github.com/kiddyuchina/knex-cloudflare-d1)

### Prisma ORM

[Prisma ORM](https://www.prisma.io/orm) is a next-generation JavaScript and TypeScript ORM that unlocks a new level of developer experience when working with databases thanks to its intuitive data model, automated migrations, type-safety and auto-completion.

* [Tutorial](/d1/tutorials/d1-and-prisma-orm/)
* [Docs](https://www.prisma.io/docs/orm/prisma-client/deployment/edge/deploy-to-cloudflare#d1)

### D1 adapter for Kysely ORM

Kysely is a type-safe and autocompletion-friendly typescript SQL query builder. With this adapter you can interact with D1 with the familiar Kysely interface.

* [Kysely GitHub](https://github.com/koskimas/kysely)
* [D1 adapter](https://github.com/aidenwallis/kysely-d1)

### feathers-kysely

The `feathers-kysely` database adapter follows the FeathersJS Query Syntax standard and works with any framework. It is built on the D1 adapter for Kysely and supports passing queries directly from client applications. Since the FeathersJS query syntax is a subset of MongoDB's syntax, this is a great tool for MongoDB users to use Cloudflare D1 without previous SQL experience.

* [feathers-kysely on npm](https://www.npmjs.com/package/feathers-kysely)
* [feathers-kysely on GitHub](https://github.com/marshallswain/feathers-kysely)

### Drizzle ORM

Drizzle is a headless TypeScript ORM with a head which runs on Node, Bun and Deno. Drizzle ORM lives on the Edge and it is a JavaScript ORM too. It comes with a drizzle-kit CLI companion for automatic SQL migrations generation. Drizzle automatically generates your D1 schema based on types you define in TypeScript, and exposes an API that allows you to query your database directly.

* [Docs](https://orm.drizzle.team/docs)
* [GitHub](https://github.com/drizzle-team/drizzle-orm)
* [D1 example](https://orm.drizzle.team/docs/connect-cloudflare-d1)

### Flyweight

Flyweight is an ORM designed specifically for databases related to SQLite. It has first-class D1 support that includes the ability to batch queries and integrate with the wrangler migration system.

* [GitHub](https://github.com/thebinarysearchtree/flyweight)

### d1-orm

Object Relational Mapping (ORM) is a technique to query and manipulate data by using JavaScript. Created by a Cloudflare Discord Community Champion, the `d1-orm` seeks to provide a strictly typed experience while using D1.

* [GitHub](https://github.com/Interactions-as-a-Service/d1-orm/issues)
* [Documentation](https://docs.interactions.rest/d1-orm/)

### workers-qb

`workers-qb` is a zero-dependency query builder that provides a simple standardized interface while keeping the benefits and speed of using raw queries over a traditional ORM. While not intended to provide ORM-like functionality, `workers-qb` makes it easier to interact with your database from code for direct SQL access.

* [GitHub](https://github.com/G4brym/workers-qb)
* [Documentation](https://workers-qb.massadas.com/)

### d1-console

Instead of running the `wrangler d1 execute` command in your terminal every time you want to interact with your database, you can interact with D1 from within the `d1-console`. Created by a Discord Community Champion, this gives the benefit of executing multi-line queries, obtaining command history, and viewing a cleanly formatted table output.

* [GitHub](https://github.com/isaac-mcfadyen/d1-console)

### L1

`L1` is a package that brings some Cloudflare Worker ecosystem bindings into PHP and Laravel via the Cloudflare API. It provides interaction with D1 via PDO, KV and Queues, with more services to add in the future, making PHP integration with Cloudflare a real breeze.

* [GitHub](https://github.com/renoki-co/l1)
* [Packagist](https://packagist.org/packages/renoki-co/l1)

### Staff Directory - a D1-based demo

Staff Directory is a demo project using D1, [HonoX](https://github.com/honojs/honox), and [Cloudflare Pages](/pages/). It uses D1 to store employee data, and is an example of a full-stack application built on top of D1.

* [GitHub](https://github.com/lauragift21/staff-directory)
* [D1 functionality](https://github.com/lauragift21/staff-directory/blob/main/app/db.ts)

### NuxtHub

`NuxtHub` is a Nuxt module that brings Cloudflare Worker bindings into your Nuxt application with no configuration. It leverages the [Wrangler Platform Proxy](/workers/wrangler/api/#getplatformproxy) in development and direct binding in production to interact with [D1](/d1/), [KV](/kv/) and [R2](/r2/) with server composables (`hubDatabase()`, `hubKV()` and `hubBlob()`).

`NuxtHub` also provides a way to use your remote D1 database in development using the `npx nuxt dev --remote` command.

* [GitHub](https://github.com/nuxt-hub/core)
* [Documentation](https://hub.nuxt.com)
* [Example](https://github.com/Atinux/nuxt-todos-edge)

## Feedback

To report a bug or file feature requests for these community projects, create an issue directly on the project's repository.

---

# Data security

URL: https://developers.cloudflare.com/d1/reference/data-security/

This page details the data security properties of D1, including:

* Encryption-at-rest (EAR).
* Encryption-in-transit (EIT).
* Cloudflare's compliance certifications.

## Encryption at Rest

All objects stored in D1, including metadata, live databases, and inactive databases are encrypted at rest. Encryption and decryption are automatic, do not require user configuration to enable, and do not impact the effective performance of D1.

Encryption keys are managed by Cloudflare and securely stored in the same key management systems we use for managing encrypted data across Cloudflare internally.

Objects are encrypted using [AES-256](https://www.cloudflare.com/learning/ssl/what-is-encryption/), a widely tested, highly performant and industry-standard encryption algorithm. D1 uses GCM (Galois/Counter Mode) as its preferred mode.

## Encryption in Transit

Data transfer between a Cloudflare Worker, and/or between nodes within the Cloudflare network and D1 is secured using the same [Transport Layer Security](https://www.cloudflare.com/learning/ssl/transport-layer-security-tls/) (TLS/SSL).

API access via the HTTP API or using the [wrangler](/workers/wrangler/install-and-update/) command-line interface is also over TLS/SSL (HTTPS).

## Compliance

To learn more about Cloudflare's adherence to industry-standard security compliance certifications, visit the Cloudflare [Trust Hub](https://www.cloudflare.com/trust-hub/compliance-resources/).

---

# Generated columns

URL: https://developers.cloudflare.com/d1/reference/generated-columns/

D1 allows you to define generated columns based on the values of one or more other columns, SQL functions, or even [extracted JSON values](/d1/sql-api/query-json/).

This allows you to normalize your data as you write to it or read it from a table, making it easier to query and reducing the need for complex application logic.

Generated columns can also have [indexes defined](/d1/best-practices/use-indexes/) against them, which can dramatically increase query performance over frequently queried fields.

## Types of generated columns

There are two types of generated columns:

* `VIRTUAL` (default): the column is generated when read. This has the benefit of not consuming storage, but can increase compute time (and thus reduce query performance), especially for larger queries.
* `STORED`: the column is generated when the row is written. The column takes up storage space just as a regular column would, but the column does not need to be generated on every read, which can improve read query performance.

When omitted from a generated column expression, generated columns default to the `VIRTUAL` type. The `STORED` type is recommended when the generated column is compute intensive. For example, when parsing large JSON structures.

## Define a generated column

Generated columns can be defined during table creation in a `CREATE TABLE` statement or afterwards via the `ALTER TABLE` statement.

To create a table that defines a generated column, you use the `AS` keyword:

```sql
CREATE TABLE some_table (
    -- other columns omitted
    some_generated_column AS <function_that_generates_the_column_data>
)
```

As a concrete example, to automatically extract the `location` value from the following JSON sensor data, you can define a generated column called `location` (of type `TEXT`), based on a `raw_data` column that stores the raw representation of our JSON data.

```json
{
    "measurement": {
        "temp_f": "77.4",
        "aqi": [21, 42, 58],
        "o3": [18, 500],
        "wind_mph": "13",
        "location": "US-NY"
    }
}
```

To define a generated column with the value of `$.measurement.location`, you can use the [`json_extract`](/d1/sql-api/query-json/#extract-values) function to extract the value from the `raw_data` column each time you write to that row:

```sql
CREATE TABLE sensor_readings (
    event_id INTEGER PRIMARY KEY,
    timestamp INTEGER NOT NULL,
    raw_data TEXT,
    location as (json_extract(raw_data, '$.measurement.location')) STORED
);
```

Generated columns can optionally be specified with the `column_name GENERATED ALWAYS AS <function> [STORED|VIRTUAL]` syntax. The `GENERATED ALWAYS` syntax is optional and does not change the behavior of the generated column when omitted.

## Add a generated column to an existing table

A generated column can also be added to an existing table. If the `sensor_readings` table did not have the generated `location` column, you could add it by running an `ALTER TABLE` statement:

```sql
ALTER TABLE sensor_readings
ADD COLUMN location as (json_extract(raw_data, '$.measurement.location'));
```

This defines a `VIRTUAL` generated column that runs `json_extract` on each read query.

Generated column definitions cannot be directly modified. To change how a generated column generates its data, you can use `ALTER TABLE table_name REMOVE COLUMN` and then `ADD COLUMN` to re-define the generated column, or `ALTER TABLE table_name RENAME COLUMN current_name TO new_name` to rename the existing column before calling `ADD COLUMN` with a new definition.

## Examples

Generated columns are not just limited to JSON functions like `json_extract`: you can use almost any available function to define how a generated column is generated.

For example, you could generate a `date` column based on the `timestamp` column from the previous `sensor_reading` table, automatically converting a Unix timestamp into a `YYYY-MM-dd` format within your database:

```sql
ALTER TABLE your_table
-- date(timestamp, 'unixepoch') converts a Unix timestamp to a YYYY-MM-dd formatted date
ADD COLUMN formatted_date AS (date(timestamp, 'unixepoch'))
```

Alternatively, you could define an `expires_at` column that calculates a future date, and filter on that date in your queries:

```sql
-- Filter out "expired" results based on your generated column:
-- SELECT * FROM your_table WHERE current_date() > expires_at
ALTER TABLE your_table
-- calculates a date (YYYY-MM-dd) 30 days from the timestamp.
ADD COLUMN expires_at AS (date(timestamp, '+30 days'));
```

## Additional considerations

* Tables must have at least one non-generated column. You cannot define a table with only generated column(s).
* Expressions can only reference other columns in the same table and row, and must only use [deterministic functions](https://www.sqlite.org/deterministic.html). Functions like `random()`, sub-queries or aggregation functions cannot be used to define a generated column.
* Columns added to an existing table via `ALTER TABLE ... ADD COLUMN` must be `VIRTUAL`. You cannot add a `STORED` column to an existing table.

---

# Glossary

URL: https://developers.cloudflare.com/d1/reference/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's D1 documentation.

<Glossary product="d1" />

---

# Reference

URL: https://developers.cloudflare.com/d1/reference/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Migrations

URL: https://developers.cloudflare.com/d1/reference/migrations/

import { WranglerConfig } from "~/components";

Database migrations are a way of versioning your database. Each migration is stored as an `.sql` file in your `migrations` folder. The `migrations` folder is created in your project directory when you create your first migration. This enables you to store and track changes throughout database development.

## Features

Currently, the migrations system aims to be simple yet effective. With the current implementation, you can:

* [Create](/workers/wrangler/commands/#d1-migrations-create) an empty migration file.
* [List](/workers/wrangler/commands/#d1-migrations-list) unapplied migrations.
* [Apply](/workers/wrangler/commands/#d1-migrations-apply) remaining migrations.

Every migration file in the `migrations` folder has a specified version number in the filename. Files are listed in sequential order. Every migration file is an SQL file where you can specify queries to be run.

## Wrangler customizations

By default, migrations are created in the `migrations/` folder in your Worker project directory. Creating migrations will keep a record of applied migrations in the `d1_migrations` table found in your database.

This location and table name can be customized in your Wrangler file, inside the D1 binding.

<WranglerConfig>

```toml
[[ d1_databases ]]
binding = "<BINDING_NAME>" # i.e. if you set this to "DB", it will be available in your Worker at `env.DB`
database_name = "<DATABASE_NAME>"
database_id = "<UUID>"
preview_database_id = "<UUID>"
migrations_table = "<d1_migrations>" # Customize this value to change your applied migrations table name
migrations_dir = "<FOLDER_NAME>" # Specify your custom migration directory
```

</WranglerConfig>

## Foreign key constraints

When applying a migration, you may need to temporarily disable [foreign key constraints](/d1/sql-api/foreign-keys/). To do so, call `PRAGMA defer_foreign_keys = true` before making changes that would violate foreign keys.

Refer to the [foreign key documentation](/d1/sql-api/foreign-keys/) to learn more about how to work with foreign keys and D1.

---

# Time Travel and backups

URL: https://developers.cloudflare.com/d1/reference/time-travel/

Time Travel is D1's approach to backups and point-in-time-recovery, and allows you to restore a database to any minute within the last 30 days.

- You do not need to enable Time Travel. It is always on.
- Database history and restoring a database incur no additional costs.
- Time Travel automatically creates [bookmarks](#bookmarks) on your behalf. You do not need to manually trigger or remember to initiate a backup.

By not having to rely on scheduled backups and/or manually initiated backups, you can go back in time and restore a database prior to a failed migration or schema change, a `DELETE` or `UPDATE` statement without a specific `WHERE` clause, and in the future, fork/copy a production database directly.

:::note[Support for Time Travel]

Databases using D1's [new storage subsystem](https://blog.cloudflare.com/d1-turning-it-up-to-11/) can use Time Travel. Time Travel replaces the [snapshot-based backups](/d1/reference/backups/) used for legacy alpha databases.

To understand which storage subsystem your database uses, run `wrangler d1 info YOUR_DATABASE` and inspect the `version` field in the output. Databases with `version: production` support the new Time Travel API. Databases with `version: alpha` only support the older, snapshot-based backup API.

:::

## Bookmarks

Time Travel introduces the concept of a "bookmark" to D1. A bookmark represents the state of a database at a specific point in time, and is effectively an append-only log.

- Bookmarks are lexicographically sortable. Sorting orders a list of bookmarks from oldest-to-newest.
- Bookmarks older than 30 days are invalid and cannot be used as a restore point.
- Restoring a database to a specific bookmark does not remove or delete older bookmarks. For example, if you restore to a bookmark representing the state of your database 10 minutes ago, and determine that you needed to restore to an earlier point in time, you can still do so.

Bookmarks can be derived from a [Unix timestamp](https://en.wikipedia.org/wiki/Unix_time) (seconds since Jan 1st, 1970), and conversion between a specific timestamp and a bookmark is deterministic (stable).

## Timestamps

Time Travel supports two timestamp formats:

- [Unix timestamps](https://developer.mozilla.org/en-US/docs/Glossary/Unix_time), which correspond to seconds since January 1st, 1970 at midnight. This is always in UTC.
- The [JavaScript date-time string format](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date#date_time_string_format), which is a simplified version of the ISO-8601 timestamp format. An valid date-time string for the July 27, 2023 at 11:18AM in Americas/New_York (EST) would look like `2023-07-27T11:18:53.000-04:00`.

## Requirements

- [`Wrangler`](/workers/wrangler/install-and-update/) `v3.4.0` or later installed to use Time Travel commands.
- A database on D1's production backend. You can check whether a database is using this backend via `wrangler d1 info DB_NAME` - the output show `version: production`.

## Retrieve a bookmark

You can retrieve a bookmark for the current timestamp by calling the `d1 info` command, which defaults to returning the current bookmark:

```sh
wrangler d1 time-travel info YOUR_DATABASE
```

```sh output
ðŸš§ Time Traveling...
âš ï¸ The current bookmark is '00000085-0000024c-00004c6d-8e61117bf38d7adb71b934ebbf891683'
âš¡ï¸ To restore to this specific bookmark, run:
 `wrangler d1 time-travel restore YOUR_DATABASE --bookmark=00000085-0000024c-00004c6d-8e61117bf38d7adb71b934ebbf891683`
```

To retrieve the bookmark for a timestamp in the past, pass the `--timestamp` flag with a valid Unix or RFC3339 timestamp:

```sh title="Using an RFC3339 timestamp, including the timezone"
wrangler d1 time-travel info YOUR_DATABASE --timestamp="2023-07-09T17:31:11+00:00"
```

## Restore a database

To restore a database to a specific point-in-time:

:::caution

Restoring a database to a specific point-in-time is a _destructive_ operation, and overwrites the database in place. In the future, D1 will support branching & cloning databases using Time Travel.

:::

```sh
wrangler d1 time-travel restore YOUR_DATABASE --timestamp=UNIX_TIMESTAMP
```

```sh output
ðŸš§ Restoring database YOUR_DATABASE from bookmark 00000080-ffffffff-00004c60-390376cb1c4dd679b74a19d19f5ca5be

âš ï¸ This will overwrite all data in database YOUR_DATABASE.
In-flight queries and transactions will be cancelled.

âœ” OK to proceed (y/N) â€¦ yes
âš¡ï¸ Time travel in progress...
âœ… Database YOUR_DATABASE restored back to bookmark 00000080-ffffffff-00004c60-390376cb1c4dd679b74a19d19f5ca5be

â†©ï¸ To undo this operation, you can restore to the previous bookmark: 00000085-ffffffff-00004c6d-2510c8b03a2eb2c48b2422bb3b33fad5
```

Note that:

- Timestamps are converted to a deterministic, stable bookmark. The same timestamp will always represent the same bookmark.
- Queries in flight will be cancelled, and an error returned to the client.
- The restore operation will return a [bookmark](#bookmarks) that allows you to [undo](#undo-a-restore) and revert the database.

## Undo a restore

You can undo a restore by:

- Taking note of the previous bookmark returned as part of a `wrangler d1 time-travel restore` operation
- Restoring directly to a bookmark in the past, prior to your last restore.

To fetch a bookmark from an earlier state:

```sh title: "Get a historical bookmark"
wrangler d1 time-travel info YOUR_DATABASE
```

```sh output
ðŸš§ Time Traveling...
âš ï¸ The current bookmark is '00000085-0000024c-00004c6d-8e61117bf38d7adb71b934ebbf891683'
âš¡ï¸ To restore to this specific bookmark, run:
 `wrangler d1 time-travel restore YOUR_DATABASE --bookmark=00000085-0000024c-00004c6d-8e61117bf38d7adb71b934ebbf891683`
```

## Export D1 into R2 using Workflows

You can automatically export your D1 database into R2 storage via REST API and Cloudflare Workflows. This may be useful if you wish to store a state of your D1 database for longer than 30 days.

Refer to the guide [Export and save D1 database](/workflows/examples/backup-d1/).

## Notes

- You can quickly get the Unix timestamp from the command-line in macOS and Windows via `date %+s`.
- Time Travel does not yet allow you to clone or fork an existing database to a new copy. In the future, Time Travel will allow you to fork (clone) an existing database into a new database, or overwrite an existing database.
- You can restore a database back to a point in time up to 30 days in the past (Workers Paid plan) or 7 days (Workers Free plan). Refer to [Limits](/d1/platform/limits/) for details on Time Travel's limits.

---

# Define foreign keys

URL: https://developers.cloudflare.com/d1/sql-api/foreign-keys/

D1 supports defining and enforcing foreign key constraints across tables in a database.

Foreign key constraints allow you to enforce relationships across tables. For example, you can use foreign keys to create a strict binding between a `user_id` in a `users` table and the `user_id` in an `orders` table, so that no order can be created against a user that does not exist.

Foreign key constraints can also prevent you from deleting rows that reference rows in other tables. For example, deleting rows from the `users` table when rows in the `orders` table refer to them.

By default, D1 enforces that foreign key constraints are valid within all queries and migrations. This is identical to the behaviour you would observe when setting `PRAGMA foreign_keys = on` in SQLite for every transaction.

## Defer foreign key constraints

When running a [query](/d1/worker-api/), [migration](/d1/reference/migrations/) or [importing data](/d1/best-practices/import-export-data/) against a D1 database, there may be situations in which you need to disable foreign key validation during table creation or changes to your schema.

D1's foreign key enforcement is equivalent to SQLite's `PRAGMA foreign_keys = on` directive. Because D1 runs every query inside an implicit transaction, user queries cannot change this during a query or migration.

Instead, D1 allows you to call `PRAGMA defer_foreign_keys = on` or `off`, which allows you to violate foreign key constraints temporarily (until the end of the current transaction).

Calling `PRAGMA defer_foreign_keys = off` does not disable foreign key enforcement outside of the current transaction. If you have not resolved outstanding foreign key violations at the end of your transaction, it will fail with a `FOREIGN KEY constraint failed` error.

To defer foreign key enforcement, set `PRAGMA defer_foreign_keys = on` at the start of your transaction, or ahead of changes that would violate constraints:

```sql
-- Defer foreign key enforcement in this transaction.
PRAGMA defer_foreign_keys = on

-- Run your CREATE TABLE or ALTER TABLE / COLUMN statements
ALTER TABLE users ...

-- This is implicit if not set by the end of the transaction.
PRAGMA defer_foreign_keys = off
```

You can also explicitly set `PRAGMA defer_foreign_keys = on` immediately after you have resolved outstanding foreign key constraints. If there are still outstanding foreign key constraints, you will receive a `FOREIGN KEY constraint failed` error and will need to resolve the violation.

## Define a foreign key relationship

A foreign key relationship can be defined when creating a table via `CREATE TABLE` or when adding a column to an existing table via an `ALTER TABLE` statement.

To illustrate this with an example based on an e-commerce website with two tables:

* A `users` table that defines common properties about a user account, including a unique `user_id` identifier.
* An `orders` table that maps an order back to a `user_id` in the user table.

This mapping is defined as `FOREIGN KEY`, which ensures that:

* You cannot delete a row from the `users` table that would violate the foreign key constraint. This means that you cannot end up with orders that do not have a valid user to map back to.
* `orders` are always defined against a valid `user_id`, mitigating the risk of creating orders that refer to invalid (or non-existent) users.

```sql
CREATE TABLE users (
    user_id INTEGER PRIMARY KEY,
    email_address TEXT,
    name TEXT,
    metadata TEXT
)

CREATE TABLE orders (
    order_id INTEGER PRIMARY KEY,
    status INTEGER,
    item_desc TEXT,
    shipped_date INTEGER,
    user_who_ordered INTEGER,
    FOREIGN KEY(user_who_ordered) REFERENCES users(user_id)
)
```

You can define multiple foreign key relationships per-table, and foreign key definitions can reference multiple tables within your overall database schema.

## Foreign key actions

You can define *actions* as part of your foreign key definitions to either limit or propagate changes to a parent row (`REFERENCES table(column)`). Defining *actions* makes using foreign key constraints in your application easier to reason about, and help either clean up related data or prevent data from being islanded.

There are five actions you can set when defining the `ON UPDATE` and/or `ON DELETE` clauses as part of a foreign key relationship. You can also define different actions for `ON UPDATE` and `ON DELETE` depending on your requirements.

* `CASCADE` - Updating or deleting a parent key deletes all child keys (rows) associated to it.
* `RESTRICT` - A parent key cannot be updated or deleted when *any* child key refers to it. Unlike the default foreign key enforcement, relationships with `RESTRICT` applied return errors immediately, and not at the end of the transaction.
* `SET DEFAULT` - Set the child column(s) referred to by the foreign key definition to the `DEFAULT` value defined in the schema. If no `DEFAULT` is set on the child columns, you cannot use this action.
* `SET NULL` - Set the child column(s) referred to by the foreign key definition to SQL `NULL`.
* `NO ACTION` - Take no action.

:::caution[CASCADE usage]

Although `CASCADE` can be the desired behavior in some cases, deleting child rows across tables can have undesirable effects and/or result in unintended side effects for your users.
:::

In the following example, deleting a user from the `users` table will delete all related rows in the `scores` table as you have defined `ON DELETE CASCADE`. Delete all related rows in the `scores` table if you do not want to retain the scores for any users you have deleted entirely. This might mean that *other* users can no longer look up or refer to scores that were still valid.

```sql
CREATE TABLE users (
    user_id INTEGER PRIMARY KEY,
    email_address TEXT,
)

CREATE TABLE scores (
    score_id INTEGER PRIMARY KEY,
    game TEXT,
    score INTEGER,
    player_id INTEGER,
    FOREIGN KEY(player_id) REFERENCES users(user_id) ON DELETE CASCADE
)
```

## Next Steps

* Read the SQLite [`FOREIGN KEY`](https://www.sqlite.org/foreignkeys.html) documentation.
* Learn how to [use the D1 Workers Binding API](/d1/worker-api/) from within a Worker.
* Understand how [database migrations work](/d1/reference/migrations/) with D1.

---

# SQL API

URL: https://developers.cloudflare.com/d1/sql-api/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Query JSON

URL: https://developers.cloudflare.com/d1/sql-api/query-json/

D1 has built-in support for querying and parsing JSON data stored within a database. This enables you to:

* [Query paths](#extract-values) within a stored JSON object - for example, extracting the value of named key or array index directly, which is especially useful with larger JSON objects.
* Insert and/or replace values within an object or array.
* [Expand the contents of a JSON object](#expand-arrays-for-in-queries) or array into multiple rows - for example, for use as part of a `WHERE ... IN` predicate.
* Create [generated columns](/d1/reference/generated-columns/) that are automatically populated with values from JSON objects you insert.

One of the biggest benefits to parsing JSON within D1 directly is that it can directly reduce the number of round-trips (queries) to your database. It reduces the cases where you have to read a JSON object into your application (1), parse it, and then write it back (2).

This allows you to more precisely query over data and reduce the result set your application needs to additionally parse and filter on.

## Types

JSON data is stored as a `TEXT` column in D1. JSON types follow the same [type conversion rules](/d1/worker-api/#type-conversion) as D1 in general, including:

* A JSON null is treated as a D1 `NULL`.
* A JSON number is treated as an `INTEGER` or `REAL`.
* Booleans are treated as `INTEGER` values: `true` as `1` and `false` as `0`.
* Object and array values as `TEXT`.

## Supported functions

The following table outlines the JSON functions built into D1 and example usage.

* The `json` argument placeholder can be a JSON object, array, string, number or a null value.
* The `value` argument accepts string literals (only) and treats input as a string, even if it is well-formed JSON. The exception to this rule is when nesting `json_*` functions: the outer (wrapping) function will interpret the inner (wrapped) functions return value as JSON.
* The `path` argument accepts path-style traversal syntax - for example, `$` to refer to the top-level object/array, `$.key1.key2` to refer to a nested object, and `$.key[2]` to index into an array.

| Function                                                    | Description                                                                                                                                                    | Example                                                                                   |
| ----------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| `json(json)`                                                | Validates the provided string is JSON and returns a minified version of that JSON object.                                                                      | `json('{"hello":["world" ,"there"] }')` returns `{"hello":["world","there"]}`             |
| `json_array(value1, value2, value3, ...)`                   | Return a JSON array from the values.                                                                                                                           | `json_array(1, 2, 3)` returns `[1, 2, 3]`                                                 |
| `json_array_length(json)` - `json_array_length(json, path)` | Return the length of the JSON array                                                                                                                            | `json_array_length('{"data":["x", "y", "z"]}', '$.data')` returns `3`                     |
| `json_extract(json, path)`                                  | Extract the value(s) at the given path using `$.path.to.value` syntax.                                                                                         | `json_extract('{"temp":"78.3", "sunset":"20:44"}', '$.temp')` returns `"78.3"`            |
| `json -> path`                                              | Extract the value(s) at the given path using path syntax and return it as JSON.                                                                                |                                                                                           |
| `json ->> path`                                             | Extract the value(s) at the given path using path syntax and return it as a SQL type.                                                                          |                                                                                           |
| `json_insert(json, path, value)`                            | Insert a value at the given path. Does not overwrite an existing value.                                                                                        |                                                                                           |
| `json_object(label1, value1, ...)`                          | Accepts pairs of (keys, values) and returns a JSON object.                                                                                                     | `json_object('temp', 45, 'wind_speed_mph', 13)` returns `{"temp":45,"wind_speed_mph":13}` |
| `json_patch(target, patch)`                                 | Uses a JSON [MergePatch](https://tools.ietf.org/html/rfc7396) approach to merge the provided patch into the target JSON object.                                |                                                                                           |
| `json_remove(json, path, ...)`                              | Remove the key and value at the specified path.                                                                                                                | `json_remove('[60,70,80,90]', '$[0]')` returns `70,80,90]`                                |
| `json_replace(json, path, value)`                           | Insert a value at the given path. Overwrites an existing value, but does not create a new key if it doesn't exist.                                             |                                                                                           |
| `json_set(json, path, value)`                               | Insert a value at the given path. Overwrites an existing value.                                                                                                |                                                                                           |
| `json_type(json)` - `json_type(json, path)`                 | Return the type of the provided value or value at the specified path. Returns one of `null`, `true`, `false`, `integer`, `real`, `text`, `array`, or `object`. | `json_type('{"temperatures":[73.6, 77.8, 80.2]}', '$.temperatures')` returns `array`      |
| `json_valid(json)`                                          | Returns 0 (false) for invalid JSON, and 1 (true) for valid JSON.                                                                                               | `json_valid({invalid:json})`returns`0\`                                                  |
| `json_quote(value)`                                         | Converts the provided SQL value into its JSON representation.                                                                                                  | `json_quote('[1, 2, 3]')` returns `[1,2,3]`                                               |
| `json_group_array(value)`                                   | Returns the provided value(s) as a JSON array.                                                                                                                 |                                                                                           |
| `json_each(value)` - `json_each(value, path)`               | Returns each element within the object as an individual row. It will only traverse the top-level object.                                                       |                                                                                           |
| `json_tree(value)` - `json_tree(value, path)`               | Returns each element within the object as an individual row. It traverses the full object.                                                                     |                                                                                           |

The SQLite [JSON extension](https://www.sqlite.org/json1.html), on which D1 builds on, has additional usage examples.

## Error Handling

JSON functions will return a `malformed JSON` error when operating over data that isn't JSON and/or is not valid JSON. D1 considers valid JSON to be [RFC 7159](https://www.rfc-editor.org/rfc/rfc7159.txt) conformant.

In the following example, calling `json_extract` over a string (not valid JSON) will cause the query to return a `malformed JSON` error:

```sql
SELECT json_extract('not valid JSON: just a string', '$')
```

This will return an error:

```txt
ERROR 9015: SQL engine error: query error: Error code 1: SQL error or missing database (malformed
  JSON)`
```

## Generated columns

D1's support for [generated columns](/d1/reference/generated-columns/) allows you to create dynamic columns that are generated based on the values of other columns, including extracted or calculated values of JSON data.

These columns can be queried like any other column, and can have [indexes](/d1/best-practices/use-indexes/) defined on them. If you have JSON data that you frequently query and filter over, creating a generated column and an index can dramatically improve query performance.

For example, to define a column based on a value within a larger JSON object, use the `AS` keyword combined with a [JSON function](#supported-functions) to generate a typed column:

```sql
CREATE TABLE some_table (
    -- other columns omitted
    raw_data TEXT -- JSON: {"measurement":{"aqi":[21,42,58],"wind_mph":"13","location":"US-NY"}}
    location AS (json_extract(raw_data, '$.measurement.location')) STORED
)
```

Refer to [Generated columns](/d1/reference/generated-columns/) to learn more about how to generate columns.

## Example usage

### Extract values

There are three ways to extract a value from a JSON object in D1:

* The `json_extract()` function - for example, `json_extract(text_column_containing_json, '$.path.to.value)`.
* The `->` operator, which returns a JSON representation of the value.
* The `->>` operator, which returns an SQL representation of the value.

The `->` and `->>` operators functions both operate similarly to the same operators in PostgreSQL and MySQL/MariaDB.

Given the following JSON object in a column named `sensor_reading`, you can extract values from it directly.

```json
{
    "measurement": {
        "temp_f": "77.4",
        "aqi": [21, 42, 58],
        "o3": [18, 500],
        "wind_mph": "13",
        "location": "US-NY"
    }
}
```

```sql
-- Extract the temperature value
json_extract(sensor_reading, '$.measurement.temp_f')-- returns "77.4" as TEXT
```

```sql
-- Extract the maximum PM2.5 air quality reading
sensor_reading -> '$.measurement.aqi[3]' -- returns 58 as a JSON number
```

```sql
-- Extract the o3 (ozone) array in full
sensor_reading -\-> '$.measurement.o3' -- returns '[18, 500]' as TEXT
```

### Get the length of an array

You can get the length of a JSON array in two ways:

1. By calling `json_array_length(value)` directly
2. By calling `json_array_length(value, path)` to specify the path to an array within an object or outer array.

For example, given the following JSON object stored in a column called `login_history`, you could get a count of the last logins directly:

```json
{
    "user_id": "abc12345",
    "previous_logins": ["2023-03-31T21:07:14-05:00", "2023-03-28T08:21:02-05:00", "2023-03-28T05:52:11-05:00"]
}
```

```sql
json_array_length(login_history, '$.previous_logins') --> returns 3 as an INTEGER
```

You can also use `json_array_length` as a predicate in a more complex query - for example, `WHERE json_array_length(some_column, '$.path.to.value') >= 5`.

### Insert a value into an existing object

You can insert a value into an existing JSON object or array using `json_insert()`. For example, if you have a `TEXT` column called `login_history` in a `users` table containing the following object:

```json
{"history": ["2023-05-13T15:13:02+00:00", "2023-05-14T07:11:22+00:00", "2023-05-15T15:03:51+00:00"]}
```

To add a new timestamp to the `history` array within our `login_history` column, write a query resembling the following:

```sql
UPDATE users
SET login_history = json_insert(login_history, '$.history[#]', '2023-05-15T20:33:06+00:00')
WHERE user_id = 'aba0e360-1e04-41b3-91a0-1f2263e1e0fb'
```

Provide three arguments to `json_insert`:

1. The name of our column containing the JSON you want to modify.
2. The path to the key within the object to modify.
3. The JSON value to insert. Using `[#]` tells `json_insert` to append to the end of your array.

To replace an existing value, use `json_replace()`, which will overwrite an existing key-value pair if one already exists. To set a value regardless of whether it already exists, use `json_set()`.

### Expand arrays for IN queries

Use `json_each` to expand an array into multiple rows. This can be useful when composing a `WHERE column IN (?)` query over several values. For example, if you wanted to update a list of users by their integer `id`, use `json_each` to return a table with each value as a column called `value`:

```sql
UPDATE users
SET last_audited = '2023-05-16T11:24:08+00:00'
WHERE id IN (SELECT value FROM json_each('[183183, 13913, 94944]'))
```

This would extract only the `value` column from the table returned by `json_each`, with each row representing the user IDs you passed in as an array.

`json_each` effectively returns a table with multiple columns, with the most relevant being:

* `key` - the key (or index).
* `value` - the literal value of each element parsed by `json_each`.
* `type` - the type of the value: one of `null`, `true`, `false`, `integer`, `real`, `text`, `array`, or `object`.
* `fullkey` - the full path to the element: e.g. `$[1]` for the second element in an array, or `$.path.to.key` for a nested object.
* `path` - the top-level path - `$` as the path for an element with a `fullkey` of `$[0]`.

In this example, `SELECT * FROM json_each('[183183, 13913, 94944]')` would return a table resembling the below:

```sql
key|value|type|id|fullkey|path
0|183183|integer|1|$[0]|$
1|13913|integer|2|$[1]|$
2|94944|integer|3|$[2]|$
```

You can use `json_each` with [D1 Workers Binding API](/d1/worker-api/) in a Worker by creating a statement and using `JSON.stringify` to pass an array as a [bound parameter](/d1/worker-api/d1-database/#guidance):

```ts
const stmt = context.env.DB
    .prepare("UPDATE users SET last_audited = ? WHERE id IN (SELECT value FROM json_each(?1))")
const resp = await stmt.bind(
    "2023-05-16T11:24:08+00:00",
    JSON.stringify([183183, 13913, 94944])
    ).run()
```

This would only update rows in your `users` table where the `id` matches one of the three provided.

---

# SQL statements

URL: https://developers.cloudflare.com/d1/sql-api/sql-statements/

import { Details, Render } from "~/components";

D1 is compatible with most SQLite's SQL convention since it leverages SQLite's query engine. D1 supports a number of database-level statements that allow you to list tables, indexes, and inspect the schema for a given table or index.

You can execute any of these statements via the D1 console in the Cloudflare dashboard, [`wrangler d1 execute`](/workers/wrangler/commands/#d1), or with the [D1 Worker Bindings API](/d1/worker-api/d1-database).

## Supported SQLite extensions

D1 supports a subset of SQLite extensions for added functionality, including:

- Default SQLite extensions.
- [FTS5 module](https://www.sqlite.org/fts5.html) for full-text search.

## Compatible PRAGMA statements

D1 supports some [SQLite PRAGMA](https://www.sqlite.org/pragma.html) statements. The PRAGMA statement is an SQL extension for SQLite. PRAGMA commands can be used to:

- Modify the behavior of certain SQLite operations.
- Query the SQLite library for internal data about schemas or tables (but note that PRAGMA statements cannot query the contents of a table).
- Control environmental variables.

<Render file="use-pragma-statements" />

## Query `sqlite_master`

You can also query the `sqlite_master` table to show all tables, indexes, and the original SQL used to generate them:

```sql
SELECT name, sql FROM sqlite_master
```

```json
      {
        "name": "users",
        "sql": "CREATE TABLE users ( user_id INTEGER PRIMARY KEY, email_address TEXT, created_at INTEGER, deleted INTEGER, settings TEXT)"
      },
      {
        "name": "idx_ordered_users",
        "sql": "CREATE INDEX idx_ordered_users ON users(created_at DESC)"
      },
      {
        "name": "Order",
        "sql": "CREATE TABLE \"Order\" ( \"Id\" INTEGER PRIMARY KEY, \"CustomerId\" VARCHAR(8000) NULL, \"EmployeeId\" INTEGER NOT NULL, \"OrderDate\" VARCHAR(8000) NULL, \"RequiredDate\" VARCHAR(8000) NULL, \"ShippedDate\" VARCHAR(8000) NULL, \"ShipVia\" INTEGER NULL, \"Freight\" DECIMAL NOT NULL, \"ShipName\" VARCHAR(8000) NULL, \"ShipAddress\" VARCHAR(8000) NULL, \"ShipCity\" VARCHAR(8000) NULL, \"ShipRegion\" VARCHAR(8000) NULL, \"ShipPostalCode\" VARCHAR(8000) NULL, \"ShipCountry\" VARCHAR(8000) NULL)"
      },
      {
        "name": "Product",
        "sql": "CREATE TABLE \"Product\" ( \"Id\" INTEGER PRIMARY KEY, \"ProductName\" VARCHAR(8000) NULL, \"SupplierId\" INTEGER NOT NULL, \"CategoryId\" INTEGER NOT NULL, \"QuantityPerUnit\" VARCHAR(8000) NULL, \"UnitPrice\" DECIMAL NOT NULL, \"UnitsInStock\" INTEGER NOT NULL, \"UnitsOnOrder\" INTEGER NOT NULL, \"ReorderLevel\" INTEGER NOT NULL, \"Discontinued\" INTEGER NOT NULL)"
      }
```

## Search with LIKE

You can perform a search using SQL's `LIKE` operator:

```js
const { results } = await env.DB.prepare(
	"SELECT * FROM Customers WHERE CompanyName LIKE ?",
)
	.bind("%eve%")
	.all();
console.log("results: ", results);
```
```js output
results:  [...]
```

## Related resources

- Learn [how to create indexes](/d1/best-practices/use-indexes/#list-indexes) in D1.
- Use D1's [JSON functions](/d1/sql-api/query-json/) to query JSON data.
- Use [`wrangler dev`](/workers/wrangler/commands/#dev) to run your Worker and D1 locally and debug issues before deploying.

---

# Tutorials

URL: https://developers.cloudflare.com/d1/tutorials/

import { GlossaryTooltip, ListTutorials } from "~/components"

View <GlossaryTooltip term="tutorial">tutorials</GlossaryTooltip> to help you get started with D1.

<ListTutorials />

---

# D1 Database

URL: https://developers.cloudflare.com/d1/worker-api/d1-database/

import { Type, MetaInfo, Details } from "~/components";

To interact with your D1 database from your Worker, you need to access it through the environment bindings provided to the Worker (`env`).

```js
async fetch(request, env) {
	// D1 database is 'env.DB', where "DB" is the binding name from the Wrangler configuration file.
}
```

A D1 binding has the type `D1Database`, and supports a number of methods, as listed below.

## Methods

### `prepare()`

Prepares a query statement to be later executed.

```js
const someVariable = `Bs Beverages`;
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind(someVariable);
```

#### Parameters

- <code>query</code>: <Type text="String"/> <MetaInfo text="Required"/>
  - The SQL query you wish to execute on the database.

#### Return values

- <code>D1PreparedStatement</code>: <Type text="Object"/>
  - An object which only contains methods. Refer to [Prepared statement methods](/d1/worker-api/prepared-statements/).

#### Guidance

You  can use the `bind` method to dynamically bind a value into the query statement, as shown below.

- Example of a static statement without using `bind`:

	```js
	const stmt = db
		.prepare("SELECT * FROM Customers WHERE CompanyName = Alfreds Futterkiste AND CustomerId = 1")
	```

- Example of an ordered statement using `bind`:

	```js
	const stmt = db
		.prepare("SELECT * FROM Customers WHERE CompanyName = ? AND CustomerId = ?")
		.bind("Alfreds Futterkiste", 1);
	```

Refer to the [`bind` method documentation](/d1/worker-api/prepared-statements/#bind) for more information.

### `batch()`

Sends multiple SQL statements inside a single call to the database. This can have a huge performance impact as it reduces latency from network round trips to D1. D1 operates in auto-commit. Our implementation guarantees that each statement in the list will execute and commit, sequentially, non-concurrently.

Batched statements are [SQL transactions](https://www.sqlite.org/lang_transaction.html). If a statement in the sequence fails, then an error is returned for that specific statement, and it aborts or rolls back the entire sequence.

To send batch statements, provide `D1Database::batch` a list of prepared statements and get the results in the same order.

```js
const companyName1 = `Bs Beverages`;
const companyName2 = `Around the Horn`;
const stmt = env.DB.prepare(`SELECT * FROM Customers WHERE CompanyName = ?`);
const batchResult = await env.DB.batch([
	stmt.bind(companyName1),
	stmt.bind(companyName2)
]);
```

#### Parameters

- <code>statements</code>: <Type text="Array"/>
  - An array of [`D1PreparedStatement`](#prepare)s.

#### Return values

- <code>results</code>: <Type text="Array"/>
  - An array of `D1Result` objects containing the results of the [`D1Database::prepare`](#prepare) statements. Each object is in the array position corresponding to the array position of the initial [`D1Database::prepare`](#prepare) statement within the `statements`.
  - Refer to [`D1Result`](/d1/worker-api/return-object/#d1result) for more information about this object.

<Details header="Example of return values" open={false}>

```js
const companyName1 = `Bs Beverages`;
const companyName2 = `Around the Horn`;
const stmt = await env.DB.batch([
	env.DB.prepare(`SELECT * FROM Customers WHERE CompanyName = ?`).bind(companyName1),
	env.DB.prepare(`SELECT * FROM Customers WHERE CompanyName = ?`).bind(companyName2)
]);
return Response.json(stmt)
```
```js output
[
  {
    "success": true,
    "meta": {
      "served_by": "miniflare.db",
      "duration": 0,
      "changes": 0,
      "last_row_id": 0,
      "changed_db": false,
      "size_after": 8192,
      "rows_read": 4,
      "rows_written": 0
    },
    "results": [
      {
        "CustomerId": 11,
        "CompanyName": "Bs Beverages",
        "ContactName": "Victoria Ashworth"
      },
      {
        "CustomerId": 13,
        "CompanyName": "Bs Beverages",
        "ContactName": "Random Name"
      }
    ]
  },
  {
    "success": true,
    "meta": {
      "served_by": "miniflare.db",
      "duration": 0,
      "changes": 0,
      "last_row_id": 0,
      "changed_db": false,
      "size_after": 8192,
      "rows_read": 4,
      "rows_written": 0
    },
    "results": [
      {
        "CustomerId": 4,
        "CompanyName": "Around the Horn",
        "ContactName": "Thomas Hardy"
      }
    ]
  }
]
```
```js
console.log(stmt[1].results);
```
```js output
[
  {
    "CustomerId": 4,
    "CompanyName": "Around the Horn",
    "ContactName": "Thomas Hardy"
  }
]
```
</Details>

#### Guidance

- You can construct batches reusing the same prepared statement:

	```js
		const companyName1 = `Bs Beverages`;
		const companyName2 = `Around the Horn`;
		const stmt = env.DB.prepare(`SELECT * FROM Customers WHERE CompanyName = ?`);
		const batchResult = await env.DB.batch([
			stmt.bind(companyName1),
			stmt.bind(companyName2)
		]);
		return Response.json(batchResult);
	```

### `exec()`

Executes one or more queries directly without prepared statements or parameter bindings.

```js
const returnValue = await env.DB.exec(`SELECT * FROM Customers WHERE CompanyName = "Bs Beverages"`);
```

#### Parameters

- <code>query</code>: <Type text="String"/> <MetaInfo text="Required"/>
  - The SQL query statement without parameter binding.

#### Return values

- <code>D1ExecResult</code>: <Type text="Object"/>
  - The `count` property contains the number of executed queries.
  - The `duration` property contains the duration of operation in milliseconds.
	- Refer to [`D1ExecResult`](/d1/worker-api/return-object/#d1execresult) for more information.

<Details header="Example of return values" open={false}>
```js
const returnValue = await env.DB.exec(`SELECT * FROM Customers WHERE CompanyName = "Bs Beverages"`);
return Response.json(returnValue);
```
```js output
{
  "count": 1,
  "duration": 1
}
```
</Details>

#### Guidance

- If an error occurs, an exception is thrown with the query and error messages, execution stops and further statements are not executed. Refer to [Errors](/d1/observability/debug-d1/#errors) to learn more.
- This method can have poorer performance (prepared statements can be reused in some cases) and, more importantly, is less safe.
- Only use this method for maintenance and one-shot tasks (for example, migration jobs).
- The input can be one or multiple queries separated by `\n`.

### `dump`

:::caution
This API only works on databases created during D1's alpha period. Check which version your database uses with `wrangler d1 info <DATABASE_NAME>`.
:::

Dumps the entire D1 database to an SQLite compatible file inside an ArrayBuffer.

```js
const dump = await db.dump();
return new Response(dump, {
	status: 200,
	headers: {
		"Content-Type": "application/octet-stream",
	},
});
```

#### Parameters

- None.

#### Return values

- None.

---

# Workers Binding API

URL: https://developers.cloudflare.com/d1/worker-api/

import { DirectoryListing, Details, Steps } from "~/components";

You can execute SQL queries on your D1 database from a Worker using the Worker Binding API. To do this, you can perform the following steps:

1. [Bind the D1 Database](/d1/worker-api/d1-database).
2. [Prepare a statement](/d1/worker-api/d1-database/#prepare).
3. [Run the prepared statement](/d1/worker-api/prepared-statements).
4. Analyze the [return object](/d1/worker-api/return-object) (if necessary).

Refer to the relevant sections for the API documentation.

## TypeScript support

D1 Worker Bindings API is fully-typed via the [`@cloudflare/workers-types`](/workers/languages/typescript/#typescript) package, and also supports [generic types](https://www.typescriptlang.org/docs/handbook/2/generics.html#generic-types) as part of its TypeScript API. A generic type allows you to provide an optional `type parameter` so that a function understands the type of the data it is handling.

When using the query statement methods [`D1PreparedStatement::run`](/d1/worker-api/prepared-statements/#run), [`D1PreparedStatement::raw`](/d1/worker-api/prepared-statements/#raw) and [`D1PreparedStatement::first`](/d1/worker-api/prepared-statements/#first), you can provide a type representing each database row. D1's API will [return the result object](/d1/worker-api/return-object/#d1result) with the correct type.

For example, providing an `OrderRow` type as a type parameter to [`D1PreparedStatement::run`](/d1/worker-api/prepared-statements/#run) will return a typed `Array<OrderRow>` object instead of the default `Record<string, unknown>` type:

```ts
// Row definition
type OrderRow = {
Id: string;
CustomerName: string;
OrderDate: number;
};

// Elsewhere in your application
const result = await env.MY_DB.prepare(
"SELECT Id, CustomerName, OrderDate FROM [Order] ORDER BY ShippedDate DESC LIMIT 100",
).run<OrderRow>();
```

## Type conversion

D1 automatically converts supported JavaScript (including TypeScript) types passed as parameters via the Workers Binding API to their associated D1 types. The type conversion is as follows:

| JavaScript           | D1                                                                           |
| -------------------- | ---------------------------------------------------------------------------- |
| null                 | `NULL`                                                                       |
| Number               | `REAL`                                                                       |
| Number <sup>1</sup>  | `INTEGER`                                                                    |
| String               | `TEXT`                                                                       |
| Boolean <sup>2</sup> | `INTEGER`                                                                    |
| ArrayBuffer          | `BLOB`                                                                       |
| undefined            | Not supported. Queries with `undefined` values will return a `D1_TYPE_ERROR` |

<sup>1</sup> D1 supports 64-bit signed `INTEGER` values internally, however
[BigInts](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt)
are not currently supported in the API yet. JavaScript integers are safe up to
[`Number.MAX_SAFE_INTEGER`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/MAX_SAFE_INTEGER).

<sup>2</sup> Booleans will be cast to an `INTEGER` type where `1` is `TRUE` and
`0` is `FALSE`.

## API playground

The D1 Worker Binding API playground is an `index.js` file where you can test each of the documented Worker Binding APIs for D1. The file builds from the end-state of the [Get started](/d1/get-started/#write-queries-within-your-worker) code.

You can use this alongside the API documentation to better understand how each API works.

Follow the steps to setup your API playground.

### 1. Complete the Get started tutorial

Complete the [Get started](/d1/get-started/#write-queries-within-your-worker) tutorial. Ensure you use JavaScript instead of TypeScript.

### 2. Modify the content of `index.js`

Replace the contents of your `index.js` file with the code below to view the effect of each API.

<Details header="index.js" open={false}>
```js
export default {
	async fetch(request, env) {
	  const { pathname } = new URL(request.url);

	//   if (pathname === "/api/beverages") {
	// 	// If you did not use `DB` as your binding name, change it here
	// 	const { results } = await env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?",).bind("Bs Beverages").all();
	// 	return Response.json(results);
	//   }

		const companyName1 = `Bs Beverages`;
		const companyName2 = `Around the Horn`;
		const stmt = env.DB.prepare(`SELECT * FROM Customers WHERE CompanyName = ?`);

	  if (pathname === `/RUN`){
		const returnValue = await stmt.bind(companyName1).run();
		return Response.json(returnValue);

	} else if (pathname === `/RAW`){
		const returnValue = await stmt.bind(companyName1).raw();
		return Response.json(returnValue);

	} else if (pathname === `/FIRST`){
		const returnValue = await stmt.bind(companyName1).first();
		return Response.json(returnValue);

	} else if (pathname === `/BATCH`) {
		const batchResult = await env.DB.batch([
			stmt.bind(companyName1),
			stmt.bind(companyName2)
		]);
		return Response.json(batchResult);

	} else if (pathname === `/EXEC`){
		const returnValue = await env.DB.exec(`SELECT * FROM Customers WHERE CompanyName = "Bs Beverages"`);
		return Response.json(returnValue);
	}

	  return new Response(
		`Welcome to the D1 API Playground!
		\nChange the URL to test the various methods inside your index.js file.`,
	  );
	},
};

```
</Details>

### 3. Deploy the Worker

<Steps>
1. Navigate to your tutorial directory you created by following step 1.
2. Run `npx wrangler dev`.
	```sh
	npx wrangler dev
	```
	```sh output
	 â›…ï¸ wrangler 3.85.0 (update available 3.86.1)
	-------------------------------------------------------

	Your worker has access to the following bindings:
	- D1 Databases:
		- DB: <DATABASE_NAME> (DATABASE_ID) (local)
	âŽ” Starting local server...
	[wrangler:inf] Ready on http://localhost:8787
	â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
	â”‚  [b] open a browser       â”‚
	â”‚  [d] open devtools        â”‚
	â”‚  [l] turn off local mode  â”‚
	â”‚  [c] clear console        â”‚
	â”‚  [x] to exit              â”‚
	â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
	```
3. Open a browser at the specified address.
</Steps>

### 4. Test the APIs

Change the URL to test the various D1 Worker Binding APIs.

---

# Prepared statement methods

URL: https://developers.cloudflare.com/d1/worker-api/prepared-statements/

import { Type, MetaInfo, Details } from "~/components";

This chapter documents the various ways you can run and retrieve the results of a query after you have [prepared your statement](/d1/worker-api/d1-database/#prepare).

## Methods

### `bind()`

Binds a parameter to the prepared statement.

```js
const someVariable = `Bs Beverages`;
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind(someVariable);
```

#### Parameter

- <code>Variable</code>: <Type text="string"/>
  - The variable to be appended into the prepared statement. See [guidance](#guidance) below.

#### Return values

- <code>D1PreparedStatement</code>: <Type text="Object"/>
  - A `D1PreparedStatement` where the input parameter has been included in the statement.

#### Guidance

- D1 follows the [SQLite convention](https://www.sqlite.org/lang_expr.html#varparam) for prepared statements parameter binding. Currently, D1 only supports Ordered (`?NNNN`) and Anonymous (`?`) parameters. In the future, D1 will support named parameters as well.

	| Syntax | Type      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
	| ------ | --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
	| `?NNN` | Ordered   | A question mark followed by a number `NNN` holds a spot for the `NNN`-th parameter. `NNN` must be between `1` and `SQLITE_MAX_VARIABLE_NUMBER`                                                                                                                                                                                                                                                                                                                                                                                                    |
	| `?`    | Anonymous | A question mark that is not followed by a number creates a parameter with a number one greater than the largest parameter number already assigned. If this means the parameter number is greater than `SQLITE_MAX_VARIABLE_NUMBER`, it is an error. This parameter format is provided for compatibility with other database engines. But because it is easy to miscount the question marks, the use of this parameter format is discouraged. Programmers are encouraged to use one of the symbolic formats below or the `?NNN` format above instead. |

	To bind a parameter, use the `.bind` method.

	Order and anonymous examples:

	```js
	const stmt = db.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind("");
	```

	```js
	const stmt = db
		.prepare("SELECT * FROM Customers WHERE CompanyName = ? AND CustomerId = ?")
		.bind("Alfreds Futterkiste", 1);
	```

	```js
	const stmt = db
		.prepare("SELECT * FROM Customers WHERE CompanyName = ?2 AND CustomerId = ?1")
		.bind(1, "Alfreds Futterkiste");
	```

#### Static statements

D1 API supports static statements. Static statements are SQL statements where the variables have been hard coded. When writing a static statement, you manually type the variable within the statement string.

:::note
The recommended approach is to bind parameters to create a prepared statement (which are precompiled objects used by the database) to run the SQL. Prepared statements lead to faster overall execution and prevent SQL injection attacks.
:::

Example of a prepared statement with dynamically bound value:

```js
const someVariable = `Bs Beverages`;
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind(someVariable);
// A variable (someVariable) will replace the placeholder '?' in the query.
// `stmt` is a prepared statement.
```

Example of a static statement:

```js
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = Bs Beverages");
// "Bs Beverages" is hard-coded into the query.
// `stmt` is a static statement.
```

### `run()`

Runs the prepared query (or queries) and returns results. The returned results includes metadata.

```js
const returnValue = await stmt.run();
```

#### Parameter

- None.

#### Return values

- <code>D1Result</code>: <Type text="Object"/>
  - An object containing the success status, a meta object, and an array of objects containing the query results.
  - For more information on the object, refer to [`D1Result`](/d1/worker-api/return-object/#d1result).

<Details header="Example of return values" open = {false}>
```js
const someVariable = `Bs Beverages`;
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind(someVariable);
const returnValue = await stmt.run();
```
```js
return Response.json(returnValue);
```
```js output
{
  "success": true,
  "meta": {
    "served_by": "miniflare.db",
    "duration": 1,
    "changes": 0,
    "last_row_id": 0,
    "changed_db": false,
    "size_after": 8192,
    "rows_read": 4,
    "rows_written": 0
  },
  "results": [
    {
      "CustomerId": 11,
      "CompanyName": "Bs Beverages",
      "ContactName": "Victoria Ashworth"
    },
    {
      "CustomerId": 13,
      "CompanyName": "Bs Beverages",
      "ContactName": "Random Name"
    }
  ]
}
```
</Details>

#### Guidance

- `results` is empty for write operations such as `UPDATE`, `DELETE`, or `INSERT`.
- When using TypeScript, you can pass a [type parameter](/d1/worker-api/#typescript-support) to [`D1PreparedStatement::run`](#run) to return a typed result object.
- [`D1PreparedStatement::run`](#run) is functionally equivalent to `D1PreparedStatement::all`, and can be treated as an alias.
- You can choose to extract only the results you expect from the statement by simply returning the `results` property of the return object.

<Details header="Example of returning only the `results`" open={false}>
```js
return Response.json(returnValue.results);
```
```js output
[
  {
    "CustomerId": 11,
    "CompanyName": "Bs Beverages",
    "ContactName": "Victoria Ashworth"
  },
  {
    "CustomerId": 13,
    "CompanyName": "Bs Beverages",
    "ContactName": "Random Name"
  }
]
```
</Details>

### `raw()`

Runs the prepared query (or queries), and returns the results as an array of arrays. The returned results do not include metadata.

Column names are not included in the result set by default. To include column names as the first row of the result array, set `.raw({columnNames: true})`.

```js
const returnValue = await stmt.raw();
```

#### Parameters

- <code>columnNames</code>: <Type text="Object"/> <MetaInfo text="Optional"/>
  - A boolean object which includes column names as the first row of the result array.

#### Return values

- <code>Array</code>: <Type text="Array"/>
  - An array of arrays. Each sub-array represents a row.

<Details header="Example of return values" open = {false}>
```js
const someVariable = `Bs Beverages`;
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind(someVariable);
const returnValue = await stmt.raw();
return Response.json(returnValue);
```
```js output
[
  [11, "Bs Beverages",
    "Victoria Ashworth"
  ],
  [13, "Bs Beverages",
    "Random Name"
  ]
]
```

With parameter `columnNames: true`:
```js
const someVariable = `Bs Beverages`;
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind(someVariable);
const returnValue = await stmt.raw({columnNames:true});
return Response.json(returnValue)
```
```js output
[
  [
    "CustomerId",
    "CompanyName",
    "ContactName"
  ],
  [11, "Bs Beverages",
    "Victoria Ashworth"
  ],
  [13, "Bs Beverages",
    "Random Name"
  ]
]
```
</Details>

#### Guidance

- When using TypeScript, you can pass a [type parameter](/d1/worker-api/#typescript-support) to [`D1PreparedStatement::raw`](#raw) to return a typed result array.

### `first()`

Runs the prepared query (or queries), and returns the first row of the query result as an object. This does not return any metadata. Instead, it directly returns the object.

```js
const values = await stmt.first();
```

#### Parameters

- <code>columnName</code>: <Type text="String"/> <MetaInfo text="Optional"/>
  - Specify a `columnName` to return a value from a specific column in the first row of the query result.
- None.
  - Do not pass a parameter to obtain all columns from the first row.

#### Return values

- <code>firstRow</code>: <Type text="Object"/> <MetaInfo text="Optional"/>
  - An object containing the first row of the query result.
  - The return value will be further filtered to a specific attribute if `columnName` was specified.

- `null`: <Type text="null"/>
  - If the query returns no rows.

<Details header ="Example of return values" open = {false}>

Get all the columns from the first row:

```js
const someVariable = `Bs Beverages`;
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind(someVariable);
const returnValue = await stmt.first();
return Response.json(returnValue)
```
```js output
{
  "CustomerId": 11,
  "CompanyName": "Bs Beverages",
  "ContactName": "Victoria Ashworth"
}
```

Get a specific column from the first row:

```js
const someVariable = `Bs Beverages`;
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind(someVariable);
const returnValue = await stmt.first(CustomerId);
return Response.json(returnValue)
```
```js output
11
```
</Details>

#### Guidance

- If the query returns rows but `column` does not exist, then [`D1PreparedStatement::first`](#first) throws the `D1_ERROR` exception.
- [`D1PreparedStatement::first`](#first) does not alter the SQL query. To improve performance, consider appending `LIMIT 1` to your statement.
- When using TypeScript, you can pass a [type parameter](/d1/worker-api/#typescript-support) to [`D1PreparedStatement::first`](#first) to return a typed result object.

---

# Return objects

URL: https://developers.cloudflare.com/d1/worker-api/return-object/

Some D1 Worker Binding APIs return a typed object.

| D1 Worker Binding API                                                                                                          | Return object |
| ------------------------------------------------------------------------------------------------------------------------------ | ------------- |
| [`D1PreparedStatement::run`](/d1/worker-api/prepared-statements/#run), [`D1Database::batch`](/d1/worker-api/d1-database/#batch)| `D1Result`    |
| [`D1Database::exec`](/d1/worker-api/d1-database/#exec)                                                                         | `D1ExecResult`|

## `D1Result`

The methods [`D1PreparedStatement::run`](/d1/worker-api/prepared-statements/#run) and [`D1Database::batch`](/d1/worker-api/d1-database/#batch) return a typed [`D1Result`](#d1result) object for each query statement. This object contains:

- The success status
- A meta object with the internal duration of the operation in milliseconds
- The results (if applicable) as an array

```js
{
  success: boolean, // true if the operation was successful, false otherwise
  meta: {
    served_by: string // the version of Cloudflare's backend Worker that returned the result
    duration: number, // the duration of the SQL query execution only, in milliseconds
		changes: number, // the number of changes made to the database
		last_row_id: number, // the last inserted row ID, only applies when the table is defined without the `WITHOUT ROWID` option
		changed_db: boolean, // true if something on the database was changed
    size_after: number, // the size of the database after the query is successfully applied
    rows_read: number, // the number of rows read (scanned) by this query
    rows_written: number // the number of rows written by this query
  }
  results: array | null, // [] if empty, or null if it does not apply
}
```

### Example

```js
const someVariable = `Bs Beverages`;
const stmt = env.DB.prepare("SELECT * FROM Customers WHERE CompanyName = ?").bind(someVariable);
const returnValue = await stmt.run();
return Response.json(returnValue)
```
```js
{
  "success": true,
  "meta": {
    "served_by": "miniflare.db",
    "duration": 1,
    "changes": 0,
    "last_row_id": 0,
    "changed_db": false,
    "size_after": 8192,
    "rows_read": 4,
    "rows_written": 0
  },
  "results": [
    {
      "CustomerId": 11,
      "CompanyName": "Bs Beverages",
      "ContactName": "Victoria Ashworth"
    },
    {
      "CustomerId": 13,
      "CompanyName": "Bs Beverages",
      "ContactName": "Random Name"
    }
  ]
}
```

## `D1ExecResult`

The method [`D1Database::exec`](/d1/worker-api/d1-database/#exec) returns a typed [`D1ExecResult`](#d1execresult) object for each query statement. This object contains:

- The number of executed queries
- The duration of the operation in milliseconds

```js
{
	"count": number, // the number of executed queries
	"duration": number // the duration of the operation, in milliseconds
}
```

### Example

```js
const returnValue = await env.DB.exec(`SELECT * FROM Customers WHERE CompanyName = "Bs Beverages"`);
return Response.json(returnValue);
```
```js output
{
  "count": 1,
  "duration": 1
}
```

---

# Cache

URL: https://developers.cloudflare.com/data-localization/how-to/cache/

import { TabItem, Tabs } from "~/components";

In the following sections, we will give you some details about how to configure Cache with Regional Services and Customer Metadata Boundary.

## Regional Services

To configure Regional Services for hostnames [proxied](/dns/proxy-status/) through Cloudflare and ensure that [eligible assets](/cache/concepts/default-cache-behavior/) are cached only in-region, follow these steps for the dashboard or API configuration:

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select an account and zone.
2. Go to the **DNS** tab.
3. Follow these steps to [create a DNS record](/dns/manage-dns-records/how-to/create-dns-records/).
4. From the **Region** dropdown, select the region you would like to use on your domain.
5. Select **Save**.

</TabItem>

<TabItem label="API">

1. To create records with the API, use the [API POST](/api/resources/dns/subresources/records/methods/create/) command.
2. Run the [API POST](/data-localization/regional-services/get-started/#configure-regional-services-via-api) command on the hostname to create a `regional_hostnames` with a specific region.

</TabItem> </Tabs>

:::note

Take into consideration that only [Generic Global Tiered Cache](/cache/how-to/tiered-cache/#generic-global-tiered-cache) and [Custom Tiered Cache](/cache/how-to/tiered-cache/#custom-tiered-cache) respect Regional Services. [Smart Tiered Cache](/cache/how-to/tiered-cache/#smart-tiered-cache) is incompatible with Regional Services.

:::

## Customer Metadata Boundary

[Cache Analytics](/cache/performance-review/cache-analytics/), Generic Global Tiered Cache and Custom Tiered Cache are compatible with Customer Metadata Boundary. With Customer Metadata Boundary set to EU, the **Caching** > **Tiered Cache** tab in the zone dashboard will not be populated.

For more information on CDN and caching, refer to the [Cache documentation](/cache/).

---

# Cloudflare for SaaS

URL: https://developers.cloudflare.com/data-localization/how-to/cloudflare-for-saas/

import { TabItem, Tabs } from "~/components";

In the following sections, we will give you some details about how to configure Cloudflare for SaaS with Regional Services and Customer Metadata Boundary.

## Regional Services

To configure Regional Services for both hostnames [proxied](/dns/proxy-status/) through Cloudflare and the fallback origin, follow these steps for the dashboard or API configuration:

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select an account and zone.
2. Go to the **Custom Hostnames** tab.
3. Follow these steps to [configure Cloudflare for SaaS](/cloudflare-for-platforms/cloudflare-for-saas/start/getting-started/).

</TabItem>

<TabItem label="API">

1. Set the [fallback record](/api/resources/custom_hostnames/subresources/fallback_origin/methods/update/).
2. Create a [Custom Hostname](/api/resources/custom_hostnames/methods/create/).
3. Run the [API POST](/data-localization/regional-services/get-started/#configure-regional-services-via-api) command on the Custom Hostname to create a `regional_hostnames` with a specific region.

</TabItem> </Tabs>

The Regional Services functionality can be extended to Custom Hostnames and this is dependent on the target of the alias.

Consider the following example.

:::note

As a SaaS provider, I might want all of my customers to connect to the nearest data center to them and for all the processing and Cloudflare features to be applied there; however, I might have a few exceptions where I want the processing to only be done in the US.

In this case, I can just keep my fallback record with `Earth` as the processing region and have all my Custom Hostnames create a CNAME record and use the fallback record as the CNAME target. For any Custom Hostnames that need to be processed in the US, I will create a DNS record for example, `us.saasprovider.com` and set the processing region to `United States of America`. In order for the US processing region to be applied, my customers must create a CNAME record and use the `us.saasprovider.com` as the CNAME target. The origin associated with the Custom Hostname is not used to set the processing region, but instead to route the traffic to the right server.

:::

Below you can find a breakdown of the different ways that you might configure Cloudflare for SaaS and the corresponding processing regions:

- No processing region: `fallback.saasprovider.com`
- Processing region is the `US`: `us.saasprovider.com`
- User location: `UK` (closest datacenter: `LHR`)

| Test | Custom Hostname                          | Target                      | Origin                         | Location |
| ---- | ---------------------------------------- | --------------------------- | ------------------------------ | -------- |
| 1    | â€‹â€‹`regionalservices-default.example.com` | `fallback.saasprovider.com` | default (fallback)             | `LHR`    |
| 2    | `regionalservices-default2.example.com`  | `us.saasprovider.com`       | default (fallback)             | `EWR`    |
| 3    | `regionalservices-custom.example.com`    | `fallback.saasprovider.com` | `us.saasprovider.com` (custom) | `LHR`    |
| 4    | `regionalservices-custom2.example.com`   | `us.saasprovider.com`       | `us.saasprovider.com` (custom) | `EWR`    |

- In order to set a processing region for the fallback record to any of the available regions for Regional Services, create a new regional hostname entry for the fallback via a [POST](/data-localization/regional-services/get-started/#configure-regional-services-via-api) request.

- To update the existing region (for example, from `EU` to `US`), make a [PATCH](/data-localization/regional-services/get-started/#configure-regional-services-via-api) request for the fallback to update the processing region accordingly.

- To remove the regional services processing region and set it back to `Earth`, make a [DELETE](/data-localization/regional-services/get-started/#configure-regional-services-via-api) request to delete the region configuration.

## Customer Metadata Boundary

Cloudflare for SaaS [Analytics](/cloudflare-for-platforms/cloudflare-for-saas/hostname-analytics/) based on [HTTP requests](/logs/reference/log-fields/zone/http_requests/) are fully supported by Customer Metadata Boundary.

Refer to [Cloudflare for SaaS documentation](/cloudflare-for-platforms/cloudflare-for-saas/) for more information.

---

# Durable Objects

URL: https://developers.cloudflare.com/data-localization/how-to/durable-objects/

In the following sections, we will give you some details about how to configure Durable Objects with Regional Services and Customer Metadata Boundary.

## Regional Services

To configure Regional Services for hostnames [proxied](/dns/proxy-status/) through Cloudflare and ensure that processing of a Durable Object (DO) occurs only in-region, follow these steps:

1. Follow the steps in the Durable Objects [Get Started](/durable-objects/get-started/) guide.
2. [Restrict Durable Objects to a jurisdiction](/durable-objects/reference/data-location/#restrict-durable-objects-to-a-jurisdiction), in order to control where the DO itself runs and persists data, by creating a jurisidictional subnamespace in your Workerâ€™s code.
3. Follow the [Workers guide](/data-localization/how-to/workers/#regional-services) to configure a custom domain with Regional Services, in order to control the regions from which Cloudflare responds to requests.

## Customer Metadata Boundary

DO Logs and Analytics are not available outside the US region when using Customer Metadata Boundary. With Customer Metadata Boundary set to `EU`, **Workers & Pages** > **Workers** > **Metrics** tab related to DO in the zone dashboard will not be populated.

Refer to the [Durable Objects documentation](/durable-objects/) for more information.

---

# Configuration guides

URL: https://developers.cloudflare.com/data-localization/how-to/

import { DirectoryListing } from "~/components";

Learn how to use Cloudflare products with the Data Localization Suite.

<DirectoryListing />

## Verify Regional Services behavior

In order to verify that Regional Services is working, customers can confirm the behavior by executing one of the following `curl` commands on a regionalized hostname:

```bash
curl -X GET -I https://<HOSTNAME>/ 2>&1 | grep cf-ray
```

```bash
curl -s https://<HOSTNAME>/cdn-cgi/trace | grep "colo="
```

The first command will return a three-letter IATA code in the [Cf-Ray](/fundamentals/reference/http-headers/#cf-ray) header, indicating the Cloudflare data center location of processing and/or TLS termination. The second command will directly return the three-letter IATA code.

For example, when a hostname is configured to use the region European Union (EU), the three-letter IATA code will always return a data center inside of the EU.

---

# Load Balancing

URL: https://developers.cloudflare.com/data-localization/how-to/load-balancing/

import { TabItem, Tabs } from "~/components";

In the following sections, we will give you some details about how to configure Load Balancing with Regional Services and Customer Metadata Boundary.

## Regional Services

You can load balance traffic at different levels of the networking stack depending on the [proxy mode](/load-balancing/understand-basics/proxy-modes/): Layer 7 (`HTTP/S`) and Layer 4 (`TCP`) are supported; however, `DNS-only` is not supported, as it is not [proxied](/dns/proxy-status/).

To configure Regional Services for hostnames [proxied](/dns/proxy-status/) through Cloudflare and ensure that the Load Balancer is available only in-region, follow these steps for the dashboard or API configuration:

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select an account and zone.
2. Go to the **Load Balancing** tab.
3. Follow the steps to [create a load balancer](/load-balancing/load-balancers/create-load-balancer/#create-a-load-balancer).
4. From the **Data Localization** dropdown, select the region you would like to use on your domain.
5. Select **Next** and continue with the regular setup.
6. Select **Save**.

</TabItem>

<TabItem label="API">

1. Follow the instructions outlined to [create a load balancer](/load-balancing/load-balancers/create-load-balancer/#create-a-load-balancer) via API.
2. Run the [API POST](/data-localization/regional-services/get-started/#configure-regional-services-via-api) command on the Load Balancer hostname to create a `regional_hostnames` with a specific region.

</TabItem> </Tabs>

## Customer Metadata Boundary

[Load Balancing Analytics](/load-balancing/reference/load-balancing-analytics/) are not available outside the US region when using Customer Metadata Boundary.

With Customer Metadata Boundary set to `EU`, **Traffic** > **Load Balancing Analytics** > **Overview and Latency** tab in the zone dashboard will not be populated.

Refer to the [Load Balancing documentation](/load-balancing/) for more information.

---

# Pages

URL: https://developers.cloudflare.com/data-localization/how-to/pages/

import { TabItem, Tabs } from "~/components";

In the following sections, we will give you some details about how to configure Pages with Regional Services and Customer Metadata Boundary.

## Regional Services

To configure Regional Services for hostnames [proxied](/dns/proxy-status/) through Cloudflare and ensure that processing of a Pages project occurs only in-region, follow these steps for the dashboard or API configuration:

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select an account.
2. Go to the **Workers & Pages** tab.
3. Select your Pages project.
4. Follow these steps to [create a Custom Domain](/pages/configuration/custom-domains/).
5. Go to the **DNS** of the zone you configured the Custom Domain for.
6. From the **Region** dropdown, select the region you would like to use on your domain.
7. Select **Save**.

</TabItem>

<TabItem label="API">

1. Use the [API POST](/api/resources/pages/subresources/projects/subresources/domains/methods/create/) command to add a Custom Domain to a Pages project.
2. Run the [API POST](/data-localization/regional-services/get-started/#configure-regional-services-via-api) command on the Pages Custom Domain to create a `regional_hostnames` with a specific Region.

</TabItem> </Tabs>

:::note

Regional Services only applies to the Custom Domain configured for a Pages project.

:::

## Customer Metadata Boundary

Customer Metadata Boundary applies to the Custom Domain configured, as well as the [\*.pages.dev](/pages/configuration/preview-deployments/) subdomain. You also have the option to disable access to the [`.dev` domain](/pages/configuration/custom-domains/#disable-access-to-pagesdev-subdomain).

For information on available Analytics and Metrics, review the [Cloudflare product compatibility](/data-localization/compatibility/) page.

It is recommended not to store any Personally Identifiable Information (PII) in the Pages project's static assets.

:::note

Page [Functions](/pages/functions/) are implemented as Cloudflare Workers. Refer to the Workers section for more information.

:::

Refer to the [Pages documentation](/pages) for more information.

---

# R2 Object Storage

URL: https://developers.cloudflare.com/data-localization/how-to/r2/

import { Details } from "~/components"

In the following sections, we will give you some details about how to configure R2 with Regional Services and Customer Metadata Boundary.

## Regional Services

To configure Regional Services for hostnames [proxied](/dns/proxy-status/) through Cloudflare and ensure that processing of requesting objects from a [R2 Bucket](/r2/buckets/) occurs only in-region, follow these steps:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select an account.
2. Go to the **R2** tab.
3. Follow the steps to [create a Bucket](/r2/buckets/create-buckets/).
4. [Connect a bucket to a custom domain](/r2/buckets/public-buckets/#connect-a-bucket-to-a-custom-domain).
5. Run the [API POST](/data-localization/regional-services/get-started/#configure-regional-services-via-api) command on the configured bucket custom domain to create a `regional_hostnames` with a specific region.

Regional Services only applies to the custom domain configured for an R2 Bucket.

### Send logs to R2 via S3-Compatible endpoint

The following instructions will show you how to set up a Logpush job using an S3-compatible endpoint to store logs in an R2 bucket in the jurisdiction of your choice.

1. Create an [R2 bucket](/r2/get-started/) in your Cloudflare account and select the [jurisdiction](/r2/reference/data-location/#set-jurisdiction-via-the-cloudflare-dashboard) you would like to use.

2. Generate an API token for your R2 bucket. You have the following two options:

<Details header="Generate a token for a specific bucket (recommended)">

Go to the R2 section of your Cloudflare dashboard and select **Manage R2 API Tokens** to generate a token directly tied to your specific bucket. You can follow the instructions in the [Authentication](/r2/api/s3/tokens/) section.

</Details>

<Details header="Generate a token for all buckets">

You can generate a API token in **Manage Account** > **Account API Tokens** or you can create a user-specific token:

1. Go to **My Profile** > **API Tokens**
2. Select **Create Token** > **Create Custom Token**
3. Choose **Account** > **Workers R2 Storage** > **Edit** to set permissions.
4. To test your token, copy the `curl` command and paste it into a terminal.

```bash
curl "https://api.cloudflare.com/client/v4/user/tokens/verify" \
--header "Authorization: Bearer <API_TOKEN>"
```

The result:

```json
{
  "result": {
    "id": "325xxxxcd",
    "status": "active"
  },
  "success": true,
  "errors": [],
  "messages": [
    {
      "code": 10000,
      "message": "This API Token is valid and active",
      "type": null
    }
  ]
}
```

5. Generate a SHA-256 hash of the token:

```bash
echo -n "<token>" | shasum -a 256
```

This command will output a hash similar to `dxxxx391b`.

</Details>

3. Set up a Logpush destination using [S3-compatible endpoint](/logs/get-started/enable-destinations/s3-compatible-endpoints/) and fill in the following fields:

- **Bucket**: Enter the name of the R2 bucket you created with the jurisdiction you would like to use.
- **Path** (optional): If you want, you can specify a folder path to organize your logs.
- **Endpoint URL**: Provide the S3 API endpoint for your bucket in the format `<account-id>.eu.r2.cloudflarestorage.com`. Do not include the bucket name, as it was set in the first field.
- **Bucket Region**: For instance, use `WEUR` to specify the EU region.
- **Access Key ID**: Enter the Token ID created previously (`325xxxxcd`).
- **Secret Access Key**: Use the SHA-256 hash of the token (`dxxxx391b`).

Complete the configuration by selecting the fields you want to push to your R2 bucket.

## Customer Metadata Boundary

With Customer Metadata Boundary set to `EU`, **R2** > **Bucket** > [**Metrics**](/r2/platform/metrics-analytics/) tab in the account dashboard will be populated.

:::note


Additionally, customers can create R2 buckets with [jurisdictional restrictions set to EU](/r2/reference/data-location/#jurisdictional-restrictions). In this case, we recommend [using jurisdictions with the S3 API](/r2/reference/data-location/#using-jurisdictions-with-the-s3-api).


:::

Refer to the [R2 documentation](/r2/) for more information.

---

# Workers

URL: https://developers.cloudflare.com/data-localization/how-to/workers/

In the following sections, we will give you some details about how to configure Workers with Regional Services and Customer Metadata Boundary.

## Regional Services

To configure Regional Services for hostnames [proxied](/dns/proxy-status/) through Cloudflare and ensure that processing of a Workers project occurs only in-region, follow these steps:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select an account.
2. Go to the **Workers & Pages** tab.
3. Select your Workers project.
4. Follow the steps to [create a custom domain](/workers/configuration/routing/custom-domains/).
5. Run the [API POST](/data-localization/regional-services/get-started/#configure-regional-services-via-api) command on the configured Workers Custom Domain to create a `regional_hostnames` with a specific region.

### Caveats

Regional Services only applies to the custom domain configured for a Workers project. Therefore, it will run only in-region Cloudflare locations.

Regional Services does not apply to [subrequests](/workers/platform/limits/#subrequests).

Regional Services does not apply to other Worker triggers, like [Queues](/queues/) or [Cron Triggers](/workers/configuration/cron-triggers/).

## Customer Metadata Boundary

Customer Metadata Boundary applies to the custom domain configured, as well as the [`*.workers.dev`](/workers/configuration/routing/workers-dev/) subdomain.

Workers [Metrics and Analytics](/workers/observability/metrics-and-analytics/) are not available outside the US region when using Customer Metadata Boundary.

With Customer Metadata Boundary set to `EU`, **Workers & Pages** > **Workers** > **Metrics** tab the zone dashboard will not be populated.

:::note


It is recommended to not store any Personally Identifiable Information (PII) in the Workers code. If sensitive information needs to be used, it is recommended to use [Secrets](/workers/configuration/secrets/).


:::

Refer to the [Workers documentation](/workers/) for more information.

---

# Zero Trust

URL: https://developers.cloudflare.com/data-localization/how-to/zero-trust/

import { Render } from "~/components";

In the following sections, we will give you some details about how different Zero Trust products can be used with the Data Localization Suite.

## Gateway

Regional Services can be used with Gateway in all [supported regions](/data-localization/region-support/). Be aware that Regional Services only apply when using the WARP client in Gateway with WARP mode.

### Egress policies

Enterprise customers can purchase a [dedicated egress IP](/cloudflare-one/policies/gateway/egress-policies/dedicated-egress-ips/) (IPv4 and IPv6) or range of IPs geolocated to one or more Cloudflare network locations.
This allows your egress traffic to geolocate to the city selected in your [egress policies](/cloudflare-one/policies/gateway/egress-policies/).

### HTTP policies

As part of Regional Services, Cloudflare Gateway will only perform [TLS decryption](/cloudflare-one/policies/gateway/http-policies/tls-decryption/) when using the [WARP client](/cloudflare-one/connections/connect-devices/warp/) (in default [Gateway with WARP mode](/cloudflare-one/connections/connect-devices/warp/configure-warp/warp-modes/)).

{/* TODO: Reintroduce */}
{/* <Render file="gateway/disable-udp" product="cloudflare-one" /> */}

#### Data Loss Prevention (DLP)

You are able to [log the payload of matched DLP rules](/cloudflare-one/policies/data-loss-prevention/dlp-policies/logging-options/#log-the-payload-of-matched-rules) and encrypt them with your public key so that only you can examine them later.

[Cloudflare cannot decrypt encrypted payloads](/cloudflare-one/policies/data-loss-prevention/dlp-policies/logging-options/#data-privacy).

### Network policies

You are able to [configure SSH proxy and command logs](/cloudflare-one/policies/gateway/network-policies/ssh-logging/). Generate a Hybrid Public Key Encryption (HPKE) key pair and upload the public key `sshkey.pub` to your dashboard. All proxied SSH commands are immediately encrypted using this public key. The matching private key â€“ which is in your possession â€“ is required to view logs.

### DNS policies

Regional Services controls where Cloudflare decrypts traffic; because most DNS traffic is not encrypted, Gateway DNS cannot be regionalized using Regional Services.

Refer to the [WARP Settings](/data-localization/how-to/zero-trust/#warp-settings) section below for more information.

### Custom certificates

You can [bring your own certificate](/cloudflare-one/connections/connect-devices/user-side-certificates/custom-certificate/) to Gateway but these cannot yet be restricted to a specific region.

### Logs and Analytics

By default, Cloudflare will store and deliver logs from data centers across our global network. To maintain regional control over your data, you can use [Customer Metadata Boundary](/data-localization/metadata-boundary/) and restrict data storage to a specific geographic region. For more information refer to the section about [Logpush datasets supported](/data-localization/metadata-boundary/logpush-datasets/).

Customers also have the option to reduce the logs that Cloudflare stores:

- You can [exclude PII from logs](/cloudflare-one/insights/logs/gateway-logs/manage-pii/)
- You can [disable logging, or only log blocked requests](/cloudflare-one/insights/logs/gateway-logs/#selective-logging).

## Access

To ensure that all reverse proxy requests for applications protected by Cloudflare Access will only occur in FedRAMP-compliant data centers, you should use [Regional Services](/data-localization/regional-services/get-started/) with the region set to FedRAMP.

## Cloudflare Tunnel

You can [configure Cloudflare Tunnel](/cloudflare-one/connections/connect-networks/configure-tunnels/cloudflared-parameters/run-parameters/#region) to only connect to data centers within the United States, regardless of where the software was deployed.

## WARP settings

### Local Domain Fallback

You can use the WARP setting [Local Domain Fallback](/cloudflare-one/connections/connect-devices/warp/configure-warp/route-traffic/local-domains/) in order to use a private DNS resolver, which you can manage yourself.

### Split Tunnels

[Split Tunnels](/cloudflare-one/connections/connect-devices/warp/configure-warp/route-traffic/split-tunnels/) allow you to decide which IP addresses/ranges and/or domains are routed through or excluded from Cloudflare.

:::caution

Gateway policies will not apply for excluded traffic.
:::

---

# Get started

URL: https://developers.cloudflare.com/data-localization/regional-services/get-started/

import { Details } from "~/components"

:::note


Interested customers need to contact their account team to enable DNS Regionalisation.


:::

You can use Regional Services through the dashboard or via API.

## Configure Regional Services in the dashboard

To use Regional Services, you need to first create a DNS record in the dashboard:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select an account and domain.
2. Go to the **DNS** tab.
3. Follow these steps to [create a DNS record](/dns/manage-dns-records/how-to/create-dns-records/).
4. From the **Region** dropdown, select the region you would like to use on your domain. This value will be applied to all DNS records on the same hostname. This means that if you have two DNS records of the same hostname and change the region for one of them, both records will have the same region.

:::note


Some regions may not appear on the dropdown because newly announced regions mentioned in the [blog post](https://blog.cloudflare.com/expanding-regional-services-configuration-flexibility-for-customers) are subject to approval by Cloudflare's internal team. For more information and entitlement reach out to your account team.


:::

Refer to the table below for the complete list of available regions and their definitions.

| Region                             | Definition                                                                                                                                                                                                                                         |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Australia                          | Cloudflare will only use data centers that are physically located within Australia to decrypt and service HTTPS traffic.                                                                                                                           |
| Canada                             | Cloudflare will only use data centers that are physically located within Canada to decrypt and service HTTPS traffic.                                                                                                                              |
| European Union                     | Cloudflare will only use data centers that are physically located within the European Union. For more details, refer to the [list of European Union countries](https://european-union.europa.eu/principles-countries-history/country-profiles_en). |
| India                              | Cloudflare will only use data centers that are physically located within India to decrypt and service HTTPS traffic.                                                                                                                               |
| Japan                              | Cloudflare will only use data centers that are physically located within Japan to decrypt and service HTTPS traffic.                                                                                                                               |
| United States of America           | Cloudflare will only use data centers that are physically located within the United States of America to decrypt and service HTTPS traffic.                                                                                                        |
| FedRAMP Compliant                  | Cloudflare will only use data centers that are FedRAMP certified.                                                                                                                                                                                  |
| ISO 27001 Certified European Union | Cloudflare will only use data centers that are physically located within the [European Union](https://european-union.europa.eu/principles-countries-history/country-profiles_en) and that adhere to the ISO 27001 certification.                   |
| Germany                            | Cloudflare will only use data centers that are physically located within Germany to decrypt and service HTTPS traffic.                                                                                                                             |
| Singapore                          | Cloudflare will only use data centers that are physically located within Singapore to decrypt and service HTTPS traffic.                                                                                                                           |
| South Korea                        | Cloudflare will only use data centers that are physically located within South Korea to decrypt and service HTTPS traffic.                                                                                                                         |
| Austria                            | Cloudflare will only use data centers that are physically located within Austria to decrypt and service HTTPS traffic.                                                                                                                             |
| Brazil                             | Cloudflare will only use data centers that are physically located within Brazil to decrypt and service HTTPS traffic.                                                                                                                              |
| Cloudflare Green Energy            | Cloudflare will only use data centers that are committed to powering their operations with renewable energy.                                                                                                                                       |
| Exclusive of Hong Kong and Macau   | Cloudflare will only use data centers that are NOT physically located within Hong Kong and Macau to decrypt and service HTTPS traffic.                                                                                                             |
| Exclusive of Russia and Belarus    | Cloudflare will only use data centers that are NOT physically located within Russia and Belarus to decrypt and service HTTPS traffic.                                                                                                              |
| France                             | Cloudflare will only use data centers that are physically located within Metropolitan France (the European territory of France) to decrypt and service HTTPS traffic.                                                                              |
| Hong Kong                          | Cloudflare will only use data centers that are physically located within Hong Kong to decrypt and service HTTPS traffic.                                                                                                                           |
| Italy                              | Cloudflare will only use data centers that are physically located within Italy to decrypt and service HTTPS traffic.                                                                                                                               |
| NATO                               | Cloudflare will only use data centers that are physically located within North Atlantic Treaty Organization (NATO) countries. For more details, refer to the [list of NATO countries](https://www.nato.int/nato-welcome/).                         |
| Netherlands                        | Cloudflare will only use data centers that are physically located within the Netherlands to decrypt and service HTTPS traffic.                                                                                                                     |
| Russia                             | Cloudflare will only use data centers that are physically located within Russia to decrypt and service HTTPS traffic.                                                                                                                              |
| Saudi Arabia                       | Cloudflare will only use data centers that are physically located within Saudi Arabia to decrypt and service HTTPS traffic.                                                                                                                        |
| South Africa                       | Cloudflare will only use data centers that are physically located within South Africa to decrypt and service HTTPS traffic.                                                                                                                        |
| Spain                              | Cloudflare will only use data centers that are physically located within Spain to decrypt and service HTTPS traffic.                                                                                                                               |
| Switzerland                        | Cloudflare will only use data centers that are physically located within Switzerland to decrypt and service HTTPS traffic.                                                                                                                         |
| Taiwan                             | Cloudflare will only use data centers that are physically located within Taiwan to decrypt and service HTTPS traffic.                                                                                                                              |
| US State of California             | Cloudflare will only use data centers that are physically located within the US State of California to decrypt and service HTTPS traffic.                                                                                                          |
| US State of Florida                | Cloudflare will only use data centers that are physically located within the US State of Florida to decrypt and service HTTPS traffic.                                                                                                             |
| US State of Texas                  | Cloudflare will only use data centers that are physically located within the US State of Texas to decrypt and service HTTPS traffic.                                                                                                               |

## Configure Regional Services via API

You can also use Regional Services via API.

Currently, only SuperAdmins and Admin roles can edit DLS configurations. Use the Zone-level **DNS: Read/Write** API permission for the `/addressing/` endpoint to read or write Regional Services configurations.

These are some examples of API requests.

<Details header="List all the available regions">

```bash title="Request"
curl "https://api.cloudflare.com/client/v4/accounts/{account_id}/addressing/regional_hostnames/regions" \
--header "Authorization: Bearer <API_TOKEN>" | jq .
```

```json title="Response"
{
  "success": true,
  "errors": [],
  "result": [
    {
      "key": "ca",
      "label": "Canada"
    },
    {
      "key": "eu",
      "label": "Europe"
    }
  ],
  "messages": []
}
```


</Details>


<Details header="Create a new regional hostname entry">

```bash title="Request"
curl "https://api.cloudflare.com/client/v4/zones/{zone_id}/addressing/regional_hostnames" \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{"hostname": "ca.regional.ipam.rocks", "region_key": "ca"}' | jq .
```

```json title="Response"
{
  "success": true,
  "errors": [],
  "result": {
    "hostname": "ca.regional.ipam.rocks",
    "region_key": "ca",
    "created_on": "2023-01-13T23:59:45.276558Z"
  },
  "messages": []
}
```


</Details>


<Details header="List all regional hostnames for a zone or get a specific one">

```bash title="Request"
curl "https://api.cloudflare.com/client/v4/zones/{zone_id}/addressing/regional_hostnames" \
--header "Authorization: Bearer <API_TOKEN>" | jq .
```

```json title="Response"
{
  "success": true,
  "errors": [],
  "result": [
    {
      "hostname": "ca.regional.ipam.rocks",
      "region_key": "ca",
      "created_on": "2023-01-14T00:47:57.060267Z"
    }
  ],
  "messages": []
}
```


</Details>


<Details header="List all regional hostnames for a specific zone">

```bash title="Request"
curl "https://api.cloudflare.com/client/v4/zones/{zone_id}/addressing/regional_hostnames/ca.regional.ipam.rocks" \
--header "Authorization: Bearer <API_TOKEN>" | jq .
```

```json title="Response"
{
  "success": true,
  "errors": [],
  "result": {
    "hostname": "ca.regional.ipam.rocks",
    "region_key": "ca",
    "created_on": "2023-01-13T23:59:45.276558Z"
  },
  "messages": []
}
```


</Details>


<Details header="Patch the region for a specific hostname">

```bash title="Request"
curl --request PATCH \
"https://api.cloudflare.com/client/v4/zones/{zone_id}/addressing/regional_hostnames/ca.regional.ipam.rocks" \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{"region_key": "eu"}' | jq .
```

```json title="Response"
{
  "success": true,
  "errors": [],
  "result": {
    "hostname": "ca.regional.ipam.rocks",
    "region_key": "eu",
    "created_on": "2023-01-13T23:59:45.276558Z"
  },
  "messages": []
}
```


</Details>


<Details header="Delete the region configuration">

```bash title="Request"
curl --request DELETE \
"https://api.cloudflare.com/client/v4/zones/{zone_id}/addressing/regional_hostnames/ca.regional.ipam.rocks" \
--header "Authorization: Bearer <API_TOKEN>" | jq .
```

```json title="Response"
{
  "success": true,
  "errors": [],
  "result": null,
  "messages": []
}
```


</Details>

## Terraform support

You can also configure Regional Services using Terraform. For more details, refer to the [`cloudflare_regional_hostname` resource](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/regional_hostname) in the Terraform documentation.

---

# Default HTTP Privacy

URL: https://developers.cloudflare.com/data-localization/regional-services/http-requests/

Cloudflare runs one of the largest global anycast networks in the world, with all current data center locations accessible on the [network map](https://www.cloudflare.com/network/).

Within the Cloudflare data centers, and between the Cloudflare network and a customer's origin, traffic is encrypted during transit. Customers have the flexibility to select which [encryption mode](/ssl/origin-configuration/ssl-modes/) and which [Cipher Suites](/ssl/edge-certificates/additional-options/cipher-suites/) they want to use.

Additionally, all request and response processing within a Cloudflare data center occurs in memory, with machine inspection used to prevent human access. Nothing is written to disk except for eligible content for caching or Cache Rules configured by the customer. Moreover, all cache disks are encrypted at rest.

![HTTP requests flow](~/assets/images/data-localization/http-requests-flow.png)

At a high level, when an end user's device connects to any Cloudflare data center, the request is processed in the following way:

1. Certain types of requests that can be used for cyber attacks are immediately dropped based on the addressing information (layer 3 / network layer).

2. Next, the encrypted request is decrypted and inspected using the customer's chosen business logic, for example, the products Configuration Rules, WAF Custom Rules, Rate Limiting Rules, following the [traffic sequence](https://blog.cloudflare.com/traffic-sequence-which-product-runs-first/) and phases. This process enables the detection and prevention of a variety of different types of cyber attacks and malicious traffic, including layer 7 / application layer DDoS attacks, automated bot traffic, credential stuffing, and SQL injection, among others.

3. The inspected request is then passed to the cache module. If the cache can fulfill the request with a cached copy of the content, it does so; if not, it forwards the request to the customer's origin server. Traffic between the Cloudflare data center and the origin server is encrypted, unless the customer decides to use a different encryption mode.

4. When the response comes from the customer's origin server, any static and eligible content is cached onto encrypted disks. The response then goes back through the business logic to the user across the Internet.

By default, Cloudflare performs TLS termination globally in every data center, where the Internet end user connects to a website or application behind Cloudflare. However, customers can configure Regional Services to specify in which regions the processing and TLS termination occurs.

---

# Regional Services

URL: https://developers.cloudflare.com/data-localization/regional-services/

import { GlossaryTooltip } from "~/components"

Regional Services gives you the ability to accommodate regional restrictions by choosing which subset of data centers decrypt and service HTTPS traffic.

Regional Services proceeds and processes traffic within certain regions for customers who have to meet regional compliance or have preferences for maintaining regional control over their data. Examples of use cases could be a customer that needs to accommodate regional restrictions like [GDPR](https://www.cloudflare.com/trust-hub/gdpr/) (General Data Protection Regulation), or customers that are bound by agreement with their own customers that include geographic restrictions on data flows or data processing.

With Regional Services, TLS is only terminated inside the configured region. For example, if a hostname is configured to regionalize to the European Union (EU), any HTTPS request from the United States (US) will route to the EU.

## Global traffic management

Regional Services globally ingests traffic, implementing [L3/L4 DDoS mitigations](/ddos-protection/about/attack-coverage/). Meanwhile, security, performance, and reliability functions are serviced at only in-region Cloudflare locations.

Regional Services ensures that all edge application services operate within the selected region. This includes (the following list is not exhaustive):

* Storing and retrieving content from Cache.
* Blocking malicious HTTP payloads with the Web Application Firewall (WAF).
* Detecting and blocking suspicious activity with Bot Management.
* Running Cloudflare Workers scripts.
* Load Balancing traffic to the best origin servers (or other <GlossaryTooltip term="endpoint">endpoints</GlossaryTooltip>).

## Request flow example

The following diagram is a high-level example of the flow of a request coming from an end user located within the US connecting to a website using Cloudflare Regional Services set to EU.

<br/>

```mermaid
sequenceDiagram
    participant User in US as End user in US
    participant CloudflarePoPNYC as Closest data center <br> in US
    participant CloudflarePoPDUB as Data center in EU
    participant EUOriginServer as Origin Server

    User in US->>CloudflarePoPNYC: TCP connection
    Note right of User in US: TLS encryption
    Note left of CloudflarePoPNYC: TCP connection<br> (no TLS unwrapping)
    Note right of CloudflarePoPNYC: L3 DDoS protection
    CloudflarePoPNYC-->>CloudflarePoPDUB: Forwards<br> encrypted request
    Note right of CloudflarePoPDUB: TLS termination (decryption)
    Note right of CloudflarePoPDUB: Applies security<br> and performance features<br> (for example, WAF, Configuration Rules, <br>Load Balancing)
    Note right of CloudflarePoPDUB: TLS encryption
    CloudflarePoPDUB-->>EUOriginServer: Requests content
    EUOriginServer-->>CloudflarePoPDUB: Response content
    Note right of CloudflarePoPDUB: TLS termination (decryption)
    Note right of CloudflarePoPDUB: Caches eligible static content<br> (on encrypted disks)
    Note right of CloudflarePoPDUB: TLS encryption
    CloudflarePoPDUB->>User in US: Forwards response with content
```

<br/>

## Additional information

For more details about the products that are compatible with Regional Services, refer to the [Cloudflare product compatibility](/data-localization/compatibility/) page. If you have purchased these products as part of your Enterprise subscription plan, Cloudflare will only terminate TLS connections for these products in the geographic region you have configured for Regional Services.

---

# FAQs

URL: https://developers.cloudflare.com/data-localization/metadata-boundary/faq/

## What data is covered by the Customer Metadata Boundary?

Nearly all end user metadata is covered by the Customer Metadata Boundary. This includes all of the end user data for which Cloudflare is a processor, as defined in the [Cloudflare Privacy Policy](https://www.cloudflare.com/privacypolicy/). Cloudflare is a data processor of Customer Logs, which are defined as end user logs that we make available to our customers via the dashboard or other online interfaces. End users are those who access or use our customers' domains, networks, websites, application programming interfaces, and applications.

Specific examples of this data include all of the analytics in our dashboard and APIs on requests, responses, and security products associated and all of the logs received through Logpush.

## What data is not covered by the Customer Metadata Boundary?

Some of the data for which Cloudflare is a controller, as defined in the [Cloudflare Privacy Policy](https://www.cloudflare.com/privacypolicy/).

Some examples:

- Customer account data (for example, name and billing information).
- Customer configuration data (for example, the content of WAF custom rules).
- Metadata that is â€œoperationalâ€ in nature â€” data needed for Cloudflare to properly operate our network. This includes metadata such as:
  - System data generated for debugging (for example, application logs from internal systems, core dumps).
  - Networking flow data (for example, sFlow from our routers), including data on DDoS attacks.

## Who can use the Customer Metadata Boundary?

Currently, this is available for Enterprise customers as part of the Data Localization Suite.

The Customer Metadata Boundary is for customers who want to limit personal data transfer outside the EU or the US (depending on the customer's selected region). These customers should already be using Regional Services, which ensures that traffic content is only ever decrypted within the geographic region specified by the customer.

## What are the analytics products available for Metadata Boundary?

HTTP and Firewall analytics are available.

At the moment, there are no analytics available for Workers, DNS, Network Analytics, Load Balancing and Rate Limiting. Additionally, there are no dashboard logs or analytics for [Gateway](/cloudflare-one/insights/logs/gateway-logs/#limitations). Enterprise users can still export Gateway logs via [Logpush](/cloudflare-one/insights/logs/logpush/).

---

# Get started

URL: https://developers.cloudflare.com/data-localization/metadata-boundary/get-started/

import { Details } from "~/components"

You can configure the Metadata Boundary to select the region where your logs and analytics are stored via API or dashboard.

Currently, this can only be applied at the account-level. If you only want the Metadata Boundary to be applied on a portion of zones beneath the same account, you will have to [move the rest of zones to a new account](/fundamentals/setup/manage-domains/move-domain/).

## Configure Customer Metadata Boundary in the dashboard

To configure Customer Metadata Boundary in the dashboard:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Go to **Manage Account** > **Configurations**.
3. In **Customer Metadata Boundary**, select the region you want to use. You have the option to select `EU` or `US`. If you want to select both regions, select `Global` instead.

## Configure Customer Metadata Boundary via API

You can also configure Customer Metadata Boundary via API.

Currently, only SuperAdmins and Admin roles can edit DLS configurations. Use the **Account-level Logs:Read/Write** API permissions for the `/logs/control/cmb` endpoint to read/write Customer Metadata Boundary configurations.

These are some examples of API requests.


<Details header="Get current regions">

Here is an example request using cURL to get current regions (if any):

```bash
curl -s -D "/dev/stderr" https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/logs/control/cmb/config -X GET \
-H "X-Auth-Email: <EMAIL>" \
-H "X-Auth-Key: <KEY>" \
| jq '.'
```


</Details>


<Details header="Setting regions">

Here is an example request using cURL to set regions:

```bash
curl -s -D "/dev/stderr" https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/logs/control/cmb/config -X POST -d '
{
    "regions": "eu"
}
' \
-H "X-Auth-Email: <EMAIL>" \
-H "X-Auth-Key: <KEY>" \
| jq '.'

```

This will overwrite any previous regions.
Change will be in effect after several minutes.


</Details>


<Details header="Delete regions">

Here is an example request using cURL to delete regions:

```bash
curl -s -D "/dev/stderr" https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/logs/control/cmb/config -X DELETE \
-H "X-Auth-Email: <EMAIL>" \
-H "X-Auth-Key: <KEY>" \
| jq '.'
```


</Details>

## View or change settings

To view or change your Customer Metadata Boundary setting:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to **Manage Account** > **Configurations** > **Preferences**.
3. Locate the **Customer Metadata Boundary** section.

---

# GraphQL datasets

URL: https://developers.cloudflare.com/data-localization/metadata-boundary/graphql-datasets/

The table below shows a non-exhaustive list of GraphQL Analytics API fields that respect CMB configuration and are available in both the US and the EU or only in the US.



 Suite/Category | Product | GraphQL Analytics API Field(s) supported in |
 --- | --- | --- |
 | Application Performance | Caching/CDN | US and EU <br/> `httpRequestsAdaptive` <br/> `httpRequestsAdaptiveGroups` <br/> `httpRequestsOverviewAdaptiveGroups` <br/> US only <br/> `httpRequests1mGroups` <br/> `httpRequests1hGroups` <br/> `httpRequests1dGroups` |
 | Cache Reserve | |  US and EU <br/> `cacheReserveOperationsAdaptiveGroups` <br/> `cacheReserveRequestsAdaptiveGroups` <br/> `cacheReserveStorageAdaptiveGroups` |
 | DNS | | US only <br/> `dnsAnalyticsAdaptive` <br/> `dnsAnalyticsAdaptiveGroups` |
 | Image Resizing | | US only <br/> `imageResizingRequests1mGroups` <br/> `imagesRequestsAdaptiveGroups` <br/> `imagesUniqueTransformations` |
 | Load Balancing | | US only <br/> [`loadBalancingRequestsAdaptive`](/load-balancing/reference/load-balancing-analytics/#graphql-analytics) <br/> [`loadBalancingRequestsAdaptiveGroups`](/load-balancing/reference/load-balancing-analytics/#graphql-analytics) <br/> `healthCheckEventsAdaptive` <br/> `healthCheckEventsAdaptiveGroups` |
 | Stream Delivery | Same as Caching/CDN |
 | Tiered Caching | | US and EU <br/> Only the field `upperTierColoName` part of `httpRequestsAdaptive` and `httpRequestsAdaptiveGroups` |
 | Secondary DNS | Same as DNS |
 | Waiting Room | | US and EU <br/> [`waitingRoomAnalyticsAdaptive`](/waiting-room/waiting-room-analytics/#graphql-analytics) <br/> [`waitingRoomAnalyticsAdaptiveGroups`](/waiting-room/waiting-room-analytics/#graphql-analytics) |
 | Web Analytics / Real User Monitoring (RUM) | | US only <br/> `rumWebVitalsEventsAdaptive` <br/> `rumWebVitalsEventsAdaptiveGroups` <br/> `rumPerformanceEventsAdaptiveGroups` <br/> `rumPageloadEventsAdaptiveGroups` |
 | Zaraz | | US and EU <br/>`zarazActionsAdaptiveGroups` <br/> `zarazTrackAdaptiveGroups` <br/> `zarazTriggersAdaptiveGroups` |  |
 | Application Security | Advanced Certificate Manager | US and EU <br/> Only the fields `clientSSLProtocol` and `ja3Hash` part of `httpRequestsAdaptive` and `httpRequestsAdaptiveGroups` | |
 | Advanced DDoS Protection | | US and EU <br/> [`dosdAttackAnalyticsGroups`](/analytics/graphql-api/migration-guides/network-analytics-v2/node-reference/) <br/> [`dosdNetworkAnalyticsAdaptiveGroups`](/analytics/graphql-api/migration-guides/network-analytics-v2/node-reference/) <br/> [`flowtrackdNetworkAnalyticsAdaptiveGroups`](/analytics/graphql-api/migration-guides/network-analytics-v2/node-reference/) <br/> `advancedTcpProtectionNetworkAnalyticsAdaptiveGroups` <br/> `advancedDnsProtectionNetworkAnalyticsAdaptiveGroups` |
 | API Shield / API Gateway | | US and EU <br/> [`apiGatewayGraphqlQueryAnalyticsGroups`](/api-shield/security/graphql-protection/configure/#gather-graphql-statistics) <br/> `apiGatewayMatchedSessionIDsAdaptiveGroups` <br/> US only <br/> `apiRequestSequencesGroups` |
 | Bot Management | | US and EU <br/>`httpRequestsAdaptive` <br/> [`httpRequestsAdaptiveGroups`](/analytics/graphql-api/migration-guides/graphql-api-analytics/) <br/> [`firewallEventsAdaptive`](/analytics/graphql-api/tutorials/querying-firewall-events/) <br/> [`firewallEventsAdaptiveGroups`](https://blog.cloudflare.com/how-we-used-our-new-graphql-api-to-build-firewall-analytics/) | |
 | DNS Firewall | Same as DNS |
 | DMARC Management | | US and EU <br/> `dmarcReportsAdaptive` <br/> `dmarcReportsSourcesAdaptiveGroups` | |
 | Page Shield | | US and EU <br/> [`pageShieldReportsAdaptiveGroups`](/page-shield/policies/violations/#get-policy-violations-via-graphql-api) |
 | SSL | | US and EU <br/> Only the fields `clientSSLProtocol` and `ja3Hash` part of `httpRequestsAdaptive` and `httpRequestsAdaptiveGroups` |  |
 | SSL 4 SaaS | | US and EU <br/> [clientRequestHTTPHost](/cloudflare-for-platforms/cloudflare-for-saas/hostname-analytics/#explore-customer-usage) <br/> Refer to [GraphQL Tutorial on querying HTTP events by hostname](/analytics/graphql-api/tutorials/end-customer-analytics/) |
 | Turnstile | | US and EU <br/> [`turnstileAdaptiveGroups`](/turnstile/turnstile-analytics/#graphql) |
 | WAF/L7 Firewall | | US and EU <br/> [`firewallEventsAdaptive`](/analytics/graphql-api/tutorials/querying-firewall-events/) <br/> [`firewallEventsAdaptiveGroups`](https://blog.cloudflare.com/how-we-used-our-new-graphql-api-to-build-firewall-analytics/) <br/> `firewallEventsAdaptiveByTimeGroups` |
 | Developer Platform | Cloudflare Images | US only <br/> `imagesRequestsAdaptiveGroups` |
 | Cloudflare Pages | | US only <br/> `pagesFunctionsInvocationsAdaptiveGroups` <br/> |
 | Durable Objects | | US only <br/> [`durableObjectsInvocationsAdaptiveGroups`](/durable-objects/observability/graphql-analytics/) <br/> [`durableObjectsPeriodicGroups`](/durable-objects/observability/graphql-analytics/) <br/> [`durableObjectsStorageGroups`](/durable-objects/observability/graphql-analytics/) <br/> [`durableObjectsSubrequestsAdaptiveGroups`](/durable-objects/observability/graphql-analytics/) |
 | Email Routing | | US and EU <br/> `emailRoutingAdaptive` <br/> `emailRoutingAdaptiveGroups` | |
 | R2 | | US and EU <br/> `r2OperationsAdaptiveGroups` <br/> `r2StorageAdaptiveGroups` | |
 | Stream | | US only <br/> [`streamMinutesViewedAdaptiveGroups`](/stream/getting-analytics/fetching-bulk-analytics/) <br/> [`videoPlaybackEventsAdaptiveGroups`](/stream/getting-analytics/fetching-bulk-analytics/) <br/> [`videoBufferEventsAdaptiveGroups`](/stream/getting-analytics/fetching-bulk-analytics/) <br/> [`videoQualityEventsAdaptiveGroups`](/stream/getting-analytics/fetching-bulk-analytics/) |
 | Workers (deployed on a Zone) | | US and EU <br/> `workerPlacementAdaptiveGroups` <br/> US only <br/> `workersAnalyticsEngineAdaptiveGroups` <br/> `workersZoneInvocationsAdaptiveGroups` <br/> `workersZoneSubrequestsAdaptiveGroups` <br/> `workersOverviewRequestsAdaptiveGroups` <br/> `workersOverviewDataAdaptiveGroups` <br/> [`workersInvocationsAdaptive`](/analytics/graphql-api/tutorials/querying-workers-metrics/) <br/> `workersInvocationsScheduled` <br/> `workersSubrequestsAdaptiveGroups` |
 | Network Services | Network Error Logging (NEL) / Edge Reachability / Last Mile Insights | US only <br/> `nelReportsAdaptiveGroups` |
 | Magic Firewall | |  US only <br/> [`magicFirewallSamplesAdaptiveGroups`](/magic-firewall/tutorials/graphql-analytics/) <br/> [`magicFirewallNetworkAnalyticsAdaptiveGroups`](/magic-firewall/tutorials/graphql-analytics/#example-queries-for-magic-firewall) |
 | Magic Network Monitoring | | US only <br/> [`mnmFlowDataAdaptiveGroups`](/magic-network-monitoring/tutorials/graphql-analytics/) |
 | Magic Transit | | US only <br/> [`magicTransitNetworkAnalyticsAdaptiveGroups`](/analytics/graphql-api/migration-guides/network-analytics-v2/node-reference/) <br/> [`flowtrackdNetworkAnalyticsAdaptiveGroups`](/analytics/graphql-api/migration-guides/network-analytics-v2/node-reference/) <br/> [`magicTransitTunnelHealthChecksAdaptiveGroups`](/analytics/graphql-api/tutorials/querying-magic-transit-tunnel-healthcheck-results/) <br/> [`magicTransitTunnelTrafficAdaptiveGroups`](/magic-transit/analytics/query-bandwidth/) |
 | Magic WAN | | US only <br/> `MagicWANConnectorMetricsAdaptiveGroups` |
 | Spectrum | | US and EU <br/> [`spectrumNetworkAnalyticsAdaptiveGroups`](/analytics/graphql-api/migration-guides/network-analytics-v2/node-reference/) |
 | Platform | GraphQL Analytics API | US and EU <br/> [All GraphQL Analytics API datasets](/analytics/graphql-api/features/discovery/introspection/) |
 | Logpush | | US only <br/> [`logpushHealthAdaptiveGroups`](/logs/get-started/alerts-and-analytics/#enable-logpush-health-analytics) <br/> |
 | Zero Trust | Access | US and EU <br/> [`accessLoginRequestsAdaptiveGroups`](/analytics/graphql-api/tutorials/querying-access-login-events/) | |
 | Browser Isolation | | US and EU <br/> Only the field `isIsolated` part of `gatewayL7RequestsAdaptiveGroups`  |
 | DLP | Part of Gateway HTTP / Gateway L7 |
 | Gateway | | US and EU <br/> `gatewayL7RequestsAdaptiveGroups` <br/> `gatewayL4SessionsAdaptiveGroups` <br/> `gatewayResolverQueriesAdaptiveGroups` <br/> `gatewayResolverByCategoryAdaptiveGroups` <br/> `gatewayResolverByRuleExecutionPerformanceAdaptiveGroups` <br/> US only <br/> `gatewayL4DownstreamSessionsAdaptiveGroups` <br/> `gatewayL4UpstreamSessionsAdaptiveGroups` |
 | WARP | | US and EU <br/> `warpDeviceAdaptiveGroups` | |

---

# Customer Metadata Boundary

URL: https://developers.cloudflare.com/data-localization/metadata-boundary/

As part of the Data Localization Suite, the Customer Metadata Boundary (CMB) ensures that any traffic metadata that can identify a customerâ€™s end user (that is, contains the customer's [Account ID](/fundamentals/setup/find-account-and-zone-ids/)) will stay in the `EU` (European Union) or in the `US` (United States), depending on the region the customer selects. For example, if a customer selects the `EU` Customer Metadata Boundary, metadata will **only** be sent to our core data center located in the European Union.

## Customer traffic metadata flow

The following diagram is a high-level example of the flow of how metadata about a customer's traffic is generated on a Cloudflare data center. Logs are exclusively sent to the EU core data center for Cloudflare customers and their authorized users to access and view.

<br/>

```mermaid
sequenceDiagram
    participant UserEU as End user
    participant CloudflarePoP as Closest data center
    participant EUCoreDC as Core data center in EU
    participant CloudflareSuperAdmin as Admin

    UserEU->>CloudflarePoP: Connects
    Note right of CloudflarePoP: Customer Logs generated <br> (for example, HTTP requests and Firewall events)
    CloudflarePoP-->>EUCoreDC: Forwards encrypted Customer Logs
    Note right of EUCoreDC: Authorized users can view Logs & Analytics <br> on the UI or via API
    CloudflareSuperAdmin->>EUCoreDC: Authenticated access
    EUCoreDC->>CloudflareSuperAdmin: Logs & Analytics
    CloudflarePoP->>UserEU: Response
```

<br/>

## Log management

Additionally, customers have the option to configure [Logpush](/logs/about/) to push their Customer Logs to various storage services, SIEMs, and log management providers.

## Product specific-behavior

For detailed information about product-specific behavior regarding Metadata Boundary, refer to the [Cloudflare product compatibility](/data-localization/compatibility/) page.

---

# Logpush datasets

URL: https://developers.cloudflare.com/data-localization/metadata-boundary/logpush-datasets/

The table below lists the [Logpush datasets](/logs/reference/log-fields/) that support zones or accounts with Customer Metadata Boundary (CMB) enabled. The column **Respects CMB** indicates whether enabling CMB impacts the dataset (yes/no). The last two columns inform you if CMB is available with US and EU.

Be aware that if you enable CMB for a dataset that does not support your region, no data will be pushed to your destination.

| Dataset name           | Level   | Respects CMB | Available with US CMB region | Available with EU CMB region |
| ---------------------- | ------- | ------------ | ---------------------------- | ---------------------------- |
| HTTP requests          | Zone    | âœ…            | âœ…                            | âœ…                            |
| Firewall events        | Zone    | âœ…            | âœ…                            | âœ…                            |
| DNS logs               | Zone    | âœ…            | âœ…                            | âœ…                            |
| NEL reports            | Zone    | âœ˜            | âœ…                            | âœ˜                            |
| Spectrum events        | Zone    | âœ…            | âœ…                            | âœ…                            |
| Page Shield            | Zone    | âœ…            | âœ…                            | âœ…                            |
| Access Requests        | Account | âœ…            | âœ…                            | âœ˜                            |
| Audit Logs             | Account | âœ˜            | âœ…                            | âœ˜                            |
| CASB Findings          | Account | âœ˜            | âœ…                            | âœ˜                            |
| Device Posture Results | Account | âœ˜            | âœ…                            | âœ˜                            |
| DNS Firewall logs      | Account | âœ…            | âœ…                            | âœ…                            |
| Gateway DNS            | Account | âœ…            | âœ…                            | âœ…                            |
| Gateway HTTP           | Account | âœ…            | âœ…                            | âœ…                            |
| Gateway Network        | Account | âœ…            | âœ…                            | âœ…                            |
| Magic IDS Detections   | Account | âœ…            | âœ…                            | âœ…                            |
| Network Analytics Logs | Account | âœ…            | âœ…                            | âœ…                            |
| Workers Trace Events   | Account | âœ…            | âœ…                            | âœ…                            |
| Zero Trust Sessions    | Account | âœ…            | âœ…                            | âœ…                            |
| Sinkhole Events        | Account | âœ…            | âœ…                            | âœ…                            |

---

# Attack coverage

URL: https://developers.cloudflare.com/ddos-protection/about/attack-coverage/

import { GlossaryTooltip, InlineBadge, Render } from "~/components"

The [DDoS Attack Protection managed rulesets](/ddos-protection/managed-rulesets/) provide protection against a variety of <GlossaryTooltip term="distributed denial-of-service (DDoS) attack">DDoS attacks</GlossaryTooltip> across L3/4 (layers 3/4) and L7 of the OSI model. Cloudflare constantly updates these managed rulesets to improve the attack coverage, increase the mitigation consistency, cover new and emerging threats, and ensure cost-efficient mitigations.

[Advanced TCP Protection](/ddos-protection/advanced-ddos-systems/overview/advanced-tcp-protection/) and [Advanced DNS Protection](/ddos-protection/advanced-ddos-systems/overview/advanced-dns-protection/), available to [Magic Transit](/magic-transit/) customers, provide additional protection against sophisticated TCP-based DDoS attacks and sophisticated and fully randomized DNS attacks, respectively.

As a general guideline, various Cloudflare products operate on different open systems interconnection (OSI) layers and you are protected up to the layer on which your service operates. You can customize the DDoS settings on the layer in which you onboarded. For example, since the CDN/WAF service is a Layer 7 (HTTP/HTTPS) service, Cloudflare provides protection from DDoS attacks on L7 downwards, including L3/4 attacks.

:::note

For Magic Transit customers, Cloudflare provides some L7 protection with a L3 service (like the Advanced DNS Protection system that is available for Magic Transit customers. DNS is considered a L7 protocol).
:::

The following table includes a sample of covered attack vectors:

<Render file="ddos-attack-coverage" product="ddos-protection" />

## Getting additional DNS protection

The Network-layer DDoS Attack Protection managed ruleset provides protection against some types of DNS attacks.

Magic Transit customers have access to [Advanced DNS Protection](/ddos-protection/advanced-ddos-systems/overview/advanced-dns-protection/) <InlineBadge preset="beta" />. Other customers might consider the following options:

- Use Cloudflare as your authoritative DNS provider ([primary DNS](/dns/zone-setups/full-setup/) or [secondary DNS](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/)).
- If you are running your own <GlossaryTooltip term="nameserver">nameservers</GlossaryTooltip>, use [DNS Firewall](/dns/dns-firewall/) to get additional protection against DNS attacks like random prefix attacks.

---

# Main components

URL: https://developers.cloudflare.com/ddos-protection/about/components/

import { GlossaryTooltip } from "~/components"

![Diagram with the main components providing protection against DDoS attacks at Cloudflare](~/assets/images/ddos-protection/ddos-diagram.png)

## Autonomous Edge

The Cloudflare Autonomous Edge is powered by the denial-of-service <GlossaryTooltip term="daemon" prepend="A daemon is ">daemon</GlossaryTooltip> (`dosd`), which is a home-grown software-defined system. The flow tracking daemon, `flowtrackd`, is our stateful mitigation platform alongside `dosd`. A `dosd` instance runs in every single server in every one of [Cloudflare global network's data centers](https://www.cloudflare.com/network/) around the world. These `dosd` instances can detect and mitigate DDoS attacks autonomously without requiring centralized consensus. Cloudflare users can configure this system through [DDoS Attack Protection managed rulesets](/ddos-protection/managed-rulesets/).

Another component of Cloudflareâ€™s Autonomous Edge includes the [Advanced TCP Protection](/ddos-protection/advanced-ddos-systems/overview/advanced-tcp-protection/) system. This is Cloudflare's TCP state tracking machine for detecting and mitigating the most randomized and sophisticated TCP-based DDoS attacks in unidirectional routing topologies â€” such as the case of [Magic Transit](/magic-transit/). Advanced TCP Protection is able to identify the state of a TCP connection and then drops, challenges, or rate-limits packets that do not belong to a legitimate connection.

For more information, refer to our blog post [A deep-dive into Cloudflareâ€™s autonomous edge DDoS protection](https://blog.cloudflare.com/deep-dive-cloudflare-autonomous-edge-ddos-protection/).

## Centralized DDoS protection system

Complementary to the Autonomous Edge, Cloudflareâ€™s entire global network is overwatched by a global version of `dosd`. This component protects Cloudflareâ€™s entire global network by detecting and mitigating globally distributed volumetric DDoS attacks.

The centralized systems run in Cloudflare's core data centers. They receive samples from every global network data center, analyze them, and automatically send mitigation instructions when detecting an attack. The system is also synchronized to each of our customers' web servers to identify their health and trigger any required mitigation actions.

---

# How DDoS protection works

URL: https://developers.cloudflare.com/ddos-protection/about/how-ddos-protection-works/

import { GlossaryTooltip } from "~/components"

To detect and mitigate <GlossaryTooltip term="distributed denial-of-service (DDoS) attack">DDoS attacks</GlossaryTooltip>, Cloudflareâ€™s autonomous edge and centralized DDoS systems analyze traffic samples out of path, which allows Cloudflare to asynchronously detect DDoS attacks without causing latency or impacting performance.

The analyzed samples include:

- **Packet fields** such as the source IP, source port, destination IP, destination port, protocol, TCP flags, sequence number, options, and packet rate.
- **HTTP request metadata** such as HTTP headers, user agent, query-string, path, host, HTTP method, HTTP version, TLS cipher version, and request rate.
- **HTTP response metrics** such as error codes returned by customersâ€™ origin servers and their rates.

Cloudflare uses a set of dynamic rules that scan for attack patterns, known attack tools, suspicious patterns, protocol violations, requests causing large amounts of origin errors, excessive traffic hitting the origin or cache, and additional attack vectors. Each rule has a predefined sensitivity level and default action that varies based on the rule's confidence that the traffic is indeed part of an attack.

:::note

You can set an override expression for the [HTTP DDoS Attack Protection](/ddos-protection/managed-rulesets/http/override-expressions/) or [Network-layer DDoS Attack Protection](/ddos-protection/managed-rulesets/network/override-expressions/) managed ruleset to define a specific scope for sensitivity level or action adjustments.
:::

Once attack traffic matches a rule, Cloudflare's systems will track that traffic and generate a real-time signature to surgically match against the attack pattern and mitigate the attack without impacting legitimate traffic. The rules are able to generate different signatures based on various properties of the attacks and the signal strength of each attribute. For example, if the attack is distributed â€” that is, originating from many source IPs â€” then the source IP field will not serve as a strong indicator, and the rule will not choose the source IP field as part of the attack signature. Once generated, the fingerprint is propagated as a mitigation rule to the most optimal location on the Cloudflare global network for cost-efficient mitigation. These mitigation rules are ephemeral and will expire shortly after the attack has ended, which happens when no additional traffic has been matched to the rule.

| Actions | Description |
| --- | --- |
| Block | Matching requests are denied access to the site. |
| Interactive Challenge | The client that made the request must pass an interactive Challenge. |
| Managed Challenge | Depending on the characteristics of a request, Cloudflare will choose an appropriate type of challenge. |
| Log | Records matching requests in the Cloudflare Logs. |
| Use rule defaults | Uses the default action that is pre-defined for each rule. |

## Thresholds

Thresholds vary for each rule and there are different thresholds globally and per colocation. Within a rule, the traffic is fingerprinted and the thresholds are per fingerprint, and it is difficult to know ahead of time which rules, colocations, or fingerprints your traffic generates, so the threshold numbers are not necessarily valuable.

Instead, Cloudflare's DDoS Protection system provides the sensitivity adjustment. If you experience a false positive, you can decrease the sensitivity. You can also use the `Log` action to help find an appropriate sensitivity level. You can decrease the sensitivity while in `Log` mode until the rule no longer matches.

## Time to mitigate

- Immediate mitigation for Advanced TCP and DNS Protection systems.
- Up to three seconds on average for the detection and mitigation of L3/4 DDoS attacks at the edge using the Network-layer DDoS Protection Managed rules.
- Up to 15 seconds on average for the detection and mitigation of HTTP DDoS attacks at the edge using the HTTP DDoS Protection Managed rules.

## Data localization

To learn more about how DDoS protection works with data localization, refer to the Data Localization Suite [product compatibility](/data-localization/compatibility/).

---

# About

URL: https://developers.cloudflare.com/ddos-protection/about/

import { GlossaryTooltip } from "~/components"

Cloudflare provides unmetered and unlimited <GlossaryTooltip term="distributed denial-of-service (DDoS) attack" link="https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/">distributed denial-of-service (DDoS)</GlossaryTooltip> protection at layers 3, 4, and 7 to all customers on all plans and services.

The protection is enabled by Cloudflareâ€™s [Autonomous DDoS Protection Edge](/ddos-protection/about/components/#autonomous-edge), which automatically detects and mitigates DDoS attacks.

The Autonomous Edge includes multiple dynamic mitigation rules exposed as [managed rulesets](/ddos-protection/managed-rulesets/), which provide comprehensive protection against a variety of DDoS attacks across layers 3/4 and layer 7 of the OSI model.

[Adaptive DDoS Protection](/ddos-protection/managed-rulesets/adaptive-protection/) also learns your unique traffic patterns and adapts to them to provide better protection against sophisticated DDoS attacks on layer 7 and layers 3/4. Your Internet properties can be secured from sophisticated TCP and DNS DDoS attacks using [Advanced DDoS Protection](/ddos-protection/advanced-ddos-systems/overview/) that leverages stateful inspection and traffic profiling.

---

# Concepts

URL: https://developers.cloudflare.com/ddos-protection/advanced-ddos-systems/concepts/

import { GlossaryTooltip, Render } from "~/components"

## Prefixes

Advanced DDoS Protection protects the IP prefixes you select from sophisticated DDoS attacks. A prefix can be an IP address or an IP range in CIDR format. You must add prefixes to Advanced DDoS Protection so that Cloudflare can analyze incoming <GlossaryTooltip term="data packet">packets</GlossaryTooltip> and offer protection against sophisticated TCP DDoS attacks.

Prefixes added to Advanced DDoS Protection must be one of the following:

- A prefix [onboarded to Magic Transit](/magic-transit/how-to/advertise-prefixes/).
- A subset of a prefix [onboarded to Magic Transit](/magic-transit/how-to/advertise-prefixes/).

You cannot add a prefix (or a subset of a prefix) that you have not onboarded to Magic Transit or whose status is still _Unapproved_. Contact your account team to get help with prefix approvals.

## Allowlist

The Advanced DDoS Protection allowlist is a list of prefixes that will bypass all configured Advanced DDoS Protection rules.

For example, you could add prefixes used only by partners of your company to the allowlist so that they are exempt from packet inspection and mitigation actions performed by Advanced DDoS Protection.

<Render file="allowlist-ip-spoofing" />

## Rule

A rule configures Advanced DDoS Protection for a given [scope](/ddos-protection/advanced-ddos-systems/concepts/#scope), according to several [settings](/ddos-protection/advanced-ddos-systems/concepts/#rule-settings): execution mode, burst sensitivity, and rate sensitivity.

Each system component (SYN flood protection and out-of-state TCP protection) has its own list of rules, and it should have at least one rule.

### Rule settings
Each rule type has the following settings: scope, mode, burst sensitivity, and rate sensitivity.

You may need to adjust the burst or rate sensitivity of a rule in case of false positives or due to specific traffic patterns.

#### Scope

Advanced TCP Protection rules can have one of the following scopes:

- **Global**: The rule will apply to all incoming packets.
- **Region**: The rule will apply to incoming packets in a selected region.
- **Data center**: The rule will apply to incoming packets in the selected Cloudflare data center.

The rule scope allows you to adjust the system's tolerance for out-of-state packets in locations where you may have more or less traffic than usual, or due to any other networking reasons.

Besides defining rules with one of the above scopes, you must also select the [prefixes](/ddos-protection/advanced-ddos-systems/concepts/#prefixes) that you wish to protect with Advanced TCP Protection.

#### Mode

The Advanced TCP Protection system constantly learns your TCP connections to mitigate DDoS attacks. Advanced TCP Protection rules can have one of the following execution modes: monitoring, mitigation (enabled), or disabled.

- **Monitoring**
  - In this mode, Advanced TCP Protection will not impact any packets. Instead, the protection system will learn your legitimate TCP connections and show you what it would have mitigated. Check Network Analytics to visualize what actions Advanced TCP Protection would have taken on incoming packets, according to the current configuration. <Render file="log-and-monitor-behavior-link" />


- **â€‹â€‹Mitigation (Enabled)**
  - In this mode, Advanced TCP Protection will learn your legitimate TCP connections and perform mitigation actions on incoming TCP DDoS attacks based on the rule configuration (burst and rate sensitivity) and your [allowlist](/ddos-protection/advanced-ddos-systems/concepts/#allowlist).

- **Disabled**
  - In this mode, a rule will not evaluate any incoming packets.

#### Burst sensitivity

The burst sensitivity is the rule's sensitivity to short-term bursts in the packet rate:

- A low sensitivity means that bigger spikes in the packet rate may trigger a mitigation action.
- A high sensitivity means that smaller spikes in the packet rate may trigger a mitigation action.

The default burst sensitivity is _Medium_.

#### Rate sensitivity

The rate sensitivity is the rule's sensitivity to the sustained packet rate:

- A low sensitivity means that higher sustained packet rates can trigger a mitigation action.
- A high sensitivity means that lower sustained packet rates may trigger a mitigation action. A high sensitivity offers increased protection, but you may get more false positives (that is, mitigated packets that belong to legitimate traffic).

The default rate sensitivity is _Medium_.

#### Profile sensitivity

:::note
Profile sensitivity is available for [Advanced DNS Protection](/ddos-protection/advanced-ddos-systems/overview/advanced-dns-protection/) only.
:::

The sensitivity to DNS queries that have not been recently seen.

- A higher sensitivity level means that the mitigation system will begin mitigating faster.
- A lower sensitivity provides more tolerance for potentially suspicious DNS queries.

The default rate sensitivity and recommended setting is _Low_. You should only increase sensitivity if it is needed based on observed attacks.

## Filter

<Render file="atp-filter-definition" />

The filter expression can reference source and destination IP addresses and ports. Each system component (SYN flood protection and out-of-state TCP protection) should have one or more [rules](#rule), but filters are optional.

Each system component has its own filters. You can configure a filter for each execution mode:

- **Mitigation Filter**: The system will drop packets matching the filter expression.
- **Monitoring Filter**: The system will log packets matching the filter expression.
- **Off Filter**: The system will ignore packets matching the filter expression.

When there is a match, a filter will alter the execution mode for all configured rules in a given system component (SYN flood protection or out-of-state TCP protection), including disabled rules.

For instructions on creating filters in the Cloudflare dashboard, refer to [Create a filter](/ddos-protection/advanced-ddos-systems/how-to/create-filter/). For API examples, refer to [Common API calls](/ddos-protection/advanced-ddos-systems/api/tcp-protection/examples/).

### Example use case

You can create a monitor filter for a new prefix that you are onboarding by using the expression to match against the prefix.

Your already onboarded prefixes can remain protected with one or more configured rules in mitigation mode.

When onboarding a new prefix, you would configure a monitoring filter for this prefix and then add it to Advanced TCP Protection.

---

## Determining the execution mode

When you have both rules and filters configured, the execution mode is determined according to the following:

1. If there is a match for one of the configured filters, use the filter's execution mode. The filter evaluation order  is based on their mode, in the following order:
   1. Mitigation filter (filter with `enabled` mode)
   2. Monitoring filter (filter with `monitoring` mode)
   3. Off filter (filter with `disabled` mode)
2. If no filter matched, use the execution mode determined by existing rules.
3. If no rules match, disable Advanced TCP Protection.

---

## Mitigation reasons

The Advanced TCP Protection system applies mitigation actions for different reasons based on the connection states. The **Mitigation reason** field shown in the **Advanced TCP Protection** tab of the [Network Analytics](/analytics/network-analytics/) dashboard will contain more information on why a given packet was dropped by the system.

The connection states are the following:

- **New**: A SYN or SYN-ACK packet has been sent to attempt to open a new connection.
- **Open**: The three-way TCP handshake has been completed and the TCP connection is open.
- **Closing**: A FIN or FIN-ACK packet has been seen attempting to close a connection.
- **Closed**: The closing three-way handshake has been completed, or an RST packet has closed the connection.

The mitigation reasons are the following:

| Reason | Description |
| --- | --- |
| **Unexpected** | Packet dropped because it was not expected given the current state of the TCP connection it was associated with. |
| **Challenge needed** | Packet challenged because the system determined that the packet is most likely part of a packet flood. |
| **Challenge passed** | Packet dropped because it belongs to a solved challenge. |
| **Not found** | Packet dropped because it is not part of an existing TCP connection and it is not establishing a new connection. |
| **Out of sequence** | Packet dropped because its properties (for example, TCP flags or sequence numbers) do not match the expected values for the existing connection. |
| **Already closed** | Packet dropped because it belongs to a connection that is already closed. |

Mitigation will only occur based on your Advanced TCP Protection configuration (rule sensitivities, configured allowlists and prefixes). The protection system will provide some tolerance to out-of-state packets to accommodate for the natural randomness of Internet routing.

---

# Best practices

URL: https://developers.cloudflare.com/ddos-protection/best-practices/

import { DirectoryListing } from "~/components"

Refer to the following pages for more information on DDoS protection best practices:

<DirectoryListing />

---

# Respond to DDoS attacks

URL: https://developers.cloudflare.com/ddos-protection/best-practices/respond-to-ddos-attacks/

import { GlossaryTooltip } from "~/components"

Cloudflare's network automatically mitigates largeÂ <GlossaryTooltip term="distributed denial-of-service (DDoS) attack" link="https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/">DDoS attacks</GlossaryTooltip>, but these attacks can still affect your application.

## All customers

All customers should perform the following steps to better secure their application:

1. Make sure all [DDoS managed rulesets](/ddos-protection/managed-rulesets/) are set to default settings (_High_ sensitivity level and mitigation actions) for optimal DDoS activation.
2. Deploy [WAF custom rules](/waf/custom-rules/) and [rate limiting rules](/waf/rate-limiting-rules/) to enforce a combined positive and negative security model. Reduce the traffic allowed to your website based on your known usage.
3. Make sure your origin is not exposed to the public Internet, meaning that access is only possible from [Cloudflare IP addresses](/fundamentals/concepts/cloudflare-ip-addresses/). As an extra security precaution, we recommend contacting your hosting provider and requesting new origin server IPs if they have been targeted directly in the past.
4. If you have [Managed IP Lists](/waf/tools/lists/managed-lists/#managed-ip-lists) or [Bot Management](/bots/plans/bm-subscription/), consider using these in WAF custom rules.
5. Enable [caching](/cache/) as much as possible to reduce the strain on your origin servers, and when using [Workers](/workers/), avoid overwhelming your origin server with more subrequests than necessary.

   To help counter attack randomization, Cloudflare recommends to update your cache settings to exclude the query string as a cache key. When the query string is excluded as a cache key, Cloudflare's cache will take in unmitigated attack requests instead of forwarding them to the origin. The cache can be a useful mechanism as part of a multilayered security posture.

## Enterprise customers

In addition to the steps for all customers, Cloudflare Enterprise customers subscribed to the Advanced DDoS Protection service should consider enabling [Adaptive DDoS Protection](/ddos-protection/managed-rulesets/adaptive-protection/), which mitigates attacks more intelligently based on your unique traffic patterns.

---

# Third-party services and DDoS protection

URL: https://developers.cloudflare.com/ddos-protection/best-practices/third-party/

import { GlossaryTooltip, RuleID } from "~/components";

## Using a third-party CDN in front of Cloudflare

Some Cloudflare customers choose to use a <GlossaryTooltip term="content delivery network (CDN)">Content Delivery Network (CDN)</GlossaryTooltip> in front of Cloudflare to cache and serve their resources.

Cloudflare recommends that you **do not use a third-party CDN in front of Cloudflare**. Some CDN providers may introduce subtleties into HTTP requests that deviate from protocol standards and/or protocol best practices. Additionally, because traffic to Cloudflare will originate from a limited set of IP addresses of the third-party CDN, in rare occasions â€” such as when using the Akamai CDN in front of Cloudflare â€” it may appear as if the CDN is launching a DDoS attack against Cloudflare due to the amount of traffic from these limited IP addresses.

Therefore, it is recommended that you **use the [Cloudflare CDN](/cache/)**, which provides the following benefits:

- You remove an additional hop between vendor data centers, thus reducing latency for your users.
- You perform DDoS filtering in the first point of contact from the Internet, which is a recommended best practice.

Note that, if you are using a third-party CDN in front of Cloudflare and Cloudflare mitigates a DDoS attack, you will still pay your first-hop CDN provider for the attack traffic that they processed before it was mitigated by Cloudflare.

### Recommended DDoS configuration adjustments

If you are using a CDN or proxy in front of Cloudflare, it is recommended that you change the action and/or sensitivity level of the following DDoS rules named:

- `HTTP requests with unusual HTTP headers or URI path (signature #1)` with the rule ID <RuleID id="0b1e17bd25c74e38834f19043486aee1" />
- `HTTP requests with unusual HTTP headers or URI path (signature #56)` with the rule ID <RuleID id="466d6c2e8ba74459a2670e91e269dfd6" />
- `HTTP requests with unusual HTTP headers or URI path (signature #57)` with the rule ID <RuleID id="12b9aecf1f6245b29d7e842bf35a42a0" />
- `Requests coming from known bad sources` with the rule ID <RuleID id="6e3ccc23900c428e8ec0fb8a3a679c52" />

You should change the ruleâ€™s action to _Log_ (only available on Enterprise plans) to view the flagged traffic in the [analytics dashboard](/ddos-protection/reference/analytics/). Alternatively, change the rule's **Sensitivity Level** to _Essentially Off_ to prevent the rule from being triggered.

For more information, refer to [HTTP DDoS Attack Protection managed ruleset: Ruleset configuration](/ddos-protection/managed-rulesets/http/#ruleset-configuration).

## Using VPNs, NATs, and other third-party services

Some Cloudflare Magic Transit customers operate <GlossaryTooltip term="Virtual Private Network (VPN)">Virtual Private Networks (VPN)</GlossaryTooltip> so that their remote employees can connect securely to the organizationâ€™s services. Additionally, larger organizations have Network Addressing Translation (NAT) systems that manage connections in and out of their network.

Cloudflare Magic Transit customers may also use third-party services such as Zoom, Webex, Microsoft Teams, and others for their internal organization communication. Because traffic to Cloudflare will be originating from a limited set of IP addresses belonging to these third-party services, it may appear as if the services are launching a DDoS attack against Cloudflare due to the amount of traffic from limited IP addresses.

Additionally, since this traffic may also be targeting a limited set of destinations (for example, the same designated service ports, VPN endpoints, or NAT IP addresses), it may appear as if the CDN is launching a DDoS attack against Cloudflare due to the amount of traffic from a limited set of IPs _to_ a limited set of IPs.

### Recommended DDoS configuration adjustments

If your organization uses VPNs, NATs, or third-party services at high rates of over 100 Mbps, it is recommended that you one of the following:

- Change the **Sensitivity Level** of the relevant rules to a lower level. Changing the level to _Essentially Off_ will prevent the rules from being triggered. Refer to [HTTP DDoS Attack Protection managed ruleset](/ddos-protection/managed-rulesets/http/) and [Network-layer DDoS Attack Protection managed ruleset](/ddos-protection/managed-rulesets/network/) for more information on the available adjustments per ruleset and how to perform them.
- Exclude the desired traffic from the Managed DDoS rule using expression filters. You can exclude a combination of source ports, source IP addresses, destination ports, destination IP addresses, and protocol. For more information, refer to [Configure Network-layer DDoS Attack Protection via API](/ddos-protection/managed-rulesets/network/configure-api/).

If you are on an Enterprise plan, you can change a ruleâ€™s action to _Log_ to view the flagged traffic in the [analytics dashboard](/ddos-protection/reference/analytics/). After gathering this information, you can later define rule adjustments as previously described.

---

# General updates

URL: https://developers.cloudflare.com/ddos-protection/change-log/general-updates/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/ddos.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Changelog

URL: https://developers.cloudflare.com/ddos-protection/change-log/

Cloudflare has a regular cadence of releasing updates and new rules to the DDoS managed rulesets. The updates either improve a rule's accuracy, lower false positives rates, or increase the protection due to a change in the threat landscape.

The release cycle for a new rule within the regular cadence follows this process:

- Cloudflare adds a new rule configured with the _Log_ action, and announces the rule in the "Scheduled changes" section of each managed ruleset.
- From that point on, if this rule matches any traffic, the matched traffic will be visible in one of the [analytics dashboards](/ddos-protection/reference/analytics/). If you suspect this might be a false positive, you can lower the sensitivity for that rule. Refer to [Handle a false positive](/ddos-protection/managed-rulesets/adjust-rules/false-positive/) for details.
- Cloudflare updates the rule action to mitigate traffic (for example, using the _Block_ action) after a period of at least seven days, usually on a Monday. The exact date is shown in the scheduled changes list.

Changes to existing rules follow the same process, except that Cloudflare will create a temporary updated rule (denoted as `BETA` in rule description) before updating the original rule on the next release cycle.

Cloudflare is very proactive in responding to new attack vectors, which may need to be released outside of the 7-day cycle, defined as an Emergency Release. This emergency release is only used to respond to new high priority threats with a low false positive probability.

---

# Alerts

URL: https://developers.cloudflare.com/ddos-protection/reference/alerts/

import { AvailableNotifications, Render } from "~/components"

Configure notifications to receive real-time alerts (within \~1 minute) about L3/4 and L7 DDoS attacks on your Internet properties, depending on your plan and services. You can choose from different delivery methods.

Each notification email includes the following information:

- Description
- Detection and mitigation time of attack
- Attack type
- Maximum rate of attack
- Attack target (zone, host, or IP address)
- Rule that matched the attack (ID and description)
- Rule override, if any

Cloudflare automatically sends weekly summaries of detected and mitigated DDoS attacks to Magic Transit and Spectrum BYOIP customers. Monthly application security reports are available for WAF/CDN customers. For more information, refer to [DDoS reports](/ddos-protection/reference/reports/).

:::note

 <Render file="alerts-and-reports-independent" />
:::

## Set up a notification for DDoS alerts

<Render file="create-notification" />

## Edit an existing notification

To edit, delete, or disable a notification, go to your [account notifications](https://dash.cloudflare.com/?to=/:account/notifications).

---

## Alert types

Cloudflare can issue notifications for different types of DDoS attack alerts.

### Standard alerts

<AvailableNotifications product="DDoS Protection" notificationFilter="HTTP DDoS Attack Alert" />

<AvailableNotifications product="DDoS Protection" notificationFilter="Layer 3/4 DDoS Attack Alert" />

### Advanced alerts

:::note

The availability of advanced DDoS attack alerts depends on your Cloudflare plan and subscribed services. Refer to [Availability](#availability) for details.
:::

Advanced DDoS attack alerts support additional configuration, allowing you to filter the notifications you wish to receive.

<AvailableNotifications product="DDoS Protection" notificationFilter="Advanced HTTP DDoS Attack Alert" /> <AvailableNotifications product="DDoS Protection" notificationFilter="Advanced Layer 3/4 DDoS Attack Alert" />

You will also receive alerts for rules with a _Log_ action, containing information on what triggered the alert.

## Availability

The available alerts depend on your Cloudflare plan and subscribed services:

| Alert type                           |     WAF/CDN     |      Spectrum      |  Spectrum BYOIP |  Magic Transit  |
| ------------------------------------ | :-------------: | :----------------: | :-------------: | :-------------: |
| HTTP DDoS Attack Alert               |       Yes       |          â€“         |        â€“        |        â€“        |
| Advanced HTTP DDoS Attack Alert      | Yes<sup>1</sup> |          â€“         |        â€“        |        â€“        |
| Layer 3/4 DDoS Attack Alert          |        â€“        | Yes<sup>2, 3</sup> |       Yes       | Yes<sup>3</sup> |
| Advanced Layer 3/4 DDoS Attack Alert |        â€“        |          â€“         | Yes<sup>2</sup> | Yes<sup>2</sup> |

<sup>1</sup> _Only available to Enterprise customers with the Advanced DDoS Protection subscription._ <br/>
<sup>2</sup> _Only available on an Enterprise plan._ <br/>
<sup>3</sup> _Refer to [Final remarks](#final-remarks) for additional notes._

## Example notification

The following image shows an example notification delivered via email:

![Example notification email of a DDoS attack](~/assets/images/ddos-protection/ddos-notification-example.png)

To investigate a possibly ongoing attack, select **View Dashboard**. To go to the rule details in the Cloudflare dashboard, select **View Rule**.

## Final remarks

- Spectrum and Magic Transit customers using [assigned Cloudflare IP addresses](/magic-transit/cloudflare-ips/) will receive layer 3/4 DDoS attack alerts where the attacked target is the Cloudflare IP or prefix. If you have [brought your own IP (BYOIP)](/byoip/) to Cloudflare Spectrum or Magic Transit, you will see your own IP addresses or prefixes as the attacked target.
- In some cases, HTTP DDoS attack alerts will reference the attacked zone name instead of the attacked hostname. This occurs when the attack signature does not include information on the attacked hostname because it is not a strong indicator for identifying attack requests. For more information on attack signatures, refer to [How DDoS protection works](/ddos-protection/about/how-ddos-protection-works/).
- DDoS alerts are currently only available for DDoS attacks detected and mitigated by the [DDoS managed rulesets](/ddos-protection/managed-rulesets/). Alerts are not yet available for DDoS attacks detected and mitigated by the [Advanced TCP Protection](/ddos-protection/advanced-ddos-systems/overview/advanced-tcp-protection/) and the [Advanced DNS Protection](/ddos-protection/advanced-ddos-systems/overview/advanced-dns-protection/) system.
- You will not receive duplicate DDoS alerts within the same one-hour time frame.
- If you configure more than one alert type for the same kind of attack (for example, both an HTTP DDoS Attack Alert and an Advanced HTTP DDoS Attack Alert) you may get more than one notification when an attack occurs. To avoid receiving duplicate notifications, delete one of the configured alerts.

---

# Analytics

URL: https://developers.cloudflare.com/ddos-protection/reference/analytics/

You can view DDoS analytics in different dashboards, depending on your service and plan:

- The [Security Events dashboard](/waf/analytics/security-events/) provides you with visibility into L7 security events that target your zone, including HTTP DDoS attacks and TCP attacks. The dashboard displays mitigations of HTTP DDoS attacks as HTTP DDoS events. These events are also available via [Cloudflare Logs](/logs/).

- The [Network Analytics dashboard](/analytics/network-analytics/) provides you with visibility into L3/4 traffic and DDoS attacks that target your IP ranges or Spectrum applications.

## Availability

| Service        | Free              | Pro             | Business        | Enterprise        |
| -------------- | ----------------- | --------------- | --------------- | ----------------- |
| WAF/CDN        | Sampled logs only | Security Events | Security Events | Security Events   |
| Spectrum/BYOIP | â€“                 | â€“               | â€“               | Network Analytics |
| Magic Transit  | â€“                 | â€“               | â€“               | Network Analytics |

## Remarks

In some situations, the analytics dashboards will not show you the ID of the DDoS managed rule that handled a packet/request. This means that an internal DDoS rule, which Cloudflare does not currently expose publicly, applied an action to the packet/request. These internal DDoS rules have a very low false positive rate and should always be enabled to protect your properties against DDoS attacks. For the same reason, DDoS rule IDs may also be unavailable in Cloudflare logs and API responses.

---

# Reference

URL: https://developers.cloudflare.com/ddos-protection/reference/

import { DirectoryListing } from "~/components"

Refer to the following pages for more information about Cloudflare DDoS protection:

<DirectoryListing />

---

# Logs

URL: https://developers.cloudflare.com/ddos-protection/reference/logs/

import { GlossaryTooltip } from "~/components"

Retrieve HTTP events using [Cloudflare Logs](/logs/) to integrate them into your <GlossaryTooltip term="SIEM">SIEM systems</GlossaryTooltip>.

Additionally, if you are a Magic Transit or a Spectrum customer on an Enterprise plan, you can export L3/4 traffic and DDoS attack logs using the [Network Analytics logs](/logs/reference/log-fields/account/network_analytics_logs/).

---

# Simulating test DDoS attacks

URL: https://developers.cloudflare.com/ddos-protection/reference/simulate-ddos-attack/

import { Render } from "~/components"

After onboarding to Cloudflare, you may want to simulate DDoS attacks against your Internet properties to test the protection, [reporting](/ddos-protection/reference/reports/), and [alerting](/ddos-protection/reference/alerts/) mechanisms. Follow the guidelines in this section to simulate a DDoS attack.

You can only launch DDoS attacks against your own Internet properties â€” your zone, Spectrum application, or IP range (depending on your Cloudflare services) â€” and provided that:

- The Internet properties are not shared with other organizations or individuals.
- The Internet properties have been onboarded to Cloudflare in an account under your name or ownership.

## Before you start

You do not have to obtain permission from Cloudflare to launch a DDoS attack simulation against your own Internet properties. However, before launching the simulated attack, you must [open a Support ticket](/support/contacting-cloudflare-support/) and provide the information below. All fields are mandatory.

It is recommended that you choose the right service and enable the correct features to test against the corresponding DDoS attacks. For example, if you want to test Cloudflare against an HTTP DDoS attack and you are only using Magic Transit, the test is going to fail because you need to onboard your HTTP application to Cloudflare's reverse proxy service to test our HTTP DDoS Protection.

<Render file="support-ticket-information" product="fundamentals" params={{ one: "Attack" }} />

---

# Reports

URL: https://developers.cloudflare.com/ddos-protection/reference/reports/

import { Render } from "~/components"

To download an ad-hoc DDoS report, generate a PDF report file by selecting **Print report** in your [analytics dashboard](/ddos-protection/reference/analytics/).

WAF/CDN customers can download a monthly report in Account Home > **Security Center**, by selecting [Security Reports](/security-center/app-security-reports/) and downloading the desired monthly report.

Additionally, if you are a Magic Transit or Spectrum BYOIP customer, you will receive weekly DDoS reports by email with a snapshot of the DDoS attacks that Cloudflare detected and mitigated in the previous week.

:::note[Note]

To receive DDoS reports by email you must have opted in to the **Analytics** category in the [communication preferences](/fundamentals/setup/account/customize-account/communication-preference/) for your profile.
:::

## Weekly DDoS reports

Cloudflare sends DDoS reports via email from `no-reply@notify.cloudflare.com` to users with the Super Administrator role on accounts with prefixes advertised by Cloudflare.

Reports contain the following information:

- Total number of DDoS attacks
- Largest DDoS attack in packets per second (pps) and bits per second (bps)
- Changes in DDoS attacks compared to the previous report
- Top attack protocols
- Top targeted IP addresses
- Top targeted destination ports
- Total potential downtime prevented (a sum of the duration of all attacks in that week)
- Total bytes mitigated (a sum of all the mitigated attack traffic)

Cloudflare issues DDoS reports via email each Tuesday. Reports summarize the attacks that occurred from Monday of the previous week to Sunday of the current week. For example, a report issued on 2020-11-10 (Tuesday) summarizes activity from 2020-11-02 (Monday) to 2020-11-08 (Sunday).

To receive real-time attack alerts, configure [DDoS alerts](/ddos-protection/reference/alerts/).

:::note[Notes]

- Information about top attack protocols, IP addresses, and destination ports is temporarily unavailable in weekly DDoS reports. Use the [Network Analytics dashboard](/analytics/network-analytics/) to get this information.
- <Render file="alerts-and-reports-independent" />
:::

### Example report

The following image shows an example DDoS report:

![Example email sent with a weekly DDoS report](~/assets/images/ddos-protection/ddos-report-email.png)

When Cloudflare does not detect any L3/4 DDoS attacks in the prior week, Cloudflare sends a confirmation report:

![Example report email sent when Cloudflare does not detect any DDoS attack in the previous week](~/assets/images/ddos-protection/ddos-report-no-attacks.png)

### Manage reporting subscriptions

Magic Transit and Spectrum BYOIP customers will receive the weekly DDoS report automatically.

To stop receiving DDoS reports, select the unsubscribe link at the bottom of the report email. To resubscribe after opting out, contact Cloudflare support.

---

# Managed rulesets

URL: https://developers.cloudflare.com/ddos-protection/managed-rulesets/

The DDoS Attack Protection managed rulesets provide comprehensive protection against a [variety of DDoS attacks](/ddos-protection/about/attack-coverage/) across L3/4 (network layer) and L7 (application layer) of the [OSI model](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/).

The available managed rulesets are:

- **[HTTP DDoS Attack Protection](/ddos-protection/managed-rulesets/http/)**
  - This ruleset includes rules to detect and mitigate DDoS attacks over HTTP and HTTPS.

- **[Network-layer DDoS Attack Protection](/ddos-protection/managed-rulesets/network/)**
  - This ruleset includes rules to detect and mitigate DDoS attacks on L3/4 of the OSI model such as UDP floods, SYN-ACK reflection attacks, SYN Floods, and DNS floods.

---

## Proactive false positive detection for new rules

:::note

Only available on Business and Enterprise plans.
:::

When Cloudflare creates a new managed rule, we check the rule impact against the traffic of Business and Enterprise zones while the rule is not blocking traffic yet.

If a [false positive](/ddos-protection/managed-rulesets/adjust-rules/false-positive/) is detected, we proactively reach out to the affected customers and help them make configuration changes (for example, to lower the sensitivity level of the new rule) before the rule starts mitigating traffic. This prevents the new rule from causing service disruptions and outages to your Internet properties.

---

# Create a sitemap from Sanity CMS with Workers

URL: https://developers.cloudflare.com/developer-spotlight/tutorials/create-sitemap-from-sanity-cms/

import { TabItem, Tabs, WranglerConfig, PackageManagers } from "~/components";

In this tutorial, you will put together a Cloudflare Worker that creates and serves a sitemap using data from [Sanity.io](https://www.sanity.io), a headless CMS.

The high-level workflow of the solution you are going to build in this tutorial is the following:

1. A URL on your domain (for example, `cms.example.com/sitemap.xml`) will be routed to a Cloudflare Worker.
2. The Worker will fetch your CMS data such as slugs and last modified dates.
3. The Worker will use that data to assemble a sitemap.
4. Finally, The Worker will return the XML sitemap ready for search engines.

## Before you begin

Before you start, make sure you have:

- A Cloudflare account. If you do not have one, [sign up](https://dash.cloudflare.com/sign-up/workers-and-pages) before continuing.
- A domain added to your Cloudflare account using a [full setup](/dns/zone-setups/full-setup/setup/), that is, using Cloudflare for your authoritative DNS nameservers.
- [npm](https://docs.npmjs.com/getting-started) and [Node.js](https://nodejs.org/en/) installed on your machine.

## Create a new Worker

Cloudflare Workers provides a serverless execution environment that allows you to create new applications or augment existing ones without configuring or maintaining infrastructure.

While you can create Workers in the Cloudflare dashboard, it is a best practice to create them locally, where you can use version control and [Wrangler](/workers/wrangler/install-and-update/), the Workers command-line interface, to deploy them.

Create a new Worker project using [C3](/pages/get-started/c3/) (`create-cloudflare` CLI):

<PackageManagers type="create" pkg="cloudflare@latest" />

In this tutorial, the Worker will be named `cms-sitemap`.

Select the options in the command-line interface (CLI) that work best for you, such as using JavaScript or TypeScript. The starter template you choose does not matter as this tutorial provides all the required code for you to paste in your project.

Next, require the `@sanity/client` package.

<Tabs> <TabItem label="pnpm">

```sh
pnpm install @sanity/client
```

</TabItem> <TabItem label="npm">

```sh
npm install @sanity/client
```

</TabItem> <TabItem label="yarn">

```sh
yarn add @sanity/client
```

</TabItem> </Tabs>

## Configure Wrangler

A default `wrangler.jsonc` was generated in the previous step.

The Wrangler file is a configuration file used to specify project settings and deployment configurations in a structured format.

For this tutorial your [Wrangler configuration file](/workers/wrangler/configuration/) should be similar to the following:

<WranglerConfig>

```toml
name = "cms-sitemap"
main = "src/index.ts"
compatibility_date = "2024-04-19"
minify = true

[vars]
# The CMS will return relative URLs, so we need to know the base URL of the site.
SITEMAP_BASE = "https://example.com"
# Modify to match your project ID.
SANITY_PROJECT_ID = "5z5j5z5j"
SANITY_DATASET = "production"
```

</WranglerConfig>

You must update the `[vars]` section to match your needs. See the inline comments to understand the purpose of each entry.

:::caution

Secrets do not belong in [Wrangler configuration file](/workers/wrangler/configuration/)s. If you need to add secrets, use `.dev.vars` for local secrets and the `wranger secret put` command for deploying secrets. For more information, refer to [Secrets](/workers/configuration/secrets/).

:::

## Add code

In this step you will add the boilerplate code that will get you close to the complete solution.

For the purpose of this tutorial, the code has been condensed into two files:

- `index.ts|js`: Serves as the entry point for requests to the Worker and routes them to the proper place.
- `Sitemap.ts|js`: Retrieves the CMS data that will be turned into a sitemap. For a better separation of concerns and organization, the CMS logic should be in a separate file.

Paste the following code into the existing `index.ts|js` file:

```ts
/**
 * Welcome to Cloudflare Workers!
 *
 * - Run `npm run dev` in your terminal to start a development server
 * - Open a browser tab at http://localhost:8787/ to see your worker in action
 * - Run `npm run deploy` to publish your worker
 *
 * Bind resources to your worker in Wrangler config file. After adding bindings, a type definition for the
 * `Env` object can be regenerated with `npm run cf-typegen`.
 *
 * Learn more at https://developers.cloudflare.com/workers/
 */

import { Sitemap } from "./Sitemap";

// Export a default object containing event handlers.
export default {
	// The fetch handler is invoked when this worker receives an HTTPS request
	// and should return a Response (optionally wrapped in a Promise).
	async fetch(request, env, ctx): Promise<Response> {
		const url = new URL(request.url);

		// You can get pretty far with simple logic like if/switch-statements.
		// If you need more complex routing, consider Hono https://hono.dev/.
		if (url.pathname === "/sitemap.xml") {
			const handleSitemap = new Sitemap(request, env, ctx);
			return handleSitemap.fetch();
		}

		return new Response(`Try requesting /sitemap.xml`, {
			headers: { "Content-Type": "text/html" },
		});
	},
} satisfies ExportedHandler<Env>;
```

You do not need to modify anything in this file after pasting the above code.

Next, create a new file named `Sitemap.ts|js` and paste the following code:

```ts
import { createClient, SanityClient } from "@sanity/client";

export class Sitemap {
	private env: Env;
	private ctx: ExecutionContext;

	constructor(request: Request, env: Env, ctx: ExecutionContext) {
		this.env = env;
		this.ctx = ctx;
	}

	async fetch(): Promise<Response> {
		// Modify the query to use your CMS's schema.
		//
		// Request these:
		// - "slug": The slug of the post.
		// - "lastmod": When the post was updated.
		//
		// Notes:
		// - The slugs are prefixed to help form the full relative URL in the sitemap.
		// - Order the slugs to ensure the sitemap is in a consistent order.
		const query = `*[defined(postFields.slug.current)] {
			_type == 'articlePost' => {
				'slug': '/posts/' + postFields.slug.current,
				'lastmod': _updatedAt,
			},
			_type == 'examplesPost' => {
				'slug': '/examples/' + postFields.slug.current,
				'lastmod': _updatedAt,
			},
			_type == 'templatesPost' => {
				'slug': '/templates/' + postFields.slug.current,
				'lastmod': _updatedAt,
			}
		} | order(slug asc)`;

		const dataForSitemap = await this.fetchCmsData(query);

		if (!dataForSitemap) {
			console.error(
				"Error fetching data for sitemap",
				JSON.stringify(dataForSitemap),
			);
			return new Response("Error fetching data for sitemap", { status: 500 });
		}

		const sitemapXml = `<?xml version="1.0" encoding="UTF-8"?>
		<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
		  ${dataForSitemap
				.filter(Boolean)
				.map(
					(item: any) => `
			<url>
			  <loc>${this.env.SITEMAP_BASE}${item.slug}</loc>
			  <lastmod>${item.lastmod}</lastmod>
			</url>
		  `,
				)
				.join("")}
		</urlset>`;

		return new Response(sitemapXml, {
			headers: {
				"content-type": "application/xml",
			},
		});
	}

	private async fetchCmsData(query: string) {
		const client: SanityClient = createClient({
			projectId: this.env.SANITY_PROJECT_ID,
			dataset: this.env.SANITY_DATASET,
			useCdn: true,
			apiVersion: "2024-01-01",
		});

		try {
			const data = await client.fetch(query);
			return data;
		} catch (error) {
			console.error(error);
		}
	}
}
```

In steps 4 and 5 you will modify the code you pasted into `src/Sitemap.ts` according to your needs.

## Query CMS data

The following query in `src/Sitemap.ts` defines which data will be retrieved from the CMS. The exact query depends on your schema:

```ts
const query = `*[defined(postFields.slug.current)] {
  _type == 'articlePost' => {
    'slug': '/posts/' + postFields.slug.current,
    'lastmod': _updatedAt,
  },
  _type == 'examplesPost' => {
    'slug': '/examples/' + postFields.slug.current,
    'lastmod': _updatedAt,
  },
  _type == 'templatesPost' => {
    'slug': '/templates/' + postFields.slug.current,
    'lastmod': _updatedAt,
  }
} | order(slug asc)`;
```

If necessary, adapt the provided query to your specific schema, taking the following into account:

- The query must return two properties: `slug` and `lastmod`, as these properties are referenced when creating the sitemap. [GROQ](https://www.sanity.io/docs/how-queries-work) (Graph-Relational Object Queries) and [GraphQL](https://www.sanity.io/docs/graphql) enable naming properties â€” for example, `"lastmod": _updatedAt` â€” allowing you to map custom field names to the required properties.
- You will likely need to prefix each slug with the base path. For `www.example.com/posts/my-post`, the slug returned is `my-post`, but the base path (`/posts/`) is what needs to be prefixed (the domain is automatically added).
- Add a sort to the query to provide a consistent order (`order(slug asc)` in the provided tutorial code).

The data returned by the query will be used to generate an XML sitemap.

## Create the sitemap from the CMS data

The relevant code from `src/Sitemap.ts` generating the sitemap and returning it with the correct content type is the following:

```ts
const sitemapXml = `<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  ${dataForSitemap
		.filter(Boolean)
		.map(
			(item: any) => `
  <url>
    <loc>${this.env.SITEMAP_BASE}${item.slug}</loc>
    <lastmod>${item.lastmod}</lastmod>
  </url>
  `,
		)
		.join("")}
</urlset>`;

return new Response(sitemapXml, {
	headers: {
		"content-type": "application/xml",
	},
});
```

The URL (`loc`) and last modification date (`lastmod`) are the only two properties added to the sitemap because, [according to Google](https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap#additional-notes-about-xml-sitemaps), other properties such as `priority` and `changefreq` will be ignored.

Finally, the sitemap is returned with the content type of `application/xml`.

At this point, you can test the Worker locally by running the following command:

```sh
wrangler dev
```

This command will output a localhost URL in the terminal. Open this URL with `/sitemap.xml` appended to view the sitemap in your browser. If there are any errors, they will be shown in the terminal output.

Once you have confirmed the sitemap is working, move on to the next step.

## Deploy the Worker

Now that your project is working locally, there are two steps left:

1. Deploy the Worker.
2. Bind it to a domain.

To deploy the Worker, run the following command in your terminal:

```sh
wrangler deploy
```

The terminal will log information about the deployment, including a new custom URL in the format `{worker-name}.{account-subdomain}.workers.dev`. While you could use this hostname to obtain your sitemap, it is a best practice to host the sitemap on the same domain your content is on.

## Route a URL to the Worker

In this step, you will make the Worker available on a new subdomain using a built-in Cloudflare feature.

One of the benefits of using a subdomain is that you do not have to worry about this sitemap conflicting with your root domain's sitemap, since both are probably using the `/sitemap.xml` path.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.

2. In Account Home, select **Workers & Pages**, and then select your Worker.

3. Go to **Settings** > **Triggers** > **Custom Domains** > **Add Custom Domain**.

4. Enter the domain or subdomain you want to configure for your Worker.

   For this tutorial, use a subdomain on the domain that is in your sitemap. For example, if your sitemap outputs URLs like `www.example.com` then a suitable subdomain is `cms.example.com`.

5. Select **Add Custom Domain**.

   After adding the subdomain, Cloudflare automatically adds the proper DNS record binding the Worker to the subdomain.

6. To verify your configuration, go to your new subdomain and append `/sitemap.xml`. For example:

   ```txt
   cms.example.com/sitemap.xml
   ```

The browser should show the sitemap as when you tested locally.

You now have a sitemap for your headless CMS using a highly maintainable and serverless setup.

---

# Adaptive DDoS Protection

URL: https://developers.cloudflare.com/ddos-protection/managed-rulesets/adaptive-protection/

Adaptive DDoS Protection learns your unique traffic patterns and adapts to them to provide better protection against sophisticated DDoS attacks on layer 7 and layers 3/4, depending on your subscribed Cloudflare services.

Adaptive DDoS Protection provides the following types of protection:

- **Adaptive DDoS Protection for Origins**: Detects and mitigates traffic that deviates from your site's origin errors profile.
- **Adaptive DDoS Protection for User-Agents**: Detects and mitigates traffic that deviates from the top User Agents seen by Cloudflare on the network. The User Agent profile is built from the entire Cloudflare network and not only from the customer's zone.
- **Adaptive DDoS Protection for Locations**: Detects and mitigates traffic that deviates from your siteâ€™s geo-distribution profile. The profile is calculated from the rate for every client country and region, using the rates from the past seven days.
- **Adaptive DDoS Protection for Protocols**: Detects and mitigates traffic that deviates from your trafficâ€™s IP protocol profile. The profile is calculated as a global rate for each of your prefixes.

## Availability

Cloudflare Adaptive DDoS Protection is available to Enterprise customers according to the following table:



| Feature                           | Profiling dimension                        | WAF/CDN<sup>1</sup> | Magic Transit /<br/>Spectrum BYOIP<sup>2</sup> |
| --------------------------------- | ------------------------------------------ | :-----------------: | :--------------------------------------------: |
| **HTTP Adaptive DDoS Protection** |                                            |                     |                                                |
| For Origins                       | Origin errors                              |         Yes         |                        â€”                       |
| For User-Agents                   | User Agent<br/>(entire Cloudflare network) |         Yes         |                        â€”                       |
| For Locations                     | Client IP country and region               |         Yes         |                        â€”                       |
| **L3/4 Adaptive DDoS Protection** |                                            |                     |                                                |
| For Protocols                     | IP protocol                                |          â€”          |                       Yes                      |
| For Protocols                     | Client IP country and Region for UDP       |          â€”          |                       Yes                      |



<sup>1</sup> _WAF/CDN customers on the Enterprise plan with the Advanced DDoS Protection subscription._<br/>
<sup>2</sup> _Magic Transit and Spectrum BYOIP customers on an Enterprise plan._

## How it works

Adaptive DDoS Protection creates a traffic profile by looking at the maximum rates of traffic every day, for the past seven days. These profiles are recalculated every day, keeping the seven-day time window. Adaptive DDoS Protection stores the maximal traffic rates seen for every predefined dimension value (the profiling dimension varies for each rule). Every profile uses one dimension, such as the source country of the request, the user agent, and the IP protocol. Incoming traffic that deviates from your profile may be malicious.

To eliminate outliers, rate calculations only consider the 95th percentile rates (discarding the top 5% of the highest rates). Cloudflare requires a minimum amount of requests per second (rps) to build traffic profiles. HTTP Adaptive DDoS Protection rules also take into account Cloudflareâ€™s [Machine Learning (ML) models](/bots/concepts/bot-score/#machine-learning) to identify traffic that is likely automated.

Cloudflare may change the logic of these protection rules from time to time to improve them. Any rule changes will appear in the [Managed rulesets changelog](/ddos-protection/change-log/) page.

### DDoS protection based on the origin HTTP error rate

Cloudflareâ€™s network is built to automatically monitor and mitigate large DDoS attacks. Cloudflare also helps mitigate smaller DDoS attacks, based on the following general rules:

- For zones on any plan, Cloudflare will apply mitigations when the HTTP error rate is above the _High_ (default) sensitivity level of 1,000 errors-per-second rate threshold. You can decrease the sensitivity level by [configuring the HTTP DDoS Attack Protection managed ruleset](/ddos-protection/managed-rulesets/http/configure-dashboard/).
- For zones on Pro, Business, and Enterprise plans, Cloudflare performs an additional check for better detection accuracy: the errors-per-second rate must also be at least five times the normal origin traffic levels before applying DDoS mitigations.

Cloudflare determines the error rate based on all HTTP errors in the 52X range (Internal Server Error) and in the 53X range, except for [error 530](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-530). Currently, for DDoS mitigations based on HTTP error rate, you cannot exclude specific HTTP error codes.

For more information on the types of DDoS attacks covered by Cloudflare's DDoS protection, refer to [DDoS attack coverage](/ddos-protection/about/attack-coverage/).

---

## View flagged traffic

To view traffic flagged by HTTP Adaptive DDoS Protection rules:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and website.
2. Go to **Security** > **Events**.
3. Filter by `Service equals HTTP DDoS` and by rule ID.

To view traffic flagged by L3/4 Adaptive DDoS Protection rules:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account.
2. Go to Account Home > **Analytics & Logs** > **Network Analytics**.
3. Filter by `Ruleset ID equals 3b64149bfa6e4220bbbc2bd6db589552` (the ID of the Network-layer DDoS Attack Protection managed ruleset) and by rule ID.

You may also obtain information about flagged traffic through [Logpush](/logs/about/) or the [GraphQL API](/analytics/graphql-api/).

## Configure the rules

You can adjust the action and sensitivity of the Adaptive DDoS Protection rules. The default action is _Log_. Use this action to first observe what traffic is flagged before deciding on a mitigation action.

To configure a rule, refer to the instructions in the following pages:

- [Configure HTTP DDoS Attack Protection in the dashboard](/ddos-protection/managed-rulesets/http/configure-dashboard/) (for L7 rules)
- [Configure Network-layer DDoS Attack Protection in the dashboard](/ddos-protection/managed-rulesets/network/configure-dashboard/) (for L3/4 rules)

For more information on the available configuration parameters, refer to the following pages:

- For the (L7) DDoS protection rules for Origins, User-Agents, and Locations:<br/>
  [HTTP DDoS Attack Protection parameters](/ddos-protection/managed-rulesets/http/override-parameters/)
- For the (L3/4) DDoS protection rules for Protocols:<br/>
  [Network-layer DDoS Attack Protection parameters](/ddos-protection/managed-rulesets/network/override-parameters/)

---

# Recommend products on e-commerce sites using Workers AI and Stripe

URL: https://developers.cloudflare.com/developer-spotlight/tutorials/creating-a-recommendation-api/

import {
	Render,
	TabItem,
	Tabs,
	PackageManagers,
	WranglerConfig,
} from "~/components";

E-commerce and media sites often work on increasing the average transaction value to boost profitability. One of the strategies to increase the average transaction value is "cross-selling," which involves recommending related products. Cloudflare offers a range of products designed to build mechanisms for retrieving data related to the products users are viewing or requesting. In this tutorial, you will experience developing functionalities necessary for cross-selling by creating APIs for related product searches and product recommendations.

## Goals

In this workshop, you will develop three REST APIs.

1. An API to search for information highly related to a specific product.
2. An API to suggest products in response to user inquiries.
3. A Webhook API to synchronize product information with external e-commerce applications.

By developing these APIs, you will learn about the resources needed to build cross-selling and recommendation features for e-commerce sites.

You will also learn how to use the following Cloudflare products:

- [**Cloudflare Workers**](/workers/): Execution environment for API applications
- [**Cloudflare Vectorize**](/vectorize/): Vector DB used for related product searches
- [**Cloudflare Workers AI**](/workers-ai/): Used for vectorizing data and generating recommendation texts

<Render file="tutorials-before-you-start" product="workers" />

<Render file="prereqs" product="workers" />

### Prerequisites

This tutorial involves the use of several Cloudflare products. Some of these products have free tiers, while others may incur minimal charges. Please review the following billing information carefully.

<Render file="ai-local-usage-charges" product="workers" />

## 1. Create a new Worker project

First, let's create a Cloudflare Workers project.

<Render file="c3-definition" product="workers" />

To efficiently create and manage multiple APIs, let's use [`Hono`](https://hono.dev). Hono is an open-source application framework released by a Cloudflare Developer Advocate. It is lightweight and allows for the creation of multiple API paths, as well as efficient request and response handling.
Open your command line interface (CLI) and run the following command:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"cross-sell-api --framework=hono"}
/>

If this is your first time running the `C3` command, you will be asked whether you want to install it. Confirm that the package name for installation is `create-cloudflare` and answer `y`.

```sh
Need to install the following packages:
create-cloudflare@latest
Ok to proceed? (y)
```

During the setup, you will be asked if you want to manage your project source code with `Git`. It is recommended to answer `Yes` as it helps in recording your work and rolling back changes. You can also choose `No`, which will not affect the tutorial progress.

```sh
â•° Do you want to use git for version control?
â€Šâ€ŠYes / No
```

Finally, you will be asked if you want to deploy the application to your Cloudflare account. For now, select `No` and start development locally.

```sh
â•­ Deploy with Cloudflare Step 3 of 3
â”‚
â•° Do you want to deploy your application?
â€Šâ€ŠYes / No
```

If you see a message like the one below, the project setup is complete. You can open the `cross-sell-api` directory in your preferred IDE to start development.

```sh
â”œ  APPLICATION CREATED  Deploy your application with npm run deploy
â”‚
â”‚ Navigate to the new directory cd cross-sell-api
â”‚ Run the development server npm run dev
â”‚ Deploy your application npm run deploy
â”‚ Read the documentation https://developers.cloudflare.com/workers
â”‚ Stuck? Join us at https://discord.cloudflare.com
â”‚
â•° See you again soon!
```

Cloudflare Workers applications can be developed and tested in a local environment. On your CLI, change directory into your newly created Workers and run `npx wrangler dev` to start the application. Using `Wrangler`, the application will start, and you'll see a URL beginning with `localhost`.

```sh
 â›…ï¸ wrangler 3.60.1
-------------------

âŽ” Starting local server...
[wrangler:inf] Ready on http://localhost:8787
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [b] open a browser, [d] open Devtools, [l] turn off local mode, [c] clear console, [x] to exit            â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

```

You can send a request to the API using the `curl` command. If you see the text `Hello Hono!`, the API is running correctly.

```sh
curl http://localhost:8787
```

```sh output
Hello Hono!
```

So far, we've covered how to create a Cloudflare Worker project and introduced tools and open-source projects like the `C3` command and the `Hono` framework that streamline development with Cloudflare. Leveraging these features will help you develop applications on Cloudflare Workers more smoothly.

## 2. Create an API to import product information

Now, we will start developing the three APIs that will be used in our cross-sell system. First, let's create an API to synchronize product information with an existing e-commerce application. In this example, we will set up a system where product registrations in [Stripe](https://stripe.com) are synchronized with the cross-sell system.

This API will receive product information sent from an external service like Stripe as a Webhook event. It will then extract the necessary information for search purposes and store it in a database for related product searches. Since vector search will be used, we also need to implement a process that converts strings to vector data using an Embedding model provided by Cloudflare Workers AI.

The process flow is illustrated as follows:

```mermaid
sequenceDiagram
    participant Stripe

    box Cloudflare
        participant CF_Workers
        participant CF_Workers_AI
        participant CF_Vectorize
    end

    Stripe->>CF_Workers: Send product registration event
    CF_Workers->>CF_Workers_AI: Request product information vectorization
    CF_Workers_AI->>CF_Workers: Send back vector data result
    CF_Workers->>CF_Vectorize: Save vector data
```

Let's start implementing step-by-step.

### Bind Workers AI and Vectorize to your Worker

This API requires the use of Workers AI and Vectorize. To use these resources from a Worker, you will need to first create the resources then [bind](/workers/runtime-apis/bindings/#what-is-a-binding) them to a Worker. First, let's create a Vectorize index with Wrangler using the command `wrangler vectorize create {index_name} --dimensions={number_of_dimensions} --metric={similarity_metric}`. The values for `dimensions` and `metric` depend on the type of [Text Embedding Model](/workers-ai/models/) you are using for data vectorization (Embedding). For example, if you are using the `bge-large-en-v1.5` model, the command is:

```sh
npx wrangler vectorize create stripe-products --dimensions=1024 --metric=cosine
```

When this command executes successfully, you will see a message like the following. It provides the items you need to add to the [Wrangler configuration file](/workers/wrangler/configuration/) to bind the Vectorize index with your Worker application.

```sh
âœ… Successfully created a new Vectorize index: 'stripe-products'
ðŸ“‹ To start querying from a Worker, add the following binding configuration into your Wrangler configuration file:

[[vectorize]]
binding = "VECTORIZE_INDEX"
index_name = "stripe-products"
```

To use the created Vectorize index from your Worker, let's add the binding. Open the [Wrangler configuration file](/workers/wrangler/configuration/) and add the copied lines.

<WranglerConfig>

```toml null {5,6,7}
name = "cross-sell-api"
main = "src/index.ts"
compatibility_date = "2024-06-05"

[[vectorize]]
binding = "VECTORIZE_INDEX"
index_name = "stripe-products"
```

</WranglerConfig>

Additionally, let's add the configuration to use Workers AI in the [Wrangler configuration file](/workers/wrangler/configuration/).

<WranglerConfig>

```toml null {9,10}
name = "cross-sell-api"
main = "src/index.ts"
compatibility_date = "2024-06-05"

[[vectorize]]
binding = "VECTORIZE_INDEX"
index_name = "stripe-products"

[ai]
binding = "AI" # available in your Worker on env.AI
```

</WranglerConfig>

When handling bound resources from your application, you can generate TypeScript type definitions to develop more safely. Run the `npm run cf-typegen` command. This command updates the `worker-configuration.d.ts` file, allowing you to use both Vectorize and Workers AI in a type-safe manner.

```sh
npm run cf-typegen
```

```sh output

> cf-typegen
> wrangler types --env-interface CloudflareBindings

 â›…ï¸ wrangler 3.60.1
-------------------

interface CloudflareBindings {
        VECTORIZE_INDEX: VectorizeIndex;
        AI: Ai;
}
```

Once you save these changes, the respective resources and APIs will be available for use in the Workers application. You can access these properties from `env`. In this example, you can use them as follows:

```ts
app.get("/", (c) => {
	c.env.AI; // Workers AI SDK
	c.env.VECTORIZE_INDEX; // Vectorize SDK
	return c.text("Hello Hono!");
});
```

Finally, rerun the `npx wrangler dev` command with the `--remote` option. This is necessary because Vectorize indexes are not supported in local mode. If you see the message, `Vectorize bindings are not currently supported in local mode. Please use --remote if you are working with them.`, rerun the command with the `--remote` option added.

```sh
npx wrangler dev --remote
```

### Create a webhook API to handle product registration events

You can receive notifications about product registration and information via POST requests using webhooks. Let's create an API that accepts POST requests. Open your `src/index.ts` file and add the following code:

```ts
app.post("/webhook", async (c) => {
	const body = await c.req.json();
	if (body.type === "product.created") {
		const product = body.data.object;
		console.log(JSON.stringify(product, null, 2));
	}
	return c.text("ok", 200);
});
```

This code implements an API that processes POST requests to the `/webhook` endpoint. The data sent by Stripe's Webhook events is included in the request body in JSON format. Therefore, we use `c.req.json()` to extract the data. There are multiple types of Webhook events that Stripe can send, so we added a conditional to only process events when a product is newly added, as indicated by the `type`.

### Add Stripe's API Key to the project

When developing a webhook API, you need to ensure that requests from unauthorized sources are rejected. To prevent unauthorized API requests from causing unintended behavior or operational confusion, you need a mechanism to verify the source of API requests. When integrating with Stripe, you can protect the API by generating a signing secret used for webhook verification.

1. Refer to the [Stripe documentation](https://docs.stripe.com/keys) to get a [secret API key for the test environment](https://docs.stripe.com/keys#reveal-an-api-secret-key-for-test-mode).
2. Save the obtained API key in a `.dev.vars` file.

```
STRIPE_SECRET_API_KEY=sk_test_XXXX
```

3. Follow the [guide](https://docs.stripe.com/stripe-cli) to install Stripe CLI.
4. Use the following Stripe CLI command to forward Webhook events from Stripe to your local application.

```sh
stripe listen --forward-to http://localhost:8787/webhook --events product.created
```

5. Copy the signing secret that starts with `whsec_` from the Stripe CLI command output.

```
> Ready! You are using Stripe API Version [2024-06-10]. Your webhook signing secret is
whsec_xxxxxx (^C to quit)
```

6. Save the obtained signing secret in the `.dev.vars` file.

```
STRIPE_WEBHOOK_SECRET=whsec_xxxxxx
```

7. Run `npm run cf-typegen` to update the type definitions in `worker-configuration.d.ts`.
8. Run `npm install stripe` to add the Stripe SDK to your application.
9. Restart the `npm run dev -- --remote` command to import the API key into your application.

Finally, modify the source code of `src/index.ts` as follows to ensure that the webhook API cannot be used from sources other than your Stripe account.

````ts
import { Hono } from "hono";
import { env } from "hono/adapter";
import Stripe from "stripe";

type Bindings = {
	[key in keyof CloudflareBindings]: CloudflareBindings[key];
};

const app = new Hono<{
	Bindings: Bindings;
	Variables: {
		stripe: Stripe;
	};
}>();

/**
 * Initialize Stripe SDK client
 * We can use this SDK without initializing on each API route,
 * just get it by the following example:
 * ```
 * const stripe = c.get('stripe')
 * ```
 */
app.use("*", async (c, next) => {
	const { STRIPE_SECRET_API_KEY } = env(c);
	const stripe = new Stripe(STRIPE_SECRET_API_KEY);
	c.set("stripe", stripe);
	await next();
});

app.post("/webhook", async (c) => {
	const { STRIPE_WEBHOOK_SECRET } = env(c);
	const stripe = c.get("stripe");
	const signature = c.req.header("stripe-signature");
	if (!signature || !STRIPE_WEBHOOK_SECRET || !stripe) {
		return c.text("", 400);
	}
	try {
		const body = await c.req.text();
		const event = await stripe.webhooks.constructEventAsync(
			body,
			signature,
			STRIPE_WEBHOOK_SECRET,
		);
		if (event.type === "product.created") {
			const product = event.data.object;
			console.log(JSON.stringify(product, null, 2));
		}
		return c.text("", 200);
	} catch (err) {
		const errorMessage = `âš ï¸  Webhook signature verification failed. ${err instanceof Error ? err.message : "Internal server error"}`;
		console.log(errorMessage);
		return c.text(errorMessage, 400);
	}
});

export default app;
````

This ensures that an HTTP 400 error is returned if the Webhook API is called directly by unauthorized sources.

```sh
curl -XPOST http://localhost:8787/webhook -I
```

```sh output
HTTP/1.1 400 Bad Request
Content-Length: 0
Content-Type: text/plain; charset=UTF-8
```

Use the Stripe CLI command to test sending events from Stripe.

```sh
stripe trigger product.created
```

```sh output
Setting up fixture for: product
Running fixture for: product
Trigger succeeded! Check dashboard for event details.
```

The product information added on the Stripe side is recorded as a log on the terminal screen where `npm run dev` is executed.

```
{
  id: 'prod_QGw9VdIqVCNABH',
  object: 'product',
  active: true,
  attributes: [],
  created: 1718087602,
  default_price: null,
  description: '(created by Stripe CLI)',
  features: [],
  images: [],
  livemode: false,
  marketing_features: [],
  metadata: {},
  name: 'myproduct',
  package_dimensions: null,
  shippable: null,
  statement_descriptor: null,
  tax_code: null,
  type: 'service',
  unit_label: null,
  updated: 1718087603,
  url: null
}
[wrangler:inf] POST /webhook 201 Created (14ms)
```

## 3. Convert text into vector data using Workers AI

We've prepared to ingest product information, so let's start implementing the preprocessing needed to create an index for search. In vector search using Cloudflare Vectorize, text data must be converted to numerical data before indexing. By storing data as numerical sequences, we can search based on the similarity of these vectors, allowing us to retrieve highly similar data.

In this step, we'll first implement the process of converting externally sent data into text data. This is necessary because the information to be converted into vector data is in text form. If you want to include product names, descriptions, and metadata as search targets, add the following processing.

```ts null {3,4,5,6,7,8,9}
if (event.type === "product.created") {
	const product = event.data.object;
	const productData = [
		`## ${product.name}`,
		product.description,
		"### metadata",
		Object.entries(product.metadata)
			.map(([key, value]) => `- ${key}: ${value}`)
			.join("\n"),
	].join("\n");
	console.log(JSON.stringify(productData, null, 2));
}
```

By adding this processing, you convert product information in JSON format into a simple Markdown format product introduction text.

```sh
## product name
product description.
### metadata
- key: value
```

Now that we've converted the data to text, let's convert it to vector data. By using the Text Embedding model of Workers AI, we can convert text into vector data of any desired dimension.

```ts null {7,8,9,10,11,12,13}
const productData = [
	`## ${product.name}`,
	product.description,
	"### metadata",
	Object.entries(product.metadata)
		.map(([key, value]) => `- ${key}: ${value}`)
		.join("\n"),
].join("\n");
const embeddings = await c.env.AI.run("@cf/baai/bge-large-en-v1.5", {
	text: productData,
});
console.log(JSON.stringify(embeddings, null, 2));
```

When using Workers AI, execute the `c.env.AI.run()` function. Specify the model you want to use as the first argument. In the second argument, input text data about the text you want to convert using the Text Embedding model or the instructions for the generated images or text. If you want to save the converted vector data using Vectorize, make sure to select a model that matches the number of `dimensions` specified in the `npx wrangler vectorize create` command. If the numbers do not match, there is a possibility that the converted vector data cannot be saved.

### Save vector data to Vectorize

Finally, let's save the created data to Vectorize. Edit `src/index.ts` to implement the indexing process using the `VECTORIZE_INDEX` binding. Since the data to be saved will be vector data, save the pre-conversion text data as metadata.

```ts null {16,17,18,19,20,21,22,23,24}
if (event.type === "product.created") {
	const product = event.data.object;
	const productData = [
		`## ${product.name}`,
		product.description,
		"### metadata",
		Object.entries(product.metadata)
			.map(([key, value]) => `- ${key}: ${value}`)
			.join("\n"),
	].join("\n");
	console.log(JSON.stringify(productData, null, 2));
	const embeddings = await c.env.AI.run("@cf/baai/bge-large-en-v1.5", {
		text: productData,
	});
	await c.env.VECTORIZE_INDEX.insert([
		{
			id: product.id,
			values: embeddings.data[0],
			metadata: {
				name: product.name,
				description: product.description || "",
				product_metadata: product.metadata,
			},
		},
	]);
}
```

With this, we have established a mechanism to synchronize the product data with the database for recommendations. Use Stripe CLI commands to save some product data.

```bash
stripe products create --name="Smartphone X" \
  --description="Latest model with cutting-edge features" \
  -d "default_price_data[currency]=usd" \
  -d "default_price_data[unit_amount]=79900" \
  -d "metadata[category]=electronics"
```

```bash
stripe products create --name="Ultra Notebook" \
  --description="Lightweight and powerful notebook computer" \
  -d "default_price_data[currency]=usd" \
  -d "default_price_data[unit_amount]=129900" \
  -d "metadata[category]=computers"
```

```bash
stripe products create --name="Wireless Earbuds Pro" \
  --description="High quality sound with noise cancellation" \
  -d "default_price_data[currency]=usd" \
  -d "default_price_data[unit_amount]=19900" \
  -d "metadata[category]=audio"
```

```bash
stripe products create --name="Smartwatch 2" \
  --description="Stay connected with the latest smartwatch" \
  -d "default_price_data[currency]=usd" \
  -d "default_price_data[unit_amount]=29900" \
  -d "metadata[category]=wearables"
```

```bash
stripe products create --name="Tablet Pro" \
  --description="Versatile tablet for work and play" \
  -d "default_price_data[currency]=usd" \
  -d "default_price_data[unit_amount]=49900" \
  -d "metadata[category]=computers"
```

If the save is successful, you will see logs like `[200] POST` in the screen where you are running the `stripe listen` command.

```sh
2024-06-11 16:41:42   --> product.created [evt_1PQPKsL8xlxrZ26gst0o1DK3]
2024-06-11 16:41:45  <--  [200] POST http://localhost:8787/webhook [evt_1PQPKsL8xlxrZ26gst0o1DK3]
2024-06-11 16:41:47   --> product.created [evt_1PQPKxL8xlxrZ26gGk90TkcK]
2024-06-11 16:41:49  <--  [200] POST http://localhost:8787/webhook [evt_1PQPKxL8xlxrZ26gGk90TkcK]
```

If you confirm one log entry for each piece of registered data, the save process is complete. Next, we will implement the API for related product searches.

## 4. Create a related products search API using Vectorize

Now that we have prepared the index for searching, the next step is to implement an API to search for related products. By utilizing a vector index, we can perform searches based on how similar the data is. Let's implement an API that searches for product data similar to the specified product ID using this method.

In this API, the product ID is received as a part of the API path. Using the received ID, vector data is retrieved from Vectorize using `c.env.VECTORIZE_INDEX.getByIds()`. The return value of this process includes vector data, which is then passed to `c.env.VECTORIZE_INDEX.query()` to conduct a similarity search. To quickly check which products are recommended, we set `returnMetadata` to `true` to obtain the stored metadata information as well. The `topK` parameter specifies the number of data items to retrieve. Change this value if you want to obtain less than 2 or more than 4 data items.

```ts
app.get("/products/:product_id", async (c) => {
	// Get the product ID from API path parameters
	const productId = c.req.param("product_id");

	// Retrieve the indexed data by the product ID
	const [product] = await c.env.VECTORIZE_INDEX.getByIds([productId]);

	// Search similar products by using the embedding data
	const similarProducts = await c.env.VECTORIZE_INDEX.query(product.values, {
		topK: 3,
		returnMetadata: true,
	});

	return c.json({
		product: {
			...product.metadata,
		},
		similarProducts,
	});
});
```

Let's run this API. Use a product ID that starts with `prod_`, which can be obtained from the result of running the `stripe products crate` command or the `stripe products list` command.

```sh
curl http://localhost:8787/products/prod_xxxx
```

If you send a request using a product ID that exists in the Vectorize index, the data for that product and two related products will be returned as follows.

```json
{
	"product": {
		"name": "Tablet Pro",
		"description": "Versatile tablet for work and play",
		"product_metadata": {
			"category": "computers"
		}
	},
	"similarProducts": {
		"count": 3,
		"matches": [
			{
				"id": "prod_QGxFoHEpIyxHHF",
				"metadata": {
					"name": "Tablet Pro",
					"description": "Versatile tablet for work and play",
					"product_metadata": {
						"category": "computers"
					}
				},
				"score": 1
			},
			{
				"id": "prod_QGxFEgfmOmy5Ve",
				"metadata": {
					"name": "Ultra Notebook",
					"description": "Lightweight and powerful notebook computer",
					"product_metadata": {
						"category": "computers"
					}
				},
				"score": 0.724717327
			},
			{
				"id": "prod_QGwkGYUcKU2UwH",
				"metadata": {
					"name": "demo product",
					"description": "aaaa",
					"product_metadata": {
						"test": "hello"
					}
				},
				"score": 0.635707003
			}
		]
	}
}
```

Looking at the `score` in `similarProducts`, you can see that there is data with a `score` of `1`. This means it is exactly the same as the query used to search. By looking at the metadata, it is evident that the data is the same as the product ID sent in the request. Since we want to search for related products, let's add a `filter` to prevent the same product from being included in the search results. Here, a filter is added to exclude data with the same product name using the `metadata` name.

```ts null {7,8,9,10,11}
app.get("/products/:product_id", async (c) => {
	const productId = c.req.param("product_id");
	const [product] = await c.env.VECTORIZE_INDEX.getByIds([productId]);
	const similarProducts = await c.env.VECTORIZE_INDEX.query(product.values, {
		topK: 3,
		returnMetadata: true,
		filter: {
			name: {
				$ne: product.metadata?.name.toString(),
			},
		},
	});

	return c.json({
		product: {
			...product.metadata,
		},
		similarProducts,
	});
});
```

After adding this process, if you run the API, you will see that there is no data with a `score` of `1`.

```json
{
	"product": {
		"name": "Tablet Pro",
		"description": "Versatile tablet for work and play",
		"product_metadata": {
			"category": "computers"
		}
	},
	"similarProducts": {
		"count": 3,
		"matches": [
			{
				"id": "prod_QGxFEgfmOmy5Ve",
				"metadata": {
					"name": "Ultra Notebook",
					"description": "Lightweight and powerful notebook computer",
					"product_metadata": {
						"category": "computers"
					}
				},
				"score": 0.724717327
			},
			{
				"id": "prod_QGwkGYUcKU2UwH",
				"metadata": {
					"name": "demo product",
					"description": "aaaa",
					"product_metadata": {
						"test": "hello"
					}
				},
				"score": 0.635707003
			},
			{
				"id": "prod_QGxFEafrNDG88p",
				"metadata": {
					"name": "Smartphone X",
					"description": "Latest model with cutting-edge features",
					"product_metadata": {
						"category": "electronics"
					}
				},
				"score": 0.632409942
			}
		]
	}
}
```

In this way, you can implement a system to search for related product information using Vectorize.

## 5. Create a recommendation API that answers user questions.

Recommendations can be more than just displaying related products; they can also address user questions and concerns. The final API will implement a process to answer user questions using Vectorize and Workers AI.

This API will implement the following processes:

1. Vectorize the user's question using the Text Embedding Model from Workers AI.
2. Use Vectorize to search and retrieve highly relevant products.
3. Convert the search results into a string in Markdown format.
4. Utilize the Text Generation Model from Workers AI to generate a response based on the search results.

This method realizes a text generation mechanism called Retrieval Augmented Generation (RAG) using Cloudflare. The bindings and other preparations are already completed, so let's add the API.

```ts
app.post("/ask", async (c) => {
	const { question } = await c.req.json();
	if (!question) {
		return c.json({
			message: "Please tell me your question.",
		});
	}
	/**
	 * Convert the question to the vector data
	 */
	const embeddedQuestion = await c.env.AI.run("@cf/baai/bge-large-en-v1.5", {
		text: question,
	});

	/**
	 * Query similarity data from Vectorize index
	 */
	const similarProducts = await c.env.VECTORIZE_INDEX.query(
		embeddedQuestion.data[0],
		{
			topK: 3,
			returnMetadata: true,
		},
	);

	/**
	 * Convert the JSON data to the Markdown text
	 **/
	const contextData = similarProducts.matches.reduce((prev, current) => {
		if (!current.metadata) return prev;
		const productTexts = Object.entries(current.metadata).map(
			([key, value]) => {
				switch (key) {
					case "name":
						return `## ${value}`;
					case "product_metadata":
						return `- ${key}: ${JSON.stringify(value)}`;
					default:
						return `- ${key}: ${value}`;
				}
			},
		);
		const productTextData = productTexts.join("\n");
		return `${prev}\n${productTextData}`;
	}, "");

	/**
	 * Generate the answer
	 */
	const response = await c.env.AI.run("@cf/meta/llama-3.1-8b-instruct", {
		messages: [
			{
				role: "system",
				content: `You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n#Context: \n${contextData} `,
			},
			{
				role: "user",
				content: question,
			},
		],
	});

	return c.json(response);
});
```

Let's use the created API to consult on a product. You can send your question in the body of a POST request. For example, if you want to ask about getting a new PC, you can execute the following command:

```sh
curl -X POST "http://localhost:8787/ask" -H "Content-Type: application/json" -d '{"question": "I want to get a new PC"}'
```

When the question is sent, a recommendation text will be generated as introduced earlier. In this example, the `Ultra Notebook` product was recommended. This is because it has a `notebook compucoter` description, which means it received a relatively high score in the Vectorize search.

```json
{
	"response": "Exciting! You're looking to get a new PC! Based on the context I retrieved, I'd recommend considering the \"Ultra Notebook\" since it's described as a lightweight and powerful notebook computer, which fits the category of \"computers\". Would you like to know more about its specifications or features?"
}
```

The text generation model generates new text each time based on the input prompt (questions or product search results). Therefore, even if you send the same request to this API, the response text may differ slightly. When developing for production, use features like logging or caching in the [AI Gateway](/ai-gateway/) to set up proper control and debugging.

## 6. Deploy the application

Before deploying the application, we need to make sure your Worker project has access to the Stripe API keys we created earlier. Since the API keys of external services are defined in `.dev.vars`, this information also needs to be set in your Worker project. To save API keys and secrets, run the `npx wrangler secret put <KEY>` command. In this tutorial, you'll execute the command twice, referring to the values set in `.dev.vars`.

```sh
npx wrangler secret put STRIPE_SECRET_API_KEY
npx wrangler secret put STRIPE_WEBHOOK_SECRET
```

Then, run `npx wrangler deploy`. This will deploy the application on Cloudflare, making it publicly accessible.

## Conclusion

As you can see, using Cloudflare Workers, Workers AI, and Vectorize allows you to easily implement related product or product recommendation APIs. Even if product data is managed on external services like Stripe, you can incorporate them by adding a webhook API. Additionally, though not introduced in this tutorial, you can save information such as user preferences and interested categories in Workers KV or D1. By using this stored information as text generation prompts, you can provide more accurate recommendation functions.

Use the experience from this tutorial to enhance your e-commerce site with new ideas.

---

# Custom access control for files in R2 using D1 and Workers

URL: https://developers.cloudflare.com/developer-spotlight/tutorials/custom-access-control-for-files/

import { Render, PackageManagers, WranglerConfig } from "~/components";

This tutorial gives you an overview on how to create a TypeScript-based Cloudflare Worker which allows you to control file access based on a simple username and password authentication. To achieve this, we will use a [D1 database](/d1/) for user management and an [R2 bucket](/r2/) for file storage.

The following sections will guide you through the process of creating a Worker using the Cloudflare CLI, creating and setting up a D1 database and R2 bucket, and then implementing the functionality to securely upload and fetch files from the created R2 bucket.

## Prerequisites

<Render file="prereqs" product="workers" />

## 1. Create a new Worker application

To get started developing your Worker you will use the [`create-cloudflare` CLI](https://github.com/cloudflare/workers-sdk/tree/main/packages/create-cloudflare). To do this, open a terminal window and run the following command:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"custom-access-control"}
/>

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "hello-world",
		type: "Hello World Worker",
		lang: "TypeScript",
	}}
/>

Then, move into your newly created Worker:

```sh
cd custom-access-control
```

## 2. Create a new D1 database and binding

Now that you have created your Worker, next you will need to create a D1 database.
This can be done through the Cloudflare Portal or the Wrangler CLI.
For this tutorial, we will use the Wrangler CLI for simplicity.

To create a D1 database, just run the following command.
If you get asked to install wrangler, just confirm by pressing `y` and then press `Enter`.

```sh
npx wrangler d1 create <YOUR_DATABASE_NAME>
```

Replace `<YOUR_DATABASE_NAME>` with the name you want to use for your database. Keep in mind that this name can't be changed later on.

After the database is successfully created, you will see the data for the binding displayed as an output.
The binding declaration will start with `[[d1_databases]]` and contain the binding name, database name and ID.
To use the database in your worker, you will need to add the binding to your Wrangler file, by copying the declaration and pasting it into the wrangler file, as shown in the example below.

<WranglerConfig>

```toml
[[d1_databases]]
binding = "DB"
database_name = "<YOUR_DATABASE_NAME>"
database_id = "<YOUR_DATABASE_ID>"
```

</WranglerConfig>

## 3. Create R2 bucket and binding

Now that the D1 database is created, you also need to create an R2 bucket which will be used to store the uploaded files.
This step can also be done through the Cloudflare Portal, but as before, we will use the Wrangler CLI for this tutorial.
To create an R2 bucket, run the following command:

```sh
npx wrangler r2 bucket create <YOUR_BUCKET_NAME>
```

This works similar to the D1 database creation, where you will need to replace `<YOUR_BUCKET_NAME>` with the name you want to use for your bucket.
To do this, go to the Wrangler file again and then add the following lines:



<WranglerConfig>

```toml
[[r2_buckets]]
binding = "BUCKET"
bucket_name = "<YOUR_BUCKET_NAME>"
```

</WranglerConfig>

Now that you have prepared the Wrangler configuration, you should update the `worker-configuration.d.ts` file to include the new bindings.
This file will then provide TypeScript with the correct type definitions for the bindings, which allows for type checking and code completion in your editor.
You could either update it manually or run the following command in the directory of your project to update it automatically based on the [Wrangler configuration file](/workers/wrangler/configuration/) (recommended).

```sh
npm run cf-typegen
```

## 4. Database preparation

Before you can start developing the Worker, you need to prepare the D1 database.

For this you need to

1. Create a table in the database which will then be used to store the user data
2. Create a unique index on the username column, which will speed up database queries and ensure that the username is unique
3. Insert a test user into the table, so you can test your code later on

As this operation only needs to be done once, this will be done through the Wrangler CLI and not in the Worker's code.
Copy the commands listed below, replace the placeholders and then run them in order to prepare the database.
For this tutorial you can replace the `<YOUR_USERNAME>` and `<YOUR_HASHED_PASSWORD>` placeholders with `admin` and `5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8` respecively.
And `<YOUR_DATABASE_NAME>` should be replaced with the name you used to create the database.

```sh
npx wrangler d1 execute <YOUR_DATABASE_NAME> --command "CREATE TABLE user (id INTEGER PRIMARY KEY NOT NULL, username STRING NOT NULL, password STRING NOT NULL)" --remote
npx wrangler d1 execute <YOUR_DATABASE_NAME> --command "CREATE UNIQUE INDEX user_username ON user (username)"  --remote
npx wrangler d1 execute <YOUR_DATABASE_NAME> --command "INSERT INTO user (username, password) VALUES ('<YOUR_USERNAME>', '<YOUR_HASHED_PASSWORD>')"  --remote
```

## 5. Implement authentication in the Worker

Now that the database and bucket are all set up, you can start to develop the Worker application.
The first thing you will need to do is to implement the authentication for the requests.

This tutorial will use a simple username and password authentication, where the username and password (hashed) are stored in the D1 database.
The requests will contain the username and password as a base64 encoded string, which is also called Basic Authentication.
Depending on the request method, this string will be retrieved from the `Authorization` header for POST requests or the `Authorization` search parameter for GET requests.

To handle the authentication, you will need to replace the current code within `index.ts` file with the following code:

```ts
export default {
	async fetch(
		request: Request,
		env: Env,
		ctx: ExecutionContext,
	): Promise<Response> {
		try {
			const url = new URL(request.url);
			let authBase64;
			if (request.method === "POST") {
				authBase64 = request.headers.get("Authorization");
			} else if (request.method === "GET") {
				authBase64 = url.searchParams.get("Authorization");
			} else {
				return new Response("Method Not Allowed!", { status: 405 });
			}
			if (!authBase64 || authBase64.substring(0, 6) !== "Basic ") {
				return new Response("Unauthorized!", { status: 401 });
			}

			const authString = atob(authBase64.substring(6));
			const [username, password] = authString.split(":");
			if (!username || !password) {
				return new Response("Unauthorized!", { status: 401 });
			}

			// TODO: Check if the username and password are correct
		} catch (error) {
			console.error("An error occurred!", error);
			return new Response("Internal Server Error!", { status: 500 });
		}
	},
};
```

The code above currently extracts the username and password from the request, but does not yet check if the username and password are correct.

To check the username and password, you will need to hash the password and then query the D1 database table `user` with the given username and hashed password.
If the username and password are correct, you will retrieve a record from D1. If the username or password is incorrect, undefined will be returned and a `401 Unauthorized` response will be sent.
To add this functionality, you will need to add the following code to the `fetch` function by replacing the TODO comment from the last code snippet:

```ts
const passwordHashBuffer = await crypto.subtle.digest(
	{ name: "SHA-256" },
	new TextEncoder().encode(password),
);
const passwordHashArray = Array.from(new Uint8Array(passwordHashBuffer));
const passwordHashString = passwordHashArray
	.map((b) => b.toString(16).padStart(2, "0"))
	.join("");

const user = await env.DB.prepare(
	"SELECT id FROM user WHERE username = ? AND password = ? LIMIT 1",
)
	.bind(username, passwordHashString)
	.first<{ id: number }>();
if (!user) {
	return new Response("Unauthorized!", { status: 401 });
}

// TODO: Implement upload functionality
```

This code will now ensure that every request is authenticated before it can be processed further.

## 6. Upload a file through the Worker

Now that the authentication is set up, you can start to implement the functionality for uploading a file through the Worker.
To do this, you will need to add a new code path that handles HTTP `POST` requests.
Then within it, you will need to get the data from the request, which is sent within the body of the request, by using the `request.blob()` function.
After that, you can upload the data to the R2 bucket by using the `env.BUCKET.put` function.
And finally, you will return a `200 OK` response to the client.

To implement this functionality, you will need to replace the TODO comment from the last code snippet with the following code:

```ts
if (request.method === "POST") {
	// Upload the file to the R2 bucket with the user id followed by a slash as the prefix and then the path of the URL
	await env.BUCKET.put(`${user.id}/${url.pathname}`, request.body);
	return new Response("OK", { status: 200 });
}
// TODO: Implement GET request handling
```

This code will now allow you to upload a file through the Worker, which will be stored in your R2 bucket.

## 7. Fetch from the R2 bucket

To round up the Worker application, you will need to implement the functionality to fetch files from the R2 bucket.
This can be done by adding a new code path that handles `GET` requests.
Within this code path, you will need to extract the URL pathname and then retrieve the asset from the R2 bucket by using the `env.BUCKET.get` function.

To finalize the code, just replace the TODO comment for handling GET requests from the last code snippet with the following code:

```ts
if (request.method === "GET") {
	const file = await env.BUCKET.get(`${user.id}/${url.pathname.slice(1)}`);
	if (!file) {
		return new Response("Not Found!", { status: 404 });
	}
	const headers = new Headers();
	file.writeHttpMetadata(headers);
	return new Response(file.body, { headers });
}
return new Response("Method Not Allowed!", { status: 405 });
```

This code now allows you to fetch and return data from the R2 bucket when a `GET` request is made to the Worker application.

## 8. Deploy your Worker

After completing the code for this Cloudflare Worker tutorial, you will need to deploy it to Cloudflare.
To do this open the terminal in the directory created for your application, and then run:

```sh
npx wrangler deploy
```

You might get asked to authenticate (if not logged in already) and select an account. After that, the Worker will be deployed to Cloudflare.
When the deployment finished successfully, you will see a success message with the URL where your Worker is now accessible.

## 9. Test your Worker (optional)

To finish this tutorial, you should test your Worker application by sending a `POST` request to upload a file and after that a `GET` request to fetch the file.
This can be done by using a tool like `curl` or `Postman`, but for simplicity, this will describe the usage of `curl`.

Copy the following command which can be used to upload a simple JSON file with the content `{"Hello": "Worker!"}`.
Replace `<YOUR_API_SECRET>` with the base64 encoded username and password combination and then run the command. For this example you can use `YWRtaW46cGFzc3dvcmQ=`, which can be decoded to `admin` and `test`, for the api secret placeholder.

```sh
curl --location '<YOUR_WORKER_URL>/myFile.json' \
--header 'Content-Type: application/json' \
--header 'Authorization: Basic <YOUR_API_SECRET>' \
--data '{
    "Hello": "Worker!"
}'
```

Then run the next command, or simply open the URL in your browser, to fetch the file you just uploaded:

```sh
curl --location '<YOUR_WORKER_URL>/myFile.json?Authorization=Basic%20YWRtaW46cGFzc3dvcmQ%3D'
```

## Next steps

If you want to learn more about Cloudflare Workers, R2, or D1 you can check out the following documentation:

- [Cloudflare Workers](/workers/)
- [Cloudflare R2](/r2/)
- [Cloudflare D1](/d1/)

---

# Setup Fullstack Authentication with Next.js, Auth.js, and Cloudflare D1

URL: https://developers.cloudflare.com/developer-spotlight/tutorials/fullstack-authentication-with-next-js-and-cloudflare-d1/

import {
	Render,
	PackageManagers,
	Type,
	TypeScriptExample,
	FileTree,
} from "~/components";

In this tutorial, you will build a [Next.js app](/workers/frameworks/framework-guides/nextjs/) with authentication powered by Auth.js, Resend, and [Cloudflare D1](/d1/).

Before continuing, make sure you have a Cloudflare account and have installed and [authenticated Wrangler](https://developers.cloudflare.com/workers/wrangler/commands/#login). Some experience with HTML, CSS, and JavaScript/TypeScript is helpful but not required. In this tutorial, you will learn:

- How to create a Next.js application and run it on Cloudflare Workers
- How to bind a Cloudflare D1 database to your Next.js app and use it to store authentication data
- How to use Auth.js to add serverless fullstack authentication to your Next.js app

You can find the finished code for this project on [GitHub](https://github.com/mackenly/auth-js-d1-example).

## Prerequisites

<Render file="prereqs" product="workers" />

3. Create or login to a [Resend account](https://resend.com/signup) and get an [API key](https://resend.com/docs/dashboard/api-keys/introduction#add-api-key).
4. [Install and authenticate Wrangler](/workers/wrangler/install-and-update/).

## 1. Create a Next.js app using Workers

From within the repository or directory where you want to create your project run:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"auth-js-d1-example --framework=next --experimental"}
/>

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "web-framework",
		framework: "Next.js",
	}}
/>

This will create a new Next.js project using [OpenNext](https://opennext.js.org/cloudflare) that will run in a Worker using [Workers Static Assets](/workers/frameworks/framework-guides/nextjs/#static-assets).

Before we get started, open your project's `tsconfig.json` file and add the following to the `compilerOptions` object to allow for top level await needed to let our application get the Cloudflare context:

```json title="tsconfig.json"
{
	"compilerOptions": {
		"target": "ES2022",
	}
}
```

Throughout this tutorial, we'll add several values to Cloudflare Secrets. For [local development](/workers/configuration/secrets/#local-development-with-secrets), add those same values to a file in the top level of your project called `.dev.vars` and make sure it is not committed into version control. This will let you work with Secret values locally. Go ahead and copy and paste the following into `.dev.vars` for now and replace the values as we go.

```sh title=".dev.vars"
AUTH_SECRET = "<replace-me>"
AUTH_RESEND_KEY = "<replace-me>"
AUTH_EMAIL_FROM = "onboarding@resend.dev"
AUTH_URL = "http://localhost:8787/"
```

:::note[Manually set URL]
Within the Workers environment, the `AUTH_URL` doesn't always get picked up automatically by Auth.js, hence why we're specifying it manually here (we'll need to do the same for prod later).
:::

## 2. Install Auth.js

Following the [installation instructions](https://authjs.dev/getting-started/installation?framework=Next.js) from Auth.js, begin by installing Auth.js:

<PackageManagers pkg="next-auth@beta" />

Now run the following to generate an `AUTH_SECRET`:

```sh
npx auth secret
```

Now, deviating from the standard Auth.js setup, locate your generated secret (likely in a file named `.env.local`) and [add the secret to your Workers application](/workers/configuration/secrets/#adding-secrets-to-your-project) by running the following and completing the steps to add a secret's value that we just generated:

```sh
npx wrangler secret put AUTH_SECRET
```

After adding the secret, update your `.dev.vars` file to include an `AUTH_SECRET` value (this secret should be different from the one you generated earlier for security purposes):

```sh title=".dev.vars"
# ...
AUTH_SECRET = "<replace-me>"
# ...
```

Next, go into the newly generated `env.d.ts` file and add the following to the <Type text="CloudflareEnv" /> interface:

```ts title="env.d.ts"
interface CloudflareEnv {
	AUTH_SECRET: string;
}
```

## 3. Install Cloudflare D1 Adapter

Now, install the Auth.js D1 adapter by running:

<PackageManagers pkg="@auth/d1-adapter" />

Create a D1 database using the following command:

```sh title="Create D1 database"
npx wrangler d1 create auth-js-d1-example-db
```

When finished you should see instructions to add the database binding to your [Wrangler configuration file](/workers/wrangler/configuration/). Example binding:

import { WranglerConfig} from "~/components";

<WranglerConfig>

```toml title="wrangler.toml"
[[d1_databases]]
binding = "DB"
database_name = "auth-js-d1-example-db"
database_id = "<unique-ID-for-your-database>"
```

</WranglerConfig>

Now, within your `env.d.ts`, add your D1 binding, like:

```ts title="env.d.ts"
interface CloudflareEnv {
	DB: D1Database;
	AUTH_SECRET: string;
}
```

## 4. Configure Credentials Provider

Auth.js provides integrations for many different [credential providers](https://authjs.dev/getting-started/authentication) such as Google, GitHub, etc. For this tutorial we're going to use [Resend for magic links](https://authjs.dev/getting-started/authentication/email). You should have already created a Resend account and have an [API key](https://resend.com/docs/dashboard/api-keys/introduction#add-api-key).

Using either a [Resend verified domain email address](https://resend.com/docs/dashboard/domains/introduction) or `onboarding@resend.dev`, add a new Secret to your Worker containing the email your magic links will come from:

```sh title="Add Resend email to secrets"
npx wrangler secret put AUTH_EMAIL_FROM
```

Next, ensure the `AUTH_EMAIL_FROM` environment variable is updated in your `.dev.vars` file with the email you just added as a secret:

```sh title=".dev.vars"
# ...
AUTH_EMAIL_FROM = "onboarding@resend.dev"
# ...
```

Now [create a Resend API key](https://resend.com/docs/dashboard/api-keys/introduction) with `Sending access` and add it to your Worker's Secrets:

```sh title="Add Resend API key to secrets"
npx wrangler secret put AUTH_RESEND_KEY
```

As with previous secrets, update your `.dev.vars` file with the new secret value for `AUTH_RESEND_KEY` to use in local development:

```sh title=".dev.vars"
# ...
AUTH_RESEND_KEY = "<replace-me>"
# ...
```

After adding both of those Secrets, your `env.d.ts` should now include the following:

```ts title="env.d.ts"
interface CloudflareEnv {
	DB: D1Database;
	AUTH_SECRET: string;
	AUTH_RESEND_KEY: string;
	AUTH_EMAIL_FROM: string;
}
```

Credential providers and database adapters are provided to Auth.js through a configuration file called `auth.ts`. Create a file within your `src/app/` directory called `auth.ts` with the following contents:

<TypeScriptExample filename="src/app/auth.ts">
```ts
import NextAuth from "next-auth";
import { NextAuthResult } from "next-auth";
import { D1Adapter } from "@auth/d1-adapter";
import Resend from "next-auth/providers/resend";
import { getCloudflareContext } from "@opennextjs/cloudflare";

const authResult = async (): Promise<NextAuthResult> => {
	return NextAuth({
		providers: [
			Resend({
				apiKey: (await getCloudflareContext()).env.AUTH_RESEND_KEY,
				from: (await getCloudflareContext()).env.AUTH_EMAIL_FROM,
			}),
		],
		adapter: D1Adapter((await getCloudflareContext()).env.DB),
	});
};

export const { handlers, signIn, signOut, auth } = await authResult();
```
</TypeScriptExample>

Now, lets add the route handler and middleware used to authenticate and persist sessions.

Create a new directory structure and route handler within `src/app/api/auth/[...nextauth]` called `route.ts`. The file should contain:

<TypeScriptExample filename="src/app/api/auth/[...nextauth]/route.ts">
```ts
import { handlers } from "../../../auth";

export const { GET, POST } = handlers;
```
</TypeScriptExample>

Now, within the `src/` directory, create a `middleware.ts` file to persist session data containing the following:

<TypeScriptExample filename="src/middleware.ts">
```ts
export { auth as middleware } from "./app/auth";
```
</TypeScriptExample>

## 5. Create Database Tables

The D1 adapter requires that tables be created within your database. It [recommends](https://authjs.dev/getting-started/adapters/d1#migrations) using the exported `up()` method to complete this. Within `src/app/api/` create a directory called `setup` containing a file called `route.ts`. Within this route handler, add the following code:

<TypeScriptExample filename="src/app/api/setup/route.ts">
```ts
import type { NextRequest } from 'next/server';
import { up } from "@auth/d1-adapter";
import { getCloudflareContext } from "@opennextjs/cloudflare";

export async function GET(request: NextRequest) {
    try {
        await up((await getCloudflareContext()).env.DB)
    } catch (e: any) {
        console.log(e.cause.message, e.message)
    }
    return new Response('Migration completed');
}

```
</TypeScriptExample>

You'll need to run this once on your production database to create the necessary tables. If you're following along with this tutorial, we'll run it together in a few steps.

:::note[Clean up]
Running this multiple times won't hurt anything since the tables are only created if they do not already exist, but it's a good idea to remove this route from your production code once you've run it since you won't need it anymore.
:::

Before we go further, make sure you've created all of the necessary files:
<FileTree>
- src/
  - app/
    - api/
      - auth/
        - [...nextauth]/
          - route.ts
      - setup/
        - route.ts
    - auth.ts
    - page.ts
  - middleware.ts
- env.d.ts
- wrangler.toml
</FileTree>

## 6. Build Sign-in Interface
We've completed the backend steps for our application. Now, we need a way to sign in. First, let's install [shadcn](https://ui.shadcn.com/):
```sh title="Install shadcn"
npx shadcn@latest init -d
```

Next, run the following to add a few components:
```sh title="Add components"
npx shadcn@latest add button input card avatar label
```

To make it easy, we've provided a basic sign-in interface for you below that you can copy into your app. You will likely want to customize this to fit your needs, but for now, this will let you sign in, see your account details, and update your user's name.

Replace the contents of `page.ts` from within the `app/` directory with the following:

```ts title="src/app/page.ts"
import { redirect } from 'next/navigation';
import { signIn, signOut, auth } from './auth';
import { updateRecord } from '@auth/d1-adapter';
import { getCloudflareContext } from '@opennextjs/cloudflare';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Card, CardContent, CardDescription, CardHeader, CardTitle, CardFooter } from '@/components/ui/card';
import { Avatar, AvatarFallback, AvatarImage } from '@/components/ui/avatar';
import { Label } from '@/components/ui/label';

async function updateName(formData: FormData): Promise<void> {
	'use server';
	const session = await auth();
	if (!session?.user?.id) {
		return;
	}
	const name = formData.get('name') as string;
	if (!name) {
		return;
	}
	const query = `UPDATE users SET name = $1 WHERE id = $2`;
	await updateRecord((await getCloudflareContext()).env.DB, query, [name, session.user.id]);
	redirect('/');
}

export default async function Home() {
	const session = await auth();
	return (
		<main className="flex items-center justify-center min-h-screen bg-background">
			<Card className="w-full max-w-md">
				<CardHeader className="space-y-1">
					<CardTitle className="text-2xl font-bold text-center">{session ? 'User Profile' : 'Login'}</CardTitle>
					<CardDescription className="text-center">
						{session ? 'Manage your account' : 'Welcome to the auth-js-d1-example demo'}
					</CardDescription>
				</CardHeader>
				<CardContent>
					{session ? (
						<div className="space-y-4">
							<div className="flex items-center space-x-4">
								<Avatar>
									<AvatarImage src={session.user?.image || ''} alt={session.user?.name || ''} />
									<AvatarFallback>{session.user?.name?.[0] || 'U'}</AvatarFallback>
								</Avatar>
								<div>
									<p className="font-medium">{session.user?.name || 'No name set'}</p>
									<p className="text-sm text-muted-foreground">{session.user?.email}</p>
								</div>
							</div>
							<div>
								<p className="text-sm font-medium">User ID: {session.user?.id}</p>
							</div>
							<form action={updateName} className="space-y-2">
								<Label htmlFor="name">Update Name</Label>
								<Input id="name" name="name" placeholder="Enter new name" />
								<Button type="submit" className="w-full">
									Update Name
								</Button>
							</form>
						</div>
					) : (
						<form
							action={async (formData) => {
								'use server';
								await signIn('resend', { email: formData.get('email') as string });
							}}
							className="space-y-4"
						>
							<div className="space-y-2">
								<Input
									type="email"
									name="email"
									placeholder="Email"
									autoCapitalize="none"
									autoComplete="email"
									autoCorrect="off"
									required
								/>
							</div>
							<Button className="w-full" type="submit">
								Sign in with Resend
							</Button>
						</form>
					)}
				</CardContent>
				{session && (
					<CardFooter>
						<form
							action={async () => {
								'use server';
								await signOut();
								Response.redirect('/');
							}}
						>
							<Button type="submit" variant="outline" className="w-full">
								Sign out
							</Button>
						</form>
					</CardFooter>
				)}
			</Card>
		</main>
	);
}
```

## 7. Preview and Deploy

Now, it's time to preview our app. Run the following to preview your application:

<PackageManagers
	type="run"
	args={"preview"}
/>

:::caution[Windows support]
OpenNext has [limited Windows support](https://opennext.js.org/cloudflare#windows-support) and recommends using WSL2 if developing on Windows.
:::

You should see our login form. But wait, we're not done yet. Remember to create your database tables by visiting `/api/setup`. You should see `Migration completed`. This means your database is ready to go.

Navigate back to your application's homepage. Enter your email and sign in (use the same email as your Resend account if you used the `onboarding@resend.dev` address). You should receive an email in your inbox (check spam). Follow the link to sign in. If everything is configured correctly, you should now see a basic user profile letting your update your name and sign out.

Now let's deploy our application to production. From within the project's directory run:

<PackageManagers
	type="run"
	args={"deploy"}
/>

This will build and deploy your application as a Worker. Note that you may need to select which account you want to deploy your Worker to. After your app is deployed, Wrangler should give you the URL on which it was deployed. It might look something like this: `https://auth-js-d1-example.example.workers.dev`. Add your URL to your Worker using:

```sh title="Add URL to secrets"
npx wrangler secret put AUTH_URL
```

After the changes are deployed, you should now be able to access and try out your new application.

You have successfully created, configured, and deployed a fullstack Next.js application with authentication powered by Auth.js, Resend, and Cloudflare D1.

## Related resources

To build more with Workers, refer to [Tutorials](/workers/tutorials/).

Find more information about the tools and services used in this tutorial at:

- [Auth.js](https://authjs.dev/getting-started)
- [Resend](https://resend.com/)
- [Cloudflare D1](/d1/)

If you have any questions, need assistance, or would like to share your project, join the Cloudflare Developer community on [Discord](https://discord.cloudflare.com) to connect with other developers and the Cloudflare team.

---

# Send form submissions using Astro and Resend

URL: https://developers.cloudflare.com/developer-spotlight/tutorials/handle-form-submission-with-astro-resend/

This tutorial will instruct you on how to send emails from [Astro](https://astro.build/) and Cloudflare Workers (via Cloudflare SSR Adapter) using [Resend](https://resend.com/).

## Prerequisites

Make sure you have the following set up before proceeding with this tutorial:

- AÂ [Cloudflare account](https://dash.cloudflare.com/sign-up/workers-and-pages)
- InstalledÂ [npm](https://docs.npmjs.com/getting-started).
- AÂ [Resend account](https://resend.com/signup).

## 1. Create a new Astro project and install Cloudflare Adapter:

Open your terminal and run the below command:

```bash title="Create Astro project"
npm create cloudflare@latest my-astro-app -- --framework=astro
```

Follow the prompts to configure your project, selecting your preferred options for TypeScript usage, TypeScript strictness, version control, and deployment.

After the initial installation change into the newly created project directory `my-astro-app` and run the following to add the Cloudflare adapter:

```bash title="Install Cloudflare Adapter"
npm run astro add cloudflare
```

The [`@astrojs/cloudflare` adapter](https://github.com/withastro/adapters/tree/main/packages/cloudflare#readme) allows Astro's Server-Side Rendered (SSR) sites and components to work on Cloudflare Pages and converts Astro's endpoints into Cloudflare Workers endpoints.

## 2. Add your domain to Resend

:::note

If you do not have a domain and just want to test you can skip to step 4 of this section.

:::

1. **Add Your Domain from Cloudflare to Resend:**
   - After signing up for Resend, navigate to the side menu and click `Domains`.
   - Look for the button to add a new domain and click it.
   - A pop-up will appear where you can type in your domain. Do so, then choose a region and click the `add` button.
   - After clicking the add button Resend will provide you with a list of DNS records (DKIM, SPF, and DMARC).
2. **Copy DNS Records from Resend to Cloudflare:**
   - Go back to your Cloudflare dashboard.
   - Select the domain you want to use and find the "DNS" section.
   - Copy and paste the DNS records from Resend to Cloudflare.
3. **Verify Your Domain:**
   - Return to Resend and click on the "Verify DNS Records" button.
   - If everything is set up correctly, your domain status will change to "Verified."
4. **Create an API Key:**
   - In Resend, find the "API Keys" option in the side menu and click it.
   - Create a new API key with a descriptive name and give Full Access permission.
5. **Save the API key for Local Development and Deployed Worker**
   - For local development, create an .env in the root folder of your Astro project and save the API key as RESEND_API_KEY='Api key here' (no quotes).
   - For a deployed Worker, run the following in your CLI and follow the instructions.

```bash
npx wrangler secret put RESEND_API_KEY
```

## 3. Create an Astro endpoint

In the `src/pages` directory, create a new folder called `api`. Inside the `api` folder, create a new file called `sendEmail.json.ts`. This will create an endpoint at `/api/sendEmail.json`.

Copy the following code into the `sendEmail.json.ts` file. This code sets up a POST route that handles form submissions, and validates the form data.

```ts
export const prerender = false; //This will not work without this line

import type { APIRoute } from "astro";

export const POST: APIRoute = async ({ request }) => {
	const data = await request.formData();
	const name = data.get("name");
	const email = data.get("email");
	const message = data.get("message"); // Validate the data - making sure values are not empty

	if (!name || !email || !message) {
		return new Response(null, {
			status: 404,
			statusText: "Did not provide the right data",
		});
	}
};
```

## 4. Send emails using Resend

Next you will need to install the Resend SDK.

```bash title="Install Resend's SDK"
npm i resend
```

Once the SDK is installed you can add in the rest of the code that sends an email using the Resend's API, and conditionally checks if the Resend response was successful or not.

```ts
export const prerender = false; //This will not work without this line

import type { APIRoute } from "astro";
import { Resend } from "resend";

const resend = new Resend(import.meta.env.RESEND_API_KEY);

export const POST: APIRoute = async ({ request }) => {
	const data = await request.formData();
	const name = data.get("name");
	const email = data.get("email");
	const message = data.get("message"); // Validate the data - making sure values are not empty

	if (!name || !email || !message) {
		return new Response(
			JSON.stringify({
				message: `Fill out all fields.`,
			}),
			{
				status: 404,
				statusText: "Did not provide the right data",
			},
		);
	} // Sending information to Resend

	const sendResend = await resend.emails.send({
		from: "support@resend.dev",
		to: "delivered@resend.dev",
		subject: `Sumbission from ${name}`,
		html: `<p>Hi ${name},</p><p>Your message was received.</p>`,
	}); // If the message was sent successfully, return a 200 response

	if (sendResend.data) {
		return new Response(
			JSON.stringify({
				message: `Message successfully sent!`,
			}),
			{
				status: 200,
				statusText: "OK",
			},
		); // If there was an error sending the message, return a 500 response
	} else {
		return new Response(
			JSON.stringify({
				message: `Message failed to send: ${sendResend.error}`,
			}),
			{
				status: 500,
				statusText: `Internal Server Error: ${sendResend.error}`,
			},
		);
	}
};
```

:::note

Make sure to change the 'to' property in 'resend.emails.send' function, if you set up your own domain in step 2. If you skipped that step, keep the value '[delivered@resend.dev](mailto:delivered@resend.dev)'; otherwise, Resend will throw an error.

:::

## 5. Create an Astro Form Component

In the `src` directory, create a new folder called `components`. Inside the `components` folder, create a new file `AstroForm.astro` and copy the provided code into it.

```typescript
---
export const prerender = false;

type formData = {
  name: string;
  email: string;
  message: string;
};

if (Astro.request.method === "POST") {
  try {
    const formData = await Astro.request.formData();
    const response = await fetch(Astro.url + "/api/sendEmail.json", {
      method: "POST",
      body: formData,
    });

    const data: formData = await response.json();

    if (response.status === 200) {
      console.log(data.message);
    }
  } catch (error) {
    if (error instanceof Error) {
      console.error(`Error: ${error.message}`);
    }
  }
}
---

<form method="POST">
Â  Â  <label>
    Â  Â  Name
    Â  Â  <input type="text" id="name" name="name" required />
Â  Â  </label>
Â  Â  <label>
    Â  Â  Email
    Â  Â  <input type="email" id="email" name="email" required />
Â  Â  </label>
Â  Â  <label>
    Â  Â  Message
    Â  Â  <textarea id="message" name="message" required />
Â  Â  </label>
Â  Â  <button>Send</button>
</form>

```

This code creates an Astro component that renders a form and handles the form submission. When the form is submitted, the component will send a POST request to the `/api/sendEmail.json` endpoint created in the previous step with the form data.

:::caution[File Extension]

Astro requires an absolute URL, which is why you should use `Astro.url + "/api/sendEmail.json`. If you use a relative path the post request will fail.

:::

Additionally, adding the `export const prerender = false;` will enable SSR; otherwise, the component will be static and unable to send a post request. If you don't enable it inside the component then you will need to enable SSR via the [template directive](https://docs.astro.build/en/reference/directives-reference/).

After creating the `AstroForm` component, add the component to your main index file located in the `src/pages` directory. Below is an example of how the main index file should look with the `AstroForm` component added.

```typescript
---
import AstroForm from '../components/AstroForm.astro'
---

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width" />
    <meta name="generator" content={Astro.generator} />
    <title>Astro</title>
  </head>
  <body>
    <AstroForm />
  </body>
</html>

```

## 6. Conclusion

You now have an Astro form component that sends emails via Resend and Cloudflare Workers. You can view your project locally via `npm run preview`, or you can deploy it live via `npm run deploy`.

---

# Tutorials

URL: https://developers.cloudflare.com/developer-spotlight/tutorials/

import { ListTutorials } from "~/components"

<ListTutorials />

---

# Manage content

URL: https://developers.cloudflare.com/docs-guide/manage-content/

import { DirectoryListing } from "~/components"

Almost more important than creating content, you need to figure out smart ways to manage that content.

<DirectoryListing />

---

# Analytics and logs

URL: https://developers.cloudflare.com/dns/additional-options/analytics/

import { FeatureTable } from "~/components"

When you use Cloudflare DNS, you can access data about DNS queries through a variety of sources.

---

## Analytics

DNS analytics allow you to evaluate data about DNS queries to your zone.

You can [use the dashboard](#view-on-the-dashboard) to get insights quickly based on a [predefined set of dimensions](#available-dimensions), or [use the API](#explore-with-the-api) to have access to all fields available in the GraphQL DNS analytics schemas.

### Availability and limits

<FeatureTable id="dns.dns_analytics" />

### View on the dashboard

For a quick summary, view your DNS analytics on the dashboard:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account and domain.
2. Go to **DNS** > **Analytics**.

The DNS analytics dashboard contains [four main panels](#dns-analytics-panels). The filters and time frame that you specify at the top of the page apply to all of them.

#### Available dimensions

- Query name
- Query type (same as DNS record type)
- Response code
- Data center
- Source IP
- Destination IP
- Protocol
- IP version

#### DNS analytics panels

- **Query overview**: the number of queries per hour and their distribution over time. This information is segmented by each of the [available dimensions](#available-dimensions) and the graph displays the top five values. You can select the dimensions through the different tabs above the graph and quickly filter for or exclude a certain value from the results by hovering over it and selecting **Filter** or **Exclude**.

- **Query statistics**: an overview of query metrics based on your filters and selected time frame. Namely, **Total queries**, **Average queries per second**, and **Average processing time**. The average processing time is displayed in milliseconds and includes upstream queries in the case of [flattened CNAME records](/dns/cname-flattening/).

  :::note
  Processing time is different from response time. Response time would have to include information that is not available to Cloudflare, such as how long the query takes from the client to the resolver and from the resolver to Cloudflare (as your authoritative DNS provider).
  :::

- **DNS queries by data center**: a map indicating which Cloudflare data centers have handled DNS queries to your zone in the selected time period. You can also find a list of the ten top results and quickly filter for or exclude a certain data center from the results by hovering over it and selecting **Filter** or **Exclude**.

- **Queries by source**: a breakdown of the top five, ten, or fifteen results - based on your selection - and grouped by the [available dimensions](#available-dimensions).


### Explore with the API

For more detailed metrics, use the [GraphQL API](/analytics/graphql-api/). Refer to the GraphQL Analytics API documentation for guidance on how to [get started](/analytics/graphql-api/getting-started/).

The DNS analytics has two [schemas](/analytics/graphql-api/getting-started/querying-basics/):

* `dnsAnalyticsAdaptive`: Retrieve information about individual DNS queries.
* `dnsAnalyticsAdaptiveGroups`: Get reports on aggregate information only.

---

## Logs

Logs let Enterprise customers view [detailed information](/logs/reference/log-fields/zone/dns_logs/) about individual DNS queries.

For help setting up Logpush, refer to [Get started with Logs](/logs/get-started/).

---

# Configure DNS zone defaults

URL: https://developers.cloudflare.com/dns/additional-options/dns-zone-defaults/

While there are default values for DNS settings that Cloudflare applies to all new zones, Enterprise accounts have the option to configure their own DNS zone defaults according to their preference.

:::caution
DNS zone defaults are only applied at the moment a new zone is created and will not impact already existing zones. Any of the values specified as default can later be adjusted within each zone, on the respective [**DNS** > **Settings**](https://dash.cloudflare.com/?to=/:account/:zone/dns/settings) or [**DNS** > **Records**](https://dash.cloudflare.com/?to=/:account/:zone/dns/records) page.
:::

## Steps

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Go to **Manage Account** > **Configurations** > **DNS Settings**. If these options are not displayed on your Cloudflare dashboard, you may need to reach out to your account team to have them added.
3. For **DNS zone defaults**, select **Configure defaults**.

The values you select for the listed settings will be automatically applied to new zones as you add them to your Cloudflare account.

## Available settings

- [Nameserver assignment](/dns/nameservers/nameserver-options/#assignment-method): Select your preferred nameserver type or assignment method that you want Cloudflare to use for your new zones. This setting applies both to primary zones ([full setup](/dns/zone-setups/full-setup/)) and [secondary zones](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/).

For primary zones:

- [Multi-provider DNS](/dns/nameservers/nameserver-options/#multi-provider-dns): Control whether or not Cloudflare will consider `NS` records you add on the zone apex and if zones that contain external nameservers listed in the registrar will be activated.
- [Nameserver TTL](/dns/nameservers/nameserver-options/#nameserver-ttl): Control how long, in seconds, your nameserver (`NS`) records are cached. The default time-to-live (TTL) is 24 hours. This setting applies both to Cloudflare nameservers and [custom nameservers](/dns/nameservers/custom-nameservers/).
- [SOA record](/dns/manage-dns-records/reference/dns-record-types/#soa): Adjust values for the start of authority (SOA) record that Cloudflare creates for your zone.

For secondary zones:

- [Secondary DNS override](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/proxy-traffic/): Enable the options to use Cloudflare [proxy](/dns/proxy-status/) and add `CNAME` records at your zone apex.

  Multi-provider DNS does not apply as a setting for secondary zones, as this is already a required behavior for this setup. `SOA` record and the `NS` record TTL are defined on your external DNS provider and only transferred into Cloudflare.

---

# Additional options

URL: https://developers.cloudflare.com/dns/additional-options/

import { DirectoryListing } from "~/components"

Beyond [setting up your zone](/dns/zone-setups/) and [updating your DNS records](/dns/manage-dns-records/), you may want to customize the following settings in Cloudflare DNS:

<DirectoryListing />

---

# Reverse zones and PTR records

URL: https://developers.cloudflare.com/dns/additional-options/reverse-zones/

import { Details, Example } from "~/components"

If you control your own IP prefix(es), you can set up reverse zones with PTR records to allow reverse DNS lookups.

## PTR records

PTR records specify the allowed hosts for a given IP address. They are the opposite of [A records](https://www.cloudflare.com/learning/dns/dns-records/dns-a-record) and used for reverse DNS lookups.

Historically, PTR records prevented outbound SMTP servers from being blocked by spam filters. However, more modern DNS records â€” [SPF, DKIM, and DMARC](/dns/manage-dns-records/how-to/email-records/#prevent-domain-spoofing) â€” provide better verifications of domain ownership.

Now, PTR records are primarily useful for those who own a dedicated IP space. They can help populate trace routes and security tools with human-readable domain names.

As PTR records are mainly used for reverse DNS lookups, they should preferably be added to reverse zones.

## Availability

The following Cloudflare customers can create reverse zones.

* Customers with an IPv4 or IPv6 address space can add the IPv4 or IPv6 reverse zone for their IP space to their account, and create the required PTR records for forward resolution.
* DNS Firewall customers need to contact their account team to add PTR records for the IPs used for their DNS Firewall clusters.

If your account does not meet these qualifications and you do not own the IP prefix you want to add PTR records on, contact the owner of the IP address based on a [whois lookup](https://lookup.icann.org/).

## Set up a reverse zone

To set up a reverse zone, you need to create a reverse DNS zone and add PTR records for forward resolution.

### 1. Create a reverse DNS zone

1. Within your account, click **Add site**.

2. For your site name, use the reverse IP address:

   * For IPv4 /24 prefixes, the pattern is:
     * **IP prefix**: `<octet_1>.<octet_2>.<octet_3>.0/24`
     * **Reverse zone address**: `<octet_3>.<octet_2>.<octet_1>.in-addr.arpa`
   * For IPv4 /16 prefixes, the pattern is:

     * **IP prefix**: `<octet_1>.<octet_2>.0.0/16`
     * **Reverse zone address**: `<octet_2>.<octet_1>.in-addr.arpa`


    <Details header="Example">

    * **IPv4 prefix**: `198.51.100.0/24`
    * **Reverse zone**: `100.51.198.in-addr.arpa`

    </Details>

  * For IPv6, consider the following examples:

  <Example>
  * **IPv6 prefix**: `2001:DB8::0/32`
  * **Reverse zone**: `8.b.d.0.1.0.0.2.ip6.arpa`
  </Example>
  <Example>
  * **IPv6 prefix**: `2001:DB8::0/48`
  * **Reverse zone**: `0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa`
  </Example>

3. If you are adding less than 200 PTR records, select the **Free** plan. If you are adding more, select a paid plan.

4. Skip the rest of the onboarding process.

### 2. Add PTR records

1. Go to **DNS** > **Records**.

2. For each IP within the prefix, add a PTR record using the least significant octet(s) as the subdomain.

  <Details header="IPv4 example">

  Suppose you have the following configuration:

  * **Reverse zone**: `100.51.198.in-addr.arpa`
  * **IP address**: `198.51.100.123`

  The subdomain for the PTR record would be `123`, making the full domain for forward lookup `123.100.51.198.in-addr.arpa`.

  <Example>

  | Type       | Name  | Domain name   | TTL    |
  | ---------- | ----- | ------------- | ------ |
  | `PTR`      | `123` | `example.com` | `Auto` |

  </Example>

  </Details>

  <Details header="IPv6 example">

  Suppose you have the following configuration:

  * **Reverse zone**: `0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa`
  * **IP address**: `2001:DB8::5`

  The subdomain for the PTR record would be `5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0`, making the full domain for forward lookup `5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa`.

  <Example>

  | Type       | Name                                      | Domain name   | TTL    |
  | ---------- | ----------------------------------------- | ------------- | ------ |
  | `PTR`      | `5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0` | `example.com` | `Auto` |

  </Example>

  </Details>

3\. Add the two Cloudflare nameservers provided for the zone at your Regional Internet Registry (RIR).

After this process, your reverse zone will be activated and you can perform reverse DNS lookups.

## Other resources

While setting up reverse zones, the following third-party tools may be useful:

* [Reverse DNS record generator](https://www.whatsmydns.net/reverse-dns-generator)
* [IPv6 subnet calculator](https://www.internex.at/de/toolbox/ipv6)

---

# Example diagram

URL: https://developers.cloudflare.com/dns/cname-flattening/cname-flattening-diagram/

import { Example } from "~/components"

With CNAME flattening, Cloudflare returns an IP address instead of the target hostname that a CNAME record points to.
This process supports a few features and delivers better performance and flexibility, as mentioned in the [CNAME flattening concept page](/dns/cname-flattening/).

Consider the diagram below to have an overview of the steps that may be involved in CNAME flattening.

:::note

Note that this is a simpler scenario. Cases where CNAME flattening is optional and/or the target hostname is not external to Cloudflare work differently.

:::

## Example use case

* `domain.test` is a zone on Cloudflare and has the following CNAME record:

<Example>

| Type       | Name          | Content                | TTL    |
| ---------- | ------------- | ---------------------- | ------ |
| `CNAME`    | `domain.test` | `external-origin.test` | `3600` |

</Example>

* `external-origin.test` is a zone on a different DNS provider and has the following A record:

<Example>

| Type       | Name                   | Content     | TTL    |
| ---------- | ---------------------- | ----------- | ------ |
| `A`        | `external-origin.test` | `192.0.2.1` | `7200` |

</Example>

In this case, the process to respond to queries for `domain.test` directly with the IP address can be represented by the following diagram:

```mermaid
flowchart BT
accTitle: CNAME flattening diagram
accDescr: Diagram of CNAME flattening process when there is a request for a domain in Cloudflare and the zone has a CNAME record at apex that points to an external A record.
  A((User)) <--query for <code>domain.test</code>--> B[Resolver] --> C
  C["Question:
  <code>domain.test IN A</code>"]
 subgraph Y[Cloudflare DNS]
 direction RL
  D{{Look up record}} --> G["Answer:
  <code>domain.test 3600 CNAME external-origin.test</code>

  This means that <code>domain.test</code> is a <code>CNAME</code> at the zone apex.
  Forced <code>CNAME</code> flattening is enabled."] --- H{{Resolve <code>external-origin.test</code>}}
  K{{Append answer with overwritten query name}} --> L["Answer:
  <code>domain.test 7200 IN A 192.0.2.1</code>"] --- M{Proxy status}
  M --Proxied--> O["Answer:
  <code>domain.test 300 IN A {$Cloudflare IP 1}</code>
  <code>domain.test 300 IN A {$Cloudflare IP 2}</code>"]
  M --DNS only--> N["Answer:
  <code>domain.test 3600 IN A 192.0.2.1</code>"]
 end

 subgraph Z [External DNS provider]
  J["Answer:
  <code>external-origin.test 7200 IN A 192.0.2.1</code>"]
 end

 C --> D
 H --- J --- K
 O --> B
 N --> B
```

## Aspects to consider

* If the CNAME record is proxied in Cloudflare, the answer is made up of multiple [Cloudflare IPs](https://www.cloudflare.com/ips/) and its Time to Live (TTL) is set to `300`.
* If the CNAME record in Cloudflare is not proxied, the flattened answer consists of the IP address from the external DNS provider and its TTL corresponds to the lower value between the external record and the Cloudflare CNAME record.

---

# CNAME flattening

URL: https://developers.cloudflare.com/dns/cname-flattening/

import { Render } from "~/components"

CNAME flattening speeds up CNAME resolution and allows you to use a CNAME record at your root/apex domain (`example.com`).

:::note


This functionality is also what allows you to use a [root custom domain](/pages/configuration/custom-domains/) with a Cloudflare Pages site.


:::

## How it works

With CNAME flattening, Cloudflare finds the IP address that a CNAME points to. This process could involve a single lookup or multiple (if your CNAME points to another CNAME). Cloudflare then returns the final IP address instead of a CNAME record, helping DNS queries resolve up to 30% faster.

For more details on the steps involved in CNAME flattening, review the [CNAME flattening diagram](/dns/cname-flattening/cname-flattening-diagram/) and refer to the [Cloudflare blog post](https://blog.cloudflare.com/introducing-cname-flattening-rfc-compliant-cnames-at-a-domains-root/).

## Aspects to keep in mind

* CNAME flattening happens by default in some cases. Refer to [Setup](/dns/cname-flattening/set-up-cname-flattening/) for details.
* <Render file="cname-flattening-callout" />

---

# Setup

URL: https://developers.cloudflare.com/dns/cname-flattening/set-up-cname-flattening/

import { Render, TabItem, Tabs, GlossaryTooltip } from "~/components"

:::note


If the CNAME target is on the same zone as the CNAME record, Cloudflare proceeds with CNAME flattening and ignores the **CNAME Flattening** setting.


:::

## For your zone apex

CNAME flattening occurs by default for all plans when your domain uses a CNAME record for its zone apex (`example.com`, meaning the record **Name** is set to `@`).

## For all CNAME records

For zones on paid plans, you can choose to flatten all CNAME records. This option is useful for <GlossaryTooltip term="proxy status">DNS-only (unproxied)</GlossaryTooltip> CNAME records. [Proxied records](/dns/proxy-status/) are flattened by default as they return Cloudflare anycast IPs.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account and domain.
2. Go to **DNS** > **Settings**.
3. Turn on the option **CNAME flattening for all CNAME records**.

</TabItem> <TabItem label="API">

Make a `PATCH` request to the [Update DNS Settings](/api/resources/dns/subresources/settings/methods/edit/) endpoint and set `flatten_all_cnames` to `true` in the request body.

</TabItem> </Tabs>

:::caution
 <Render file="cname-flattening-callout" />
:::

## Per record

Paid zones also have the option of flattening specific CNAME records.

If you use this option, a special [tag](/dns/manage-dns-records/reference/record-attributes/) `cf-flatten-cname` will be added to the respective flattened CNAME records in your zone file, allowing you to [export and import records](/dns/manage-dns-records/how-to/import-and-export/) without losing this configuration.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account and domain.
2. In **DNS** > **Settings**, make sure that **CNAME flattening for all CNAME records** is turned off.
3. Go to **DNS** > **Records** and find the CNAME record you would like to flatten.
4. Select **Edit** and turn on the **Flatten** option.
5. Select **Save** to confirm.

:::note[Unavailable flatten option]
For the following cases, **Flatten** will not be available:
- The record is at the [zone apex](#for-your-zone-apex).
- The record is already proxied, which means it will be flattened by default.
- **CNAME flattening for all CNAME records** is turned on, which means you cannot override it per record.
:::

</TabItem> <TabItem label="API">

With the available [API endpoints](/api/resources/dns/subresources/records/methods/create/), specify the following for each CNAME record in the request body:

```txt
"settings": {
  "flatten_cname": true
}
```

</TabItem> </Tabs>

---

# Analytics

URL: https://developers.cloudflare.com/dns/dns-firewall/analytics/

To access analytics for your DNS Firewall, use the [Cloudflare API](/api/resources/dns_firewall/subresources/analytics/subresources/reports/methods/get/).

Alternatively, [set up Logpush](/logs/about/) to deliver [DNS Firewall logs](/logs/reference/log-fields/account/dns_firewall_logs/) to a storage service, SIEM, or log management provider.

## Response reasons

When analyzing why Cloudflare DNS Firewall responded in one way or another to a specific query, consider the `responseReason` log field.

The following table provides a description for each of the values that might be returned as a response reason:

| Value                     | Description                                                                                                                                                    |
| ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `success`                 | Response was successfully served, either from Cloudflare cache or forwarded from the upstream.                                                                 |
| `upstream_failure`        | Response could not be fetched from the upstream due to the upstream failing to respond.                                                                        |
| `upstream_servfail`       | Response could not be fetched from the upstream due to the upstream responding with `SERVFAIL`.                                                                |
| `invalid_query`           | Query is invalid and cannot be processed.                                                                                                                      |
| `any_type_blocked`        | Query of type `ANY` was blocked according to your [DNS Firewall settings](/dns/dns-firewall/setup/) ([RFC 8482](https://www.rfc-editor.org/rfc/rfc8482.html)). |
| `rate_limit`              | Query was rate limited according to your [DNS Firewall settings](/dns/dns-firewall/setup/).                                                                    |
| `chaos_success`           | Response for [Chaos class](https://en.wikipedia.org/wiki/Chaosnet) was successfully served.                                                                    |
| `attack_mitigation_block` | Query was blocked as part of [random prefix attack mitigation](/dns/dns-firewall/random-prefix-attacks/).                                                      |
| `unknown`                 | There was an unknown error.                                                                                                                                    |

---

# FAQ

URL: https://developers.cloudflare.com/dns/dns-firewall/faq/

import { Details, GlossaryTooltip } from "~/components";

<Details header="How does DNS Firewall choose a backend nameserver to query upstream?">

DNS Firewall alternates between a customer's nameservers, using an algorithm is more likely to send queries to the faster upstream nameservers than slower nameservers.

</Details>

<Details header="How long does DNS Firewall cache a stale object?">

DNS Firewall sets cache longevity according to allocated memory.

As long as there is enough allocated memory, Cloudflare does not clear items from the cache forcefully, even when the TTL expires. This feature allows Cloudflare to serve stale objects from cache if your nameservers are offline.

</Details>

<Details header="Does the DNS Firewall cache SERVFAIL?">

Yes. `SERVFAIL` is treated like any other negative answer for caching purposes. The default TTL is 30 seconds. You can use the [API](/api/resources/dns_firewall/methods/edit/) to set a different `negative_cache_ttl`.

</Details>

<Details header="Does DNS Firewall support EDNS Client Subnet (ECS)?">

Yes. Often, DNS providers want to see a client's IP via <GlossaryTooltip term="EDNS Client Subnet (ECS)">EDNS Client Subnet (ECS)</GlossaryTooltip> ([RFC 7871](https://www.rfc-editor.org/rfc/rfc7871.html)) because they serve geographically specific DNS answers based on the client's IP. With EDNS Client Subnet enabled, the DNS Firewall will forward the client's IP subnet along with the DNS query to the upstream nameserver.

When EDNS is enabled, the DNS Firewall gives out the geographically correct answer in cache based on the client IP subnet. To do this, the DNS Firewall segments its cache. For example:

1. A resolver says it is looking for an answer for client `192.0.2.0/24`.
2. The DNS Firewall will proxy the request to the upstream nameserver for the answer.
3. The DNS Firewall will cache the answer from the upstream nameserver, but only for that `/24`.
4. `203.0.113.0/24` now asks the same DNS question and the answer is again returned from the upstream nameserver instead of the cache.

:::note

EDNS limits the effectiveness of the DNS cache.

:::

Some resolvers might not be sending any EDNS data. When you set the `ecs_fallback` parameter to `true` via the [API](/api/resources/dns_firewall/methods/edit/), DNS Firewall will forward the IP subnet of the resolver instead only if there is no EDNS data present in incoming the DNS query.

</Details>

<Details header="Does DNS Firewall cache negative answers?">

Yes. The default TTL is 30 seconds. You can set `negative_cache_ttl` via the [API](/api/resources/dns_firewall/methods/edit/). This will affect the TTL of responses with status `REFUSED`, `NXDOMAIN`, or `SERVFAIL`.

</Details>

<Details header="How can I set PTR records for nameserver hostnames?">

To set up PTR records for the DNS Firewall cluster IPs that point to your nameserver hostnames, use the following API endpoints:

- [Show DNS Firewall Cluster Reverse DNS](/api/resources/dns_firewall/subresources/reverse_dns/methods/get/)
- [Update DNS Firewall Cluster Reverse DNS](/api/resources/dns_firewall/subresources/reverse_dns/methods/edit/)

</Details>

---

# DNS Firewall

URL: https://developers.cloudflare.com/dns/dns-firewall/

import { Description, Plan } from "~/components"

<Description>

Speed up and protect entire authoritative nameservers
</Description>

<Plan type="ent-add-on" />

Cloudflare DNS Firewall proxies all DNS queries to your nameservers through Cloudflareâ€™s global network. This action protects upstream nameservers from DDoS attacks and reduces load by caching DNS responses.

![Diagram showing protection provided by DNS Firewall. For more details, read further.](~/assets/images/dns/dns-firewall-overview.png)

DNS Firewall is for customers who need to speed up and protect entire authoritative nameservers. If you need to speed up and protect individual zones, refer to Cloudflare DNS [Setups](/dns/zone-setups/).

***

## How DNS Firewall works

When a DNS query for your domain takes place:

1. Queries go to the Cloudflare data center that is closest to the website visitor. This is determined by the location of the DNS resolver.
2. Cloudflare tries to return a DNS response from cache.
3. If the response is not available in cache, Cloudflare queries the upstream authoritative nameservers.
4. After returning the response from the nameservers, Cloudflare temporarily caches it for subsequent DNS queries.

***

## Benefits

DNS Firewall provides the following benefits while allowing your organization total control over your authoritative nameservers:

* DDoS mitigation
* High availability
* Global distribution
* Enhanced performance
* Bandwidth savings
* [Rate limiting per data center](/dns/dns-firewall/setup/#additional-options)
* Minimum and maximum cache TTL specification
* DNS [ANY](https://datatracker.ietf.org/doc/html/rfc8482) query type block

---

# Setup

URL: https://developers.cloudflare.com/dns/dns-firewall/setup/

import { TabItem, Tabs } from "~/components";

## Prerequisites

Prior to setting up DNS Firewall, you need:

- Account access to DNS Firewall (provided by your Enterprise account team).
- Access to **DNS Administrator** or **Super Administrator** privileges on your account.
- Newly updated IP addresses for your nameservers (protects against previously compromised IP addresses).

## Configure DNS Firewall

### Create a DNS Firewall cluster

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. Log in to the [Cloudflare account](https://dash.cloudflare.com) with DNS Firewall.
2. On the account homepage, click **DNS Firewall**.
3. Click **Add Firewall Cluster**.
4. Fill out the required fields, including:
   - **IP Addresses**: The upstream IPv4 and/or IPv6 addresses of your authoritative nameservers.
   - **Minimum Cache TTL**: Recommended setting of **30 seconds**.
   - **Maximum Cache TTL**: Recommended setting of **4 hours**. Larger values increase the cache hit ratio, but also increase the time required for DNS changes to propagate.
   - **ANY queries**: Recommended setting is **Off** because these are often used as part of DDoS attacks. Also refer to this [blog post](https://blog.cloudflare.com/rfc8482-saying-goodbye-to-any/).
5. Click **Continue**.
6. On the following screen, save the values for **Your new DNS Firewall IP Addresses**.

:::note[Note:]

If you forget to save your new IP addresses, find your cluster and click **IP Addresses**.

If you delete your cluster, the assigned set of IPs will be lost. If you recreate the cluster you will get a different set of IPs.

:::

</TabItem> <TabItem label="API">

You can also create a DNS Firewall cluster by sending a [POST request](/api/resources/dns_firewall/methods/create/) to the API.

</TabItem> </Tabs>

### Update registrar settings

Update the `A/AAAA` glue records for your nameserver hostnames at your registrar with your DNS Firewall cluster IP addresses.

### Update DNS servers

At your DNS servers, update the `A/AAAA` records for your nameserver hostnames in your DNS zone file with your DNS Firewall cluster IP addresses.

### Test DNS resolution

Confirm that your nameservers are functioning correctly by running a `dig` command.

### Update security policies

Configure security policy in your DNS servers and Firewall to allow only [Cloudflare IPs](https://cloudflare.com/ips) and TCP/UDP port 53.

## Additional options

When you use the API, you can also specify other parameters, such as rate limit (in queries per second per data center). You can find the parameters descriptions and examples in the [API documentation](/api/resources/dns_firewall/methods/create/).

To configure rate limiting and other options for already existing clusters, use the [Update DNS Firewall Cluster](/api/resources/dns_firewall/methods/edit/) endpoint.

---

# Advanced nameservers

URL: https://developers.cloudflare.com/dns/foundation-dns/advanced-nameservers/

import { Details, GlossaryTooltip, Render } from "~/components"

Advanced nameservers included with [Foundation DNS](/dns/foundation-dns/) offer improved resiliency and more consistent nameserver assignment.

Consider the sections below for details about advanced nameservers, and refer to [Set up advanced nameservers](/dns/foundation-dns/setup/) to learn how to enable this feature.

:::caution
 <Render file="ns-advanced-vs-custom" /> 
:::

## Anycast network groups

To increase resiliency, advanced nameserver IPs are advertised by only one of two <GlossaryTooltip term="anycast">anycast</GlossaryTooltip> network groups.

The two groups consist of data centers that are geographically equally distributed.


<Details header="United Kingdom example">

| IPs             | Group | Data centers         |
| --------------- | ----- | -------------------- |
| `108.162.198.1` | A     | London and Edinburgh |
| `172.64.40.1`   | B     | Manchester           |
| `162.159.60.1`  | A     | London and Edinburgh |


</Details>

In DNS resolution, a resolver eventually acquires a list of all IPs where authoritative nameservers for a domain can be reached, and will then usually prefer the IP with the best resolution performance.

When, instead of advertising all IPs in all data centers, this group logic is applied, resiliency is improved because, if one of the data centers experiences a localized issue, the resolver can fall back to an IP advertised by the next closest data center.

Refer to [our blog post](https://blog.cloudflare.com/foundation-dns-launch) for an in-depth explanation.

## Dedicated release process

Zones using advanced nameservers are less exposed to incidents or software regression.

The dedicated release process means that only changes that have been in production for a while will reach advanced nameservers.

## Nameservers hosting and assignment

While standard Cloudflare nameservers are hosted under `ns.cloudflare.com` or `secondary.cloudflare.com`, advanced nameservers use different domains:

* `foundationdns.com`
* `foundationdns.net`
* `foundationdns.org`

Using the different TLDs (`.com`, `.net`, and `.org`) and making these available only to enterprise accounts allows for better predictability and consistency in nameserver assignment.

There should also be less conflicts when guaranteeing that directly descending zones do not have the same nameserver set.


<Details header="Descending zones example">

Consider the domain `example.com`, and subdomains `abc.example.com` and `123.example.com`:

* `abc.example.com` and `123.example.com` directly descend from `example.com` and cannot have the same nameservers as `example.com`.
* `abc.example.com` and `123.example.com` are sibling domains and can have the same nameservers.
* `new.abc.example.com` directly descends from both `abc.example.com` and `example.com`, and cannot have the same nameservers as them, but can have the same nameservers as `123.example.com`.


</Details>

---

# Foundation DNS

URL: https://developers.cloudflare.com/dns/foundation-dns/

Foundation DNS is the Cloudflare DNS offering for enterprise customers.

With Foundation DNS, you get access to increased reliability, security, and insights. Features include the following:

* [Advanced nameservers](/dns/foundation-dns/advanced-nameservers/) that provide:
  * Strategically distributed IPs to enhance resiliency
  * Reduced exposure to incidents or software regression
  * More consistent nameserver assignment
* DNSSEC keys unique to your zone
* Additional DNS settings, including:
  * [Zone defaults](/dns/additional-options/dns-zone-defaults/)
  * [Account custom nameservers](/dns/nameservers/custom-nameservers/account-custom-nameservers/)
  * Custom [SOA record](/dns/manage-dns-records/reference/dns-record-types/#soa) and [Nameserver TTL](/dns/nameservers/nameserver-options/#nameserver-ttl)

## Availability

Foundation DNS is only available to Enterprise customers.

Advanced nameservers are an opt-in configuration. Refer to [set up advanced nameservers](/dns/foundation-dns/setup/).

## Related resources

* [Release blog post](https://blog.cloudflare.com/foundation-dns-launch)
* [Product page](https://www.cloudflare.com/dns/foundation-dns/)

---

# Set up advanced nameservers

URL: https://developers.cloudflare.com/dns/foundation-dns/setup/

import { TabItem, Tabs } from "~/components";

Advanced nameservers included with [Foundation DNS](/dns/foundation-dns/) are an opt-in configuration.

## Enable on a zone

To enable advanced nameservers on an existing zone:

1. Opt for advanced nameservers on your zone:

    <Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

    1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account and domain.
    2. Go to **DNS** > **Records**.
    3. In the **Cloudflare nameservers** card, enable **Advanced nameservers**.
    4. After you refresh the page, the card will display the values for your advanced nameservers `NS` records.

    </TabItem> <TabItem label="API">

    Use the [Update DNS Settings](/api/resources/dns/subresources/settings/methods/edit/) endpoint to send a PATCH request like the following:

    ```bash
    curl --request PATCH \
    "https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_settings" \
    --header "X-Auth-Email: <EMAIL>" \
    --header "X-Auth-Key: <API_KEY>" \
    --header "Content-Type: application/json" \
    --data '{
      "foundation_dns": true
    }'
    ```

    The response body will contain your assigned namservers in the `nameservers` object. You will use these nameservers in the next step.

    </TabItem> </Tabs>

2. Update the authoritative nameservers at your registrar. This step depends on whether you are using [Cloudflare Registrar](/registrar/):
   - If you are using Cloudflare Registrar, [contact Cloudflare Support](/support/contacting-cloudflare-support/) to have your nameservers updated.
   - If you are using a different registrar or if your zone is delegated to a parent zone, [manually update your nameservers](/dns/nameservers/update-nameservers/#specific-processes).

      :::caution

      Make sure the values for your assigned nameservers are copied exactly.
      :::

---

# Migrate an existing zone with DNSSEC enabled

URL: https://developers.cloudflare.com/dns/dnssec/dnssec-active-migration/

import { Details } from "~/components";

Follow this tutorial to migrate an existing DNS zone to Cloudflare without having to disable DNSSEC.

:::caution

This procedure involves cross-importing the [zone signing keys (ZSKs)](https://www.cloudflare.com/learning/dns/dns-records/dnskey-ds-records/) from one provider to the other. To learn more about this, consider this article [about multi-signer DNSSEC](/dns/dnssec/multi-signer-dnssec/about/) or refer to [RFC 8901](https://www.rfc-editor.org/rfc/rfc8901.html).
:::

This is an advanced procedure and assume some familiarity with [DNS concepts](/dns/concepts/), [API operations](/fundamentals/api/), and basic setup steps. Assumed knowledge that is not detailed in this tutorial can be referenced through the linked content in each of the steps.

## Requirement

The provider you are migrating from must allow you to add DNSKEY records on the zone apex and use these records in responses to DNS queries.

## 1. Set up Cloudflare

1. [Add your zone to Cloudflare](/fundamentals/setup/manage-domains/add-site/).

   To add your zone using the API, refer to the [Create Zone endpoint](/api/resources/zones/methods/create/).

2. [Review the records found by the automatic scan](/dns/manage-dns-records/how-to/create-dns-records/) or [import your zone file](/dns/manage-dns-records/how-to/import-and-export/).

   To import the zone file using the API, refer to the [Import DNS Records endpoint](/api/resources/dns/subresources/records/methods/import/).

3. Go to **DNS** > **Settings**, and select **Enable DNSSEC**. Or use the following [API request](/api/resources/dns/subresources/dnssec/methods/edit/).

```bash
curl --request PATCH \
https://api.cloudflare.com/client/v4/zones/{zone_id}/dnssec \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{"status": "active"}'
```

4. Go to **DNS** > **Settings**, and enable **Multi-signer DNSSEC**. Or use the following [API request](/api/resources/dns/subresources/dnssec/methods/edit/).

```bash
curl --request PATCH \
https://api.cloudflare.com/client/v4/zones/{zone_id}/dnssec \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{"dnssec_multi_signer": true}'
```

## 2. Cross-import ZSKs

1. Add the [ZSK](https://www.cloudflare.com/learning/dns/dns-records/dnskey-ds-records/) of your previous provider to Cloudflare by creating a DNSKEY record on your zone.

You can do this [on the dashboard](/dns/manage-dns-records/how-to/create-dns-records/#create-dns-records) or through the [Create DNS Record endpoint](/api/resources/dns/subresources/records/methods/create/), as in the following example.

```bash
curl https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '{
    "type": "DNSKEY",
    "name": "<ZONE_NAME>",
    "data": {
      "flags": 256,
      "protocol": 3,
      "algorithm": 13,
      "public_key": "<PUBLIC_KEY>"
    },
    "ttl":3600
   }'
```

2. Get Cloudflare's ZSK using either the API or a query from one of the assigned Cloudflare nameservers.

API example:

```bash
curl https://api.cloudflare.com/client/v4/zones/{zone_id}/dnssec/zsk \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>"
```

Command line query example:

```sh
dig <ZONE_NAME> dnskey @<CLOUDFLARE_NAMESERVER> +noall +answer | grep 256
```

3. Add Cloudflare's ZSK that you fetched in the last step to your previous provider.

:::note

You can check if both providers are responding with both ZSKs by running one `dig` command for each, as in the following example. You can also use [Dig Web Interface](https://www.digwebinterface.com/?type=DNSKEY).

```sh
dig <ZONE_NAME> dnskey @<PREVIOUS_PROVIDER_NAMESERVER> +noall +answer
dig <ZONE_NAME> dnskey @<CLOUDFLARE_NAMESERVER> +noall +answer
```

Both queries should return both ZSKs (identified with tag `256`).

<Details header="Example">

```sh
dig multisigner.info dnskey @dns1.p01.nsone.net. +noall +answer
```

```sh output
multisigner.info.    3600    IN    DNSKEY    257 3 13 t+4D<bla_bla_bla>JBmA==
multisigner.info.    3600    IN    DNSKEY    256 3 13 pxEU<bla_bla_bla>0xOg==
multisigner.info.    3600    IN    DNSKEY    256 3 13 oJM<bla_bla_bla>XhSA==
```

```sh
dig multisigner.info dnskey @ashley.ns.cloudflare.com +noall +answer
```

```sh output
multisigner.info.    3600    IN    DNSKEY    257 3 13 mdss<bla_bla_bla>eKGQ==
multisigner.info.    3600    IN    DNSKEY    256 3 13 oJM<bla_bla_bla>XhSA==
multisigner.info.    3600    IN    DNSKEY    256 3 13 pxEU<bla_bla_bla>0xOg==
```

</Details>

:::

## 3. Set up registrar

1. Add Cloudflare DS record to your registrar. You can see your Cloudflare DS record on the [dashboard](https://dash.cloudflare.com/?to=/:account/:zone/dns) by going to **DNS** > **Settings** > **DS Record**.
2. Add Cloudflare assigned nameservers to your registrar. You can see your Cloudflare nameservers by going to **DNS** > **Records**.

At this point your zone is in a [multi-signer DNSSEC setup](/dns/dnssec/multi-signer-dnssec/).

## 4. Remove previous provider

1. Remove your previous provider's DS record from your registrar.
2. Remove your previous provider's nameservers from your registrar.
3. After waiting at least one and a half times the [TTL](https://www.cloudflare.com/learning/cdn/glossary/time-to-live-ttl/) of your previous provider DS record, you can remove the DNSKEY record (containing your previous provider ZSK) that you added to your Cloudflare zone in [step 2](#2-cross-import-zsks).

:::note

You can find out the TTL of your previous provider DS record by running a `dig` command, as in the following example, or by using this [Dig Web Interface link](https://www.digwebinterface.com/?type=DS).

```sh
dig multisigner.info ds +noall +answer
```

```sh output
multisigner.info. 3600 IN DS 2371 13 2 227B4C7FF3E1D49D59BAF39BDA54CA0839DE700DD9896076AA3E6AD7 19A0CF55
multisigner.info. 3600 IN DS 48553 13 2 893709B51A9C53D011A4054B15FC5454BEDF68E739BB3B3FA1E333DA 7B8DACFE
```

In this example, both DS records have a TTL of `3600` seconds. Cloudflare's DS record always has the key tag set to `2371`, so the second line of the response is the DS record of the other provider.

:::

---

# DNSSEC states

URL: https://developers.cloudflare.com/dns/dnssec/dnssec-states/

This page describes different DNSSEC states and how they relate to the responses you get from the [DNSSEC details API endpoint](/api/resources/dns/subresources/dnssec/methods/get/).

| State            | API response                                                     | Description                                                                                                                                                                       |
| ---------------- | ---------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Pending          | `"status":"pending"`<br /> `"modified_on":<TIME_STAMP>`          | DNSSEC has been enabled but the Cloudflare DS record has not been added at the registrar.                                                                                         |
| Active           | `"status":"active"`<br /> `"modified_on":<TIME_STAMP>`           | DNSSEC has been enabled and the Cloudflare DS record is present at the registrar.                                                                                                 |
| Pending-disabled | `"status":"pending-disabled"`<br /> `"modified_on":<TIME_STAMP>` | DNSSEC has been disabled but the Cloudflare DS record is still added at the registrar.                                                                                            |
| Disabled         | `"status":"disabled"`<br /> `"modified_on":<TIME_STAMP>`         | DNSSEC has been disabled and the Cloudflare DS record has been removed from the registrar.                                                                                        |
| Deleted          | `"status":"disabled"`<br /> `"modified_on": null`                | DNSSEC has never been enabled for the zone or DNSSEC has been disabled and then deleted using the [Delete DNSSEC records endpoint](/api/resources/dns/subresources/dnssec/methods/delete/). |

:::caution

Once you have enabled DNSSEC on a zone for the first time, you cannot transition directly from an `active` state to a `deleted` state. You can only [delete DNSSEC records](/api/resources/dns/subresources/dnssec/methods/delete/) once your zone DNSSEC is in a `disabled` state. Cloudflare prevents you from deleting DNSSEC records before removing the DS record from the registrar to avoid DNS resolution issues.
:::

In both `pending` and `active` states, Cloudflare signs the zone and responds with RRSIG, NSEC, DNSKEY, CDS, and CDNSKEY record types.

In `pending-disabled` and `disabled` states, Cloudflare still signs the zone and serves RRSIG, NSEC, and DNSKEY record types, but the CDS and CDNSKEY records are set to zero ([RFC 8078](https://www.rfc-editor.org/rfc/rfc8078.html#section-4)), signaling to the registrar that DNSSEC should be disabled.

In `deleted` state, Cloudflare does **not** sign the zone and does **not** respond with RRSIG, NSEC, DNSKEY, CDS, and CDNSKEY record types.

Refer to [How DNSSEC works](https://www.cloudflare.com/dns/dnssec/how-dnssec-works/) to learn more about the authentication process and records involved.

---

# DNSSEC

URL: https://developers.cloudflare.com/dns/dnssec/

import { Render } from "~/components"

<Render file="dnssec-definition" product="dns" />

***

## Disable DNSSEC

<Render file="disable_dnssec" product="dns" />

<Render file="dnssec-enabled-migration" product="dns" />

***

## Enable DNSSEC

When you enable DNSSEC, Cloudflare signs your zone, publishes your public signing keys, and generates your **DS** record.

### 1. Activate DNSSEC in Cloudflare

<Render file="dnssec-cloudflare-steps" product="dns" />

### 2. Add DS record to your registrar

<Render file="dnssec-registrar-steps" product="dns" />

<Render file="dnssec-auto-add" product="dns" />

***

## Other DNSSEC setup options

If you are using Cloudflare as your Secondary DNS provider and want to configure DNSSEC on your secondary zone(s), you have [three options](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/dnssec-for-secondary/) depending on your setup.

If you want to set up DNSSEC on a subdomain zone, refer to [Subdomain DNSSEC](/dns/zone-setups/subdomain-setup/dnssec/).

***

## Limitations

If your registrar does not support DNSSEC with Cloudflare's preferred cipher choice (Algorithm 13), you have several options:

* Contact your registrar to ask for DNSSEC with modern encryption.
* Transfer your domain to a different registrar that supports DNSSEC with Algorithm 13
* File a [complaint with ICANN](https://www.icann.org/compliance/complaint), citing your registrar's lack of compliance.

If your top-level domain does not support DNSSEC with Algorithm 13 (also known as *ECDSA Curve P-256 with SHA-256*), [contact that top-level domain](https://www.iana.org/domains/root/db).

---

# Troubleshooting

URL: https://developers.cloudflare.com/dns/dnssec/troubleshooting/

import { Example } from "~/components";

Learn more about how to troubleshoot issues with DNSSEC.

## Test DNSSEC with Dig

`Dig`Â is a command-line tool to query a nameserver for DNS records.

For instance,Â `dig`Â canÂ ask a DNS resolver for the IP address ofÂ `www.cloudflare.com`:

<Example>

```sh
dig www.cloudflare.com +short
```

```sh output
198.41.215.162
198.41.214.162
```

The optionÂ `+short`Â outputs the result only.

</Example>

UseÂ `+dnssec`Â to verify that the DNS records are signed:

<Example>

```sh
dig www.cloudflare.com +dnssec +short
```

```sh output
198.41.214.162
198.41.215.162
A 13 3 300 20180927180434 20180925160434 35273 cloudflare.com. DYYZ/bhHSAIlpvu/HEUsxlzkC9NsswbCQ7dcfcuiNBrbhYV7k3AI8t46 QMnOlfhwT6jqsfN7ePV6Fwpym3B0pg==
```

In this example, the last line of output is theÂ `RRSIG`Â record. `RRSIG`Â is the DNSSEC signature attached to the record. With theÂ `RRSIG`, a DNS resolver determines whether a DNS response is trusted.

</Example>

`Dig`Â can also retrieve the public key used to verify the DNS record, `DNSKEY`:

<Example>

```sh
dig DNSKEY cloudflare.com +short
```

```sh output
257 3 13 mdsswUyr3DPW132mOi8V9xESWE8jTo0dxCjjnopKl+GqJxpVXckHAeF+ KkxLbxILfDLUT0rAK9iUzy1L53eKGQ==
256 3 13 koPbw9wmYZ7ggcjnQ6ayHyhHaDNMYELKTqT+qRGrZpWSccr/lBcrm10Z 1PuQHB3Azhii+sb0PYFkH1ruxLhe5g==
```

A domain's DNS records are all signed with the same public key. Therefore,Â query for the apex domain (`cloudflare.com`) public key,Â not the subdomain (`www.cloudflare.com`) public key.

The DNS response includes two records:

- `DNSKEY` record **256** is the public key called zone signing key (ZSK). ZSKs are used to verify the DNS record signatures forÂ `A`, `MX`, `CNAME`, `SRV`, etc.
- `DNSKEY` record **257** is called the key signing key (KSK). KSKs are used to verify the signatures of theÂ `DNSKEY`, `CDS`, and `CDNSKEY` records.

</Example>

:::note

Details on how to verify the signatures with the public key are beyond the scope of this article.
:::

When not using theÂ `+short`Â option withÂ `dig`, a DNS response is DNSSEC authenticated if theÂ `ad`Â flag appears in the response header:

<Example>

```sh
dig www.cloudflare.com
```

```sh output
[...]
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 65326
;; flags: qr rd ra ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1
[...]
;; QUESTION SECTION:
;www.cloudflare.com.        IN  A
[...]
;; ANSWER SECTION:
www.cloudflare.com. 15  IN  A   198.41.215.162
www.cloudflare.com. 15  IN  A   198.41.214.162
```

</Example>

---

## Troubleshoot DNSSEC validation using DNSViz

:::note

DNSViz is a public, free online tool to visualize and help discover issues with your DNSSEC configuration and is **not** associated Cloudflare.
:::

To visualize and discover potential issues with DNSSEC:

1. Browse toÂ [http://dnsviz.net/](http://dnsviz.net/)
2. Enter a domain name in the text field that appears.
3. If DNSViz has never analyzed the site before, click theÂ **Analyze** button that appears.
4. If the site has been analyzed by DNSViz before,Â click theÂ **Update Now** button that appears.

### Example with missing or incorrect RRSIG record on authoritative nameserver

Below is an example of how dnsviz.net will display incorrect delegation when no valid DNSKEY records are provided by the authoritative nameserver to match the DS record published by the TLD nameserver:

![Incorrect delegation when no valid DNSKEY records are provided](~/assets/images/support/troubleshoot_dnssec-example_no_rrsig.png)

---

## View the DNSSEC chain of trust with Dig

Full verification of domain signatures (for example,Â `cloudflare.com`) involves verifying the key signing key at the top-level domain (for example,Â `.com`).

Similar verification isÂ then performed by checking the key-signingÂ key ofÂ `.com`Â at the root server level. DNSSEC root keys are distributed to DNS clients to complete the chain of trust.

When DNSSEC is enabled, aÂ `DS` recordÂ is required at the registrar's DNS. The `DS` recordÂ contains a hash of the public key signing key as well as metadata about the key.

<Example>

UseÂ `dig`Â toÂ find aÂ `DS` record:

```sh
dig +short DS cloudflare.com
```

```sh output
2371 13 2 32996839A6D808AFE3EB4A795A0E6A7A39A76FC52FF228B22B76F6D6 3826F2B9
```

When using theÂ `+trace`Â option,Â `dig`Â confirms whether an answer isÂ returned by the nameserver forÂ `cloudflare.com`Â or the nameserver forÂ `.com`.Â  In this example, theÂ `DS` recordÂ for `cloudflare.com`Â is returned byÂ `e.gtld-servers.net`:

```sh
dig DS cloudflare.com +trace
```

```sh output
[...]
cloudflare.com.     86400   IN  DS  2371 13 2 32996839A6D808AFE3EB4A795A0E6A7A39A76FC52FF228B22B76F6D6 3826F2B9
[...]
com.            172800  IN  NS  e.gtld-servers.net.
[...]
;; Received 1213 bytes from 2001:502:1ca1::30#53(e.gtld-servers.net) in 37 ms

```

</Example>

An easier alternative to manually running the steps above is to use the third-party toolÂ [DNSViz](#troubleshoot-dnssec-validation-using-dnsviz).

---

## Troubleshoot DNSSEC validation with Dig

Issues occur if authoritative DNS providers are changed without updating or removing old DNSSEC records at the registrar:

<Example>

```sh
dig A brokendnssec.net @1.0.0.1
```

```sh output
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0
;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 10663
```

Confirm whether aÂ `SERVFAIL`Â response is related to DNSSEC by runningÂ `dig`Â with theÂ `+cd`Â option. TheÂ `+cd`Â option provides DNS results without any DNSSEC validation in place.

```sh
dig A brokendnssec.net @1.0.0.1 +dnssec +cd +short
```

```sh output
104.20.49.61
104.20.48.61
```

In this example, DNSSEC is misconfigured if a proper DNS response is received when using theÂ `+cd`Â option but queries using DNSSEC return aÂ `SERVFAIL`Â response.Â This issue often happensÂ when authoritative nameservers are changed butÂ `DS` recordsÂ are not updated.Â The issue can also occur if an attacker attempts to forge a response to a query.

</Example>

---

## Next steps

If a problem is discovered with DNSSEC implementation, contact the domain's registrar and confirm theÂ `DS` recordÂ matches what the authoritative DNS provider has specified. If Cloudflare is the authoritative DNS provider, follow the instructions forÂ [configuring DNSSEC with Cloudflare](/dns/dnssec/).

---

# Manage DNS views

URL: https://developers.cloudflare.com/dns/internal-dns/dns-views/

import { Details, Render } from "~/components";

Internal DNS views are logical groupings of [internal DNS zones](/dns/internal-dns/internal-zones/). As explained in the [architecture overview](/dns/internal-dns/#architecture-overview), DNS views are referenced by [Gateway resolver policies](/cloudflare-one/policies/gateway/resolver-policies/) to define how a specific query should be resolved.

Refer to the sections below for details on how to manage your DNS views, or consider the [get started](/dns/internal-dns/#architecture-overview) for a complete workflow.

## Configuration conditions

When setting up DNS views, observe the following conditions:

<Render file="internal-dns-view-conditions" />

## Create a view

Use the [Create Internal DNS View](/api/resources/dns/subresources/settings/subresources/views/methods/create/) endpoint. For each view you create, list all the internal zones that should be grouped under that view.

## Delete a view

Use the [Delete Internal DNS View](/api/resources/dns/subresources/settings/subresources/views/methods/delete/) endpoint.

DNS views can be deleted even if they still have internal zones linked to them. The internal DNS zones will continue to exist but will be unlinked once the view is deleted.

It is also possible to delete a DNS view that is being referenced by a Gateway resolver policy. In this case, queries matching the policy will return SERVFAIL.

## Other actions

- [Update a DNS view](/api/resources/dns/subresources/settings/subresources/views/methods/edit/) (`PATCH`)
- [Get view details](/api/resources/dns/subresources/settings/subresources/views/methods/get/) (`GET`)
- [List DNS views](/api/resources/dns/subresources/settings/subresources/views/methods/list/) (`GET`)

---

# Analytics and logs

URL: https://developers.cloudflare.com/dns/internal-dns/analytics/

Internal DNS leverages [Gateway analytics](/cloudflare-one/insights/analytics/gateway/). Below you can find information about specific fields and different methods you can use to access this data.

## GraphQL

For detailed metrics, use the [GraphQL API](/analytics/graphql-api/). Refer to the GraphQL Analytics API documentation for guidance on how to [get started](/analytics/graphql-api/getting-started/).

The [fields](/analytics/graphql-api/getting-started/querying-basics/) added to cover Internal DNS are the following:

- `InternalDNSFallbackStrategy`: The fallback strategy applied to the internal DNS response. Empty if no fallback strategy was applied.
- `InternalDNSRCode`: The response code sent back by the internal DNS service.
- `InternalDNSViewID`: The view identifier that was sent to the internal DNS service.
- `InternalDNSZoneID`: The internal zone identifier returned by the internal DNS service.

## Logs

Leverage Logpush jobs for [Gateway DNS](/logs/reference/log-fields/account/gateway_dns/#internaldnsfallbackstrategy). For help setting up Logpush, refer to [Get started with Logs](/logs/get-started/).

You can also set up [Logpush filters](/logs/reference/filters/) to only push logs related to a specific [internal zone](/dns/internal-dns/internal-zones/) or [view](/dns/internal-dns/dns-views/) ID.

---

# Connect to Gateway resolver

URL: https://developers.cloudflare.com/dns/internal-dns/connectivity/

To connect to Cloudflare Gateway resolver - which is [required to reach private resources in Internal DNS](/dns/internal-dns/#architecture-overview) - you can use the following options:

- DNS endpoints supported with [DNS locations](/cloudflare-one/connections/connect-devices/agentless/dns/locations/)
    - DNS over UDP/TCP port 53 (IPv4 or IPv6)
    - DNS over TLS
    - DNS over HTTPS
- [Proxy Auto-Configuration (PAC) files](/cloudflare-one/connections/connect-devices/agentless/pac-files/)
- [WARP device client](/cloudflare-one/connections/connect-devices/warp/)
- [Clientless browser isolation](/cloudflare-one/policies/browser-isolation/setup/clientless-browser-isolation/#filter-dns-queries)
- [Magic WAN](/magic-wan/zero-trust/cloudflare-gateway/)

---

# Get started

URL: https://developers.cloudflare.com/dns/internal-dns/get-started/

import { TabItem, Tabs, Details, Example, Render } from "~/components";

Follow this guide to get started with Internal DNS.

Although there are some steps that can be achieved on the dashboard, currently the whole process can only be completed via API.

## Before you begin

<Render file="internal-dns-beta-note" />

- Make sure you have an Enterprise account with access to [Gateway resolver policies](/cloudflare-one/policies/gateway/resolver-policies/) and [Internal DNS](/dns/internal-dns/).
- Consider the different ways in which you can [connect to Gateway resolver](/dns/internal-dns/connectivity/).
- If you are not familiar with how to use Cloudflare API, refer to [Fundamentals](/fundamentals/api/get-started/).
- If you will be using an API token for authentication, make sure you have the following permissions:

<Details header="API token configuration">
<Example>
**Permissions**
- *Account* - *DNS Views* - *Edit*
- *Zone* - *DNS* - *Edit*
- *Account* - *Account Settings* - *Edit*
- *Zone* - *DNS Settings* - *Edit*
- *Zone* - *Zone* - *Edit*

**Account Resources**
- *Include* - *(Your account)*

**Zone Resources**
- *Include* - *All zones*

</ Example>
</ Details>


## 1. Set up your internal DNS zone

1. Use the [Create Zone](/api/resources/zones/) endpoint to create an [internal zone](/dns/internal-dns/internal-zones/). Specify your account ID and set the `type` to `internal`.

<Details header="Internal zone configuration conditions">
<Render file="internal-zones-conditions" />
</Details>

2. Add DNS records to your internal zone using your preferred option:
- [Import](/api/resources/dns/subresources/records/methods/import/) a formatted BIND file. Refer to the [DNS records how-to](/dns/manage-dns-records/how-to/import-and-export/) for guidance.
- Use other API endpoints, such as [`/batch`](/api/resources/dns/subresources/records/methods/batch/), to manage DNS records. Refer to [Batch record changes](/dns/manage-dns-records/how-to/batch-record-changes/#use-the-api) for details.
3. Repeat this process for each internal zone you wish to add.

### (Optional) Reference a zone from another zone

1. Use the [Update DNS settings](/api/resources/dns/subresources/settings/methods/edit/) endpoint to add a reference from an internal zone to another internal zone. In `--data`, specify the `internal_dns` object with the parameter `reference_zone_id`. For details, refer to [reference zones](/dns/internal-dns/internal-zones/#reference-zones).

<Example>
<Render file="internal-reference-zone-api"/>
</ Example>

## 2. Link your internal zone to a view

Since the resolver policy will require a [DNS view](/dns/internal-dns/dns-views/), you must have at least one view to be able to route requests to internal zones.

1. Use the [Create Internal DNS View](/api/resources/dns/subresources/settings/subresources/views/methods/create/) endpoint. For each view you create, list all the internal zones that should be grouped under that view.

<Details header="DNS view configuration conditions">
<Render file="internal-dns-view-conditions" />
</Details>

## 3. Configure Gateway policies

:::note
The Gateway configuration must exist within the same Cloudflare account where the internal zone exists.
:::

Besides selecting an internal DNS view when setting up your resolver policies, you can also enable the **fallback through public DNS** option.

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

1. In [Zero Trust](https://one.dash.cloudflare.com/), go to **Gateway** > **Resolver policies**.
2. Select **Add a policy** and enter a name and description.
3. Create an expression for the traffic you wish to route. For guidance about selectors, operators, and values, refer to [Gateway](/cloudflare-one/policies/gateway/resolver-policies/#selectors).
4. Select **Use DNS view**. In the dropdown, choose the view that queries matching the expression should be sent to.
5. (Optional) Adjust the option to **fallback through public DNS** according to your use case.
- Off: Gateway DNS resolver returns the response as-is to the client.
- On: In case the response from the internal zone is REFUSED, NXDOMAIN, or a response with a CNAME type, Gateway DNS resolver sends the query to Cloudflare 1.1.1.1 public resolver and tries to resolve the query via public DNS.
6. Select **Create policy** to confirm.

</TabItem> <TabItem label="API">

Use the API endpoints under [Zero Trust > Gateway > Rules](/api/resources/zero_trust/subresources/gateway/subresources/rules/) to set up resolver policies. For guidance about selectors, operators, and values, refer to [Gateway](/cloudflare-one/policies/gateway/resolver-policies/#selectors).

Use the rule settings object to define `resolve_dns_internally`, specifying `view_id` and `fallback` option. The fallback options behave as follows:

- `none`: Gateway DNS resolver returns the response as-is to the client.
- `public_dns`: In case the response from the internal zone is REFUSED, NXDOMAIN, or a response with a CNAME type, Gateway DNS resolver sends the query to Cloudflare 1.1.1.1 public resolver and tries to resolve the query via public DNS.

</TabItem> </Tabs>

---

# Internal DNS (beta)

URL: https://developers.cloudflare.com/dns/internal-dns/

import { Render, Description, Plan, RelatedProduct, DirectoryListing, GlossaryTooltip, Example } from "~/components";

<Description>
Simplify private network management with Cloudflare DNS for your internal resources.
</Description>

<Plan type="enterprise" />

Manage DNS records that should only be accessible within your private network. Internal DNS [zones](/dns/internal-dns/internal-zones/) and [views](/dns/internal-dns/dns-views/) pair up with [Gateway resolver policies](/cloudflare-one/policies/gateway/resolver-policies/) so that you can control how a DNS query should be responded to according to the query context, such as its source IP.

<Render file="internal-dns-beta-note" />

## Architecture overview

You can use different [connectivity options](/dns/internal-dns/connectivity/) to on-ramp your traffic to Cloudflare. Then, Cloudflare Gateway resolver acts as an interface between the DNS client and internal DNS zones.

Internal DNS zones do not get assigned Cloudflare nameservers and can only be queried via Cloudflare Gateway resolver.

```mermaid
flowchart LR
        accTitle: Internal DNS query overview
        accDescr: Diagram comparing internal DNS query with public DNS
        A[Client]
        subgraph Cloudflare account
        subgraph Gateway
				B[Default 1.1.1.1 resolver]
        X[Resolver policy selecting an internal DNS view]
        end
        subgraph Authoritative DNS
        Y[(Public DNS)]
				Z[(Internal DNS)]
        end
        end

			  C[Public resolver]

        B --Query--> Y
        X --Query + View ID--> Z
        A --Query--> B
				A --Query--> X
				C --Query--> Y
```

Internal DNS zones are grouped into DNS views, which are selected by the resolver policy you define. Views are usually logical groupings relevant to your organization, such as different geographical locations.

```mermaid
flowchart LR
        accTitle: Internal DNS views and zones
        accDescr: Diagram exemplifying Internal DNS views and zones relationship
        subgraph Internal DNS
        subgraph View 111 - London
        Y[Zone 600 <br /> example.local]
				Z[Zone 601 <br /> local]
        end
        subgraph View 110 - San Francisco
        X[Zone 101 <br /> example.com]
				B[Zone 100 <br /> example.local]
				S[Zone 102 <br /> com]
        end
				W[Zone 701 <br /> net]
				end
```

Internal DNS zones contain the <GlossaryTooltip term="DNS record" link="/dns/manage-dns-records/">DNS records</ GlossaryTooltip> that should be used to resolve an internal DNS query. Also, if no internal record is found within a matching internal zone, Cloudflare will check if the matching internal zone is [referencing another internal zone](/dns/internal-dns/internal-zones/#reference-zones).

<Example>

```mermaid
flowchart LR
        accTitle: Internal DNS zones and internal records
        accDescr: Diagram exemplifying Internal DNS zones and records relationship
        subgraph View 111 - London
				subgraph Zone 601 - local
				S[@ A 192.0.2.10]
				T[ghi.example A 192.0.2.15]
				end
        subgraph Zone 600 - example.local
				X[@ A 192.0.2.1]
				Y[abc A 192.0.2.6]
				Z[def A 192.0.2.9]
				end
				end
```
In this example, a query for `ghi.example.local` routed to view ID 111 would go to zone 600, which presents the longest matching zone name (`example.local`). Zone 600 does not contain a record for `ghi` but, if it is referencing zone 601, Cloudflare will then look for the queried record within the reference zone.

</Example>

## Resources

<DirectoryListing />

## Related products

<RelatedProduct header="Cloudflare Gateway" href="/cloudflare-one/policies/gateway/" product="privacy-gateway">
Set up policies to inspect DNS, Network, HTTP, and Egress traffic.
</RelatedProduct>

<RelatedProduct header="Cloudflare Magic WAN" href="/magic-wan/" product="magic-wan">
Improve security and performance for your entire corporate networking, reducing cost and operation complexity.
</RelatedProduct>

---

# DNS records

URL: https://developers.cloudflare.com/dns/manage-dns-records/

import { DirectoryListing, FeatureTable, GlossaryTooltip, Details, Example } from "~/components"

DNS records contain information about your domain and are used to make your website or application available to visitors and other web services.

Each DNS record belongs to a different type, and each type serves a different purpose. For background about the different types of DNS records, refer to the [Learning Center](https://www.cloudflare.com/learning/dns/dns-records/). To quickly find reference information about a specific type, refer to [DNS record types](/dns/manage-dns-records/reference/dns-record-types/).

Depending on the providers you used to [get your domain name](/fundamentals/setup/manage-domains/#get-a-domain-name) and [host your website or application](/fundamentals/setup/manage-domains/#host-your-domain), it is expected that DNS records were automatically created on your behalf. According to your [setup](/dns/zone-setups/), you can use Cloudflare to manage your DNS records.

## DNS records table

When managing your records at Cloudflare, besides the common record fields described below, you may also find an option for [Proxy status](/dns/proxy-status/) and [CNAME flattening](/dns/cname-flattening/). These are specific features offered by Cloudflare.

<Details header="Record fields">
- **Type**: Defines the purpose of a record. Different types of record require different information in their corresponding `Content` field.
- **Name**: Identifies the resource that the record resolves to. Depending on the purpose of the record, the value you want to add to this field will also change.
- **Content**: Contains the core value of a record, depending on the record type.
- **TTL**: Controls how long each record is cached by DNS resolvers.
</Details>

<Details header="Example">

<Example>
DNS management for **example.com**:

| Type | Name    | Content      | Proxy status | TTL    |
| ---- | ------- | ------------ | ------------ | ------ |
| A    | `blog`  | `192.0.2.1`  | Proxied      | Auto   |

In this example, an IP address resolution record of type `A` is indicating that the resources that correspond to the subdomain `blog.example.com` can be reached on the IPv4 address `192.0.2.1`.

Also, as this record is <GlossaryTooltip term="proxy status" link="/dns/proxy-status/">proxied</GlossaryTooltip>, Cloudflare automatically defines for how long this information should be cached by DNS resolvers.
</Example>

</Details>

## DNS records quota

There is a limit to the number of records you can create on a single <GlossaryTooltip term="DNS zone">zone</GlossaryTooltip>.

- Free zones created before `2024-09-01 00:00:00 UTC`: 1,000
- Free zones created on or after `2024-09-01 00:00:00 UTC`: 200
- Pro: 3,500
- Business: 3,500
- Enterprise: 3,500

:::note[For more DNS records]

If you are an Enterprise customer and require more DNS records, contact your account team. Cloudflare can support millions of DNS records on a single zone.

:::

## Resources

### How to

<DirectoryListing folder="dns/manage-dns-records/how-to" />

### Reference

<DirectoryListing folder="dns/manage-dns-records/reference" />

### Troubleshooting

<DirectoryListing folder="dns/manage-dns-records/troubleshooting" />

---

# Nameservers

URL: https://developers.cloudflare.com/dns/nameservers/

import { Render } from "~/components"

As explained in [How DNS works](https://www.cloudflare.com/learning/dns/what-is-dns/), from the moment a user types an address (`www.example.com`) into their web browser, the resolution of a DNS query takes place. Also, the process behind DNS resolution involves different computers (or servers).

In the context of Cloudflare DNS, nameservers refer to authoritative nameservers, which are the last stop in the DNS query resolution. When a nameserver is authoritative for `example.com`, it means that DNS resolvers will consider responses from this nameserver when a user tries to access `example.com`.

## Authoritative nameservers offering

Within Cloudflare, and depending on your plan, you can choose between using Cloudflare-branded nameservers or setting up your own custom nameservers. The names for Cloudflare-branded nameservers are automatically assigned and cannot be changed.

Regardless of the type you choose, for these nameservers to be authoritative for your domain, you need to [update your domain nameservers](/dns/nameservers/update-nameservers/). Updating your nameservers is required to activate your domain on Cloudflare and use most of our [application services](/fundamentals/concepts/how-cloudflare-works/).

### Standard nameservers

Unless your account has a specific [DNS zone defaults](/dns/additional-options/dns-zone-defaults/) configuration, when you add a domain on a [primary (full)](/dns/zone-setups/full-setup/) or [secondary](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/) DNS setup, Cloudflare automatically assigns two standard nameservers for your zone.

Standard nameservers are hosted on `ns.cloudflare.com` and follow the pattern `<proper_name>.ns.cloudflare.com`.

To know the reason behind these nameserver names, refer to [our blog](https://blog.cloudflare.com/whats-the-story-behind-the-names-of-cloudflares-name-servers/).

### Advanced nameservers

Enterprise accounts on [Foundation DNS](/dns/foundation-dns/) have access to advanced nameservers.

[Advanced nameservers](/dns/foundation-dns/advanced-nameservers/) are hosted on `foundationdns.com`, `foundationdns.net`, and `foundationdns.org`.

Each zone that uses advanced nameservers is assigned a set of three nameservers names: `<color>.foundationdns.com`, `<color>.foundationdns.net`, and `<color>.foundationdns.org`.

### Custom nameservers

With [custom nameservers](/dns/nameservers/custom-nameservers/), your nameservers are hosted on your own domain (or domains) and, in this sense, are not Cloudflare branded.

You provide fully qualified domain names (`ns1.mydomain.com`) for your nameservers, and Cloudflare assigns one IPv4 and one IPv6 to each of your custom nameservers.

:::caution
 <Render file="ns-advanced-vs-custom" />
:::

---

# Nameserver options

URL: https://developers.cloudflare.com/dns/nameservers/nameserver-options/

import { Example } from "~/components"

Refer to the sections below to learn about different nameserver options.

## Assignment method

When you add a domain on a full or secondary setup, Cloudflare automatically assigns your nameservers.

The [default assignment method](/dns/zone-setups/reference/nameserver-assignment/) is to use standard nameservers and favor consistent nameserver names across all zones within an account. Nonetheless, in case there are conflicts - for example, if someone else has already added the same zone to a different account - you may get different nameserver names.

To have control over what nameservers are assigned for different zones within an account, you can use [account custom nameservers](/dns/nameservers/custom-nameservers/account-custom-nameservers/).

### DNS zone defaults

If you have an Enterprise account, you also have the option to [configure your own DNS zone defaults](/dns/additional-options/dns-zone-defaults/) and change how Cloudflare handles nameserver assignment when you add a new zone to your account:

- **Standard nameservers randomized**: instead of attempting consistency, Cloudflare assigns random pairs of nameserver names every time you add a new domain to your account.
- **Advanced nameservers**: Cloudflare uses the same method as the default - trying to keep nameserver names consistent for different zones within an account - but uses the specific [Foundation DNS nameservers](/dns/foundation-dns/advanced-nameservers/).
- **Account custom nameservers**: Cloudflare automatically assigns a set of [account custom nameservers](/dns/nameservers/custom-nameservers/account-custom-nameservers/) that you have previously configured for your account. In this method, **Set 1** will be attempted first and, in case of any conflicts, Cloudflare will cycle through the other nameserver sets, in ascending order.

## Multi-provider DNS

Multi-provider DNS is an optional setting for zones using [full setup](/dns/zone-setups/full-setup/) and is an enforced default behavior for zones using [secondary setup](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/).

When you enable multi-provider DNS on a primary (full setup) zone:

* Cloudflare will no longer ignore `NS` records created on the zone apex, as in the example below.

  <Example>

  | Type | Name | Nameserver         |
  | ---- | ---- | ------------------ |
  | `NS` | `@`  | `ns1.external.com` |

  </Example>

This means that responses to DNS queries made to the zone apex and requesting `NS` records will contain both Cloudflare's and your other DNS providers' nameservers.

* Cloudflare will activate a primary (full setup) zone even if its [nameservers listed at the registrar](/dns/nameservers/update-nameservers/) include nameservers from other DNS providers.

:::caution

If you choose this option and you also want to use DNSSEC on your zone, make sure to set up [multi-signer DNSSEC](/dns/dnssec/multi-signer-dnssec/).

:::

## Nameserver TTL

For both Cloudflare nameservers (standard or advanced) and custom nameservers, the `NS` record time-to-live (TTL) is controlled by the specific setting in **DNS** > **Records** > **DNS record options**.

The default TTL is 24 hours (or 86,400 seconds), but you have the option to lower this value depending on your needs. For example, shorter TTLs can be useful when you are changing nameservers or migrating a zone. Accepted values range from 30 to 86,400 seconds.

This setting can also be configured as a [DNS zone default](/dns/additional-options/dns-zone-defaults/), meaning new zones created in your account will automatically start with the value you define.

---

# Update nameservers

URL: https://developers.cloudflare.com/dns/nameservers/update-nameservers/

import { Render } from "~/components"

To use Cloudflare DNS as an authoritative DNS provider - be it in a [primary (full)](/dns/zone-setups/full-setup/) or [secondary](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/) setup -, your domain nameservers must point to nameservers that you get from your Cloudflare account. Updating your nameservers is required to activate your domain on Cloudflare and use most of our [application services](/fundamentals/concepts/how-cloudflare-works/).

## Specific processes

Although Cloudflare will [provide you the nameservers](/dns/nameservers/#authoritative-nameservers-offering) or allow you to create your own [custom nameservers](/dns/nameservers/custom-nameservers/), the final step to make Cloudflare an authoritative DNS provider for your domain may have to be done outside of Cloudflare.

Unless you are using [Cloudflare Registrar](/registrar/), consider which of the following sections correspond to your use case.

### Your domain uses a different registrar

If you have acquired your domain from a [registrar](https://www.cloudflare.com/learning/dns/glossary/what-is-a-domain-name-registrar/) other than Cloudflare Registrar - and it has not been [delegated to another zone](#your-domain-is-delegated-to-another-zone) - you need to update your nameservers at your registrar.

<Render file="ns-update-providers" />

If you do not know who your registrar is, you can use aÂ Whois search, such as [ICANN Lookup](https://lookup.icann.org/). If the registrar indicated on your Whois search result is not a service that you have interacted directly with, you may [have acquired your domain from a reseller](#you-have-acquired-your-domain-from-a-reseller).

### You have acquired your domain from a reseller

Some services, such as website builders ([Squarespace](https://support.squarespace.com/hc/articles/115003671428-Who-s-my-domain-provider), for example), are not registrars but act as a [reseller](https://www.icann.org/resources/pages/reseller-2013-05-03-en), allowing you to buy domains directly from them.

In that case, you may have to update your nameservers in the reseller platform, not at the registrar.

:::note

Refer to [Squarespace documentation](https://support.squarespace.com/hc/articles/4404183898125-Nameservers-and-DNSSEC-for-Squarespace-managed-domains#toc-open-the-domain-s-advanced-settings) on how to update nameservers in their platform. 
:::

### Your domain is delegated to another zone

If you are onboarding a subdomain `shop.example.com` as a [child domain](/dns/zone-setups/subdomain-setup/), it is expected that the child domain has been delegated to the parent domain.

Delegation means that `shop.example.com` has specific `NS` records set up for it within the DNS records management of the parent zone (`example.com`).

If that is the case, when setting up your zone in Cloudflare or opting for a different set of [nameservers](/dns/nameservers/), you have to update the `NS` records in the parent domain, and not at the registrar.

---

# Proxying limitations

URL: https://developers.cloudflare.com/dns/proxy-status/limitations/

import { Render, GlossaryTooltip } from "~/components";

This page describes expected limitations when <GlossaryTooltip term="proxy status">proxying DNS records</GlossaryTooltip>. For further information about proxying, refer to [How Cloudflare works](/fundamentals/concepts/how-cloudflare-works/).

## Proxy eligibility

Only A, AAAA, and CNAME DNS records that serve HTTP or HTTPS traffic can be proxied. Other record types cannot be proxied.

If you encounter a CNAME record that you cannot proxy â€” usually associated with another CDN provider â€” a proxied version of that record will cause connectivity errors. Cloudflare is purposely preventing that record from being proxied to protect you from a misconfiguration.

### Pre-signed DNSSEC

If you use Cloudflare as your [secondary DNS provider](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/) and leverage [Secondary DNS Overrides](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/proxy-traffic/) to set records to proxied, note that opting for [Pre-signed DNSSEC](/dns/zone-setups/zone-transfers/cloudflare-as-secondary/dnssec-for-secondary/) will cause Cloudflare to treat your records as DNS-only.

## Ports and protocols

To proxy HTTP/HTTPS traffic on [non-standard ports](/fundamentals/reference/network-ports/) or to proxy a TCP or UDP based application, use [Cloudflare Spectrum](/spectrum/).

## Pending domains

<Render file="onboard-warning" product="dns" />

This means that DNS records â€” even those set to [proxy traffic through Cloudflare](#proxy-eligibility) â€” will be [DNS-only](/dns/proxy-status/#dns-only-records) until your zone has been activated and any requests to your DNS records will return your origin server's IP address.

If this warning is still present after 24 hours, refer to [Troubleshooting](/dns/troubleshooting/).

For enhanced security, we recommend rolling your origin IP addresses at your hosting provider after your zone has been activated. This action prevents your origin IPs from being leaked during onboarding.

## Windows authentication

Because Microsoft Integrated Windows Authentication, NTLM, and Kerberos violate HTTP/1.1 specifications, they are not compatible with proxied DNS records.

---

# Proxy status

URL: https://developers.cloudflare.com/dns/proxy-status/

import { Render, Example, Details, GlossaryTooltip } from "~/components";

While your [DNS records](/dns/manage-dns-records/) make your website or application available to visitors and other web services, the **Proxy status** of a DNS record defines how Cloudflare treats incoming DNS queries for that record.

The records you can proxy through Cloudflare are [records used for IP address resolution](/dns/manage-dns-records/reference/dns-record-types/#ip-address-resolution) â€” meaning A, AAAA, or CNAME records.

Cloudflare recommends setting to proxied all A, AAAA, and CNAME records that are used for serving web traffic. For example, CNAME records being used to verify your domain for a third-party service should not be proxied.

:::note
Proxying is on by default when you onboard a domain via the dashboard.
:::

### Benefits

When you set a DNS record to **Proxied** (also known as orange-clouded), Cloudflare can:

- Protect your origin server from [DDoS attacks](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/).
- [Optimize, cache, and protect](/fundamentals/setup/manage-domains/connect-your-domain/#domain-configurations) all requests to your application.
- Apply your configurations for a variety of Cloudflare products.

:::caution
When you [add a domain](/fundamentals/setup/manage-domains/add-site/) to Cloudflare, Cloudflare protection will be in a [pending state](/dns/zone-setups/reference/domain-status/) until we can verify ownership. This could take up to 24 hours to complete. Refer to [Limitations](/dns/proxy-status/limitations/#pending-domains) for further guidance.
:::

### Example

<Render file="proxy-status-dns-table" />

This means that:

- A DNS query to the proxied record `blog.example.com` will be answered with a Cloudflare [anycast IP address](/fundamentals/concepts/cloudflare-ip-addresses/) instead of `192.0.2.1`. This ensures that HTTP/HTTPS requests for this name will be sent to Cloudflare's network and can be proxied, which allows the [benefits listed above](#benefits).
- A DNS query to the DNS-only record `shop.example.com` will be answered with the actual origin IP address, `192.0.2.2`. In addition to exposing your origin IP address and not benefitting from several features, Cloudflare cannot provide HTTP/HTTPS analytics on those requests (only DNS analytics).

For further context, refer to [How Cloudflare works](/fundamentals/concepts/how-cloudflare-works/).

---

## Proxied records

The sections below describe specific behaviors and expected outcomes when you have DNS records set to <GlossaryTooltip term="proxy status">proxied</GlossaryTooltip>. There may also be some [limitations](/dns/proxy-status/limitations/) in specific scenarios.

### Predefined time to live

By default, all proxied records have a time to live (TTL) of **Auto**, which is set to 300 seconds. This value cannot be edited.

Since only [records used for IP address resolution](/dns/manage-dns-records/reference/dns-record-types/#ip-address-resolution) can be proxied, this setting ensures that potential changes to the assigned [anycast IP address](/fundamentals/concepts/cloudflare-ip-addresses/) will take effect quickly, as recursive resolvers will not cache them for longer than 300 seconds (five minutes).

:::note
It may take longer than five minutes for you to actually experience record changes, as your local DNS cache may take longer to update.
:::

### Mix proxied and unproxied

If you have multiple A or AAAA records on the same name and at least one of them is proxied, Cloudflare will treat all A or AAAA records on this name as being proxied.

<Details header="Example">

<Example>
DNS management for **example.com**:

| Type | Name    | Content      | Proxy status | TTL    |
| ---- | ------- | ------------ | ------------ | ------ |
| A    | `blog`  | `192.0.2.1`  | Proxied      | Auto   |
| A    | `blog`  | `192.0.2.5`  | DNS only     | Auto   |

In this example, all traffic intended for `blog.example.com` will be treated as if both records were **Proxied**.

</Example>

</Details>

### Protocol optimization

For proxied records, if your domain has [HTTP/2 or HTTP/3 enabled](/speed/optimization/protocol/), Cloudflare automatically generates corresponding [HTTPS Service (HTTPS) records](/dns/manage-dns-records/reference/dns-record-types/#svcb-and-https) on the fly. HTTPS records allow you to provide a client with information about how it should connect to a server upfront, without the need of an initial plaintext HTTP connection.

:::note
Both HTTP/2 and HTTP/3 configurations also require that you have an SSL/TLS certificate served by Cloudflare. This means that disabling [Universal SSL](/ssl/edge-certificates/universal-ssl/), for example, could impact this behavior.
:::

---

## DNS-only records

When an A, AAAA, or CNAME record is **DNS-only** â€” also known as being gray-clouded â€” DNS queries for these will resolve to the record's origin IP address, as described in the [example](#example).

In addition to potentially exposing your origin IP addresses to bad actors and [DDoS attacks](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/), leaving your records as **DNS-only** means that Cloudflare cannot [optimize, cache, and protect](/fundamentals/concepts/how-cloudflare-works/) requests to your application or provide analytics on those requests.

<Render file="mix-proxied-and-unproxied" product="dns" />

---

# Features and plans

URL: https://developers.cloudflare.com/dns/reference/all-features/

import { ProductFeatures } from "~/components"

Cloudflare provides the following features for different [plans](https://www.cloudflare.com/plans/).

## Features

<ProductFeatures id="dns" />

---

# Analytics API properties

URL: https://developers.cloudflare.com/dns/reference/analytics-api-properties/

import { Details, Render } from "~/components"

This page describes API properties that you can use in requests to the [DNS analytics API](/api/resources/dns/subresources/analytics/subresources/reports/methods/get/).

:::caution[Warning]
The [DNS analytics API](/api/resources/dns/subresources/analytics/subresources/reports/methods/get/), along with the following [API properties](/dns/reference/analytics-api-properties/), will be deprecated soon.

To access the new analytics dashboard, go to **DNS** > **Analytics**. Refer to [Analytics and logs](/dns/additional-options/analytics/) for details.
:::

## Metrics

A metric is a numerical value based on an attribute of the data, for example a query count.

In API requests, metrics are set in the `metrics` parameter. If you need to list multiple metrics, separate them with commas.



| Metric             | Name                          | Example | Unit                 |
| ------------------ | ----------------------------- | ------- | -------------------- |
| queryCount         | Query count                   | `1000`  | Count                |
| uncachedCount      | Uncached query count          | `1`     | Count                |
| staleCount         | Stale query count             | `1`     | Count                |
| responseTimeAvg    | Average response time         | `1.0`   | Time in milliseconds |
| responseTimeMedian | Median response time          | `1.0`   | Time in milliseconds |
| responseTime90th   | 90th percentile response time | `1.0`   | Time in milliseconds |
| responseTime99th   | 99th percentile response time | `1.0`   | Time in milliseconds |



## Dimensions

Dimensions can be used to break down the data by given attributes.

In API requests, dimensions are set in the `dimensions` parameter. If you need to list multiple dimensions, separate them with commas.



| Dimension          | Name                 | Example       | Notes                                                                                                                                     |
| ------------------ | -------------------- | ------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| queryName          | Query Name           | `example.com` |                                                                                                                                           |
| queryType          | Query Type           | `AAAA`        | [Types defined by IANA](http://www.iana.org/assignments/dns-parameters/dns-parameters.xhtml#dns-parameters-4). Unknown types are empty.   |
| responseCode       | Response Code        | `NOERROR`     | [Response codes defined by IANA](http://www.iana.org/assignments/dns-parameters/dns-parameters.xhtml#dns-parameters-6). Always uppercase. |
| responseCached     | Response Cached      | `Cached`      | Either `Cached` or `Uncached`.                                                                                                            |
| coloName           | Colo Name            | `SJC`         | PoP code.                                                                                                                                 |
| origin             | Origin               | `2001:db8::1` | Origin used to resolve the query. Empty if N/A or if the query was answered from cache.                                                   |
| dayOfWeek          | Day Of Week          | `1`           | Break down by day of week. Monday is `1`, and Sunday is `7`.                                                                              |
| tcp                | TCP                  | `1`           | Either `1` or `0` depending on the protocol used.                                                                                         |
| ipVersion          | IP Version           | `6`           | IP protocol version used (currently `4` or `6`).                                                                                          |
| querySizeBucket    | Query Size Bucket    | `16-31`       | Query size bucket by multiples of 16.                                                                                                     |
| responseSizeBucket | Response Size Bucket | `16-31`       | Response size bucket by multiples of 16.                                                                                                  |



## Filters

Filters use the form `dimension operator expression`, where each part corresponds to the following:

* **Dimension**: Specifies the [dimension](#dimensions) to filter on. For example, `queryName`.
* **Operator**: Defines the type of filter match to use. Operators are specific to dimensions.
* **Expression**: States the values to include or exclude from the results. Expressions use regular expression (regex) syntax.

### Filter operators

<Render file="api-filter-operators" product="fundamentals" params={{ one: "queryName", two: "example.com", three: "responseCode", four: "NOERROR" }} />

### Combining filters

<Render file="api-combine-filters" product="fundamentals" />


<Details header="Examples using OR">

* `responseCode==NOERROR,responseCode==NXDOMAIN` indicates that response code is either `NOERROR` or `NXDOMAIN`.
* `coloName==SJC OR coloName==LAX` indicates queries in either `SJC` or `LAX`.


</Details>


<Details header="Examples using AND">

* `responseCode==NOERROR;queryType==AAAA` indicates that response code is `NOERROR` and query type is `AAAA`.
* `queryType==AAAA AND coloName==SJC` indicates `AAAA` queries in `SJC`.


</Details>

---

# Domain Connect

URL: https://developers.cloudflare.com/dns/reference/domain-connect/

If you are a service provider, consider this page for information on how Cloudflare supports [Domain Connect](https://www.domainconnect.org/) and how you can onboard your template.

## What is Domain Connect

Domain Connect is an open standard that allows service providers - such as email or web hosting platforms - to make it easier for their end users to configure functionality, without having to manually edit DNS records.

This is achieved with templates that close the gap between necessary configurations (required by the service provider) and necessary DNS records changes (that must happen at the authoritative DNS provider).

In practice, this means that when a user that owns `example.com` and has Cloudflare as their authoritative DNS provider wants to use your service, instead of having to manually update their DNS records, they will only have to authenticate themselves and the necessary changes will be applied automatically.

## Setup

### Before you begin

* Note that Cloudflare only supports the [Domain Connect synchronous flow](https://www.domainconnect.org/getting-started/).
* Domain Connect templates and tools are published on GitHub, so you must have a GitHub account and be familiar with [GitHub forks and pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks).

### 1. Add templates to the repository

Domain Connect templates are published and maintained on a GitHub repository.

1. Create a fork of the [templates repository](https://github.com/Domain-Connect/Templates).
2. Add your template. You can create a copy of one of the existing templates and edit it according to your needs.
   * Refer to the [Domain Connect Specification](https://github.com/Domain-Connect/spec/blob/master/Domain%20Connect%20Spec%20Draft.adoc#52-template-definition) for details on the different available fields.
   * If present, you must set the `syncBlock` field on your template to `false`. This means the template flow will be synchronous, which is the only option supported by Cloudflare.
   * You must also provide a synchronous public key domain (`syncPubKeyDomain` [^1]). When your template is in use, synchronous calls will be digitally signed.
3. Make sure you follow the naming format defined by Domain Connect: `<providerId>.<serviceId>.json`.

:::note[Tip]

You can use Domain Connect's [linter tool](https://github.com/Domain-Connect/dc-template-linter) with the option `-cloudflare` enabled to check your template against Cloudflare specific rules.
:::

4. Submit a pull request to have your template(s) added to the repository.

Once your pull request has been reviewed and merged, contact Cloudflare as specified below.

### 2. Contact Cloudflare to onboard your template

When your template is onboarded, a graphical user interface flow will be available to your end users.

Send an email to `domain-connect@cloudflare.com`, including the following information:

1. List of template(s) you want to onboard, with their corresponding GitHub hyperlinks.
2. A logo to be displayed as part of the Domain Connect flow. Preferably in `SVG` format.
3. The default [proxy status](/dns/proxy-status/) you would like Cloudflare to set for `A`, `AAAA`, and `CNAME` records that are part of your template(s). Proxying other record types is not supported.

   :::note

   Proxy status is applied per template. If needed, organize the records in different templates to specify a different default proxy status per template. Once the records have been created, the domain owner can always change the proxy status for `A`, `AAAA`, and `CNAME` records later.
   :::
4. (Optional) A Cloudflare [account ID](/fundamentals/setup/find-account-and-zone-ids/) for you to test the flow.

   If you have a [DNS provider discovery](https://github.com/Domain-Connect/spec/blob/master/Domain%20Connect%20Spec%20Draft.adoc#dns-provider-discovery) automation in place and will not list new DNS providers manually, Cloudflare can initially restrict your template to be exposed to the specified account only. Once you confirm everything is working as expected, Cloudflare will publish your template on the discovery endpoint, to be picked up by your automation.

## Template updates

Since September, 2024, template updates are picked up by an automation.

The automation compares the template version number in Cloudflare with the authoritative source of the template on the Internet. This check runs multiple times a day. Although Cloudflare cannot guarantee when exactly each update will be picked up, the process is expected to take no longer than eight hours.

:::note
The authoritative source must be in raw `json` format for the automation to work correctly, as in [this example](https://raw.githubusercontent.com/Domain-Connect/Templates/master/exampleservice.domainconnect.org.template1.json).

If the source template is unavailable, or technically invalid, Cloudflare will keep the previous template in use until the updated version is fixed.
:::

You can contact Cloudflare to opt out of the automatic updates. Once the automation is disabled, you can request template updates individually, by writing to `domain-connect@cloudflare.com`.

### Troubleshooting

Send an email to `domain-connect@cloudflare.com` with the following information:

1. Detailed description of what is wrong:
   - List the record(s) that the issue is related with.
   - Describe what the template did.
   - Describe what you expected the template to do.

2. A [HAR file](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/#generate-a-har-file) attachment containing the problematic update.

[^1]: A domain that can be queried for `TXT` records containing a public key to verify your digital signature.

---

# Reference

URL: https://developers.cloudflare.com/dns/reference/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Recommended third-party tools

URL: https://developers.cloudflare.com/dns/reference/recommended-third-party-tools/

You can use the third-party tools listed below to test and troubleshoot DNS settings.

* [DNSViz](https://dnsviz.net): A web-based tool for visualizing the status of a DNS zone to understand and troubleshoot the deployment of DNS Security Extensions (DNSSEC).

* [Dig Web Interface](https://digwebinterface.com): An online DNS lookup tool based on the command line interface `dig`. Users can skip the process of entering commands with complicated parameters in the terminal by entering the same information in this web tool and getting the same results.

* [dns.google](https://dns.google): A web-based tool, similar to Dig Web Interface, where users can get DNS responses for specific queries.

* [Mess with DNS](https://messwithdns.net): An educational resource that encourages users to experiment with DNS records by providing users with a domain where they are free to play around and break things during the learning process.

---

# DNS setups

URL: https://developers.cloudflare.com/dns/zone-setups/

import { Details } from "~/components"

When using Cloudflare DNS, you have a few options for your DNS zone setup:

* [Full setup](/dns/zone-setups/full-setup/) (most common): Use Cloudflare as your primary DNS provider and manage your DNS records on Cloudflare.
* [Partial (CNAME) setup](/dns/zone-setups/partial-setup/): Keep your primary DNS provider and only use Cloudflare's reverse proxy for individual subdomains.
* [Subdomain setup](/dns/zone-setups/subdomain-setup/): With your apex domain (`example.com`) on a partial or full setup, independently manage the settings for a specific subdomain (`blog.example.com`) within a separate zone and, potentially, a separate account.
* [Zone transfers](/dns/zone-setups/zone-transfers/): Use Cloudflare and another DNS provider together across your entire zone to increase availability and fault tolerance. DNS records will be transferred between providers using [AXFR](https://datatracker.ietf.org/doc/html/rfc5936) or [IXFR](https://datatracker.ietf.org/doc/html/rfc1995).

:::note[Note:]


If you run your own authoritative nameservers but still want to benefit from Cloudflare's global anycast network, check out [DNS Firewall](/dns/dns-firewall/).


:::

## Common use cases and availability

If you are unsure of which setup to use, consider the questions below for an overview of common use cases and their correspondence to each setup and [different pricing plans](https://www.cloudflare.com/plans/#overview).


<Details header="Are you on a Free or Pro plan?">

If you are on a Free or Pro plan, [full setup](/dns/zone-setups/full-setup/) is the only one available. This is the recommended and most common option.


</Details>


<Details header="Will you be using Cloudflare with other DNS providers?">

If you are on a Business or Enterprise plan, you can use [partial (CNAME) setup](/dns/zone-setups/partial-setup/) to keep your primary DNS provider and only proxy individual subdomains through Cloudflare.

If you are on an Enterprise plan, you also have the option to use [zone transfers](/dns/zone-setups/zone-transfers/) to set up Cloudflare as either a primary or a secondary DNS provider.
</Details>


<Details header="Do you need to manage subdomains separately?">

If you are on an Enterprise plan, you can use [subdomain setup](/dns/zone-setups/subdomain-setup/) to manage the Cloudflare settings for one or more subdomains separately from your domain apex.


</Details>

---

# Zone removal

URL: https://developers.cloudflare.com/dns/zone-setups/removal/

If domains on Free zones remain in the [Pending](/dns/zone-setups/reference/domain-status/#pending) or [Moved](/dns/zone-setups/reference/domain-status/#moved) status for too long, Cloudflare automatically removes them from your account and the Cloudflare network. Refer to [zone statuses](/dns/zone-setups/reference/domain-status/) for more details.

You can also [manually remove a domain](/fundamentals/setup/manage-domains/remove-domain/) from Cloudflare.

If you need to re-add a domain to your account, follow the [regular onboarding flow](/fundamentals/setup/manage-domains/add-site/).

:::caution[Purged zones]


By default, your zone will be automatically purged seven days after the removal. In this case, even if you re-add the domain to the same Cloudflare account, none of the zone settings are expected to be restored. Refer to [zone statuses](/dns/zone-setups/reference/domain-status/) for more details.


:::

---

# DNS_PROBE_FINISHED_NXDOMAIN

URL: https://developers.cloudflare.com/dns/troubleshooting/dns-probe-finished-nxdomain/

import { Render } from "~/components"

<Render file="dns-errors-ts-intro" params={{ one: "DNS_PROBE_FINISHED_NXDOMAIN" }} />

## Background

`DNS_PROBE_FINISHED` means that the DNS request for a resource timed out and `NXDOMAIN` stands for non-existent domain. Together, these messages mean that the DNS query for a specific resource could not locate an associated domain.

Though visitors sometimes encounter this error â€” or similarly worded messages from Safari, Edge, or Firefox â€” because of network or local DNS issues, it might point to an issue with your DNS records in Cloudflare.

## Potential solutions

<Render file="dns-errors-ts-action" params={{ one: "DNS_PROBE_FINISHED_NXDOMAIN" }} />

:::note


For additional troubleshooting help, refer to our [Community troubleshooting guide](https://community.cloudflare.com/t/community-tip-fixing-the-dns-probe-finished-nxdomain-error/42818).


:::

---

# DNS_PROBE_POSSIBLE

URL: https://developers.cloudflare.com/dns/troubleshooting/dns-probe-possible/

import { Render } from "~/components"

<Render file="dns-errors-ts-intro" params={{ one: "DNS_PROBE_POSSIBLE" }} />

## Background

`DNS_PROBE_POSSIBLE` means that the resolver could not find DNS records for the requested hostname.

Though visitors sometimes encounter this error â€” or similarly worded messages from Safari, Edge, or Firefox â€” because of network or local DNS issues, it might point to an issue with your DNS records in Cloudflare.

## Potential solutions

<Render file="dns-errors-ts-action" params={{ one: "DNS_PROBE_POSSIBLE" }} />

---

# Email issues

URL: https://developers.cloudflare.com/dns/troubleshooting/email-issues/

import { Render } from "~/components"

If you have issues sending or receiving mail, follow these troubleshooting steps.

## Are your records correct?

Consult with your mail administrator or mail provider to ensure you have valid DNS record content.

## Are DNSÂ records missing?

Contact your mail administrator to confirm the DNS records for your domain are correct. Refer to our guide onÂ [managing DNS records in Cloudflare](/dns/manage-dns-records/how-to/create-dns-records)Â if you need assistance to add or edit DNS records.

## Do you have CNAME Flattening enabled?

When set to [Flatten all CNAMEs](/dns/cname-flattening/set-up-cname-flattening/) in your Cloudflare DNS settings, queries to all `CNAME` records will flatten to an `A` record; no `CNAME` records will be returned.

Also, if `CNAME` records are not returned by the queried nameserver (sometimes nameservers will return TXT records), this may result in nothing being returned when ***Flatten all CNAMEs*** is enabled. Changing to ***Flatten at the root*** should fix any issues with your CNAME records not being returned.

## Is Cloudflare Spectrum enabled on your account?

Cloudflare does not proxy traffic on port 25 (SMTP) unlessÂ [Cloudflare Spectrum](/spectrum/reference/configuration-options#smtp)Â is enabled and configured to proxy email traffic across Cloudflare. If you do not have Spectrum enabled, then no email traffic (SMTP) will actually pass through Cloudflare, and we will simply resolve the DNS. This also means that any DNS record used to send email traffic must be DNS-only to bypass the Cloudflare network. CheckÂ [Identifying subdomains compatible with Cloudflare's proxy](/dns/proxy-status/) for more details.

## Contact your mail provider for assistance

If your email does not work shortly after editing DNS records, contact your mail administrator or mail provider for further assistance in troubleshooting so that data about the issue can be provided to Cloudflare support.

## dc-######### subdomain

The dc-##### subdomain is added to overcome a conflict created when your `SRV` orÂ `MX` recordÂ resolves to a domain configured to [proxy](/dns/proxy-status/) to Cloudflare.

Therefore, Cloudflare will create a `dc-#####` DNS record that resolves to the origin IP address. The `dc-#####` record ensures that traffic for your `MX` or `SRV` record is not proxied (it directly resolves to your origin IP) while the Cloudflare proxy works for all other traffic.

For example, before using Cloudflare, suppose your DNS records for mail are as follows:

`example.com MX example.com` `example.com A 192.0.2.1`

After using Cloudflare and proxying theÂ `A` record, Cloudflare will provide DNS responses with a Cloudflare IP (`203.0.113.1` in the example below):

`example.com MX example.com` `example.com A 203.0.113.1`

Since proxying mail traffic to Cloudflare would break your mail services, Cloudflare detects this situation and creates a `dc-#####` record:

`example.com MX dc-1234abcd.example.com` `dc-1234abcd.example.com A 192.0.2.1` `example.com A 203.0.113.1`

Removing the `dc-######` record is only possible via one of these methods:

* If no mail is received for the domain, delete the `MX` record.
* If mail is received for the domain, update the `MX` record to resolve to a separateÂ `A` record for a mail subdomain that is not proxied by Cloudflare:

`example.com MX mail.example.com` `mail.example.com A 192.0.2.1` `example.com A 203.0.113.1`

:::caution

If your mail server resides on the same IP as your web server, your MX
record will expose your origin IP address.
:::

***

## Best practices for MX records on Cloudflare

<Render file="email-record-origin-ip" product="learning-paths" />

---

# General DNS issues

URL: https://developers.cloudflare.com/dns/troubleshooting/dns-issues/

In web browsers such as Safari or Chrome, there are several commonly observable DNS errors:

* `This site canâ€™t be reached`
* `This webpage is not available`
* `err_name_not_resolved`
* `Can't find the server`
* [`Error 1001 DNS resolution error`](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-1xxx-errors/#error-1001-dns-resolution-error)

## Common causes and resolutions

Below are the most common causes for DNS resolution errors along with suggested solutions.

### Mistyped domain or subdomain

Verify that the domain or subdomain was correctly spelled in the request URL.

### Missing DNS records

Ensure that you have the necessary DNS records in theÂ **DNS**Â app of your Cloudflare dashboard. This includes having the following records:

* The [zone apex](/dns/manage-dns-records/how-to/create-zone-apex/) (e.g., `example.com`) record.
* Existing [subdomains](/dns/manage-dns-records/how-to/create-subdomain/) (`www.example.com`, `blog.example.com`) records.

:::note

If you have a [partial (CNAME) setup](/dns/zone-setups/partial-setup), ensure your DNS records also exist in your authoritative nameservers.
:::

### DNSSEC was not disabled before the domain was added to Cloudflare

DNS resolution failures occur ifÂ [DNSSEC is not disabled](/dns/dnssec/#disable-dnssec)Â at your domain provider before you add the domain to Cloudflare.

### Nameservers no longer point to Cloudflare

If you manage DNS records via theÂ **DNS**Â app in Cloudflare's Dashboard and your domain stops pointing to Cloudflare's nameservers, DNS resolution will stop functioning.

This can occur if your domain registrar switches the nameservers for your domain to point to their default nameservers.Â To confirm if this is the problem,Â [check whether your domain uses Cloudflare's nameservers](/dns/zone-setups/full-setup/setup/#verify-changes).

### Unresolved IP address

In rare cases, the DNS resolver in the client requesting the URL might fail to resolve a DNS record to a valid IP address.

Reload the page after a short wait to note if the problem disappears.Â This issue is unrelated to Cloudflare, but usingÂ [Cloudflare's DNS resolver](/1.1.1.1/setup/)Â may help.Â Contact your hosting provider for additional help with your current DNS resolver.

---

# General FAQ

URL: https://developers.cloudflare.com/dns/troubleshooting/faq/

import { Render, GlossaryTooltip } from "~/components";

## Is Cloudflare a free DNS (domain nameserver) provider?

Yes. Cloudflare offersÂ [free DNS services](https://www.cloudflare.com/dns)Â to customers on all plans. Note that:

1. You do not need to change your hosting provider to use Cloudflare.
2. You do not need to move away from your registrar. The only change you make with your registrar is to point the authoritative nameservers to the Cloudflare nameservers.

---

## Does Cloudflare charge for or limit DNS queries?

Cloudflare never limits or caps DNS queries, but the pricing depends on your plan level.

For customers on Free, Pro, or Business plans, Cloudflare does not charge for DNS queries.

For customers on Enterprise plans, Cloudflare uses the number of monthly DNS queries as a pricing input to generate a custom quote.

---

## Where do I change my nameservers to point to Cloudflare?

Make the change at your registrar, which is where you registered your domain. This may or may not be your hosting provider. If you don't know who your registrar is for the domain, you can find this by doing aÂ WHOIS search. You can use [ICANN Lookup](https://lookup.icann.org/), for example.

:::caution

Some country code TLDs may not be supported by ICANN Lookup. If that is the case, use a different WHOIS search tool.
:::

Once you identify your registrar, follow the instructions inÂ [change nameservers to Cloudflare](/dns/zone-setups/full-setup/setup/#update-your-nameservers).

---

## Does Cloudflare limit the number of DNS records a domain can have?

Yes. All customers have a limit on the number of DNS records they can create.

- Free: 200
- Pro: 3,500
- Business: 3,500
- Enterprise: 3,500

Free zones created before 2024-09-01 00:00:00 UTC have an increased limit of 1,000.

:::note[For more DNS records]

If you are an Enterprise customer and require more DNS records, contact your account team. Cloudflare can support millions of DNS records on a single zone.

:::


---

## Which record types can Cloudflare proxy?

Only `A`, `AAAA`, and `CNAME` records can be proxied. Cloudflare will not proxy any other [DNS record types](/dns/manage-dns-records/reference/dns-record-types/).

---

## How do I add ANAME records on Cloudflare?

<Render file="aname-alias-callout" />

---

## Can I CNAME a domain not on Cloudflare to a domain that is on Cloudflare?

No. If you would like to do a redirect for a site not on Cloudflare, then set up a traditional `301` or `302` redirect on your origin web server.

Redirecting non-Cloudflare sites via `CNAME` records would cause a DNS resolution error. Since Cloudflare is a reverse proxy for the domain that is on Cloudflare, the `CNAME` redirect for the domain (not on Cloudflare) would not know where to send the traffic to.

---

## Does Cloudflare support wildcard DNS entries?

Cloudflare supports wildcard '*' DNS records, both proxied and unproxied, on all plans.

---

## How long does it take for a DNS change I made to push out?

By default, any changes or additions you make to your Cloudflare zone file will take effect globally within 5 minutes, usually much less.

Depending on the Time-to-Live (TTL) set on the previous [DNS record](/dns/manage-dns-records/how-to/create-dns-records/), old data may still remain cached until the TTL expires. Proxied records expire after 5 minutes ("Automatic"), but the TTL for unproxied records can be customized.

If changes to records with large TTLs are anticipated, it may make sense to reduce the TTL ahead of time so that the change takes effect as quickly as possible.

---

## Does Cloudflare offer domain masking?

No. Cloudflare does not offer domain masking or DNS redirect services (your hosting provider might). However, we do offer URL forwarding through [Bulk Redirects](/rules/url-forwarding/bulk-redirects/).

---

## Why can't I make ANY queries to Cloudflare DNS servers?

`ANY` queries are special and often misunderstood. They are usually used to get all record types available on a DNS name, but what they return is just any type in the cache of recursive resolvers. This can cause confusion when they are used for debugging.

Because of Cloudflare's many advanced DNS features like CNAME flattening, it can be complex and even impossible to give correct answers to `ANY` queries. For example, when DNS records dynamically come and go or are stored remotely, it can be taxing or even impossible to get all the results at the same time.

`ANY` is rarely used in production, but is often used in DNS reflection attacks, taking advantage of the lengthy answer returned by `ANY`.

Instead of using `ANY` queries to list records, Cloudflare customers can get a better overview of their DNS records by logging in and checking their DNS app settings.

The decision to block `ANY` queries was implemented for all Authoritative DNS customers in September 2015, and does not affect DNS Firewall customers.

ReadÂ [Deprecating the DNS ANY meta-query type](https://blog.cloudflare.com/deprecating-dns-any-meta-query-type/)Â on the Cloudflare blog.

---

## Why do I have to remove my `DS` record when signing up for Cloudflare?

<Render file="dnssec-providers" />

For more help, refer to [Enabling DNSSEC in Cloudflare](/dns/dnssec/).

---

## What happens when I remove the `DS` record?

When you remove your DS record, an invalidation process begins which results in the unsigning of your domainâ€™s DNS records. This will allow your authoritative nameservers to be changed. If you are an existing customer, this will not affect your ability to use Cloudflare. New customers will need to complete this step before Cloudflare can be used successfully.

---

## Does Cloudflare support EDNS0 (extension mechanisms for DNS)?

Yes, Cloudflare DNS supports EDNS0. EDNS0 is enabled for all Cloudflare customers. It is a building block for modern DNS implementations that adds support for signaling if the DNS Resolver (recursive DNS provider) supports larger message sizes and DNSSEC.

EDNS0 is the first approved set of mechanisms forÂ [DNS extensions](http://en.wikipedia.org/wiki/Extension_mechanisms_for_DNS), originally published asÂ [RFC 2671](https://datatracker.ietf.org/doc/html/rfc2671).

---

## What should I do if I change my server IP address or hosting provider?

After switching hosting providers or server IP addresses, update the IP addresses in your CloudflareÂ **DNS**Â app. Your new hosting provider will provide the new IP addresses that your DNS should use.Â  To modify DNS record content in theÂ **DNS**Â app, click on the IP address, and enter the new IP address.

---

## Where can I find my Cloudflare nameservers?

Under theÂ **DNS**Â app of your Cloudflare account, review theÂ **Cloudflare Nameservers**.

The IP address associated with a specific Cloudflare nameserver can be retrieved via a dig command or a third-party DNS lookup tool hosted online such asÂ [whatsmydns.net](https://www.whatsmydns.net/):

```sh
dig kate.ns.cloudflare.com
```

```sh output
kate.ns.cloudflare.com.    68675    IN    A    173.245.58.124.
```

---

## Why are Cloudflare's A or AAAA records / IP addresses for my domain's DNS responses appearing?

For DNS records proxied to Cloudflare, Cloudflare's IP addresses are returned in DNS queries instead of your original server IP address. This allows Cloudflare to optimize, cache, and protect all requests for your website.

---

## Can subdomains be added directly to Cloudflare?

Only Enterprise customers can add subdomains directly to Cloudflare viaÂ [Subdomain Support](/dns/zone-setups/subdomain-setup/).

---

## 403 Authentication error when creating DNS records using Terraform

**Problem Description**

`Error: failed to create DNS record: HTTP status 403: Authentication error (10000)` is returned when using Terraform with Cloudflare API.

**Root Cause**

Error seems to be misleading, as the error was found to be in customer code syntax, specifically: zone_id = data.cloudflare_zones.example_com.id

**Solution**

Make sure the argument `zone_id = data.cloudflare_zones.example_com.zones[0].id`. A more detailed use case can be found in [this](https://github.com/cloudflare/terraform-provider-cloudflare/issues/913) GitHub thread.

---

## Why am I getting hundreds of random DNS records after adding my domain?

This can happen when you had a wildcard `*` record configured at your previous authoritative DNS, and for some reason the wildcard record wasn't detected. You can remove these records in bulk [using the API](/api/resources/dns/subresources/records/methods/delete/).

Alternatively, you can also:

1. [Remove your domain](/fundamentals/setup/manage-domains/remove-domain/) from Cloudflare.
2. Delete the wildcard record from your authoritative DNS.
3. [Re-add](/fundamentals/setup/manage-domains/add-site/) the domain.

---

## What IP should I use for parked domain / redirect-only / originless setup?

In the case a placeholder address is needed for â€œoriginlessâ€ setups, use the IPv6 reserved address `100::` or the IPv4 reserved address `192.0.2.0` in your Cloudflare DNS to create a [proxied DNS record](/dns/proxy-status/) that can use Cloudflare [Redirect Rules](/rules/url-forwarding/), [Page Rules](/rules/page-rules/), or [Cloudflare Workers](/workers/).

---

## Why are DNS queries returning incorrect results?

Third-party tools can sometimes fail to return correct DNS results if a recursive DNS cache fails to refresh. In this circumstance, purge your public DNS cache via these methods:

- [Purging your DNS cache at OpenDNS](http://www.opendns.com/support/cache/)
- [Purging your DNS cache at Google](https://developers.google.com/speed/public-dns/cache)
- [Purging your DNS cache locally](https://docs.cpanel.net/knowledge-base/dns/how-to-clear-your-dns-cache/)

---

## Why have I received an email: Your Name Servers have Changed?

For domains where Cloudflare hosts the DNS, Cloudflare continuously checks whether the domain uses Cloudflareâ€™s nameservers for DNS resolution. If Cloudflare's nameservers are not used, the [domain status](/dns/zone-setups/reference/domain-status/) is updated fromÂ *Active*Â toÂ *Moved*Â in the CloudflareÂ **Overview**Â appÂ and an email is sent to the customer.

This is important because, if a domain is in a _Moved_ state for a [long enough period of time](/dns/zone-setups/reference/domain-status/), it will be deleted from Cloudflare.

<Render file="recover-deleted-domain" />

---

## How to configure records on local DNS servers?

You can create CNAME records pointing to `cdn.cloudflare.net` in your local DNS to locally resolve hostnames through Cloudflare.
For example, if you need to resolve `example.com` through Cloudflare in your local DNS server, you need to create a CNAME record such as:

```txt
example.com CNAME example.com.cdn.cloudflare.net
```

---

## Why am I getting a warning for hostname not covered even if I have a custom certificate?

If the [custom certificate](/ssl/edge-certificates/custom-certificates/) has been in place before our new certificate management pipeline, the following warning is displayed but can be discarded.

`This hostname is not covered by a certificate.`

The warning will be gone when you upload a new custom certificate, or start using another type of certificate for this hostname.

---

# Troubleshooting

URL: https://developers.cloudflare.com/dns/troubleshooting/

import { TroubleshootingList } from "~/components"

The following topics are useful for troubleshooting DNS issues.

<TroubleshootingList />

---

# Alarms

URL: https://developers.cloudflare.com/durable-objects/api/alarms/

import { Type, GlossaryTooltip } from "~/components";

## Background

Durable Objects alarms allow you to schedule the Durable Object to be woken up at a time in the future. When the alarm's scheduled time comes, the `alarm()` handler method will be called. Alarms are modified using the <GlossaryTooltip term="Storage API">Storage API</GlossaryTooltip>, and alarm operations follow the same rules as other storage operations.

Notably:

- Each Durable Object is able to schedule a single alarm at a time by calling `setAlarm()`.
- Alarms have guaranteed at-least-once execution and are retried automatically when the `alarm()` handler throws.
- Retries are performed using exponential backoff starting at a 2 second delay from the first failure with up to 6 retries allowed.

:::note[How are alarms different from Cron Triggers?]

Alarms are more fine grained than [Cron Triggers](/workers/configuration/cron-triggers/). A Worker can have up to three Cron Triggers configured at once, but it can have an unlimited amount of Durable Objects, each of which can have an alarm set.

Alarms are directly scheduled from within your Durable Object. Cron Triggers, on the other hand, are not programmatic. [Cron Triggers](/workers/configuration/cron-triggers/) execute based on their schedules, which have to be configured through the Cloudflare dashboard or API.

:::

Alarms can be used to build distributed primitives, like queues or batching of work atop Durable Objects. Alarms also provide a mechanism to guarantee that operations within a Durable Object will complete without relying on incoming requests to keep the Durable Object alive. For a complete example, refer to [Use the Alarms API](/durable-objects/examples/alarms-api/).

## Storage methods

### `getAlarm`

- <code>getAlarm()</code>: <Type text="number | null" />

  - If there is an alarm set, then return the currently set alarm time as the number of milliseconds elapsed since the UNIX epoch. Otherwise, return `null`.

  - If `getAlarm` is called while an [`alarm`](/durable-objects/api/alarms/#alarm) is already running, it returns `null` unless `setAlarm` has also been called since the alarm handler started running.

### `setAlarm`

- <code>{" "}setAlarm(scheduledTimeMs <Type text="number" />)</code>
  : <Type text="void" />

  - Set the time for the alarm to run. Specify the time as the number of milliseconds elapsed since the UNIX epoch.

### `deleteAlarm`

- `deleteAlarm()`: <Type text='void' />

  - Unset the alarm if there is a currently set alarm.

  - Calling `deleteAlarm()` inside the `alarm()` handler may prevent retries on a best-effort basis, but is not guaranteed.

## Handler methods

### `alarm`

- <code>alarm(`alarmInfo`<Type text="Object"/>)</code>`: <Type text='void' />

  - Called by the system when a scheduled alarm time is reached.

  - The optional parameter `alarmInfo` object has two properties:
    - `retryCount` <Type text="number"/>: The number of times this alarm event has been retried.
    - `isRetry` <Type text="boolean"/>: A boolean value to indicate if the alarm has been retried. This value is `true` if this alarm event is a retry.


  - The `alarm()` handler has guaranteed at-least-once execution and will be retried upon failure using exponential backoff, starting at 2 second delays for up to 6 retries. Retries will be performed if the method fails with an uncaught exception.

  - This method can be `async`.

## Example

This example shows how to both set alarms with the `setAlarm(timestamp)` method and handle alarms with the `alarm()` handler within your Durable Object.

- The `alarm()` handler will be called once every time an alarm fires.
- If an unexpected error terminates the Durable Object, the `alarm()` handler may be re-instantiated on another machine.
- Following a short delay, the `alarm()` handler will run from the beginning on the other machine.

```js
import { DurableObject } from "cloudflare:workers";

export default {
	async fetch(request, env) {
		let id = env.ALARM_EXAMPLE.idFromName("foo");
		return await env.ALARM_EXAMPLE.get(id).fetch(request);
	},
};

const SECONDS = 1000;

export class AlarmExample extends DurableObject {
	constructor(ctx, env) {
		this.ctx = ctx;
		this.storage = ctx.storage;
	}
	async fetch(request) {
		// If there is no alarm currently set, set one for 10 seconds from now
		let currentAlarm = await this.storage.getAlarm();
		if (currentAlarm == null) {
			this.storage.setAlarm(Date.now() + 10 * SECONDS);
		}
	}
	async alarm() {
		// The alarm handler will be invoked whenever an alarm fires.
		// You can use this to do work, read from the Storage API, make HTTP calls
		// and set future alarms to run using this.storage.setAlarm() from within this handler.
	}
}
```

The following example shows how to use the `alarmInfo` property to identify if the alarm event has been attempted before.

```js
class MyDurableObject extends DurableObject {
  async alarm(alarmInfo) {
    if (alarmInfo?.retryCount != 0) {
      console.log("This alarm event has been attempted ${alarmInfo?.retryCount} times before.");
    }
  }
}
```

## Related resources

- Understand how to [use the Alarms API](/durable-objects/examples/alarms-api/) in an end-to-end example.
- Read the [Durable Objects alarms announcement blog post](https://blog.cloudflare.com/durable-objects-alarms/).
- Review the [Storage API](/durable-objects/api/storage-api/) documentation for Durable Objects.

---

# Durable Object Base Class

URL: https://developers.cloudflare.com/durable-objects/api/base/

import { Render, Tabs, TabItem, GlossaryTooltip, Type, MetaInfo, TypeScriptExample } from "~/components";

The `DurableObject` base class is an abstract class which all Durable Objects inherit from. This base class provides a set of optional methods, frequently referred to as handler methods, which can respond to events, for example a webSocketMessage when using the [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api). To provide a concrete example, here is a Durable Object `MyDurableObject` which extends `DurableObject` and implements the fetch handler to return "Hello, World!" to the calling Worker.

<TypeScriptExample>
```ts
export class MyDurableObject extends DurableObject {
	constructor(ctx: DurableObjectState, env: Env) {
		super(ctx, env);
	}

	async fetch(request: Request) {
		return new Response("Hello, World!");
	}
}
```
</TypeScriptExample>

## Methods

### `fetch`

- <code>fetch(<Type text="Request"/>)</code>: <Type text="Response"/> | <Type text="Promise <Response>"/>

  - Takes an HTTP request object and returns an HTTP response object. This method allows the Durable Object to emulate an HTTP server where a Worker with a binding to that object is the client.

  - This method can be `async`.

### `alarm`

- <code>alarm(`alarmInfo`<Type text="Object"/>)</code>: <Type text="Promise <void>"/>

  - Called by the system when a scheduled alarm time is reached.

  - The optional parameter `alarmInfo` object has two properties:
    - `retryCount` <Type text="number"/>: The number of times this alarm event has been retried.
    - `isRetry` <Type text="boolean"/>: A boolean value to indicate if the alarm has been retried. This value is `true` if this alarm event is a retry.

  - The `alarm()` handler has guaranteed at-least-once execution and will be retried upon failure using exponential backoff, starting at two second delays for up to six retries. Retries will be performed if the method fails with an uncaught exception.

  - This method can be `async`.

  - Refer to [`alarm`](/durable-objects/api/alarms/#alarm) for more information.

### `webSocketMessage`

- <code> webSocketMessage(ws <Type text="WebSocket" />, message{" "} <Type text="string | ArrayBuffer" />)</code>: <Type text="void" />

  - Called by the system when an accepted WebSocket receives a message.

  - This method can be `async`.

  - This method is not called for WebSocket control frames. The system will respond to an incoming [WebSocket protocol ping](https://www.rfc-editor.org/rfc/rfc6455#section-5.5.2) automatically without interrupting hibernation.

### `webSocketClose`

- <code> webSocketClose(ws <Type text="WebSocket" />, code <Type text="number" />, reason <Type text="string" />, wasClean <Type text="boolean" />)</code>: <Type text="void" />

  - Called by the system when a WebSocket is closed. `wasClean()` is true if the connection closed cleanly, false otherwise.

  - This method can be `async`.

### `webSocketError`

- <code> webSocketError(ws <Type text="WebSocket" />, error <Type text="any" />)</code> : <Type text="void" />

  - Called by the system when any non-disconnection related errors occur.

  - This method can be `async`.

## Properties

### `DurableObjectState`

See [`DurableObjectState` documentation](/durable-objects/api/state/).

### `Env`

A list of bindings which are available to the Durable Object.

## Related resources

- Refer to [Use WebSockets](/durable-objects/best-practices/websockets/) for more information on examples of WebSocket methods and best practices.

---

# Durable Object ID

URL: https://developers.cloudflare.com/durable-objects/api/id/

import { Render, Tabs, TabItem, GlossaryTooltip } from "~/components";

## Description

A Durable Object ID is a 64-digit hexadecimal number used to identify a <GlossaryTooltip term="Durable Object">Durable Object</GlossaryTooltip>. Not all 64-digit hex numbers are valid IDs. Durable Object IDs are constructed indirectly via the [`DurableObjectNamespace`](/durable-objects/api/namespace) interface.

The `DurableObjectId` interface refers to a new or existing Durable Object. This interface is most frequently used by [`DurableObjectNamespace::get`](/durable-objects/api/namespace/#get) to obtain a [`DurableObjectStub`](/durable-objects/api/stub) for submitting requests to a Durable Object. Note that creating an ID for a Durable Object does not create the Durable Object. The Durable Object is created lazily after creating a stub from a `DurableObjectId`. This ensures that objects are not constructed until they are actually accessed.

:::note[Logging]

If you are experiencing an issue with a particular Durable Object, you may wish to log the `DurableObjectId` from your Worker and include it in your Cloudflare support request.

:::

## Methods

### `toString`

`toString` converts a `DurableObjectId` to a 64 digit hex string. This string is useful for logging purposes or storing the `DurableObjectId` elsewhere, for example, in a session cookie. This string can be used to reconstruct a `DurableObjectId` via `DurableObjectNamespace::idFromString`.

<Render file="example-id-from-string" />

#### Parameters

- None.

#### Return values

- A 64 digit hex string.

### `equals`

`equals` is used to compare equality between two instances of `DurableObjectId`.

```js
const id1 = env.MY_DURABLE_OBJECT.newUniqueId();
const id2 = env.MY_DURABLE_OBJECT.newUniqueId();
console.assert(!id1.equals(id2), "Different unique ids should never be equal.");
```

#### Parameters

- A required `DurableObjectId` to compare against.

#### Return values

- A boolean. True if equal and false otherwise.

## Properties

### `name`

`name` is an optional property of a `DurableObjectId`, which returns the name that was used to create the `DurableObjectId` via [`DurableObjectNamespace::idFromName`](/durable-objects/api/namespace/#idfromname). This value is undefined if the `DurableObjectId` was constructed using [`DurableObjectNamespace::newUniqueId`](/durable-objects/api/namespace/#newuniqueid).

```js
const uniqueId = env.MY_DURABLE_OBJECT.newUniqueId();
const fromNameId = env.MY_DURABLE_OBJECT.idFromName("foo");
console.assert(uniqueId.name === undefined, "unique ids have no name");
console.assert(
	fromNameId.name === "foo",
	"name matches parameter to idFromName",
);
```

## Related resources

- [Durable Objects: Easy, Fast, Correct â€“ Choose Three](https://blog.cloudflare.com/durable-objects-easy-fast-correct-choose-three/).

---

# Workers Binding API

URL: https://developers.cloudflare.com/durable-objects/api/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# SQL Storage

URL: https://developers.cloudflare.com/durable-objects/api/sql-storage/

import { Render, Type, MetaInfo, GlossaryTooltip } from "~/components";

The `SqlStorage` interface encapsulates methods that modify the SQLite database embedded within a Durable Object. The `SqlStorage` interface is accessible via the `sql` property of `DurableObjectStorage` class.

For example, using `sql.exec()`, a user can create a table, then insert rows into the table.

```ts
import { DurableObject } from "cloudflare:workers";

export class MyDurableObject extends DurableObject {
  sql: SqlStorage;
  constructor(ctx: DurableObjectState, env: Env) {
    super(ctx, env);
    this.sql = ctx.storage.sql;

    this.sql.exec(`CREATE TABLE IF NOT EXISTS artist(
      artistid    INTEGER PRIMARY KEY,
      artistname  TEXT
    );INSERT INTO artist (artistid, artistname) VALUES
      (123, 'Alice'),
      (456, 'Bob'),
      (789, 'Charlie');`
    );
  }
}
```

:::note[SQLite in Durable Objects Beta]
SQL API methods accessed with `ctx.storage.sql` are only allowed on [Durable Object classes with SQLite storage backend](/durable-objects/reference/durable-objects-migrations/#enable-sqlite-storage-backend-on-new-durable-object-class-migration) and will return an error if called on Durable Object classes with a key-value storage backend.
:::

:::note[Writing to indexes or virtual tables]
When writing data, every index counts as an additional row. However, indexes may be beneficial for read-heavy use cases. Refer to [Index for SQLite Durable Objects](/durable-objects/best-practices/access-durable-objects-storage/#index-for-sqlite-durable-objects).

Writing data to [SQLite virtual tables](https://www.sqlite.org/vtab.html) also counts towards rows written.
:::

Specifically for Durable Object classes with SQLite storage backend, KV operations which were previously asynchronous (for example, [`get`](/durable-objects/api/storage-api/#get), [`put`](/durable-objects/api/storage-api/#put), [`delete`](/durable-objects/api/storage-api/#delete), [`deleteAll`](/durable-objects/api/storage-api/#deleteall), [`list`](/durable-objects/api/storage-api/#list)) are synchronous, even though they return promises. These methods will have completed their operations before they return the promise.

## Methods

### `exec`

<code>exec(query: <Type text='string'/>, ...bindings: <Type text='any[]'/>)</code>: <Type text='SqlStorageCursor' />

#### Parameters

* `query`: <Type text ='string' />
  * The SQL query string to be executed. `query` can contain `?` placeholders for parameter bindings. Multiple SQL statements, separated with a semicolon, can be executed in the `query`. With multiple SQL statements, any parameter bindings are applied to the last SQL statement in the `query`, and the returned cursor is only for the last SQL statement.
* `...bindings`: <Type text='any[]' /> <MetaInfo text='Optional' />
  * Optional variable number of arguments that correspond to the `?` placeholders in `query`.

#### Returns

A cursor (`SqlStorageCursor`) to iterate over query row results as objects. `SqlStorageCursor` is a JavaScript [Iterable](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#the_iterable_protocol), which supports iteration using `for (let row of cursor)`. `SqlStorageCursor` is also a JavaScript [Iterator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#the_iterator_protocol), which supports iteration using `cursor.next()`.

`SqlStorageCursor` supports the following methods:

* `next()`
  * Returns an object representing the next value of the cursor. The returned object has `done` and `value` properties adhering to the JavaScript [Iterator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#the_iterator_protocol). `done` is set to `false` when a next value is present, and `value` is set to the next row object in the query result. `done` is set to `true` when the entire cursor is consumed, and no `value` is set.
* `toArray()`
  * Iterates through remaining cursor value(s) and returns an array of returned row objects.
* `one()`
  * Returns a row object if query result has exactly one row. If query result has zero rows or more than one row, `one()` throws an exception.
* `raw()`: <Type text='Iterator' />
  * Returns an Iterator over the same query results, with each row as an array of column values (with no column names) rather than an object.
  * Returned Iterator supports `next()`, `toArray()`, and `one()` methods above.
  * Returned cursor and `raw()` iterator iterate over the same query results and can be combined. For example:

```ts
let cursor = this.sql.exec("SELECT * FROM artist ORDER BY artistname ASC;");
let rawResult = cursor.raw().next();

if (!rawResult.done) {
  console.log(rawResult.value); // prints [ 123, 'Alice' ]
} else {
  // query returned zero results
}

console.log(cursor.toArray()); // prints [{ artistid: 456, artistname: 'Bob' },{ artistid: 789, artistname: 'Charlie' }]
```

`SqlStorageCursor` had the following properties:

* `columnNames`: <Type text='string[]' />
  * The column names of the query in the order they appear in each row array returned by the `raw` iterator.
*  `rowsRead`: <Type text='number' />
  * The number of rows read so far as part of this SQL `query`. This may increase as you iterate the cursor. The final value is used for [SQL billing](/durable-objects/platform/pricing/#sqlite-storage-backend).
*  `rowsWritten`: <Type text='number' />
  * The number of rows written so far as part of this SQL `query`. This may increase as you iterate the cursor. The final value is used for [SQL billing](/durable-objects/platform/pricing/#sqlite-storage-backend).

Note that `sql.exec()` cannot execute transaction-related statements like `BEGIN TRANSACTION` or `SAVEPOINT`. Instead, use the [`ctx.storage.transaction()`](/durable-objects/api/storage-api/#transaction) or [`ctx.storage.transactionSync()`](/durable-objects/api/storage-api/#transactionsync) APIs to start a transaction, and then execute SQL queries in your callback.

#### Examples

<Render file="durable-objects-sql" />

### `databaseSize`

`databaseSize`: <Type text ='number' />

#### Returns

The current SQLite database size in bytes.

```ts
let size = ctx.storage.sql.databaseSize;
```

## Point in time recovery

For [Durable Objects classes with SQL storage](/durable-objects/reference/durable-objects-migrations/#enable-sqlite-storage-backend-on-new-durable-object-class-migration), the following point-in-time-recovery (PITR) API methods are available to restore a Durable Object's embedded SQLite database to any point in time in the past 30 days. These methods apply to the entire SQLite database contents, including both the object's stored SQL data and stored key-value data using the key-value `put()` API. The PITR API is not supported in local development because a durable log of data changes is not stored locally.

The PITR API represents points in times using 'bookmarks'. A bookmark is a mostly alphanumeric string like `0000007b-0000b26e-00001538-0c3e87bb37b3db5cc52eedb93cd3b96b`. Bookmarks are designed to be lexically comparable: a bookmark representing an earlier point in time compares less than one representing a later point, using regular string comparison.

### `getCurrentBookmark`

<code>ctx.storage.getCurrentBookmark()</code>: <Type text='Promise<string>' />

* Returns a bookmark representing the current point in time in the object's history.

### `getBookmarkForTime`

<code>ctx.storage.getBookmarkForTime(timestamp: <Type text='number | Date'/>)</code>: <Type text='Promise<string>' />

* Returns a bookmark representing approximately the given point in time, which must be within the last 30 days. If the timestamp is represented as a number, it is converted to a date as if using `new Date(timestamp)`.

### `onNextSessionRestoreBookmark`

<code>ctx.storage.onNextSessionRestoreBookmark(bookmark: <Type text='string'/>)</code>: <Type text='Promise<string>' />

  * Configures the Durable Object so that the next time it restarts, it should restore its storage to exactly match what the storage contained at the given bookmark. After calling this, the application should typically invoke `ctx.abort()` to restart the Durable Object, thus completing the point-in-time recovery.

This method returns a special bookmark representing the point in time immediately before the recovery takes place (even though that point in time is still technically in the future). Thus, after the recovery completes, it can be undone by performing a second recovery to this bookmark.


```ts
let now = new Date();
// restore to 2 days ago
let bookmark = ctx.storage.getBookmarkForTime(now - 2);
ctx.storage.onNextSessionRestoreBookmark(bookmark);
```

## TypeScript and query results

You can use TypeScript [type parameters](https://www.typescriptlang.org/docs/handbook/2/generics.html#working-with-generic-type-variables) to provide a type for your results, allowing you to benefit from type hints and checks when iterating over the results of a query.

:::caution

Providing a type parameter does _not_ validate that the query result matches your type definition. In TypeScript, properties (fields) that do not exist in your result type will be silently dropped.

:::

Your type must conform to the shape of a TypeScript [Record](https://www.typescriptlang.org/docs/handbook/utility-types.html#recordkeys-type) type representing the name (`string`) of the column and the type of the column. The column type must be a valid `SqlStorageValue`: one of `ArrayBuffer | string | number | null`.

For example, 

```ts
type User = {
	id: string;
	name: string;
	email_address: string;
	version: number;
}
```

This type can then be passed as the type parameter to a `sql.exec` call:

```ts
// The type parameter is passed between the "pointy brackets" before the function argument:
const result = this.ctx.storage.sql.exec<User>("SELECT id, name, email_address, version FROM users WHERE id = ?", user_id).one()
// result will now have a type of "User"

// Alternatively, if you are iterating over results using a cursor
let cursor = this.sql.exec<User>("SELECT id, name, email_address, version FROM users WHERE id = ?", user_id)
for (let row of cursor) {
	// Each row object will be of type User
}

// Or, if you are using raw() to convert results into an array, define an array type:
type UserRow = [
  id: string,
  name: string,
  email_address: string,
  version: number,
];

// ... and then pass it as the type argument to the raw() method:
let cursor = sql.exec("SELECT id, name, email_address, version FROM users WHERE id = ?", user_id).raw<UserRow>();

for (let row of cursor) {
	// row is of type User
}
```

You can represent the shape of any result type you wish, including more complex types. If you are performing a JOIN across multiple tables, you can compose a type that reflects the results of your queries.

---

# Durable Object Namespace

URL: https://developers.cloudflare.com/durable-objects/api/namespace/

import { Render, Tabs, TabItem, GlossaryTooltip } from "~/components";

## Description

A Durable Object namespace is a set of Durable Objects that are backed by the same <GlossaryTooltip term="Durable Object class">Durable Object class</GlossaryTooltip>. There is only one Durable Object namespace per class. A Durable Object namespace can contain any number of Durable Objects.

The `DurableObjectNamespace` interface is used to obtain a reference to new or existing Durable Objects. The interface is accessible from the fetch handler on a Cloudflare Worker via the `env` parameter, which is the standard interface when referencing bindings declared in the [Wrangler configuration file](/workers/wrangler/configuration/).

This interface defines several [methods](/durable-objects/api/namespace/#methods) that can be used to create an ID for a Durable Object. Note that creating an ID for a Durable Object does not create the Durable Object. The Durable Object is created lazily after calling [`DurableObjectNamespace::get`](/durable-objects/api/namespace/#get) to create a [`DurableObjectStub`](/durable-objects/api/stub) from a `DurableObjectId`. This ensures that objects are not constructed until they are actually accessed.

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Durable Object
export class MyDurableObject extends DurableObject {
  ...
}

// Worker
export default {
  async fetch(request, env) {
    // Every unique ID refers to an individual instance of the Durable Object class
    const id = env.MY_DURABLE_OBJECT.idFromName("foo");

    // A stub is a client Object used to invoke methods defined by the Durable Object
    const stub = env.MY_DURABLE_OBJECT.get(id);
    ...
  }
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
  MY_DURABLE_OBJECT: DurableObjectNamespace<MyDurableObject>;
}

// Durable Object
export class MyDurableObject extends DurableObject {
  ...
}

// Worker
export default {
  async fetch(request, env) {
    // Every unique ID refers to an individual instance of the Durable Object class
    const id = env.MY_DURABLE_OBJECT.idFromName("foo");

    // A stub is a client Object used to invoke methods defined by the Durable Object
    const stub = env.MY_DURABLE_OBJECT.get(id);
    ...
  }
} satisfies ExportedHandler<Env>;
```

</TabItem> </Tabs>

## Methods

### `idFromName`

`idFromName` creates a unique [`DurableObjectId`](/durable-objects/api/id) which refers to an individual instance of the Durable Object class. Named Durable Objects are the most common method of referring to Durable Objects.

```js
const fooId = env.MY_DURABLE_OBJECT.idFromName("foo");
const barId = env.MY_DURABLE_OBJECT.idFromName("bar");
```

#### Parameters

- A required string to be used to generate a [`DurableObjectId`](/durable-objects/api/id) corresponding to the name of a Durable Object.

#### Return values

- A [`DurableObjectId`](/durable-objects/api/id) referring to an instance of a Durable Object class.

### `newUniqueId`

`newUniqueId` creates a randomly generated and unique [`DurableObjectId`](/durable-objects/api/id) which refers to an individual instance of the Durable Object class. IDs created using `newUniqueId`, will need to be stored as a string in order to refer to the same Durable Object again in the future. For example, the ID can be stored in Workers KV, another Durable Object, or in a cookie in the user's browser.

```js
const id = env.MY_DURABLE_OBJECT.newUniqueId();
const euId = env.MY_DURABLE_OBJECT.newUniqueId({ jurisdiction: "eu" });
```

:::note[`newUniqueId` results in lower request latency at first use]

The first time you get a Durable Object stub based on an ID derived from a name, the system has to take into account the possibility that a Worker on the opposite side of the world could have coincidentally accessed the same named Durable Object at the same time. To guarantee that only one instance of the Durable Object is created, the system must check that the Durable Object has not been created anywhere else. Due to the inherent limit of the speed of light, this round-the-world check can take up to a few hundred milliseconds. `newUniqueId` can skip this check.

After this first use, the location of the Durable Object will be cached around the world so that subsequent lookups are faster.

:::

#### Parameters

- An optional object with the key `jurisdiction` and value of a [jurisdiction](/durable-objects/reference/data-location/#restrict-durable-objects-to-a-jurisdiction) string.

#### Return values

- A [`DurableObjectId`](/durable-objects/api/id) referring to an instance of the Durable Object class.

### `idFromString`

`idFromString` creates a [`DurableObjectId`](/durable-objects/api/id) from a previously generated ID that has been converted to a string. This method throws an exception if the ID is invalid, for example, if the ID was not created from the same `DurableObjectNamespace`.

<Render file="example-id-from-string" />

#### Parameters

- A required string corresponding to a [`DurableObjectId`](/durable-objects/api/id) previously generated either by `newUniqueId` or `idFromName`.

#### Return values

- A [`DurableObjectId`](/durable-objects/api/id) referring to an instance of a Durable Object class.

### `get`

`get` obtains a [`DurableObjectStub`](/durable-objects/api/stub) from a [`DurableObjectId`](/durable-objects/api/id) which can be used to invoke methods on a Durable Object.

This method returns the <GlossaryTooltip term="stub">stub</GlossaryTooltip> immediately, often before a connection has been established to the Durable Object. This allows requests to be sent to the instance right away, without waiting for a network round trip.

```js
const id = env.MY_DURABLE_OBJECT.newUniqueId();
const stub = env.MY_DURABLE_OBJECT.get(id);
```

#### Parameters

- A required [`DurableObjectId`](/durable-objects/api/id)
- An optional object with the key `locationHint` and value of a [locationHint](/durable-objects/reference/data-location/#provide-a-location-hint) string.

#### Return values

- A [`DurableObjectStub`](/durable-objects/api/stub) referring to an instance of a Durable Object class.

### `jurisdiction`

`jurisdiction` creates a subnamespace from a namespace where all Durable Object IDs and references created from that subnamespace will be restricted to the specified [jurisdiction](/durable-objects/reference/data-location/#restrict-durable-objects-to-a-jurisdiction).

```js
const subnamespace = env.MY_DURABLE_OBJECT.jurisdiction("foo");
const euId = subnamespace.idFromName("foo");
```

#### Parameters

- A required [jurisdiction](/durable-objects/reference/data-location/#restrict-durable-objects-to-a-jurisdiction) string.

#### Return values

- A `DurableObjectNamespace` scoped to a particular geographic jurisdiction.

## Related resources

- [Durable Objects: Easy, Fast, Correct â€“ Choose Three](https://blog.cloudflare.com/durable-objects-easy-fast-correct-choose-three/).

---

# Durable Object Storage

URL: https://developers.cloudflare.com/durable-objects/api/storage-api/

import {
	Render,
	Type,
	MetaInfo,
	GlossaryTooltip,
	TypeScriptExample,
} from "~/components";

The Durable Object Storage API allows <GlossaryTooltip term="Durable Object">Durable Objects</GlossaryTooltip> to access transactional and strongly consistent storage. A Durable Object's attached storage is private to its unique instance and cannot be accessed by other objects.

:::note[Scope of Durable Object storage]
Note that Durable Object storage is scoped by individual <GlossaryTooltip term="Durable Object">Durable Objects</GlossaryTooltip>.

- An account can have many Durable Object <GlossaryTooltip term="namespace">namespaces</GlossaryTooltip>.
- A namespace can have many Durable Objects.

However, storage is scoped per individual Durable Object.
:::

Durable Objects gain access to a persistent Durable Object Storage API via the `DurableObjectStorage` interface and accessed by the `DurableObjectState::storage` property. This is frequently accessed via `this.ctx.storage` when the `ctx` parameter passed to the Durable Object constructor.

JavaScript is a single-threaded and event-driven programming language. This means that JavaScript runtimes, by default, allow requests to interleave with each other which can lead to concurrency bugs. The Durable Objects runtime uses a combination of <GlossaryTooltip term="input gate">input gates</GlossaryTooltip> and <GlossaryTooltip term="output gate">output gates</GlossaryTooltip> to avoid this type of concurrency bug when performing storage operations.

The following code snippet shows you how to store and retrieve data using the Durable Object Storage API.

<TypeScriptExample>
```ts
export class Counter extends DurableObject {
	constructor(ctx: DurableObjectState, env: Env) {
		super(ctx, env);
	}

    async increment(): Promise<number> {
    	let value: number = (await this.ctx.storage.get('value')) || 0;
    	value += 1;
    	await this.ctx.storage.put('value', value);
    	return value;
    }

}

```
</TypeScriptExample>

## Methods

:::note[SQLite in Durable Objects Beta]

The new beta version of Durable Objects is available where each Durable Object has a private, embedded SQLite database. When deploying a new Durable Object class, users can [opt-in to a SQLite storage backend](/durable-objects/best-practices/access-durable-objects-storage/#sqlite-storage-backend) to access the new [SQL API](/durable-objects/api/sql-storage/#exec). Otherwise, a Durable Object class has a key-value storage backend.

:::

The Durable Object Storage API comes with several methods, including key-value (KV) API, SQL API, and point-in-time-recovery (PITR) API.

- Durable Object classes with the default, key-value storage backend can use KV API.
- Durable Object classes with the [SQLite storage backend](/durable-objects/best-practices/access-durable-objects-storage/#sqlite-storage-backend) can use KV API, SQL API, and PITR API. KV API methods like `get()`, `put()`, `delete()`, or `list()` store data in a hidden SQLite table.

Each method is implicitly wrapped inside a transaction, such that its results are atomic and isolated from all other storage operations, even when accessing multiple key-value pairs.

### `get`

- <code>get(key <Type text="string" />, options <Type text="Object" />{" "}<MetaInfo text="optional" />)</code>: <Type text="Promise<any>" />

  - Retrieves the value associated with the given key. The type of the returned value will be whatever was previously written for the key, or undefined if the key does not exist.

- <code>get(keys <Type text="Array<string>" />, options <Type text="Object" />{" "}<MetaInfo text="optional" />)</code>: <Type text="Promise<Map<string, any>>" />

  - Retrieves the values associated with each of the provided keys. The type of each returned value in the [`Map`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map) will be whatever was previously written for the corresponding key. Results in the `Map` will be sorted in increasing order of their UTF-8 encodings, with any requested keys that do not exist being omitted. Supports up to 128 keys at a time.

#### Supported options

- `allowConcurrency`: <Type text='boolean' />

  - By default, the system will pause delivery of I/O events to the Object while a storage operation is in progress, in order to avoid unexpected race conditions. Pass `allowConcurrency: true` to opt out of this behavior and allow concurrent events to be delivered.

- `noCache`: <Type text='boolean'/>

  - If true, then the key/value will not be inserted into the in-memory cache. If the key is already in the cache, the cached value will be returned, but its last-used time will not be updated. Use this when you expect this key will not be used again in the near future. This flag is only a hint. This flag will never change the semantics of your code, but it may affect performance.

### `put`

- <code>put(key <Type text="string" />, value <Type text="any" />, options{" "}<Type text="Object" /> <MetaInfo text="optional" />)</code>: <Type text="Promise" />

  - Stores the value and associates it with the given key. The value can be any type supported by the [structured clone algorithm](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm), which is true of most types. Keys are limited to a max size of 2,048 bytes and values are limited to 128 KiB (131,072 bytes).<br/><br/>

- <code>put(entries <Type text="Object" />, options <Type text="Object" />{" "}<MetaInfo text="optional" />)</code>: <Type text="Promise" />

  - Takes an Object and stores each of its keys and values to storage.
  - Each value can be any type supported by the [structured clone algorithm](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm), which is true of most types.
  - Supports up to 128 key-value pairs at a time. Each key is limited to a maximum size of 2,048 bytes and each value is limited to 128 KiB (131,072 bytes).

### `delete`

- <code>delete(key <Type text="string" />, options <Type text="Object" />{" "}<MetaInfo text="optional" />)</code>: <Type text="Promise<boolean>" />

  - Deletes the key and associated value. Returns `true` if the key existed or `false` if it did not.

- <code>delete(keys <Type text="Array<string>" />, options <Type text="Object" />{" "}<MetaInfo text="optional" />)</code>: <Type text="Promise<number>" />

  - Deletes the provided keys and their associated values. Supports up to 128 keys at a time. Returns a count of the number of key-value pairs deleted.

### `deleteAll`

- <code>deleteAll(options <Type text="Object" /> <MetaInfo text="optional" />)</code>: <Type text="Promise" />

  - Deletes all stored data, effectively deallocating all storage used by the Durable Object. For Durable Objects with a key-value storage backend, `deleteAll()` removes all keys and associated values for an individual Durable Object. For Durable Objects with a [SQLite storage backend](/durable-objects/best-practices/access-durable-objects-storage/#sqlite-storage-backend), `deleteAll()` removes the entire contents of a Durable Object's private SQLite database, including both SQL data and key-value data.
  - For Durable Objects with a key-value storage backend, an in-progress `deleteAll()` operation can fail, which may leave a subset of data undeleted. Durable Objects with a SQLite storage backend do not have a partial `deleteAll()` issue because `deleteAll()` operations are atomic (all or nothing).
  - `deleteAll()` does not proactively delete [Alarms](/durable-objects/api/alarms/). Use [`deleteAlarm()`](/durable-objects/api/alarms/#deletealarm) to delete an alarm.

#### Supported options

- `put()`, `delete()` and `deleteAll()` support the following options:

- `allowUnconfirmed` <Type text ='boolean' />

  - By default, the system will pause outgoing network messages from the Durable Object until all previous writes have been confirmed flushed to disk. If the write fails, the system will reset the Object, discard all outgoing messages, and respond to any clients with errors instead.

  - This way, Durable Objects can continue executing in parallel with a write operation, without having to worry about prematurely confirming writes, because it is impossible for any external party to observe the Object's actions unless the write actually succeeds.

  - After any write, subsequent network messages may be slightly delayed. Some applications may consider it acceptable to communicate on the basis of unconfirmed writes. Some programs may prefer to allow network traffic immediately. In this case, set `allowUnconfirmed` to `true` to opt out of the default behavior.

  - If you want to allow some outgoing network messages to proceed immediately but not others, you can use the allowUnconfirmed option to avoid blocking the messages that you want to proceed and then separately call the [`sync()`](/durable-objects/api/storage-api/#sync) method, which returns a promise that only resolves once all previous writes have successfully been persisted to disk.

- `noCache` <Type text ='boolean' />

  - If true, then the key/value will be discarded from memory as soon as it has completed writing to disk.

  - Use `noCache` if the key will not be used again in the near future. `noCache` will never change the semantics of your code, but it may affect performance.

  - If you use `get()` to retrieve the key before the write has completed, the copy from the write buffer will be returned, thus ensuring consistency with the latest call to `put()`.

:::note[Automatic write coalescing]

If you invoke `put()` (or `delete()`) multiple times without performing any `await` in the meantime, the operations will automatically be combined and submitted atomically. In case of a machine failure, either all of the writes will have been stored to disk or none of the writes will have been stored to disk.
:::

:::note[Write buffer behavior]

The `put()` method returns a `Promise`, but most applications can discard this promise without using `await`. The `Promise` usually completes immediately, because `put()` writes to an in-memory write buffer that is flushed to disk asynchronously. However, if an application performs a large number of `put()` without waiting for any I/O, the write buffer could theoretically grow large enough to cause the isolate to exceed its 128 MB memory limit. To avoid this scenario, such applications should use `await` on the `Promise` returned by `put()`. The system will then apply backpressure onto the application, slowing it down so that the write buffer has time to flush. Using `await` will disable automatic write coalescing.
:::

### `list`

- <code>list(options <Type text="Object" /> <MetaInfo text="optional" />)</code>: <Type text="Promise<Map<string, any>>" />

  - Returns all keys and values associated with the current Durable Object in ascending sorted order based on the keys' UTF-8 encodings.

  - The type of each returned value in the [`Map`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map) will be whatever was previously written for the corresponding key.

  - Be aware of how much data may be stored in your Durable Object before calling this version of `list` without options because all the data will be loaded into the Durable Object's memory, potentially hitting its [limit](/durable-objects/platform/limits/). If that is a concern, pass options to `list` as documented below.

#### Supported options

- `start` <Type text ='string' />

  - Key at which the list results should start, inclusive.

- `startAfter` <Type text ='string' />

  - Key after which the list results should start, exclusive. Cannot be used simultaneously with `start`.

- `end` <Type text ='string' />

  - Key at which the list results should end, exclusive.

- `prefix` <Type text ='string' />

  - Restricts results to only include key-value pairs whose keys begin with the prefix.

- `reverse` <Type text='boolean' />

  - If true, return results in descending order instead of the default ascending order.
  - Enabling `reverse` does not change the meaning of `start`, `startKey`, or `endKey`. `start` still defines the smallest key in lexicographic order that can be returned (inclusive), effectively serving as the endpoint for a reverse-order list. `end` still defines the largest key in lexicographic order that the list should consider (exclusive), effectively serving as the starting point for a reverse-order list.

- `limit` <Type text ='number' />

  - Maximum number of key-value pairs to return.

- `allowConcurrency` <Type text ='boolean' />

  - Same as the option to `get()`, above.

- `noCache` <Type text ='boolean' />

  - Same as the option to `get()`, above.

### `transaction`

- `transaction(closureFunction(txn))`: <Type text='Promise' />

  - Runs the sequence of storage operations called on `txn` in a single transaction that either commits successfully or aborts.

  - Explicit transactions are no longer necessary. Any series of write operations with no intervening `await` will automatically be submitted atomically, and the system will prevent concurrent events from executing while `await` a read operation (unless you use `allowConcurrency: true`). Therefore, a series of reads followed by a series of writes (with no other intervening I/O) are automatically atomic and behave like a transaction.

- `txn`

  - Provides access to the `put()`, `get()`, `delete()` and `list()` methods documented above to run in the current transaction context. In order to get transactional behavior within a transaction closure, you must call the methods on the `txn` Object instead of on the top-level `ctx.storage` Object.<br/><br/>Also supports a `rollback()` function that ensures any changes made during the transaction will be rolled back rather than committed. After `rollback()` is called, any subsequent operations on the `txn` Object will fail with an exception. `rollback()` takes no parameters and returns nothing to the caller.

  * When using [the SQLite-backed storage engine](/durable-objects/best-practices/access-durable-objects-storage/#sqlite-storage-backend), the `txn` object is obsolete. Any storage operations performed directly on the `ctx.storage` object, including SQL queries using [`ctx.storage.sql.exec()`](/durable-objects/api/sql-storage/#exec), will be considered part of the transaction.

### `transactionSync`

- `transactionSync(callback)`: <Type text='any' />

  - Only available when using [the SQLite-backed storage engine](/durable-objects/best-practices/access-durable-objects-storage/#sqlite-storage-backend).

  - Invokes `callback()` wrapped in a transaction, and returns its result.

  - If `callback()` throws an exception, the transaction will be rolled back.

  * The callback must complete synchronously, that is, it should not be declared `async` nor otherwise return a Promise. Only synchronous storage operations can be part of the transaction. This is intended for use with SQL queries using [`ctx.storage.sql.exec()`](/durable-objects/api/sql-storage/#exec), which complete sychronously.

### `sync`

- `sync()`: <Type text='Promise' />

  - Synchronizes any pending writes to disk.

  - This is similar to normal behavior from automatic write coalescing. If there are any pending writes in the write buffer (including those submitted with [the `allowUnconfirmed` option](/durable-objects/api/storage-api/#supported-options-1)), the returned promise will resolve when they complete. If there are no pending writes, the returned promise will be already resolved.

### `getAlarm`

- <code>getAlarm(options <Type text="Object" /> <MetaInfo text="optional" />)</code>: <Type text="Promise<Number | null>" />

  - Retrieves the current alarm time (if set) as integer milliseconds since epoch. The alarm is considered to be set if it has not started, or if it has failed and any retry has not begun. If no alarm is set, `getAlarm()` returns `null`.

#### Supported options

- Same options as [`get()`](/durable-objects/api/storage-api/#get), but without `noCache`.

### `setAlarm`

- <code>setAlarm(scheduledTime <Type text="Date | number" />, options{" "}<Type text="Object" /> <MetaInfo text="optional" />)</code>: <Type text="Promise" />

  - Sets the current alarm time, accepting either a JavaScript `Date`, or integer milliseconds since epoch.

    If `setAlarm()` is called with a time equal to or before `Date.now()`, the alarm will be scheduled for asynchronous execution in the immediate future. If the alarm handler is currently executing in this case, it will not be canceled. Alarms can be set to millisecond granularity and will usually execute within a few milliseconds after the set time, but can be delayed by up to a minute due to maintenance or failures while failover takes place.

### `deleteAlarm`

- <code>deleteAlarm(options <Type text="Object" /> <MetaInfo text="optional" />)</code>: <Type text="Promise" />

  - Deletes the alarm if one exists. Does not cancel the alarm handler if it is currently executing.

#### Supported options

- `setAlarm()` and `deleteAlarm()` support the same options as [`put()`](/durable-objects/api/storage-api/#put), but without `noCache`.

## Properties

### `sql`

`sql` is a readonly property of type `DurableObjectStorage` encapsulating the [SQL API](/durable-objects/api/sql-storage/).

## Related resources

- [Durable Objects: Easy, Fast, Correct â€“ Choose Three](https://blog.cloudflare.com/durable-objects-easy-fast-correct-choose-three/)
- [WebSockets API](/durable-objects/best-practices/websockets/)
```

---

# Durable Object State

URL: https://developers.cloudflare.com/durable-objects/api/state/

import { Tabs, TabItem, GlossaryTooltip, Type, MetaInfo } from "~/components";

## Description

The `DurableObjectState` interface is accessible as an instance property on the <GlossaryTooltip term="Durable Object class">Durable Object class</GlossaryTooltip>. This interface encapsulates methods that modify the state of a Durable Object, for example which WebSockets are attached to a Durable Object or how the runtime should handle concurrent Durable Object requests.

The `DurableObjectState` interface is different from the <GlossaryTooltip term="Storage API">Storage API</GlossaryTooltip> in that it does not have top-level methods which manipulate persistent application data. These methods are instead encapsulated in the [`DurableObjectStorage`](/durable-objects/api/storage-api) interface and accessed by [`DurableObjectState::storage`](/durable-objects/api/state/#storage).

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Durable Object
export class MyDurableObject extends DurableObject {
  // DurableObjectState is accessible via the ctx instance property
	constructor(ctx, env) {
		super(ctx, env);
	}
  ...
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
  MY_DURABLE_OBJECT: DurableObjectNamespace<MyDurableObject>;
}

// Durable Object
export class MyDurableObject extends DurableObject {
  // DurableObjectState is accessible via the ctx instance property
	constructor(ctx: DurableObjectState, env: Env) {
		super(ctx, env);
	}
  ...
}
```

</TabItem> </Tabs>

## Methods

### `waitUntil`

`waitUntil` waits until the promise which is passed as a parameter resolves and can extend a <GlossaryTooltip term= "request context">request context</GlossaryTooltip> up to 30 seconds after the last client disconnects.

:::note[`waitUntil` is not necessary]

The request context for a Durable Objects extends at least 60 seconds after the last client disconnects. So `waitUntil` is not necessary. It remains part of the `DurableObjectState` interface to remain compatible with [Workers Runtime APIs](/workers/runtime-apis/context/#waituntil).

:::

#### Parameters

- A required promise of any type.

#### Return values

- None.

### `blockConcurrencyWhile`

`blockConcurrencyWhile` executes an async callback while blocking any other events from being delivered to the Durable Object until the callback completes. This method guarantees ordering and prevents concurrent requests. All events that were not explicitly initiated as part of the callback itself will be blocked. Once the callback completes, all other events will be delivered.

`blockConcurrencyWhile` is commonly used within the constructor of the Durable Object class to enforce initialization to occur before any requests are delivered. Another use case is executing `async` operations based on the current state of the Durable Object and using `blockConcurrencyWhile` to prevent that state from changing while yielding the event loop.

If the callback throws an exception, the object will be terminated and reset. This ensures that the object cannot be left stuck in an uninitialized state if something fails unexpectedly. To avoid this behavior, enclose the body of your callback in a `try...catch` block to ensure it cannot throw an exception.

To help mitigate deadlocks there is a 30 second timeout applied when executing the callback. If this timeout is exceeded, the Durable Object will be reset. It is best practice to have the callback do as little work as possible to improve overall request throughput to the Durable Object.

```js
// Durable Object
export class MyDurableObject extends DurableObject {
	initialized = false;

	constructor(ctx, env) {
		super(ctx, env);

		// blockConcurrencyWhile will ensure that initialized will always be true
		this.ctx.blockConcurrencyWhile(async () => {
			this.initialized = true;
		});
	}
  ...
}
```

#### Parameters

- A required callback which returns a `Promise<T>`.

#### Return values

- A `Promise<T>` returned by the callback.

### `acceptWebSocket`

`acceptWebSocket` is part of the [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api), which allows a Durable Object to be removed from memory to save costs while keeping its WebSockets connected.

`acceptWebSocket` adds a WebSocket to the set of WebSockets attached to the Durable Object. Once called, any incoming messages will be delivered by calling the Durable Object's `webSocketMessage` handler, and `webSocketClose` will be invoked upon disconnect. After calling `acceptWebSocket`, the WebSocket is accepted and its `send` and `close` methods can be used.

The [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api) takes the place of the standard [WebSockets API](/workers/runtime-apis/websockets/). Therefore, `ws.accept` must not have been called separately and `ws.addEventListener` method will not receive events as they will instead be delivered to the Durable Object.

The WebSocket Hibernation API permits a maximum of 32,768 WebSocket connections per Durable Object, but the CPU and memory usage of a given workload may further limit the practical number of simultaneous connections.

#### Parameters

- A required `WebSocket` with name `ws`.
- An optional `Array<string>` of associated tags. Tags can be used to retrieve WebSockets via [`DurableObjectState::getWebSockets`](/durable-objects/api/state/#getwebsockets). Each tag is a maximum of 256 characters and there can be at most 10 tags associated with a WebSocket.

#### Return values

- None.

### `getWebSockets`

`getWebSockets` is part of the [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api), which allows a Durable Object to be removed from memory to save costs while keeping its WebSockets connected.

`getWebSockets` returns an `Array<WebSocket>` which is the set of WebSockets attached to the Durable Object. An optional tag argument can be used to filter the list according to tags supplied when calling [`DurableObjectState::acceptWebSocket`](/durable-objects/api/state/#acceptwebsocket).

:::note[`waitUntil` is not necessary]

Disconnected WebSockets are not returned by this method, but `getWebSockets` may still return WebSockets even after `ws.close` has been called. For example, if the server-side WebSocket sends a close, but does not receive one back (and has not detected a disconnect from the client), then the connection is in the CLOSING 'readyState'. The client might send more messages, so the WebSocket is technically not disconnected.

:::

#### Parameters

- An optional tag of type `string`.

#### Return values

- An `Array<WebSocket>`.

### `setWebSocketAutoResponse`

`setWebSocketAutoResponse` is part of the [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api), which allows a Durable Object to be removed from memory to save costs while keeping its WebSockets connected.

`setWebSocketAutoResponse` sets an automatic response, auto-response, for the request provided for all WebSockets attached to the Durable Object. If a request is received matching the provided request then the auto-response will be returned without waking WebSockets in hibernation and incurring billable duration charges.

`setWebSocketAutoResponse` is a common alternative to setting up a server for static ping/pong messages because this can be handled without waking hibernating WebSockets.

#### Parameters

- An optional `WebSocketRequestResponsePair(request string, response string)` enabling any WebSocket accepted via [`DurableObjectState::acceptWebSocket`](/durable-objects/api/state/#acceptwebsocket) to automatically reply to the provided response when it receives the provided request. Both request and response are limited to 2,048 characters each. If the parameter is omitted, any previously set auto-response configuration will be removed. [`DurableObjectState::getWebSocketAutoResponseTimestamp`](/durable-objects/api/state/#getwebsocketautoresponsetimestamp) will still reflect the last timestamp that an auto-response was sent.

#### Return values

- None.

### `getWebSocketAutoResponse`

`getWebSocketAutoResponse` returns the `WebSocketRequestResponsePair` object last set by [`DurableObjectState::setWebSocketAutoResponse`](/durable-objects/api/state/#setwebsocketautoresponse), or null if not auto-response has been set.

:::note[inspect `WebSocketRequestResponsePair`]

`WebSocketRequestResponsePair` can be inspected further by calling `getRequest` and `getResponse` methods.

:::

#### Parameters

- None.

#### Return values

- A `WebSocketRequestResponsePair` or null.

### `getWebSocketAutoResponseTimestamp`

`getWebSocketAutoResponseTimestamp` is part of the [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api), which allows a Durable Object to be removed from memory to save costs while keeping its WebSockets connected.

`getWebSocketAutoResponseTimestamp` gets the most recent `Date` on which the given WebSocket sent an auto-response, or null if the given WebSocket never sent an auto-response.

#### Parameters

- A required `WebSocket`.

#### Return values

- A `Date` or null.

### `setHibernatableWebSocketEventTimeout`

`setHibernatableWebSocketEventTimeout` is part of the [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api), which allows a Durable Object to be removed from memory to save costs while keeping its WebSockets connected.

`setHibernatableWebSocketEventTimeout` sets the maximum amount of time in milliseconds that a WebSocket event can run for.

If no parameter or a parameter of `0` is provided and a timeout has been previously set, then the timeout will be unset. The maximum value of timeout is 604,800,000 ms (7 days).

#### Parameters

- An optional `number`.

#### Return values

- None.

### `getHibernatableWebSocketEventTimeout`

`getHibernatableWebSocketEventTimeout` is part of the [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api), which allows a Durable Object to be removed from memory to save costs while keeping its WebSockets connected.

`getHibernatableWebSocketEventTimeout` gets the currently set hibernatable WebSocket event timeout if one has been set via [`DurableObjectState::setHibernatableWebSocketEventTimeout`](/durable-objects/api/state/#sethibernatablewebsocketeventtimeout).

#### Parameters

- None.

#### Return values

- A number, or null if the timeout has not been set.

### `getTags`

`getTags` is part of the [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api), which allows a Durable Object to be removed from memory to save costs while keeping its WebSockets connected.

`getTags` returns tags associated with a given WebSocket. This method throws an exception if the WebSocket has not been associated with the Durable Object via [`DurableObjectState::acceptWebSocket`](/durable-objects/api/state/#acceptwebsocket).

#### Parameters

- A required `WebSocket`.

#### Return values

- An `Array<string>` of tags.

### `abort`

`abort` is used to forcibly reset a Durable Object. A JavaScript `Error` with the message passed as a parameter will be logged. This error is not able to be caught within the application code.

```js
// Durable Object
export class MyDurableObject extends DurableObject {
	constructor(ctx: DurableObjectState, env: Env) {
		super(ctx, env);
	}

  async sayHello() {
    // Error: Hello, World! will be logged
    this.ctx.abort("Hello, World!");
  }
}
```

:::caution[Not available in local development]

`abort` is not available in local development with the `wrangler dev` CLI command.

:::

#### Parameters

- An optional `string` .

#### Return values

- None.

## Properties

### `id`

`id` is a readonly property of type `DurableObjectId` corresponding to the [`DurableObjectId`](/durable-objects/api/id) of the Durable Object.

### `storage`

`storage` is a readonly property of type `DurableObjectStorage` encapsulating the [Storage API](/durable-objects/api/storage-api).

## Related resources

- [Durable Objects: Easy, Fast, Correct â€“ Choose Three](https://blog.cloudflare.com/durable-objects-easy-fast-correct-choose-three/).

---

# Durable Object Stub

URL: https://developers.cloudflare.com/durable-objects/api/stub/

import { Render, GlossaryTooltip } from "~/components";

## Description

The `DurableObjectStub` interface is a client used to invoke methods on a remote <GlossaryTooltip term="Durable Object">Durable Object</GlossaryTooltip>. The type of `DurableObjectStub` is generic to allow for RPC methods to be invoked on the stub.

Durable Objects implement E-order semantics, a concept deriving from the [E distributed programming language](<https://en.wikipedia.org/wiki/E_(programming_language)>). When you make multiple calls to the same Durable Object, it is guaranteed that the calls will be delivered to the remote Durable Object in the order in which you made them. E-order semantics makes many distributed programming problems easier. E-order is implemented by the [Cap'n Proto](https://capnproto.org) distributed object-capability RPC protocol, which Cloudflare Workers uses for internal communications.

If an exception is thrown by a Durable Object <GlossaryTooltip term="stub">stub</GlossaryTooltip> all in-flight calls and future calls will fail with [exceptions](/durable-objects/observability/troubleshooting/). To continue invoking methods on a remote Durable Object a Worker must recreate the stub. There are no ordering guarantees between different stubs.

<Render file="example-rpc" />

## Properties

### `id`

`id` is a property of the `DurableObjectStub` corresponding to the [`DurableObjectId`](/durable-objects/api/id) used to create the stub.

```js
const id = env.MY_DURABLE_OBJECT.newUniqueId();
const stub = env.MY_DURABLE_OBJECT.get(id);
console.assert(id.equals(stub.id), "This should always be true");
```

### `name`

`name` is an optional property of a `DurableObjectStub`, which returns the name that was used to create the [`DurableObjectId`](/durable-objects/api/id) via [`DurableObjectNamespace::idFromName`](/durable-objects/api/namespace/#idfromname) which was then used to create the `DurableObjectStub`. This value is undefined if the [`DurableObjectId`](/durable-objects/api/id) used to create the `DurableObjectStub` was constructed using [`DurableObjectNamespace::newUniqueId`](/durable-objects/api/namespace/#newuniqueid).

```js
const id = env.MY_DURABLE_OBJECT.idFromName("foo");
const stub = env.MY_DURABLE_OBJECT.get(id);
console.assert(stub.name === "foo", "This should always be true");
```

## Related resources

- [Durable Objects: Easy, Fast, Correct â€“ Choose Three](https://blog.cloudflare.com/durable-objects-easy-fast-correct-choose-three/).

---

# WebGPU

URL: https://developers.cloudflare.com/durable-objects/api/webgpu/

:::caution

The WebGPU API is only available in local development. You cannot deploy Durable Objects to Cloudflare that rely on the WebGPU API. See [Workers AI](/workers-ai/) for information on running machine learning models on the GPUs in Cloudflare's global network.

:::

The [WebGPU API](https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API) allows you to use the GPU directly from JavaScript.

The WebGPU API is only accessible from within [Durable Objects](/durable-objects/). You cannot use the WebGPU API from within Workers.

To use the WebGPU API in local development, enable the `experimental` and `webgpu` [compatibility flags](/workers/configuration/compatibility-flags/) in the [Wrangler configuration file](/workers/wrangler/configuration/) of your Durable Object.

```
compatibility_flags = ["experimental", "webgpu"]
```

The following subset of the WebGPU API is available from within Durable Objects:

| API                                                                                                                | Supported? | Notes |
| ------------------------------------------------------------------------------------------------------------------ | ---------- | ----- |
| [`navigator.gpu`](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/gpu)                                  | âœ…         |       |
| [`GPU.requestAdapter`](https://developer.mozilla.org/en-US/docs/Web/API/GPU/requestAdapter)                        | âœ…         |       |
| [`GPUAdapterInfo`](https://developer.mozilla.org/en-US/docs/Web/API/GPUAdapterInfo)                                | âœ…         |       |
| [`GPUAdapter`](https://developer.mozilla.org/en-US/docs/Web/API/GPUAdapter)                                        | âœ…         |       |
| [`GPUBindGroupLayout`](https://developer.mozilla.org/en-US/docs/Web/API/GPUBindGroupLayout)                        | âœ…         |       |
| [`GPUBindGroup`](https://developer.mozilla.org/en-US/docs/Web/API/GPUBindGroup)                                    | âœ…         |       |
| [`GPUBuffer`](https://developer.mozilla.org/en-US/docs/Web/API/GPUBuffer)                                          | âœ…         |       |
| [`GPUCommandBuffer`](https://developer.mozilla.org/en-US/docs/Web/API/GPUCommandBuffer)                            | âœ…         |       |
| [`GPUCommandEncoder`](https://developer.mozilla.org/en-US/docs/Web/API/GPUCommandEncoder)                          | âœ…         |       |
| [`GPUComputePassEncoder`](https://developer.mozilla.org/en-US/docs/Web/API/GPUComputePassEncoder)                  | âœ…         |       |
| [`GPUComputePipeline`](https://developer.mozilla.org/en-US/docs/Web/API/GPUComputePipeline)                        | âœ…         |       |
| [`GPUComputePipelineError`](https://developer.mozilla.org/en-US/docs/Web/API/GPUPipelineError)                     | âœ…         |       |
| [`GPUDevice`](https://developer.mozilla.org/en-US/docs/Web/API/GPUDevice)                                          | âœ…         |       |
| [`GPUOutOfMemoryError`](https://developer.mozilla.org/en-US/docs/Web/API/GPUOutOfMemoryError)                      | âœ…         |       |
| [`GPUValidationError`](https://developer.mozilla.org/en-US/docs/Web/API/GPUValidationError)                        | âœ…         |       |
| [`GPUInternalError`](https://developer.mozilla.org/en-US/docs/Web/API/GPUInternalError)                            | âœ…         |       |
| [`GPUDeviceLostInfo`](https://developer.mozilla.org/en-US/docs/Web/API/GPUDeviceLostInfo)                          | âœ…         |       |
| [`GPUPipelineLayout`](https://developer.mozilla.org/en-US/docs/Web/API/GPUPipelineLayout)                          | âœ…         |       |
| [`GPUQuerySet`](https://developer.mozilla.org/en-US/docs/Web/API/GPUQuerySet)                                      | âœ…         |       |
| [`GPUQueue`](https://developer.mozilla.org/en-US/docs/Web/API/GPUQueue)                                            | âœ…         |       |
| [`GPUSampler`](https://developer.mozilla.org/en-US/docs/Web/API/GPUSampler)                                        | âœ…         |       |
| [`GPUCompilationMessage`](https://developer.mozilla.org/en-US/docs/Web/API/GPUCompilationMessage)                  | âœ…         |       |
| [`GPUShaderModule`](https://developer.mozilla.org/en-US/docs/Web/API/GPUShaderModule)                              | âœ…         |       |
| [`GPUSupportedFeatures`](https://developer.mozilla.org/en-US/docs/Web/API/GPUSupportedFeatures)                    | âœ…         |       |
| [`GPUSupportedLimits`](https://developer.mozilla.org/en-US/docs/Web/API/GPUSupportedLimits)                        | âœ…         |       |
| [`GPUMapMode`](https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API#reading_the_results_back_to_javascript) | âœ…         |       |
| [`GPUShaderStage`](https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API#create_a_bind_group_layout)         | âœ…         |       |
| [`GPUUncapturedErrorEvent`](https://developer.mozilla.org/en-US/docs/Web/API/GPUUncapturedErrorEvent)              | âœ…         |       |

The following subset of the WebGPU API is not yet supported:

| API                                                                                                             | Supported? | Notes |
| --------------------------------------------------------------------------------------------------------------- | ---------- | ----- |
| [`GPU.getPreferredCanvasFormat`](https://developer.mozilla.org/en-US/docs/Web/API/GPU/getPreferredCanvasFormat) |            |       |
| [`GPURenderBundle`](https://developer.mozilla.org/en-US/docs/Web/API/GPURenderBundle)                           |            |       |
| [`GPURenderBundleEncoder`](https://developer.mozilla.org/en-US/docs/Web/API/GPURenderBundleEncoder)             |            |       |
| [`GPURenderPassEncoder`](https://developer.mozilla.org/en-US/docs/Web/API/GPURenderPassEncoder)                 |            |       |
| [`GPURenderPipeline`](https://developer.mozilla.org/en-US/docs/Web/API/GPURenderPipeline)                       |            |       |
| [`GPUShaderModule`](https://developer.mozilla.org/en-US/docs/Web/API/GPUShaderModule)                           |            |       |
| [`GPUTexture`](https://developer.mozilla.org/en-US/docs/Web/API/GPUTexture)                                     |            |       |
| [`GPUTextureView`](https://developer.mozilla.org/en-US/docs/Web/API/GPUTextureView)                             |            |       |
| [`GPUExternalTexture`](https://developer.mozilla.org/en-US/docs/Web/API/GPUExternalTexture)                     |            |       |

## Examples

- [workers-wonnx](https://github.com/cloudflare/workers-wonnx/) â€” Image classification, running on a GPU via the WebGPU API, using the [wonnx](https://github.com/webonnx/wonnx) model inference runtime.

---

# Access Durable Objects Storage

URL: https://developers.cloudflare.com/durable-objects/best-practices/access-durable-objects-storage/

import { Render, GlossaryTooltip, WranglerConfig } from "~/components";

<GlossaryTooltip term="Durable Object">Durable Objects</GlossaryTooltip> are a powerful compute API that provides a compute with storage building block. Each Durable Object has its own private, transactional and strongly consistent storage. Durable Objects <GlossaryTooltip term="Storage API">Storage API</GlossaryTooltip> provides access to a Durable Object's attached storage.

A Durable Object's [in-memory state](/durable-objects/reference/in-memory-state/) is preserved as long as the Durable Object is not evicted from memory. Inactive Durable Objects with no incoming request traffic can be evicted. There are normal operations like [code deployments](/workers/configuration/versions-and-deployments/) that trigger Durable Objects to restart and lose their in-memory state. For these reasons, you should use Storage API to persist state durably on disk that needs to survive eviction or restart of Durable Objects.

## Access storage

By default, a <GlossaryTooltip term="Durable Object class">Durable Object class</GlossaryTooltip> leverages a key-value storage backend. New Durable Object classes can opt-in to using a [SQLite storage backend](/durable-objects/best-practices/access-durable-objects-storage/#sqlite-storage-backend).

[Storage API methods](/durable-objects/api/storage-api/#methods) are available on `ctx.storage` parameter passed to the Durable Object constructor. Storage API has key-value APIs and SQL APIs. Only Durable Object classes with a SQLite storage backend can access SQL API.

A common pattern is to initialize a Durable Object from [persistent storage](/durable-objects/api/storage-api/) and set instance variables the first time it is accessed. Since future accesses are routed to the same Durable Object, it is then possible to return any initialized values without making further calls to persistent storage.

```ts
import { DurableObject } from "cloudflare:workers";

export class Counter extends DurableObject {
  value: number;

  constructor(ctx: DurableObjectState, env: Env) {
    super(ctx, env);

    // `blockConcurrencyWhile()` ensures no requests are delivered until
    // initialization completes.
    ctx.blockConcurrencyWhile(async () => {
      // After initialization, future reads do not need to access storage.
      this.value = (await ctx.storage.get("value")) || 0;
    });
  }

  async getCounterValue() {
    return this.value;
  }
}
```
### Removing a Durable Object's storage

A Durable Object fully ceases to exist if, when it shuts down, its storage is empty. If you never write to a Durable Object's storage at all (including setting <GlossaryTooltip term="alarm">alarms</GlossaryTooltip>), then storage remains empty, and so the Durable Object will no longer exist once it shuts down.

However if you ever write using [Storage API](/durable-objects/api/storage-api/), including setting alarms, then you must explicitly call [`storage.deleteAll()`](/durable-objects/api/storage-api/#deleteall) to empty storage. It is not sufficient to simply delete the specific data that you wrote, such as deleting a key or dropping a table, as some metadata may remain. The only way to remove all storage is to call `deleteAll()`. Calling `deleteAll()` ensures that a Durable Object will not be billed for storage.

## SQLite storage backend

:::note[SQLite in Durable Objects Beta]

The new beta version of Durable Objects is available where each Durable Object has a private, embedded SQLite database. When deploying a new Durable Object class, users can opt-in to a SQLite storage backend in order to access new [SQL API](/durable-objects/api/sql-storage/#exec). Otherwise, a Durable Object class has a key-value storage backend.

:::

To allow a new Durable Object class to use SQLite storage backend, use `new_sqlite_classes` on the migration in your Worker's Wrangler file:

<WranglerConfig>

```toml
[[migrations]]
tag = "v1" # Should be unique for each entry
new_sqlite_classes = ["MyDurableObject"] # Array of new classes
```

</WranglerConfig>

[SQL API](/durable-objects/api/sql-storage/#exec) is available on `ctx.storage.sql` parameter passed to the Durable Object constructor.

### Examples

<Render file="durable-objects-sql" />

<Render file="durable-objects-vs-d1" />

## Index for SQLite Durable Objects

Creating indexes for your most queried tables and filtered columns reduces how much data is scanned and improves query performance at the same time. If you have a read-heavy workload (most common), this can be particularly advantageous. Writing to columns referenced in an index will add at least one (1) additional row written to account for updating the index, but this is typically offset by the reduction in rows read due to the benefits of an index.

## Related resources

* [Zero-latency SQLite storage in every Durable Object blog post](https://blog.cloudflare.com/sqlite-in-durable-objects)

---

# Invoking methods

URL: https://developers.cloudflare.com/durable-objects/best-practices/create-durable-object-stubs-and-send-requests/

import { Render, Tabs, TabItem, GlossaryTooltip } from "~/components";

## Invoking methods on a Durable Object

All new projects and existing projects with a compatibility date greater than or equal to [`2024-04-03`](/workers/configuration/compatibility-flags/#durable-object-stubs-and-service-bindings-support-rpc) should prefer to invoke [Remote Procedure Call (RPC)](/workers/runtime-apis/rpc/) methods defined on a <GlossaryTooltip term="Durable Object class">Durable Object class</GlossaryTooltip>. Legacy projects can continue to invoke the `fetch` handler on the Durable Object class indefinitely.

### Invoke RPC methods

By writing a Durable Object class which inherits from the built-in type `DurableObject`, public methods on the Durable Objects class are exposed as [RPC methods](/workers/runtime-apis/rpc/), which you can call using a [DurableObjectStub](/durable-objects/api/stub) from a Worker.

All RPC calls are [asynchronous](/workers/runtime-apis/rpc/lifecycle/), accept and return [serializable types](/workers/runtime-apis/rpc/), and [propagate exceptions](/workers/runtime-apis/rpc/error-handling/) to the caller without a stack trace. Refer to [Workers RPC](/workers/runtime-apis/rpc/) for complete details.

<Render file="example-rpc" />

:::note

With RPC, the `DurableObject` superclass defines `ctx` and `env` as class properties. What was previously called `state` is now called `ctx` when you extend the `DurableObject` class. The name `ctx` is adopted rather than `state` for the `DurableObjectState` interface to be consistent between `DurableObject` and `WorkerEntrypoint` objects.

:::

Refer to [Build a Counter](/durable-objects/examples/build-a-counter/) for a complete example.

### Invoking the `fetch` handler

If your project is stuck on a compatibility date before [`2024-04-03`](/workers/configuration/compatibility-flags/#durable-object-stubs-and-service-bindings-support-rpc), or has the need to send a [`Request`](/workers/runtime-apis/request/) object and return a `Response` object, then you should send requests to a Durable Object via the fetch handler.

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Durable Object
export class MyDurableObject extends DurableObject {
	constructor(ctx, env) {
		super(ctx, env);
	}

	async fetch(request) {
		return new Response("Hello, World!");
	}
}

// Worker
export default {
	async fetch(request, env) {
		// Every unique ID refers to an individual instance of the Durable Object class
		const id = env.MY_DURABLE_OBJECT.idFromName("foo");

		// A stub is a client used to invoke methods on the Durable Object
		const stub = env.MY_DURABLE_OBJECT.get(id);

		// Methods on the Durable Object are invoked via the stub
		const response = await stub.fetch(request);

		return response;
	},
};
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
	MY_DURABLE_OBJECT: DurableObjectNamespace<MyDurableObject>;
}

// Durable Object
export class MyDurableObject extends DurableObject {
	constructor(ctx: DurableObjectState, env: Env) {
		super(ctx, env);
	}

	async fetch(request: Request): Promise<Response> {
		return new Response("Hello, World!");
	}
}

// Worker
export default {
	async fetch(request, env) {
		// Every unique ID refers to an individual instance of the Durable Object class
		const id = env.MY_DURABLE_OBJECT.idFromName("foo");

		// A stub is a client used to invoke methods on the Durable Object
		const stub = env.MY_DURABLE_OBJECT.get(id);

		// Methods on the Durable Object are invoked via the stub
		const response = await stub.fetch(request);

		return response;
	},
} satisfies ExportedHandler<Env>;
```

</TabItem> </Tabs>

The `URL` associated with the [`Request`](/workers/runtime-apis/request/) object passed to the `fetch()` handler of your Durable Object must be a well-formed URL, but does not have to be a publicly-resolvable hostname.

Without RPC, customers frequently construct requests which corresponded to private methods on the Durable Object and dispatch requests from the `fetch` handler. RPC is obviously more ergonomic in this example.

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Durable Object
export class MyDurableObject extends DurableObject {
	constructor(ctx: DurableObjectState, env: Env) {
		super(ctx, env);
	}

	private hello(name) {
		return new Response(`Hello, ${name}!`);
	}

	private goodbye(name) {
		return new Response(`Goodbye, ${name}!`);
	}

	async fetch(request) {
		const url = new URL(request.url);
		let name = url.searchParams.get("name");
		if (!name) {
			name = "World";
		}

		switch (url.pathname) {
			case "/hello":
				return this.hello(name);
			case "/goodbye":
				return this.goodbye(name);
			default:
				return new Response("Bad Request", { status: 400 });
		}
	}
}

// Worker
export default {
	async fetch(_request, env, _ctx) {
		// Every unique ID refers to an individual instance of the Durable Object class
		const id = env.MY_DURABLE_OBJECT.idFromName("foo");

		// A stub is a client used to invoke methods on the Durable Object
		const stub = env.MY_DURABLE_OBJECT.get(id);

		// Invoke the fetch handler on the Durable Object stub
		let response = await stub.fetch("http://do/hello?name=World");

		return response;
	},
};
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
	MY_DURABLE_OBJECT: DurableObjectNamespace<MyDurableObject>;
}

// Durable Object
export class MyDurableObject extends DurableObject {
	constructor(ctx: DurableObjectState, env: Env) {
		super(ctx, env);
	}

	private hello(name: string) {
		return new Response(`Hello, ${name}!`);
	}

	private goodbye(name: string) {
		return new Response(`Goodbye, ${name}!`);
	}

	async fetch(request: Request): Promise<Response> {
		const url = new URL(request.url);
		let name = url.searchParams.get("name");
		if (!name) {
			name = "World";
		}

		switch (url.pathname) {
			case "/hello":
				return this.hello(name);
			case "/goodbye":
				return this.goodbye(name);
			default:
				return new Response("Bad Request", { status: 400 });
		}
	}
}

// Worker
export default {
	async fetch(_request, env, _ctx) {
		// Every unique ID refers to an individual instance of the Durable Object class
		const id = env.MY_DURABLE_OBJECT.idFromName("foo");

		// A stub is a client used to invoke methods on the Durable Object
		const stub = env.MY_DURABLE_OBJECT.get(id);

		// Invoke the fetch handler on the Durable Object stub
		let response = await stub.fetch("http://do/hello?name=World");

		return response;
	},
} satisfies ExportedHandler<Env>;
```

</TabItem> </Tabs>

---

# Error handling

URL: https://developers.cloudflare.com/durable-objects/best-practices/error-handling/

import { GlossaryTooltip } from "~/components";


Any uncaught exceptions thrown by a <GlossaryTooltip term="Durable Object">Durable Object</GlossaryTooltip> or thrown by Durable Objects' infrastructure (such as overloads or network errors) will be propagated to the callsite of the client. Catching these exceptions allows you to retry creating the [`DurableObjectStub`](/durable-objects/api/stub) and sending requests.

JavaScript Errors with the property `.retryable` set to True are suggested to be retried if requests to the Durable Object are idempotent, or can be applied multiple times without changing the response. If requests are not idempotent, then you will need to decide what is best for your application.

JavaScript Errors with the property `.overloaded` set to True should not be retried. If a Durable Object is overloaded, then retrying will worsen the overload and increase the overall error rate.

It is strongly recommended to retry requests following the exponential backoff algorithm in production code when the error properties indicate that it is safe to do so.

## How exceptions are thrown

Durable Objects can throw exceptions in one of two ways:

- An exception can be thrown within the user code which implements a <GlossaryTooltip term="Durable Object class">Durable Object class</GlossaryTooltip>. The resulting exception will have a `.remote` property set to `True` in this case.
- An exception can be generated by Durable Object's infrastructure. Some sources of infrastructure exceptions include: transient internal errors, sending too many requests to a single Durable Object, and too many requests being queued due to slow or excessive I/O (external API calls or storage operations) within an individual Durable Object. Some infrastructure exceptions may also have the `.remote` property set to `True` -- for example, when the Durable Object exceeds its memory or CPU limits.

Refer to [Troubleshooting](/durable-objects/observability/troubleshooting/) to review the types of errors returned by a Durable Object and/or Durable Objects infrastructure and how to prevent them.

## Example

This example demonstrates retrying requests using the recommended exponential backoff algorithm.

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
	ErrorThrowingObject: DurableObjectNamespace;
}

export default {
	async fetch(request, env, ctx) {
		let userId = new URL(request.url).searchParams.get("userId") || "";
		const id = env.ErrorThrowingObject.idFromName(userId);

		// Retry behavior can be adjusted to fit your application.
		let maxAttempts = 3;
		let baseBackoffMs = 100;
		let maxBackoffMs = 20000;

		let attempt = 0;
		while (true) {
			// Try sending the request
			try {
				// Create a Durable Object stub for each attempt, because certain types of
				// errors will break the Durable Object stub.
				const doStub = env.ErrorThrowingObject.get(id);
				const resp = await doStub.fetch("http://your-do/");

				return Response.json(resp);
			} catch (e: any) {
				if (!e.retryable) {
					// Failure was not a transient internal error, so don't retry.
					break;
				}
			}
			let backoffMs = Math.min(
				maxBackoffMs,
				baseBackoffMs * Math.random() * Math.pow(2, attempt),
			);
			attempt += 1;
			if (attempt >= maxAttempts) {
				// Reached max attempts, so don't retry.
				break;
			}
			await scheduler.wait(backoffMs);
		}
		return new Response("server error", { status: 500 });
	},
} satisfies ExportedHandler<Env>;

export class ErrorThrowingObject extends DurableObject {
	constructor(state: DurableObjectState, env: Env) {
		super(state, env);

		// Any exceptions that are raised in your constructor will also set the
		// .remote property to True
		throw new Error("no good");
	}

	async fetch(req: Request) {
		// Generate an uncaught exception
		// A .remote property will be added to the exception propagated to the caller
		// and will be set to True
		throw new Error("example error");

		// We never reach this
		return Response.json({});
	}
}
```

---

# Best practices

URL: https://developers.cloudflare.com/durable-objects/best-practices/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Using WebSockets

URL: https://developers.cloudflare.com/durable-objects/best-practices/websockets/

import { Tabs, TabItem, GlossaryTooltip, Type } from "~/components";

WebSockets are long-lived TCP connections that enable bi-directional, real-time communication between client and server. Both Cloudflare Durable Objects and Workers can act as WebSocket endpoints â€“ either as a client or as a server. Because WebSocket sessions are long-lived, applications commonly use Durable Objects to accept either the client or server connection. While there are other use cases for using Workers exclusively with WebSockets, for example proxying WebSocket messages, WebSockets are most useful when combined with Durable Objects.

Because Durable Objects provide a single-point-of-coordination between [Cloudflare Workers](/workers/), a single Durable Object instance can be used in parallel with WebSockets to coordinate between multiple clients, such as participants in a chat room or a multiplayer game. Refer to [Cloudflare Edge Chat Demo](https://github.com/cloudflare/workers-chat-demo) for an example of using Durable Objects with WebSockets.

Both Cloudflare Durable Objects and Workers can use the [Web Standard WebSocket API](/workers/runtime-apis/websockets/) to build applications, but a major differentiator of Cloudflare Durable Objects relative to other platforms is the ability to Hibernate WebSocket connections to save costs.

This guide covers:
1. Building a WebSocket server using Web Standard APIs
2. Using WebSocket Hibernation APIs.

## WebSocket Standard API

WebSocket connections are established by making an HTTP GET request with the `Upgrade: websocket` header. A Cloudflare Worker is commonly used to validate the request, proxy the request to the Durable Object to accept the server side connection, and return the client side connection in the response.

:::note[Validate requests in a Worker]

Both Workers and Durable Objects are billed, in part, based on the number of requests they receive. To avoid being billed for requests against a Durable Object for invalid requests, be sure to validate requests in your Worker.

:::

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
// Worker
export default {
	async fetch(request, env, ctx) {
		if (request.method === "GET" && request.url.endsWith("/websocket")) {
			// Expect to receive a WebSocket Upgrade request.
			// If there is one, accept the request and return a WebSocket Response.
			const upgradeHeader = request.headers.get("Upgrade");
			if (!upgradeHeader || upgradeHeader !== "websocket") {
				return new Response(null, {
					status: 426,
					statusText: "Durable Object expected Upgrade: websocket",
					headers: {
						"Content-Type": "text/plain",
					},
				});
			}

			// This example will refer to a single Durable Object instance, since the name "foo" is
			// hardcoded
			let id = env.WEBSOCKET_SERVER.idFromName("foo");
			let stub = env.WEBSOCKET_SERVER.get(id);

			// The Durable Object's fetch handler will accept the server side connection and return
			// the client
			return stub.fetch(request);
		}

		return new Response(null, {
			status: 400,
			statusText: "Bad Request",
			headers: {
				"Content-Type": "text/plain",
			},
		});
	},
};
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
// Worker
export default {
	async fetch(request, env, ctx): Promise<Response> {
		if (request.method === "GET" && request.url.endsWith("/websocket")) {
			// Expect to receive a WebSocket Upgrade request.
			// If there is one, accept the request and return a WebSocket Response.
			const upgradeHeader = request.headers.get("Upgrade");
			if (!upgradeHeader || upgradeHeader !== "websocket") {
				return new Response(null, {
					status: 426,
					statusText: "Durable Object expected Upgrade: websocket",
					headers: {
						"Content-Type": "text/plain",
					},
				});
			}

			// This example will refer to a single Durable Object instance, since the name "foo" is
			// hardcoded
			let id = env.WEBSOCKET_SERVER.idFromName("foo");
			let stub = env.WEBSOCKET_SERVER.get(id);

			// The Durable Object's fetch handler will accept the server side connection and return
			// the client
			return stub.fetch(request);
		}

		return new Response(null, {
			status: 400,
			statusText: "Bad Request",
			headers: {
				"Content-Type": "text/plain",
			},
		});
	},
} satisfies ExportedHandler<Env>;
```

</TabItem> </Tabs>

Each WebSocket server in this example is represented by a Durable Object. This WebSocket server creates a single WebSocket connection and responds to all messages over that connection with the total number of accepted WebSocket connections. In the Durable Object's fetch handler we create client and server connections and add event listeners for relevant event types.

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Durable Object
export class WebSocketServer extends DurableObject {
	currentlyConnectedWebSockets;

	constructor(ctx, env) {
		// This is reset whenever the constructor runs because
		// regular WebSockets do not survive Durable Object resets.
		//
		// WebSockets accepted via the Hibernation API can survive
		// a certain type of eviction, but we will not cover that here.
		super(ctx, env);
		this.currentlyConnectedWebSockets = 0;
	}

	async fetch(request) {
		// Creates two ends of a WebSocket connection.
		const webSocketPair = new WebSocketPair();
		const [client, server] = Object.values(webSocketPair);

		// Calling `accept()` tells the runtime that this WebSocket is to begin terminating
		// request within the Durable Object. It has the effect of "accepting" the connection,
		// and allowing the WebSocket to send and receive messages.
		server.accept();
		this.currentlyConnectedWebSockets += 1;

		// Upon receiving a message from the client, the server replies with the same message,
		// and the total number of connections with the "[Durable Object]: " prefix
		server.addEventListener("message", (event) => {
			server.send(
				`[Durable Object] currentlyConnectedWebSockets: ${this.currentlyConnectedWebSockets}`,
			);
		});

		// If the client closes the connection, the runtime will close the connection too.
		server.addEventListener("close", (cls) => {
			this.currentlyConnectedWebSockets -= 1;
			server.close(cls.code, "Durable Object is closing WebSocket");
		});

		return new Response(null, {
			status: 101,
			webSocket: client,
		});
	}
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
// Durable Object
export class WebSocketServer extends DurableObject {
	currentlyConnectedWebSockets: number;

	constructor(ctx: DurableObjectState, env: Env) {
		// This is reset whenever the constructor runs because
		// regular WebSockets do not survive Durable Object resets.
		//
		// WebSockets accepted via the Hibernation API can survive
		// a certain type of eviction, but we will not cover that here.
		super(ctx, env);
		this.currentlyConnectedWebSockets = 0;
	}

	async fetch(request: Request): Promise<Response> {
		// Creates two ends of a WebSocket connection.
		const webSocketPair = new WebSocketPair();
		const [client, server] = Object.values(webSocketPair);

		// Calling `accept()` tells the runtime that this WebSocket is to begin terminating
		// request within the Durable Object. It has the effect of "accepting" the connection,
		// and allowing the WebSocket to send and receive messages.
		server.accept();
		this.currentlyConnectedWebSockets += 1;

		// Upon receiving a message from the client, the server replies with the same message,
		// and the total number of connections with the "[Durable Object]: " prefix
		server.addEventListener("message", (event: MessageEvent) => {
			server.send(
				`[Durable Object] currentlyConnectedWebSockets: ${this.currentlyConnectedWebSockets}`,
			);
		});

		// If the client closes the connection, the runtime will close the connection too.
		server.addEventListener("close", (cls: CloseEvent) => {
			this.currentlyConnectedWebSockets -= 1;
			server.close(cls.code, "Durable Object is closing WebSocket");
		});

		return new Response(null, {
			status: 101,
			webSocket: client,
		});
	}
}
```

</TabItem> </Tabs>

To execute this code, configure your Wrangler file to include a Durable Object [binding](/durable-objects/get-started/tutorial/#5-configure-durable-object-bindings) and [migration](/durable-objects/reference/durable-objects-migrations/) based on the <GlossaryTooltip term="namespace">namespace</GlossaryTooltip> and class name chosen previously.

```toml title="wrangler.toml"
name = "websocket-server"

[[durable_objects.bindings]]
name = "WEBSOCKET_SERVER"
class_name = "WebSocketServer"

[[migrations]]
tag = "v1"
new_classes = ["WebSocketServer"]
```

A full example can be found in [Build a WebSocket server](/durable-objects/examples/websocket-server/).

:::caution[WebSockets disconnection]

Code updates will disconnect all WebSockets. If you deploy a new version of a Worker, every Durable Object is restarted. Any connections to old Durable Objects will be disconnected.

:::

## WebSocket Hibernation API

In addition to [Workers WebSocket API](/workers/runtime-apis/websockets/), Cloudflare Durable Objects can use the WebSocket Hibernation API which extends the Web Standard WebSocket API to reduce costs. Specifically, [billable Duration (GB-s) charges](/durable-objects/platform/pricing/) are not incurred during periods of inactivity. Note that other events, for example [alarms](/durable-objects/api/alarms/), can prevent a Durable Object from being inactive and therefore prevent this cost saving.

The WebSocket consists of Cloudflare-specific extensions to the Web Standard WebSocket API. These extensions are either present on the [DurableObjectState](/durable-objects/api/state) interface, or as handler methods on the Durable Object class.

:::note

Hibernation is only supported when a Durable Object acts as a WebSocket server. Currently, outgoing WebSockets cannot hibernate.

:::

The Worker used in the WebSocket Standard API example does not require any code changes to make use of the WebSocket Hibernation API. The changes to the Durable Object are described in the code sample below. In summary, [`DurableObjectState::acceptWebSocket`](/durable-objects/api/state/#acceptwebsocket) is called to accept the server side of the WebSocket connection, and handler methods are defined on the Durable Object class for relevant event types rather than adding event listeners.

If an event occurs for a hibernated Durable Object's corresponding handler method, it will return to memory. This will call the Durable Object's constructor, so it is best to minimize work in the constructor when using WebSocket hibernation.

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Durable Object
export class WebSocketHibernationServer extends DurableObject {
	async fetch(request) {
		// Creates two ends of a WebSocket connection.
		const webSocketPair = new WebSocketPair();
		const [client, server] = Object.values(webSocketPair);

		// Calling `acceptWebSocket()` informs the runtime that this WebSocket is to begin terminating
		// request within the Durable Object. It has the effect of "accepting" the connection,
		// and allowing the WebSocket to send and receive messages.
		// Unlike `ws.accept()`, `state.acceptWebSocket(ws)` informs the Workers Runtime that the WebSocket
		// is "hibernatable", so the runtime does not need to pin this Durable Object to memory while
		// the connection is open. During periods of inactivity, the Durable Object can be evicted
		// from memory, but the WebSocket connection will remain open. If at some later point the
		// WebSocket receives a message, the runtime will recreate the Durable Object
		// (run the `constructor`) and deliver the message to the appropriate handler.
		this.ctx.acceptWebSocket(server);

		return new Response(null, {
			status: 101,
			webSocket: client,
		});
	}

	async webSocketMessage(ws, message) {
		// Upon receiving a message from the client, reply with the same message,
		// but will prefix the message with "[Durable Object]: " and return the
		// total number of connections.
		ws.send(
			`[Durable Object] message: ${message}, connections: ${this.ctx.getWebSockets().length}`,
		);
	}

	async webSocketClose(ws, code, reason, wasClean) {
		// If the client closes the connection, the runtime will invoke the webSocketClose() handler.
		ws.close(code, "Durable Object is closing WebSocket");
	}
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
	WEBSOCKET_HIBERNATION_SERVER: DurableObjectNamespace<WebSocketHibernationServer>;
}

// Durable Object
export class WebSocketHibernationServer extends DurableObject {
	async fetch(request: Request): Promise<Response> {
		// Creates two ends of a WebSocket connection.
		const webSocketPair = new WebSocketPair();
		const [client, server] = Object.values(webSocketPair);

		// Calling `acceptWebSocket()` informs the runtime that this WebSocket is to begin terminating
		// request within the Durable Object. It has the effect of "accepting" the connection,
		// and allowing the WebSocket to send and receive messages.
		// Unlike `ws.accept()`, `state.acceptWebSocket(ws)` informs the Workers Runtime that the WebSocket
		// is "hibernatable", so the runtime does not need to pin this Durable Object to memory while
		// the connection is open. During periods of inactivity, the Durable Object can be evicted
		// from memory, but the WebSocket connection will remain open. If at some later point the
		// WebSocket receives a message, the runtime will recreate the Durable Object
		// (run the `constructor`) and deliver the message to the appropriate handler.
		this.ctx.acceptWebSocket(server);

		return new Response(null, {
			status: 101,
			webSocket: client,
		});
	}

	async webSocketMessage(ws: WebSocket, message: ArrayBuffer | string) {
		// Upon receiving a message from the client, the server replies with the same message,
		// and the total number of connections with the "[Durable Object]: " prefix
		ws.send(
			`[Durable Object] message: ${message}, connections: ${this.ctx.getWebSockets().length}`,
		);
	}

	async webSocketClose(
		ws: WebSocket,
		code: number,
		reason: string,
		wasClean: boolean,
	) {
		// If the client closes the connection, the runtime will invoke the webSocketClose() handler.
		ws.close(code, "Durable Object is closing WebSocket");
	}
}
```

</TabItem> </Tabs>

Similar to the WebSocket Standard API example, to execute this code, configure your Wrangler file to include a Durable Object [binding](/durable-objects/get-started/tutorial/#5-configure-durable-object-bindings) and [migration](/durable-objects/reference/durable-objects-migrations/) based on the <GlossaryTooltip term="namespace">namespace</GlossaryTooltip> and class name chosen previously.

```toml title="wrangler.toml"
name = "websocket-hibernation-server"

[[durable_objects.bindings]]
name = "WEBSOCKET_HIBERNATION_SERVER"
class_name = "WebSocketHibernationServer"

[[migrations]]
tag = "v1"
new_classes = ["WebSocketHibernationServer"]
```

A full example can be found in [Build a WebSocket server with WebSocket Hibernation](/durable-objects/examples/websocket-hibernation-server/).

:::caution[Support for local development]

Prior to `wrangler@3.13.2` and Miniflare `v3.20231016.0`, WebSockets did not hibernate when using local development environments such as `wrangler dev` or Miniflare.

If you are using older versions, note that while hibernatable WebSocket events such as [`webSocketMessage()`](/durable-objects/api/base/#websocketmessage) will still be delivered, the Durable Object will never be evicted from memory.

:::

## Extended methods

### `serializeAttachment`

- <code> serializeAttachment(value <Type text="any" />)</code>: <Type text="void" />

  - Keeps a copy of `value` associated with the WebSocket to survive hibernation. The value can be any type supported by the [structured clone algorithm](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm), which is true of most types. If the value needs to be durable please use [Durable Object Storage](/durable-objects/api/storage-api/).

  - If you modify `value` after calling this method, those changes will not be retained unless you call this method again. The serialized size of `value` is limited to 2,048 bytes, otherwise this method will throw an error. If you need larger values to survive hibernation, use the [Storage API](/durable-objects/api/storage-api/) and pass the corresponding key to this method so it can be retrieved later.

### `deserializeAttachment`

- `deserializeAttachment()`: <Type text='any' />

  - Retrieves the most recent value passed to `serializeAttachment()`, or `null` if none exists.

## Related resources

- [Mozilla Developer Network's (MDN) documentation on the WebSocket class](https://developer.mozilla.org/en-US/docs/Web/API/WebSocket)
- [Cloudflare's WebSocket template for building applications on Workers using WebSockets](https://github.com/cloudflare/websocket-template)
- [Durable Object base class](/durable-objects/api/base/)
- [Durable Object State interface](/durable-objects/api/state/)

---

# Use the Alarms API

URL: https://developers.cloudflare.com/durable-objects/examples/alarms-api/

import { GlossaryTooltip, WranglerConfig } from "~/components";

This example implements an <GlossaryTooltip term="alarm">`alarm()`</GlossaryTooltip> handler that allows batching of requests to a single Durable Object.

When a request is received and no alarm is set, it sets an alarm for 10 seconds in the future. The `alarm()` handler processes all requests received within that 10-second window.

If no new requests are received, no further alarms will be set until the next request arrives.

```js
import { DurableObject } from 'cloudflare:workers';

// Worker
export default {
  async fetch(request, env) {
    let id = env.BATCHER.idFromName("foo");
    return await env.BATCHER.get(id).fetch(request);
  },
};

const SECONDS = 10;

// Durable Object
export class Batcher extends DurableObject {
  constructor(state, env) {
    this.state = state;
    this.storage = state.storage;
    this.state.blockConcurrencyWhile(async () => {
      let vals = await this.storage.list({ reverse: true, limit: 1 });
      this.count = vals.size == 0 ? 0 : parseInt(vals.keys().next().value);
    });
  }

  async fetch(request) {
    this.count++;

    // If there is no alarm currently set, set one for 10 seconds from now
    // Any further POSTs in the next 10 seconds will be part of this batch.
    let currentAlarm = await this.storage.getAlarm();
    if (currentAlarm == null) {
      this.storage.setAlarm(Date.now() + (1000 * SECONDS));
    }

    // Add the request to the batch.
    await this.storage.put(this.count, await request.text());
    return new Response(JSON.stringify({ queued: this.count }), {
      headers: {
        "content-type": "application/json;charset=UTF-8",
      },
    });
  }

  async alarm() {
    let vals = await this.storage.list();
    await fetch("http://example.com/some-upstream-service", {
      method: "POST",
      body: Array.from(vals.values()),
    });
    await this.storage.deleteAll();
    this.count = 0;
  }
}
```

The `alarm()` handler will be called once every 10 seconds. If an unexpected error terminates the Durable Object, the `alarm()` handler will be re-instantiated on another machine. Following a short delay, the `alarm()` handler will run from the beginning on the other machine.

Finally, configure your Wrangler file to include a Durable Object [binding](/durable-objects/get-started/tutorial/#5-configure-durable-object-bindings) and [migration](/durable-objects/reference/durable-objects-migrations/) based on the namespace and class name chosen previously.

<WranglerConfig>

```toml title="wrangler.toml"
name = "durable-object-alarm"

[[durable_objects.bindings]]
name = "BATCHER"
class_name = "Batcher"

[[migrations]]
tag = "v1"
new_classes = ["Batcher"]
```

</WranglerConfig>

---

# Build a counter

URL: https://developers.cloudflare.com/durable-objects/examples/build-a-counter/

import { TabItem, Tabs, WranglerConfig } from "~/components"

This example shows how to build a counter using Durable Objects and Workers with [RPC methods](/workers/runtime-apis/rpc) that can print, increment, and decrement a `name` provided by the URL query string parameter, for example, `?name=A`.

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Worker
export default {
  async fetch(request, env) {
    let url = new URL(request.url);
    let name = url.searchParams.get("name");
    if (!name) {
      return new Response(
        "Select a Durable Object to contact by using" +
          " the `name` URL query string parameter, for example, ?name=A"
      );
    }

    // Every unique ID refers to an individual instance of the Counter class that
    // has its own state. `idFromName()` always returns the same ID when given the
    // same string as input (and called on the same class), but never the same
    // ID for two different strings (or for different classes).
    let id = env.COUNTERS.idFromName(name);

    // Construct the stub for the Durable Object using the ID.
    // A stub is a client Object used to send messages to the Durable Object.
    let stub = env.COUNTERS.get(id);

    // Send a request to the Durable Object using RPC methods, then await its response.
    let count = null;
    switch (url.pathname) {
      case "/increment":
        count = await stub.increment();
        break;
      case "/decrement":
        count = await stub.decrement();
        break;
      case "/":
        // Serves the current value.
        count = await stub.getCounterValue();
        break;
      default:
        return new Response("Not found", { status: 404 });
    }

    return new Response(`Durable Object '${name}' count: ${count}`);
  }
};

// Durable Object
export class Counter extends DurableObject {

  async getCounterValue() {
    let value = (await this.ctx.storage.get("value")) || 0;
    return value;
  }

  async increment(amount = 1) {
    let value = (await this.ctx.storage.get("value")) || 0;
    value += amount;
    // You do not have to worry about a concurrent request having modified the value in storage.
    // "input gates" will automatically protect against unwanted concurrency.
    // Read-modify-write is safe.
    await this.ctx.storage.put("value", value);
    return value;
  }

  async decrement(amount = 1) {
    let value = (await this.ctx.storage.get("value")) || 0;
    value -= amount;
    await this.ctx.storage.put("value", value);
    return value;
  }
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
  COUNTERS: DurableObjectNamespace<Counter>;
}

// Worker
export default {
  async fetch(request, env) {
    let url = new URL(request.url);
    let name = url.searchParams.get("name");
    if (!name) {
      return new Response(
        "Select a Durable Object to contact by using" +
          " the `name` URL query string parameter, for example, ?name=A"
      );
    }

    // Every unique ID refers to an individual instance of the Counter class that
    // has its own state. `idFromName()` always returns the same ID when given the
    // same string as input (and called on the same class), but never the same
    // ID for two different strings (or for different classes).
    let id = env.COUNTERS.idFromName(name);

    // Construct the stub for the Durable Object using the ID.
    // A stub is a client Object used to send messages to the Durable Object.
    let stub = env.COUNTERS.get(id);

    let count = null;
    switch (url.pathname) {
      case "/increment":
        count = await stub.increment();
        break;
      case "/decrement":
        count = await stub.decrement();
        break;
      case "/":
        // Serves the current value.
        count = await stub.getCounterValue();
        break;
      default:
        return new Response("Not found", { status: 404 });
    }

    return new Response(`Durable Object '${name}' count: ${count}`);
  }
} satisfies ExportedHandler<Env>;

// Durable Object
export class Counter extends DurableObject {

  async getCounterValue() {
    let value = (await this.ctx.storage.get("value")) || 0;
    return value;
  }

  async increment(amount = 1) {
    let value: number = (await this.ctx.storage.get("value")) || 0;
    value += amount;
    // You do not have to worry about a concurrent request having modified the value in storage.
    // "input gates" will automatically protect against unwanted concurrency.
    // Read-modify-write is safe.
    await this.ctx.storage.put("value", value);
    return value;
  }

  async decrement(amount = 1) {
    let value: number = (await this.ctx.storage.get("value")) || 0;
    value -= amount;
    await this.ctx.storage.put("value", value);
    return value;
  }
}
```

</TabItem> </Tabs>

Finally, configure your Wrangler file to include a Durable Object [binding](/durable-objects/get-started/tutorial/#5-configure-durable-object-bindings) and [migration](/durable-objects/reference/durable-objects-migrations/) based on the namespace and class name chosen previously.

<WranglerConfig>

```toml title="wrangler.toml"
name = "my-counter"

[[durable_objects.bindings]]
name = "COUNTERS"
class_name = "Counter"

[[migrations]]
tag = "v1"
new_classes = ["Counter"]
```

</WranglerConfig>

### Related resources

* [Workers RPC](/workers/runtime-apis/rpc/)
* [Durable Objects: Easy, Fast, Correct â€” Choose three](https://blog.cloudflare.com/durable-objects-easy-fast-correct-choose-three/).

---

# Build a rate limiter

URL: https://developers.cloudflare.com/durable-objects/examples/build-a-rate-limiter/

import { TabItem, Tabs, GlossaryTooltip, WranglerConfig } from "~/components"

This example shows how to build a rate limiter using <GlossaryTooltip term="Durable Object">Durable Objects</GlossaryTooltip> and Workers that can be used to protect upstream resources, including third-party APIs that your application relies on and/or services that may be costly for you to invoke.

This example also discusses some decisions that need to be made when designing a system, such as a rate limiter, with Durable Objects.

The Worker creates a `RateLimiter` Durable Object on a per IP basis to protect upstream resources. IP based rate limiting can be effective without negatively impacting latency because any given IP will remain within a small geographic area colocated with the `RateLimiter` Durable Object. Furthermore, throughput is also improved because each IP gets its own Durable Object.

It might seem simpler to implement a global rate limiter, `const id = env.RATE_LIMITER.idFromName("global");`, which can provide better guarantees on the request rate to the upstream resource. However:

* This would require all requests globally to make a sub-request to a single Durable Object.
* Implementing a global rate limiter would add additional latency for requests not colocated with the Durable Object, and global throughput would be capped to the throughput of a single Durable Object.
* A single Durable Object that all requests rely on is typically considered an anti-pattern. Durable Objects work best when they are scoped to a user, room, service and/or the specific subset of your application that requires global co-ordination.

:::note

If you do not need unique or custom rate-limiting capabilities, refer to [Rate limiting rules](/waf/rate-limiting-rules/) that are part of Cloudflare's Web Application Firewall (WAF) product.

:::

The Durable Object uses a token bucket algorithm to implement rate limiting. The naive idea is that each request requires a token to complete, and the tokens are replenished according to the reciprocal of the desired number of requests per second. As an example, a 1000 requests per second rate limit will have a token replenished every millisecond (as specified by milliseconds\_per\_request) up to a given capacity limit.

This example uses Durable Object's [Alarms API](/durable-objects/api/alarms) to schedule the Durable Object to be woken up at a time in the future.

* When the alarm's scheduled time comes, the `alarm()` handler method is called, and in this case, the <GlossaryTooltip term="alarm">alarm</GlossaryTooltip> will add a token to the "Bucket".
* The implementation is made more efficient by adding tokens in bulk (as specified by milliseconds\_for\_updates) and preventing the alarm handler from being invoked every millisecond. More frequent invocations of Durable Objects will lead to higher invocation and duration charges.

The first implementation of a rate limiter is below:

<Tabs>
<TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Worker
export default {
  async fetch(request, env, _ctx) {
    // Determine the IP address of the client
    const ip = request.headers.get("CF-Connecting-IP");
    if (ip === null) {
      return new Response("Could not determine client IP", { status: 400 });
    }

    // Obtain an identifier for a Durable Object based on the client's IP address
    const id = env.RATE_LIMITER.idFromName(ip);

    try {
      const stub = env.RATE_LIMITER.get(id);
      const milliseconds_to_next_request = await stub.getMillisecondsToNextRequest();
      if (milliseconds_to_next_request > 0) {
        // Alternatively one could sleep for the necessary length of time
        return new Response("Rate limit exceeded", { status: 429 });
      }
    } catch (error) {
      return new Response("Could not connect to rate limiter", { status: 502 });
    }

    // TODO: Implement me
    return new Response("Call some upstream resource...")
  }
};

// Durable Object
export class RateLimiter extends DurableObject {
  static milliseconds_per_request = 1;
  static milliseconds_for_updates = 5000;
  static capacity = 10000;

  constructor(ctx, env) {
    super(ctx, env);
    this.tokens = RateLimiter.capacity;
  }

  async getMillisecondsToNextRequest() {
    this.checkAndSetAlarm()

    let milliseconds_to_next_request = RateLimiter.milliseconds_per_request;
    if (this.tokens > 0) {
      this.tokens -= 1;
      milliseconds_to_next_request = 0;
    }

    return milliseconds_to_next_request;
  }

  async checkAndSetAlarm() {
    let currentAlarm = await this.ctx.storage.getAlarm();
    if (currentAlarm == null) {
      this.ctx.storage.setAlarm(Date.now() +
        RateLimiter.milliseconds_for_updates * RateLimiter.milliseconds_per_request);
    }
  }

  async alarm() {
    if (this.tokens < RateLimiter.capacity) {
      this.tokens = Math.min(RateLimiter.capacity,
        this.tokens + RateLimiter.milliseconds_for_updates);
      this.checkAndSetAlarm()
    }
  }
}
```

</TabItem>
<TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
  RATE_LIMITER: DurableObjectNamespace<RateLimiter>;
}

// Worker
export default {
  async fetch(request, env, _ctx): Promise<Response> {
    // Determine the IP address of the client
    const ip = request.headers.get("CF-Connecting-IP");
    if (ip === null) {
      return new Response("Could not determine client IP", { status: 400 });
    }

    // Obtain an identifier for a Durable Object based on the client's IP address
    const id = env.RATE_LIMITER.idFromName(ip);

    try {
      const stub = env.RATE_LIMITER.get(id);
      const milliseconds_to_next_request = await stub.getMillisecondsToNextRequest();
      if (milliseconds_to_next_request > 0) {
        // Alternatively one could sleep for the necessary length of time
        return new Response("Rate limit exceeded", { status: 429 });
      }
    } catch (error) {
      return new Response("Could not connect to rate limiter", { status: 502 });
    }

    // TODO: Implement me
    return new Response("Call some upstream resource...")
  }
} satisfies ExportedHandler<Env>;

// Durable Object
export class RateLimiter extends DurableObject {
  static readonly milliseconds_per_request = 1;
  static readonly milliseconds_for_updates = 5000;
  static readonly capacity = 10000;

  tokens: number;

  constructor(ctx: DurableObjectState, env: Env) {
    super(ctx, env);
    this.tokens = RateLimiter.capacity;
  }

  async getMillisecondsToNextRequest(): Promise<number> {
    this.checkAndSetAlarm()

    let milliseconds_to_next_request = RateLimiter.milliseconds_per_request;
    if (this.tokens > 0) {
      this.tokens -= 1;
      milliseconds_to_next_request = 0;
    }

    return milliseconds_to_next_request;
  }

  private async checkAndSetAlarm() {
    let currentAlarm = await this.ctx.storage.getAlarm();
    if (currentAlarm == null) {
      this.ctx.storage.setAlarm(Date.now() +
        RateLimiter.milliseconds_for_updates * RateLimiter.milliseconds_per_request);
    }
  }

  async alarm() {
    if (this.tokens < RateLimiter.capacity) {
      this.tokens = Math.min(RateLimiter.capacity,
        this.tokens + RateLimiter.milliseconds_for_updates);
      this.checkAndSetAlarm()
    }
  }
}
```

</TabItem>
</Tabs>

While the token bucket algorithm is popular for implementing rate limiting and uses Durable Object features, there is a simpler approach:

<Tabs>
<TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Durable Object
export class RateLimiter extends DurableObject {
  static milliseconds_per_request = 1;
  static milliseconds_for_grace_period = 5000;

  constructor(ctx, env) {
    super(ctx, env);
    this.nextAllowedTime = 0;
  }

  async getMillisecondsToNextRequest() {
    const now = Date.now();

    this.nextAllowedTime = Math.max(now, this.nextAllowedTime);
    this.nextAllowedTime += RateLimiter.milliseconds_per_request;

    const value = Math.max(0,
      this.nextAllowedTime - now - RateLimiter.milliseconds_for_grace_period);
    return value;
  }
}
```

</TabItem>
<TabItem label="TypeScript" icon="seti:typescript">

```ts title="index.ts"
import { DurableObject } from "cloudflare:workers";

// Durable Object
export class RateLimiter extends DurableObject {
  static milliseconds_per_request = 1;
  static milliseconds_for_grace_period = 5000;

  nextAllowedTime: number;

  constructor(ctx: DurableObjectState, env: Env) {
    super(ctx, env);
    this.nextAllowedTime = 0;
  }

  async getMillisecondsToNextRequest(): Promise<number> {
    const now = Date.now();

    this.nextAllowedTime = Math.max(now, this.nextAllowedTime);
    this.nextAllowedTime += RateLimiter.milliseconds_per_request;

    const value = Math.max(0,
      this.nextAllowedTime - now - RateLimiter.milliseconds_for_grace_period);
    return value;
  }
}
```

</TabItem>
</Tabs>

Finally, configure your Wrangler file to include a Durable Object [binding](/durable-objects/get-started/tutorial/#5-configure-durable-object-bindings) and [migration](/durable-objects/reference/durable-objects-migrations/) based on the namespace and class name chosen previously.

<WranglerConfig>

```toml title="wrangler.toml"
name = "my-counter"

[[durable_objects.bindings]]
name = "RATE_LIMITER"
class_name = "RateLimiter"

[[migrations]]
tag = "v1"
new_classes = ["RateLimiter"]
```

</WranglerConfig>

### Related resources

* Learn more about Durable Object's [Alarms API](/durable-objects/api/alarms) and how to configure alarms.
* [Understand how to troubleshoot](/durable-objects/observability/troubleshooting/) common errors related with Durable Objects.
* Review how [Durable Objects are priced](/durable-objects/platform/pricing/), including pricing examples.

---

# Durable Object in-memory state

URL: https://developers.cloudflare.com/durable-objects/examples/durable-object-in-memory-state/

import { WranglerConfig } from "~/components";

This example shows you how Durable Objects are stateful, meaning in-memory state can be retained between requests. After a brief period of inactivity, the Durable Object will be evicted, and all in-memory state will be lost. The next request will reconstruct the object, but instead of showing the city of the previous request, it will display a message indicating that the object has been reinitialized. If you need your applications state to survive eviction, write the state to storage by using the [Storage API](/durable-objects/api/storage-api/), or by storing your data elsewhere.

```js
import { DurableObject } from 'cloudflare:workers';

// Worker
export default {
  async fetch(request, env) {
    return await handleRequest(request, env);
  }
}

async function handleRequest(request, env) {
  let id = env.LOCATION.idFromName("A");
  let obj = env.LOCATION.get(id);
  // Forward the request to the remote Durable Object.
  let resp = await obj.fetch(request);
  // Return the response to the client.
  return new Response(await resp.text());
}

// Durable Object
export class Location extends DurableObject {
  constructor(state, env) {
    this.state = state;
    // Upon construction, you do not have a location to provide.
    // This value will be updated as people access the Durable Object.
    // When the Durable Object is evicted from memory, this will be reset.
    this.location = null
  }

  // Handle HTTP requests from clients.
  async fetch(request) {
    let response = null

    if (this.location == null) {
      response = new String(`
This is the first request, you called the constructor, so this.location was null.
You will set this.location to be your city: (${request.cf.city}). Try reloading the page.`);
    } else {
      response = new String(`
The Durable Object was already loaded and running because it recently handled a request.

Previous Location: ${this.location}
New Location: ${request.cf.city}`);
    }

    // You set the new location to be the new city.
    this.location = request.cf.city;
    console.log(response);
    return new Response(response);
  }
}
```

Finally, configure your Wrangler file to include a Durable Object [binding](/durable-objects/get-started/tutorial/#5-configure-durable-object-bindings) and [migration](/durable-objects/reference/durable-objects-migrations/) based on the namespace and class name chosen previously.

<WranglerConfig>

```toml title="wrangler.toml"
name = "durable-object-in-memory-state"

[[durable_objects.bindings]]
name = "LOCATION"
class_name = "Location"

[[migrations]]
tag = "v1"
new_classes = ["Location"]
```

</WranglerConfig>

---

# Durable Object Time To Live

URL: https://developers.cloudflare.com/durable-objects/examples/durable-object-ttl/

import { TabItem, Tabs, GlossaryTooltip, WranglerConfig } from "~/components";

A common feature request for Durable Objects is a Time To Live (TTL) for Durable Object instances. Durable Objects give developers the tools to implement a custom TTL in only a few lines of code. This example demonstrates how to implement a TTL making use of <GlossaryTooltip term="alarm">`alarms`</GlossaryTooltip>. While this TTL will be extended upon every new request to the Durable Object, this can be customized based on a particular use case.

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Durable Object
export class MyDurableObject extends DurableObject {
    // Time To Live (TTL) in milliseconds
  timeToLiveMs = 1000;

    constructor(ctx, env) {
    super(ctx, env);

    this.ctx.blockConcurrencyWhile(async () => {
    await this.ctx.storage.setAlarm(Date.now() + this.timeToLiveMs);
    });
  }

  async fetch(_request) {
      // Increment the TTL immediately following every request to a Durable Object
    await this.ctx.storage.setAlarm(Date.now() + this.timeToLiveMs);
      ...
   }

  async alarm() {
    await this.ctx.storage.deleteAll();
  }
}

// Worker
export default {
  async fetch(request, env) {
    const id = env.MY_DURABLE_OBJECT.idFromName("foo");
        const stub = env.MY_DURABLE_OBJECT.get(id)
    return await stub.fetch(request);
  },
};
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
  MY_DURABLE_OBJECT: DurableObjectNamespace<MyDurableObject>;
}

// Durable Object
export class MyDurableObject extends DurableObject {
    // Time To Live (TTL) in milliseconds
  timeToLiveMs = 1000;

    constructor(ctx: DurableObjectState, env: Env) {
    super(ctx, env);

    this.ctx.blockConcurrencyWhile(async () => {
    await this.ctx.storage.setAlarm(Date.now() + this.timeToLiveMs);
    });
  }

  async fetch(_request: Request) {
      // Increment the TTL immediately following every request to a Durable Object
    await this.ctx.storage.setAlarm(Date.now() + this.timeToLiveMs);
      ...
   }

  async alarm() {
    await this.ctx.storage.deleteAll();
  }
}

// Worker
export default {
  async fetch(request, env) {
    const id = env.MY_DURABLE_OBJECT.idFromName("foo");
        const stub = env.MY_DURABLE_OBJECT.get(id)
    return await stub.fetch(request);
  },
} satisfies ExportedHandler<Env>;
```

</TabItem> </Tabs>

To test and deploy this example, configure your Wrangler file to include a Durable Object [binding](/durable-objects/get-started/tutorial/#5-configure-durable-object-bindings) and [migration](/durable-objects/reference/durable-objects-migrations/) based on the namespace and class name chosen previously.

<WranglerConfig>

```toml title="wrangler.toml"
name = "durable-object-ttl"

[[durable_objects.bindings]]
name = "MY_DURABLE_OBJECT"
class_name = "MyDurableObject"

[[migrations]]
tag = "v1"
new_classes = ["MyDurableObject"]
```

</WranglerConfig>

---

# Examples

URL: https://developers.cloudflare.com/durable-objects/examples/

import { ListExamples, GlossaryTooltip } from "~/components";

Explore the following <GlossaryTooltip term="code example">examples</GlossaryTooltip> for Durable Objects.

<ListExamples />

---

# Testing with Durable Objects

URL: https://developers.cloudflare.com/durable-objects/examples/testing-with-durable-objects/

```ts
import { unstable_dev } from "wrangler"
import type { UnstableDevWorker } from "wrangler"
import { describe, expect, it, beforeAll, afterAll } from "vitest"

describe("Worker", () => {
  let worker: UnstableDevWorker

  beforeAll(async () => {
    worker = await unstable_dev("src/index.ts", {
      experimental: { disableExperimentalWarning: true },
    });
  });

  afterAll(async () => {
    await worker.stop()
  })

  it("should deny request for short paths", async () => {
    const cases = {
      failures: ["/", "/foo", "/foo/", "/%2F"],
    }
    for (const path of cases.failures) {
      const resp = await worker.fetch(`http://example.com${path}`)
      if (resp) {
        const text = await resp.text()
        expect(text).toMatchInlineSnapshot('"path must be at least 5 characters"')
      }
    }
  })

  describe("durable object", () => {
    it("Should send text from a POST to a matching GET", async () => {
      const path = "/stuff1"
      const url = `http://example.com${path}`

      // The get request should wait for the post request to complete
      const getResponsePromise = worker.fetch(url)

      // The post request to the same path should receive a response that the text was consumed
      const postResponse = await worker.fetch(url, { method: "POST", body: "Hello World 12345" })
      expect(postResponse.status).toBe(200)
      const postText = await postResponse.text()
      expect(postText).toBe("The text was consumed!")

      // The get request should now receive the text
      const getResponse = await getResponsePromise
      expect(getResponse.status).toBe(200)
      const text = await getResponse.text()
      expect(text).toBe("Hello World 12345")
    })

    it("Shouldn't send text from a POST to a different GET", async () => {
      const path1 = "/stuff1"
      const path2 = "/stuff2"
      const url = (p: string) => `http://example.com${p}`

      // The get request should wait for the post request to complete
      const getResponsePromise1 = worker.fetch(url(path1))
      const getResponsePromise2 = worker.fetch(url(path2))

      // The post request to the same path should receive a response that the text was consumed
      const postResponse1 = await worker.fetch(url(path1), { method: "POST", body: "Hello World 12345" })
      expect(postResponse1.status).toBe(200)
      const postText1 = await postResponse1.text()
      expect(postText1).toBe("The text was consumed!")

      const postResponse2 = await worker.fetch(url(path2), { method: "POST", body: "Hello World 789" })
      expect(postResponse2.status).toBe(200)
      const postText2 = await postResponse2.text()
      expect(postText2).toBe("The text was consumed!")

      // The get request should now receive the text
      const getResponse1 = await getResponsePromise1
      expect(getResponse1.status).toBe(200)
      const text1 = await getResponse1.text()
      expect(text1).toBe("Hello World 12345")

      const getResponse2 = await getResponsePromise2
      expect(getResponse2.status).toBe(200)
      const text2 = await getResponse2.text()
      expect(text2).toBe("Hello World 789")
    })

    it("Should not send the same POST twice", async () => {
      const path = "/stuff1"
      const url = (p: string) => `http://example.com${p}`

      // The get request should wait for the post request to complete
      const getResponsePromise1 = worker.fetch(url(path))

      // The post request to the same path should receive a response that the text was consumed
      const postResponse1 = await worker.fetch(url(path), { method: "POST", body: "Hello World 12345" })
      expect(postResponse1.status).toBe(200)
      const postText1 = await postResponse1.text()
      expect(postText1).toBe("The text was consumed!")

      // The get request should now receive the text
      const getResponse1 = await getResponsePromise1
      expect(getResponse1.status).toBe(200)
      const text1 = await getResponse1.text()
      expect(text1).toBe("Hello World 12345")

      // The next get request should wait for the next post request to complete
      const getResponsePromise2 = worker.fetch(url(path))

      // Send a new POST with different text
      const postResponse2 = await worker.fetch(url(path), { method: "POST", body: "Hello World 789" })
      expect(postResponse2.status).toBe(200)
      const postText2 = await postResponse2.text()
      expect(postText2).toBe("The text was consumed!")

      // The get request should receive the new text, not the old text
      const getResponse2 = await getResponsePromise2
      expect(getResponse2.status).toBe(200)
      const text2 = await getResponse2.text()
      expect(text2).toBe("Hello World 789")
    })
  })
})
```

Find the [full code for this example on GitHub](https://github.com/jahands/do-demo).

---

# Use Workers KV from Durable Objects

URL: https://developers.cloudflare.com/durable-objects/examples/use-kv-from-durable-objects/

import { GlossaryTooltip, WranglerConfig } from "~/components";

The following Worker script shows you how to configure a <GlossaryTooltip term="Durable Object">Durable Object</GlossaryTooltip> to read from and/or write to a [Workers KV namespace](/kv/concepts/how-kv-works/). This is useful when using a Durable Object to coordinate between multiple clients, and allows you to serialize writes to KV and/or broadcast a single read from KV to hundreds or thousands of clients connected to a single Durable Object [using WebSockets](/durable-objects/best-practices/websockets/).

Prerequisites:

* A [KV namespace](/kv/api/) created via the Cloudflare dashboard or the [wrangler CLI](/workers/wrangler/install-and-update/).
* A [configured binding](/kv/concepts/kv-bindings/) for the `kv_namespace` in the Cloudflare dashboard or Wrangler file.
* A [Durable Object namespace binding](/workers/wrangler/configuration/#durable-objects).

Configure your Wrangler file as follows:

<WranglerConfig>

```toml
name = "my-worker"

kv_namespaces = [
  { binding = "YOUR_KV_NAMESPACE", id = "<id_of_your_namespace>" }
]

[durable_objects]
bindings = [
  { name = "YOUR_DO_CLASS", class_name = "YourDurableObject" }
]
```

</WranglerConfig>

```ts
import { DurableObject } from 'cloudflare:workers';

interface Env {
  YOUR_KV_NAMESPACE: KVNamespace;
  YOUR_DO_CLASS: DurableObjectNamespace;
}

export default {
  async fetch(req: Request, env: Env): Promise<Response> {
    // Assume each Durable Object is mapped to a roomId in a query parameter
    // In a production application, this will likely be a roomId defined by your application
    // that you validate (and/or authenticate) first.
    let url = new URL(req.url);
    let roomIdParam = url.searchParams.get("roomId");

    if (roomIdParam) {
      // Create (or get) a Durable Object based on that roomId.
      let durableObjectId = env.YOUR_DO_CLASS.idFromName(roomIdParam);
      // Get a "stub" that allows you to call that Durable Object
      let durableObjectStub = env.YOUR_DO_CLASS.get(durableObjectId);

      // Pass the request to that Durable Object and await the response
      // This invokes the constructor once on your Durable Object class (defined further down)
      // on the first initialization, and the fetch method on each request.
      //
      // You could pass the original Request to the Durable Object's fetch method
      // or a simpler URL with just the roomId.
      let response = await durableObjectStub.fetch(`http://do/${roomId}`);

      // This would return the value you read from KV *within* the Durable Object.
      return response;
    }
  }
}

export class YourDurableObject extends DurableObject {
  constructor(public state: DurableObjectState, env: Env) {
    this.state = state;
    // Ensure you pass your bindings and environmental variables into
    // each Durable Object when it is initialized
    this.env = env;
  }

  async fetch(request: Request) {
    // Error handling elided for brevity.
    // Write to KV
    await this.env.YOUR_KV_NAMESPACE.put("some-key");

    // Fetch from KV
    let val = await this.env.YOUR_KV_NAMESPACE.get("some-other-key");

    return Response.json(val);
  }
}
```

---

# Build a WebSocket server with WebSocket Hibernation

URL: https://developers.cloudflare.com/durable-objects/examples/websocket-hibernation-server/

import { TabItem, Tabs, WranglerConfig } from "~/components"

This example is similar to the [Build a WebSocket server](/durable-objects/examples/websocket-server/) example, but uses the WebSocket Hibernation API. The WebSocket Hibernation API should be preferred for WebSocket server applications built on Durable Objects, since it significantly decreases duration charge, and provides additional features that pair well with WebSocket applications. For more information, refer to [Use Durable Objects with WebSockets](/durable-objects/best-practices/websockets/).

:::note


WebSocket Hibernation is unavailable for outgoing WebSocket use cases. Hibernation is only supported when the Durable Object acts as a server. For use cases where outgoing WebSockets are required, refer to [Write a WebSocket client](/workers/examples/websockets/#write-a-websocket-client).


:::

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Worker
export default {
  async fetch(request, env, ctx) {
    if (request.url.endsWith("/websocket")) {
      // Expect to receive a WebSocket Upgrade request.
      // If there is one, accept the request and return a WebSocket Response.
      const upgradeHeader = request.headers.get('Upgrade');
      if (!upgradeHeader || upgradeHeader !== 'websocket') {
        return new Response('Durable Object expected Upgrade: websocket', { status: 426 });
      }

      // This example will refer to the same Durable Object,
      // since the name "foo" is hardcoded.
      let id = env.WEBSOCKET_HIBERNATION_SERVER.idFromName("foo");
      let stub = env.WEBSOCKET_HIBERNATION_SERVER.get(id);

      return stub.fetch(request);
    }

    return new Response(null, {
      status: 400,
      statusText: 'Bad Request',
      headers: {
        'Content-Type': 'text/plain',
      },
    });
  }
};

// Durable Object
export class WebSocketHibernationServer extends DurableObject {

  async fetch(request) {
    // Creates two ends of a WebSocket connection.
    const webSocketPair = new WebSocketPair();
    const [client, server] = Object.values(webSocketPair);

    // Calling `acceptWebSocket()` informs the runtime that this WebSocket is to begin terminating
    // request within the Durable Object. It has the effect of "accepting" the connection,
    // and allowing the WebSocket to send and receive messages.
    // Unlike `ws.accept()`, `state.acceptWebSocket(ws)` informs the Workers Runtime that the WebSocket
    // is "hibernatable", so the runtime does not need to pin this Durable Object to memory while
    // the connection is open. During periods of inactivity, the Durable Object can be evicted
    // from memory, but the WebSocket connection will remain open. If at some later point the
    // WebSocket receives a message, the runtime will recreate the Durable Object
    // (run the `constructor`) and deliver the message to the appropriate handler.
    this.ctx.acceptWebSocket(server);

    return new Response(null, {
      status: 101,
      webSocket: client,
    });
  }

  async webSocketMessage(ws, message) {
    // Upon receiving a message from the client, reply with the same message,
    // but will prefix the message with "[Durable Object]: " and return the
    // total number of connections.
    ws.send(`[Durable Object] message: ${message}, connections: ${this.ctx.getWebSockets().length}`);
  }

  async webSocketClose(ws, code, reason, wasClean) {
    // If the client closes the connection, the runtime will invoke the webSocketClose() handler.
    ws.close(code, "Durable Object is closing WebSocket");
  }
}

```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
  WEBSOCKET_HIBERNATION_SERVER: DurableObjectNamespace<WebSocketHibernationServer>;
}

// Worker
export default {
  async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {
    if (request.url.endsWith("/websocket")) {
      // Expect to receive a WebSocket Upgrade request.
      // If there is one, accept the request and return a WebSocket Response.
      const upgradeHeader = request.headers.get('Upgrade');
      if (!upgradeHeader || upgradeHeader !== 'websocket') {
        return new Response('Durable Object expected Upgrade: websocket', { status: 426 });
      }

      // This example will refer to the same Durable Object,
      // since the name "foo" is hardcoded.
      let id = env.WEBSOCKET_HIBERNATION_SERVER.idFromName("foo");
      let stub = env.WEBSOCKET_HIBERNATION_SERVER.get(id);

      return stub.fetch(request);
    }

    return new Response(null, {
      status: 400,
      statusText: 'Bad Request',
      headers: {
        'Content-Type': 'text/plain',
      },
    });
  }
};

// Durable Object
export class WebSocketHibernationServer extends DurableObject {

  async fetch(request: Request): Promise<Response> {
    // Creates two ends of a WebSocket connection.
    const webSocketPair = new WebSocketPair();
    const [client, server] = Object.values(webSocketPair);

    // Calling `acceptWebSocket()` informs the runtime that this WebSocket is to begin terminating
    // request within the Durable Object. It has the effect of "accepting" the connection,
    // and allowing the WebSocket to send and receive messages.
    // Unlike `ws.accept()`, `state.acceptWebSocket(ws)` informs the Workers Runtime that the WebSocket
    // is "hibernatable", so the runtime does not need to pin this Durable Object to memory while
    // the connection is open. During periods of inactivity, the Durable Object can be evicted
    // from memory, but the WebSocket connection will remain open. If at some later point the
    // WebSocket receives a message, the runtime will recreate the Durable Object
    // (run the `constructor`) and deliver the message to the appropriate handler.
    this.ctx.acceptWebSocket(server);

    return new Response(null, {
      status: 101,
      webSocket: client,
    });
  }

  async webSocketMessage(ws: WebSocket, message: ArrayBuffer | string) {
    // Upon receiving a message from the client, the server replies with the same message,
    // and the total number of connections with the "[Durable Object]: " prefix
    ws.send(`[Durable Object] message: ${message}, connections: ${this.ctx.getWebSockets().length}`);
  }

  async webSocketClose(ws: WebSocket, code: number, reason: string, wasClean: boolean) {
    // If the client closes the connection, the runtime will invoke the webSocketClose() handler.
    ws.close(code, "Durable Object is closing WebSocket");
  }
}
```

</TabItem> </Tabs>

Finally, configure your Wrangler file to include a Durable Object [binding](/durable-objects/get-started/tutorial/#5-configure-durable-object-bindings) and [migration](/durable-objects/reference/durable-objects-migrations/) based on the namespace and class name chosen previously.

<WranglerConfig>

```toml title="wrangler.toml"
name = "websocket-hibernation-server"

[[durable_objects.bindings]]
name = "WEBSOCKET_HIBERNATION_SERVER"
class_name = "WebSocketHibernationServer"

[[migrations]]
tag = "v1"
new_classes = ["WebSocketHibernationServer"]
```

</WranglerConfig>

### Related resources

* [Durable Objects: Edge Chat Demo with Hibernation](https://github.com/cloudflare/workers-chat-demo/).

---

# Build a WebSocket server

URL: https://developers.cloudflare.com/durable-objects/examples/websocket-server/

import { TabItem, Tabs, GlossaryTooltip, WranglerConfig } from "~/components"

This example shows how to build a WebSocket server using <GlossaryTooltip term="Durable Object">Durable Objects</GlossaryTooltip> and Workers. The example exposes an endpoint to create a new WebSocket connection. This WebSocket connection echos any message while including the total number of WebSocket connections currently established. For more information, refer to [Use Durable Objects with WebSockets](/durable-objects/best-practices/websockets/).

:::caution


WebSocket connections pin your Durable Object to memory, and so duration charges will be incurred so long as the WebSocket is connected (regardless of activity). To avoid duration charges during periods of inactivity, use the [WebSocket Hibernation API](/durable-objects/examples/websocket-hibernation-server/), which only charges for duration when JavaScript is actively executing.


:::

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

// Worker
export default {
  async fetch(request, env, ctx) {
    if (request.url.endsWith("/websocket")) {
      // Expect to receive a WebSocket Upgrade request.
      // If there is one, accept the request and return a WebSocket Response.
      const upgradeHeader = request.headers.get('Upgrade');
      if (!upgradeHeader || upgradeHeader !== 'websocket') {
        return new Response('Durable Object expected Upgrade: websocket', { status: 426 });
      }

      // This example will refer to the same Durable Object,
      // since the name "foo" is hardcoded.
      let id = env.WEBSOCKET_SERVER.idFromName("foo");
      let stub = env.WEBSOCKET_SERVER.get(id);

      return stub.fetch(request);
    }

    return new Response(null, {
      status: 400,
      statusText: 'Bad Request',
      headers: {
        'Content-Type': 'text/plain',
      },
    });
  }
};

// Durable Object
export class WebSocketServer extends DurableObject {
  currentlyConnectedWebSockets;

  constructor(ctx, env) {
    // This is reset whenever the constructor runs because
    // regular WebSockets do not survive Durable Object resets.
    //
    // WebSockets accepted via the Hibernation API can survive
    // a certain type of eviction, but we will not cover that here.
    super(ctx, env);
    this.currentlyConnectedWebSockets = 0;
  }

  async fetch(request) {
    // Creates two ends of a WebSocket connection.
    const webSocketPair = new WebSocketPair();
    const [client, server] = Object.values(webSocketPair);

    // Calling `accept()` tells the runtime that this WebSocket is to begin terminating
    // request within the Durable Object. It has the effect of "accepting" the connection,
    // and allowing the WebSocket to send and receive messages.
    server.accept();
    this.currentlyConnectedWebSockets += 1;

    // Upon receiving a message from the client, the server replies with the same message,
    // and the total number of connections with the "[Durable Object]: " prefix
    server.addEventListener('message', (event) => {
      server.send(`[Durable Object] currentlyConnectedWebSockets: ${this.currentlyConnectedWebSockets}`);
    });

    // If the client closes the connection, the runtime will close the connection too.
    server.addEventListener('close', (cls) => {
      this.currentlyConnectedWebSockets -= 1;
      server.close(cls.code, "Durable Object is closing WebSocket");
    });

    return new Response(null, {
      status: 101,
      webSocket: client,
    });
  }
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export interface Env {
  WEBSOCKET_SERVER: DurableObjectNamespace<WebSocketServer>;
}

// Worker
export default {
  async fetch(request, env, ctx): Promise<Response> {
    if (request.url.endsWith("/websocket")) {
      // Expect to receive a WebSocket Upgrade request.
      // If there is one, accept the request and return a WebSocket Response.
      const upgradeHeader = request.headers.get('Upgrade');
      if (!upgradeHeader || upgradeHeader !== 'websocket') {
        return new Response('Durable Object expected Upgrade: websocket', { status: 426 });
      }

      // This example will refer to the same Durable Object,
      // since the name "foo" is hardcoded.
      let id = env.WEBSOCKET_SERVER.idFromName("foo");
      let stub = env.WEBSOCKET_SERVER.get(id);

      return stub.fetch(request);
    }

    return new Response(null, {
      status: 400,
      statusText: 'Bad Request',
      headers: {
        'Content-Type': 'text/plain',
      },
    });
  }
} satisfies ExportedHandler<Env>;

// Durable Object
export class WebSocketServer extends DurableObject {
  currentlyConnectedWebSockets: number;

  constructor(ctx: DurableObjectState, env: Env) {
    // This is reset whenever the constructor runs because
    // regular WebSockets do not survive Durable Object resets.
    //
    // WebSockets accepted via the Hibernation API can survive
    // a certain type of eviction, but we will not cover that here.
    super(ctx, env);
    this.currentlyConnectedWebSockets = 0;
  }

  async fetch(request: Request): Promise<Response> {
    // Creates two ends of a WebSocket connection.
    const webSocketPair = new WebSocketPair();
    const [client, server] = Object.values(webSocketPair);

    // Calling `accept()` tells the runtime that this WebSocket is to begin terminating
    // request within the Durable Object. It has the effect of "accepting" the connection,
    // and allowing the WebSocket to send and receive messages.
    server.accept();
    this.currentlyConnectedWebSockets += 1;

    // Upon receiving a message from the client, the server replies with the same message,
    // and the total number of connections with the "[Durable Object]: " prefix
    server.addEventListener('message', (event: MessageEvent) => {
      server.send(`[Durable Object] currentlyConnectedWebSockets: ${this.currentlyConnectedWebSockets}`);
    });

    // If the client closes the connection, the runtime will close the connection too.
    server.addEventListener('close', (cls: CloseEvent) => {
      this.currentlyConnectedWebSockets -= 1;
      server.close(cls.code, "Durable Object is closing WebSocket");
    });

    return new Response(null, {
      status: 101,
      webSocket: client,
    });
  }
}
```

</TabItem> </Tabs>

Finally, configure your Wrangler file to include a Durable Object [binding](/durable-objects/get-started/tutorial/#5-configure-durable-object-bindings) and [migration](/durable-objects/reference/durable-objects-migrations/) based on the <GlossaryTooltip term="namespace">namespace</GlossaryTooltip> and class name chosen previously.

<WranglerConfig>

```toml title="wrangler.toml"
name = "websocket-server"

[[durable_objects.bindings]]
name = "WEBSOCKET_SERVER"
class_name = "WebSocketServer"

[[migrations]]
tag = "v1"
new_classes = ["WebSocketServer"]
```

</WranglerConfig>

### Related resources

* [Durable Objects: Edge Chat Demo](https://github.com/cloudflare/workers-chat-demo).

---

# Get started

URL: https://developers.cloudflare.com/durable-objects/get-started/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Tutorial with SQL API

URL: https://developers.cloudflare.com/durable-objects/get-started/tutorial-with-sql-api/

import { Render, TabItem, Tabs, PackageManagers, WranglerConfig } from "~/components";

This guide will instruct you through:

- Writing a JavaScript class that defines a Durable Object.
- Using Durable Objects SQL API to query a Durable Object's private, embedded SQLite database.
- Instantiating and communicating with a Durable Object from another Worker.
- Deploying a Durable Object and a Worker that communicates with a Durable Object.

If you wish to learn more about Durable Objects, refer to [What are Durable Objects?](/durable-objects/what-are-durable-objects/).

:::note[SQLite in Durable Objects Beta]

The new beta version of Durable Objects is available where each Durable Object has a private, embedded SQLite database. When deploying a new Durable Object class, users can [opt-in to a SQLite storage backend](/durable-objects/best-practices/access-durable-objects-storage/#sqlite-storage-backend) in order to access new [SQL API](/durable-objects/api/sql-storage/#exec), part of Durable Objects Storage API.

:::

## Prerequisites

<Render file="prereqs" product="workers" />

## 1. Enable Durable Objects in the dashboard

To enable Durable Objects, you will need to purchase the Workers Paid plan:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. Go to **Workers & Pages** > **Plans**.
3. Select **Purchase Workers Paid** and complete the payment process to enable Durable Objects.

## 2. Create a Worker project to access Durable Objects

You will access your Durable Object from a [Worker](/workers/). Your Worker application is an interface to interact with your Durable Object.

To create a Worker project, run:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"durable-object-starter"}
/>

Running `create cloudflare@latest` will install [Wrangler](/workers/wrangler/install-and-update/), the Workers CLI. You will use Wrangler to test and deploy your project.

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "hello-world",
		type: "Hello World Worker Using Durable Objects",
		lang: "TypeScript",
	}}
/>

This will create a new directory, which will include either a `src/index.js` or `src/index.ts` file to write your code and a [`wrangler.jsonc`](/workers/wrangler/configuration/) configuration file.

Move into your new directory:

```sh
cd durable-object-starter
```

## 3. Write a class to define a Durable Object that uses SQL API

Before you create and access a Durable Object, its behavior must be defined by an ordinary exported JavaScript class.

:::note

If you do not use JavaScript or TypeScript, you will need a [shim](https://developer.mozilla.org/en-US/docs/Glossary/Shim) to translate your class definition to a JavaScript class.
:::

Your `MyDurableObject` class will have a constructor with two parameters. The first parameter, `ctx`, passed to the class constructor contains state specific to the Durable Object, including methods for accessing storage. The second parameter, `env`, contains any bindings you have associated with the Worker when you uploaded it.

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
export class MyDurableObject extends DurableObject {
	constructor(ctx, env) {}
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
export class MyDurableObject extends DurableObject {
	constructor(ctx: DurableObjectState, env: Env) {
		// Required, as we're extending the base class.
		super(ctx, env)
	}
}
```

</TabItem> </Tabs>

Workers communicate with a Durable Object using [remote-procedure call](/workers/runtime-apis/rpc/#_top). Public methods on a Durable Object class are exposed as [RPC methods](/durable-objects/best-practices/create-durable-object-stubs-and-send-requests/) to be called by another Worker.

Your file should now look like:

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
export class MyDurableObject extends DurableObject {
	constructor(ctx: DurableObjectState, env: Env) {}

	async sayHello() {
		let result = this.ctx.storage.sql
			.exec("SELECT 'Hello, World!' as greeting")
			.one();
		return result.greeting;
	}
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
export class MyDurableObject extends DurableObject {
	constructor(ctx: DurableObjectState, env: Env) {
		// Required, as we're extending the base class.
		super(ctx, env)
	}

	async sayHello() {
		let result = this.ctx.storage.sql
			.exec("SELECT 'Hello, World!' as greeting")
			.one();
		return result.greeting;
	}
}
```

</TabItem> </Tabs>

In the code above, you have:

1. Defined a RPC method, `sayHello()`, that can be called by a Worker to communicate with a Durable Object.
2. Accessed a Durable Object's attached storage, which is a private SQLite database only accessible to the object, using [SQL API](/durable-objects/api/sql-storage/#exec) methods (`sql.exec()`) available on `ctx.storage` .
3. Returned an object representing the single row query result using `one()`, which checks that the query result has exactly one row.
4. Return the `greeting` column from the row object result.

## 4. Instantiate and communicate with a Durable Object

:::note

Durable Objects do not receive requests directly from the Internet. Durable Objects receive requests from Workers or other Durable Objects.
This is achieved by configuring a binding in the calling Worker for each Durable Object class that you would like it to be able to talk to. These bindings must be configured at upload time. Methods exposed by the binding can be used to communicate with particular Durable Objects.
:::

A Worker is used to [access Durable Objects](/durable-objects/best-practices/create-durable-object-stubs-and-send-requests/).

To communicate with a Durable Object, the Worker's fetch handler should look like the following:

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
export default {
	async fetch(request, env) {
		let id = env.MY_DURABLE_OBJECT.idFromName(new URL(request.url).pathname);

		let stub = env.MY_DURABLE_OBJECT.get(id);

		let greeting = await stub.sayHello();

		return new Response(greeting);
	},
};
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
export default {
	async fetch(request, env, ctx): Promise<Response> {
		let id = env.MY_DURABLE_OBJECT.idFromName(new URL(request.url).pathname);

		let stub = env.MY_DURABLE_OBJECT.get(id);

		let greeting = await stub.sayHello();

		return new Response(greeting);
	},
} satisfies ExportedHandler<Env>;
```

</TabItem> </Tabs>

In the code above, you have:

1. Exported your Worker's main event handlers, such as the `fetch()` handler for receiving HTTP requests.
2. Passed `env` into the `fetch()` handler. Bindings are delivered as a property of the environment object passed as the second parameter when an event handler or class constructor is invoked. By calling the `idFromName()` function on the binding, you use a string-derived object ID. You can also ask the system to [generate random unique IDs](/durable-objects/api/namespace/#newuniqueid). System-generated unique IDs have better performance characteristics, but require you to store the ID somewhere to access the Object again later.
3. Derived an object ID from the URL path. `MY_DURABLE_OBJECT.idFromName()` always returns the same ID when given the same string as input (and called on the same class), but never the same ID for two different strings (or for different classes). In this case, you are creating a new object for each unique path.
4. Constructed the stub for the Durable Object using the ID. A stub is a client object used to send messages to the Durable Object.
5. Called a Durable Object by invoking a RPC method, `sayHello()`, on the Durable Object, which returns a `Hello, World!` string greeting.
6. Received an HTTP response back to the client by constructing a HTTP Response with `return new Response()`.

Refer to [Access a Durable Object from a Worker](/durable-objects/best-practices/create-durable-object-stubs-and-send-requests/) to learn more about communicating with a Durable Object.

## 5. Configure Durable Object bindings

[Bindings](/workers/runtime-apis/bindings/) allow your Workers to interact with resources on the Cloudflare developer platform. The Durable Object bindings in your Worker project's [Wrangler configuration file](/workers/wrangler/configuration/) will include a binding name (for this guide, use `MY_DURABLE_OBJECT`) and the class name (`MyDurableObject`).

<WranglerConfig>

```toml
[[durable_objects.bindings]]
name = "MY_DURABLE_OBJECT"
class_name = "MyDurableObject"
```

</WranglerConfig>

The `[[durable_objects.bindings]]` section contains the following fields:

- `name` - Required. The binding name to use within your Worker.
- `class_name` - Required. The class name you wish to bind to.
- `script_name` - Optional. Defaults to the current [environment's](/durable-objects/reference/environments/) Worker code.

## 6. Configure Durable Object class with SQLite storage backend

A migration is a mapping process from a class name to a runtime state. You perform a migration when creating a new Durable Object class, or when renaming, deleting or transferring an existing Durable Object class.

Migrations are performed through the `[[migrations]]` configurations key in your Wrangler file.

The Durable Object migration to create a new Durable Object class with SQLite storage backend will look like the following in your Worker's Wrangler file:



<WranglerConfig>

```toml
[[migrations]]
tag = "v1" # Should be unique for each entry
new_sqlite_classes = ["MyDurableObject"] # Array of new classes
```

</WranglerConfig>

Refer to [Durable Objects migrations](/durable-objects/reference/durable-objects-migrations/) to learn more about the migration process.

## 7. Develop a Durable Object Worker locally

To test your Durable Object locally, run [`wrangler dev`](/workers/wrangler/commands/#dev):

```sh
npx wrangler dev
```

In your console, you should see a`Hello world` string returned by the Durable Object.

## 8. Deploy your Durable Object Worker

To deploy your Durable Object Worker:

```sh
npx wrangler deploy
```

Once deployed, you should be able to see your newly created Durable Object Worker on the [Cloudflare dashboard](https://dash.cloudflare.com/), **Workers & Pages** > **Overview**.

Preview your Durable Object Worker at `<YOUR_WORKER>.<YOUR_SUBDOMAIN>.workers.dev`.

By finishing this tutorial, you have successfully created, tested and deployed a Durable Object.

### Related resources

- [Create Durable Object stubs](/durable-objects/best-practices/create-durable-object-stubs-and-send-requests/)
- [Access Durable Objects Storage](/durable-objects/best-practices/access-durable-objects-storage/)
- [Miniflare](https://github.com/cloudflare/workers-sdk/tree/main/packages/miniflare) - Helpful tools for mocking and testing your Durable Objects.

---

# Tutorial

URL: https://developers.cloudflare.com/durable-objects/get-started/tutorial/

import { Render, TabItem, Tabs, PackageManagers, WranglerConfig } from "~/components";

This guide will instruct you through:

- Writing a Durable Object class.
- Writing a Worker which invokes methods on a Durable Object.
- Deploying a Durable Object.

If you wish to learn more about Durable Objects, refer to [What are Durable Objects?](/durable-objects/what-are-durable-objects/).

## Prerequisites

<Render file="prereqs" product="workers" />

## 1. Enable Durable Objects in the dashboard

To enable Durable Objects, you will need to purchase the Workers Paid plan:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account.
2. Go to **Workers & Pages** > **Plans**.
3. Select **Purchase Workers Paid** and complete the payment process to enable Durable Objects.

## 2. Create a Worker project

Durable Objects are accessed from a [Worker](/workers/).

To create a Worker project, run:

<PackageManagers
	type="create"
	pkg="cloudflare@latest"
	args={"durable-object-starter"}
/>

Running `create cloudflare@latest` will install [Wrangler](/workers/wrangler/install-and-update/), the Workers CLI. You will use Wrangler to test and deploy your project.

<Render
	file="c3-post-run-steps"
	product="workers"
	params={{
		category: "hello-world",
		type: "Hello World Worker using Durable Objects",
		lang: "JavaScript / TypeScript",
	}}
/>

This will create a new directory, which will include either a `src/index.js` or `src/index.ts` file to write your code and a [`wrangler.jsonc`](/workers/wrangler/configuration/) configuration file.

Move into your new directory:

```sh
cd durable-object-starter
```

## 3. Write a Durable Object class

Durable Objects are defined by a exporting a standard JavaScript class which extends from the `DurableObject` base class.

:::note

If you do not use JavaScript or TypeScript, you will need a [shim](https://developer.mozilla.org/en-US/docs/Glossary/Shim) to translate your class definition to a JavaScript class.
:::

Your `MyDurableObject` class will have a constructor with two parameters. The first parameter, `state`, passed to the class constructor contains state specific to the Durable Object, including methods for accessing storage. The second parameter, `env`, contains any bindings you have associated with the Worker when you uploaded it.

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

export class MyDurableObject extends DurableObject {
	constructor(state, env) {}
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export class MyDurableObject extends DurableObject {
	constructor(state: DurableObjectState, env: Env) {}
}
```

</TabItem> </Tabs>

Workers can invoke public methods defined on a Durable Object via Remote Procedure Call (RPC).

The `sayHello` method demonstrates this capability:

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
import { DurableObject } from "cloudflare:workers";

export class MyDurableObject extends DurableObject {
	constructor(state, env) {}

	async sayHello() {
		return "Hello, World!";
	}
}
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
import { DurableObject } from "cloudflare:workers";

export class MyDurableObject extends DurableObject {
	constructor(state: DurableObjectState, env: Env) {}

	async sayHello(): Promise<string> {
		return "Hello, World!";
	}
}
```

</TabItem> </Tabs>

## 4. Invoke methods on a Durable Object class

As mentioned previously, methods on a Durable Object class are invoked by a Worker. This is done by creating an ID refering to an instance of the Durable Object class, getting a stub that refers to a particular instance of a Durable Object class, and invoking methods on that stub.

The fetch handler should look like the following:

<Tabs> <TabItem label="JavaScript" icon="seti:javascript">

```js
// Worker
export default {
	async fetch(request, env) {
		// Every unique ID refers to an individual instance of the Durable Object class
		const id = env.MY_DURABLE_OBJECT.idFromName("foo");

		// A stub is a client used to invoke methods on the Durable Object
		const stub = env.MY_DURABLE_OBJECT.get(id);

		// Methods on the Durable Object are invoked via the stub
		const rpcResponse = await stub.sayHello();

		return new Response(rpcResponse);
	},
};
```

</TabItem> <TabItem label="TypeScript" icon="seti:typescript">

```ts
// Worker
export default {
	async fetch(request, env, ctx): Promise<Response> {
		// Every unique ID refers to an individual instance of the Durable Object class
		const id = env.MY_DURABLE_OBJECT.idFromName("foo");

		// A stub is a client used to invoke methods on the Durable Object
		const stub = env.MY_DURABLE_OBJECT.get(id);

		// Methods on the Durable Object are invoked via the stub
		const rpcResponse = await stub.sayHello();

		return new Response(rpcResponse);
	},
} satisfies ExportedHandler<Env>;
```

</TabItem> </Tabs>

## 5. Configure Durable Object bindings

To allow a Worker to invoke methods on a Durable Object, the Worker must have a [Durable Object binding](/workers/runtime-apis/bindings/) in the project's [Wrangler configuration file](/workers/wrangler/configuration/#durable-objects). The binding is configured to use a particular Durable Object class.

<WranglerConfig>

```toml
[[durable_objects.bindings]]
name = "MY_DURABLE_OBJECT"
class_name = "MyDurableObject"
```

</WranglerConfig>

The `[[durable_objects.bindings]]` section contains the following fields:

- `name` - Required. The binding name to use within your Worker.
- `class_name` - Required. The class name you wish to bind to.
- `script_name` - Optional. The name of the Worker if the Durable Object is external to this Worker.
- `environment` - Optional. The environment of the `script_name` to bind to.

Refer to [Wrangler Configuration](/workers/wrangler/configuration/#durable-objects) for more detail.

## 6. Configure Durable Object classes with migrations

A migration is a mapping process from a class name to a runtime state. You perform a migration when creating a new Durable Object class, or when renaming, deleting or transferring an existing Durable Object class.

Migrations are performed through the `[[migrations]]` configurations key in your Wrangler file.

The Durable Object migration to create a new Durable Object class will look like the following in your Worker's Wrangler file:



<WranglerConfig>

```toml
[[migrations]]
tag = "v1" # Should be unique for each entry
new_classes = ["MyDurableObject"] # Array of new classes
```

</WranglerConfig>

### 6.a Optional: Configure new Durable Object class for SQL storage

:::note[SQLite in Durable Objects Beta]

New beta version of Durable Objects is available where each Durable Object has a private, embedded SQLite database. SQL storage is opt-in during beta; otherwise, a Durable Object class has the standard, private key-value storage. Objects can access long-lived durable storage with the [Storage API](/durable-objects/api/storage-api/).

:::

A Durable Object class can only have a single storage type, which cannot be changed after the Durable Object class is created.

To configure SQL storage and API, replace `new_classes` with `new_sqlite_classes` in your Worker's Wrangler file:



<WranglerConfig>

```toml
[[migrations]]
tag = "v1" # Should be unique for each entry
new_sqlite_classes = ["MyDurableObject"] # Array of new classes
```

</WranglerConfig>

Refer to [Durable Objects migrations](/durable-objects/reference/durable-objects-migrations/) to learn more about the migration process.

## 7. Develop a Durable Object Worker locally

To test your Durable Object locally, run [`wrangler dev`](/workers/wrangler/commands/#dev):

```sh
npx wrangler dev
```

In your console, you should see a`Hello world` string returned by the Durable Object.

## 8. Deploy your Durable Object Worker

To deploy your Durable Object Worker:

```sh
npx wrangler deploy
```

Once deployed, you should be able to see your newly created Durable Object Worker on the [Cloudflare dashboard](https://dash.cloudflare.com/), **Workers & Pages** > **Overview**.

Preview your Durable Object Worker at `<YOUR_WORKER>.<YOUR_SUBDOMAIN>.workers.dev`.

By finishing this tutorial, you have successfully created, tested and deployed a Durable Object.

### Related resources

- [Send requests to Durable Objects](/durable-objects/best-practices/create-durable-object-stubs-and-send-requests/)
- [Miniflare](https://github.com/cloudflare/workers-sdk/tree/main/packages/miniflare) - Helpful tools for mocking and testing your Durable Objects.

---

# Metrics and GraphQL analytics

URL: https://developers.cloudflare.com/durable-objects/observability/graphql-analytics/

import { GlossaryTooltip } from "~/components";

<GlossaryTooltip term="Durable Object">Durable Objects</GlossaryTooltip> expose analytics for Durable Object namespace-level and request-level metrics.

The metrics displayed in the [Cloudflare dashboard](https://dash.cloudflare.com/) charts are queried from Cloudflareâ€™s [GraphQL Analytics API](/analytics/graphql-api/). You can access the metrics [programmatically via GraphQL](#query-via-the-graphql-api) or HTTP client.

:::note[Durable Object namespace]

A Durable Object namespace is a set of Durable Objects that can be addressed by name, backed by the same class. There is only one Durable Object namespace per class. A Durable Object namespace can contain any number of Durable Objects.
:::

## View metrics and analytics via the dashboard

Per-namespace analytics for Durable Objects are available in the Cloudflare dashboard. To view current and historical metrics for a namespace:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to [**Workers & Pages** > **Durable Objects**](https://dash.cloudflare.com/?to=/:account/workers/durable-objects).
3. View account-level Durable Objects usage.
4. Select an existing namespace.
5. Select the **Metrics** tab.

You can optionally select a time window to query. This defaults to the last 24 hours.

## Query via the GraphQL API

Durable Object metrics are powered by GraphQL.

The datasets that include Durable Object metrics include:

* `durableObjectsInvocationsAdaptiveGroups`
* `durableObjectsPeriodicGroups`
* `durableObjectsStorageGroups`
* `durableObjectsSubrequestsAdaptiveGroups`

Use [GraphQL Introspection](/analytics/graphql-api/features/discovery/introspection/) to get information on the fields exposed by each datasets.

### WebSocket metrics

Durable Objects using [WebSockets](/durable-objects/best-practices/websockets/) will see request metrics across several GraphQL datasets because WebSockets have different types of requests.

* Metrics for a WebSocket connection itself is represented in `durableObjectsInvocationsAdaptiveGroups` once the connection closes. Since WebSocket connections are long-lived, connections often do not terminate until the Durable Object terminates.
* Metrics for incoming and outgoing WebSocket messages on a WebSocket connection are available in `durableObjectsPeriodicGroups`. If a WebSocket connection uses [WebSocket Hibernation](/durable-objects/best-practices/websockets/#websocket-hibernation-api), incoming WebSocket messages are instead represented in `durableObjectsInvocationsAdaptiveGroups`.

## Example GraphQL query for Durable Objects

```js
  viewer {
    /*
    Replace with your account tag, the 32 hex character id visible at the beginning of any url
    when logged in to dash.cloudflare.com or under "Account ID" on the sidebar of the Workers & Pages Overview
    */
    accounts(filter: {accountTag: "your account tag here"}) {
      // Replace dates with a recent date
      durableObjectsInvocationsAdaptiveGroups(filter: {date_gt: "2023-05-23"}, limit: 1000) {
        sum {
          // Any other fields found through introspection can be added here
          requests
          responseBodySize
        }
      }
      durableObjectsPeriodicGroups(filter: {date_gt: "2023-05-23"}, limit: 1000) {
        sum {
          cpuTime
        }
      }
      durableObjectsStorageGroups(filter: {date_gt: "2023-05-23"}, limit: 1000) {
        max {
          storedBytes
        }
      }
    }
  }
```

Refer to the [Querying Workers Metrics with GraphQL](/analytics/graphql-api/tutorials/querying-workers-metrics/) tutorial for authentication and to learn more about querying Workers datasets.

---

# Observability

URL: https://developers.cloudflare.com/durable-objects/observability/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Platform

URL: https://developers.cloudflare.com/durable-objects/platform/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Troubleshooting

URL: https://developers.cloudflare.com/durable-objects/observability/troubleshooting/

## Debugging

[`wrangler dev`](/workers/wrangler/commands/#dev) and [`wrangler tail`](/workers/wrangler/commands/#tail) are both available to help you debug your Durable Objects.

The `wrangler dev --remote` command opens a tunnel from your local development environment to Cloudflare's global network, letting you test your Durable Objects code in the Workers environment as you write it.

`wrangler tail` displays a live feed of console and exception logs for each request served by your Worker code, including both normal Worker requests and Durable Object requests. After running `npx wrangler deploy`, you can use `wrangler tail` in the root directory of your Worker project and visit your Worker URL to see console and error logs in your terminal.

## Common errors

### No event handlers were registered. This script does nothing.

In your Wrangler file, make sure the `dir` and `main` entries point to the correct file containing your Worker code, and that the file extension is `.mjs` instead of `.js` if using ES modules syntax.

### Cannot apply `--delete-class` migration to class.

When deleting a migration using `npx wrangler deploy --delete-class <ClassName>`, you may encounter this error: `"Cannot apply --delete-class migration to class <ClassName> without also removing the binding that references it"`. You should remove the corresponding binding under `[durable_objects]` in the [Wrangler configuration file](/workers/wrangler/configuration/) before attempting to apply `--delete-class` again.

### Durable Object is overloaded.

A single instance of a Durable Object cannot do more work than is possible on a single thread. These errors mean the Durable Object has too much work to keep up with incoming requests:

- `Error: Durable Object is overloaded. Too many requests queued.` The total count of queued requests is too high.
- `Error: Durable Object is overloaded. Too much data queued.` The total size of data in queued requests is too high.
- `Error: Durable Object is overloaded. Requests queued for too long.` The oldest request has been in the queue too long.
- `Error: Durable Object is overloaded. Too many requests for the same object within a 10 second window.` The number of requests for a Durable Object is too high within a short span of time (10 seconds). This error indicates a more extreme level of overload.

To solve this error, you can either do less work per request, or send fewer requests. For example, you can split the requests among more instances of the Durable Object.

These errors and others that are due to overload will have an [`.overloaded` property](/durable-objects/best-practices/error-handling) set on their exceptions, which can be used to avoid retrying overloaded operations.

### Your account is generating too much load on Durable Objects. Please back off and try again later.

There is a limit on how quickly you can create new [stubs](/durable-objects/api/stub) for new or existing Durable Objects. Those lookups are usually cached, meaning attempts for the same set of recently accessed Durable Objects should be successful, so catching this error and retrying after a short wait is safe. If possible, also consider spreading those lookups across multiple requests.

### Durable Object reset because its code was updated.

Reset in error messages refers to in-memory state. Any durable state that has already been successfully persisted via `state.storage` is not affected.

Refer to [Global Uniqueness](/durable-objects/platform/known-issues/#global-uniqueness).

### Durable Object storage operation exceeded timeout which caused object to be reset.

To prevent indefinite blocking, there is a limit on how much time storage operations can take. In Durable Objects containing a sufficiently large number of key-value pairs, `deleteAll()` may hit that time limit and fail. When this happens, note that each `deleteAll()` call does make progress and that it is safe to retry until it succeeds. Otherwise contact [Cloudflare support](/support/contacting-cloudflare-support/).

### Your account is doing too many concurrent storage operations. Please back off and try again later.

Besides the suggested approach of backing off, also consider changing your code to use `state.storage.get(keys Array<string>)` rather than multiple individual `state.storage.get(key)` calls where possible.

---

# Known issues

URL: https://developers.cloudflare.com/durable-objects/platform/known-issues/

import { GlossaryTooltip } from "~/components";

Durable Objects is generally available. However, there are some known issues.

## Global uniqueness

Global uniqueness guarantees there is only a single instance of a <GlossaryTooltip term="Durable Object class">Durable Object class</GlossaryTooltip> with a given ID running at once, across the world.

Uniqueness is enforced upon starting a new event (such as receiving an HTTP request), and upon accessing storage.

After an event is received, if the event takes some time to execute and does not ever access its durable storage, then it is possible that the Durable Object may no longer be current, and some other instance of the same Durable Object ID will have been created elsewhere. If the event accesses storage at this point, it will receive an [exception](/durable-objects/observability/troubleshooting/). If the event completes without ever accessing storage, it may not ever realize that the Durable Object was no longer current.

A Durable Object may be replaced in the event of a network partition or a software update (including either an update of the Durable Object's class code, or of the Workers system itself). Enabling `wrangler tail` or [Cloudflare dashboard](https://dash.cloudflare.com/) logs requires a software update.

## Code updates

Code changes for Workers and Durable Objects are released globally in an eventually consistent manner. Because each Durable Object is globally unique, the situation can arise that a request arrives to the latest version of your Worker (running in one part of the world), which then calls to a unique Durable Object running the previous version of your code for a short period of time (typically seconds to minutes). If you create a [gradual deployment](/workers/configuration/versions-and-deployments/gradual-deployments/), this period of time is determined by how long your live deployment is configured to use more than one version.

For this reason, it is best practice to ensure that API changes between your Workers and Durable Objects are forward and backward compatible across code updates.

## Development tools

[`wrangler tail`](/workers/wrangler/commands/#tail) logs from requests that are upgraded to WebSockets are delayed until the WebSocket is closed. `wrangler tail` should not be connected to a Worker that you expect will receive heavy volumes of traffic.

The Workers editor in the [Cloudflare dashboard](https://dash.cloudflare.com/) allows you to interactively edit and preview your Worker and Durable Objects. In the editor, Durable Objects can only be talked to by a preview request if the Worker being previewed both exports the Durable Object class and binds to it. Durable Objects exported by other Workers cannot be talked to in the editor preview.

[`wrangler dev`](/workers/wrangler/commands/#dev) has read access to Durable Object storage, but writes will be kept in memory and will not affect persistent data. However, if you specify the `script_name` explicitly in the [Durable Object binding](/workers/runtime-apis/bindings/), then writes will affect persistent data. Wrangler will emit a warning in that case.

## Alarms in local development

Currently, when developing locally (using `npx wrangler dev`), Durable Object [alarm methods](/durable-objects/api/alarms) may fail after a hot reload (if you edit the code while the code is running locally).

To avoid this issue, when using Durable Object alarms, close and restart your `wrangler dev` command after editing your code.

---

# Limits

URL: https://developers.cloudflare.com/durable-objects/platform/limits/

import { Render, GlossaryTooltip } from "~/components";

Durable Objects are only available on the [Workers Paid plan](/workers/platform/pricing/#workers). Durable Objects limits are the same as [Workers Limits](/workers/platform/limits/), as well as the following limits that are specific to Durable Objects:

| Feature                                  | Limit for class with key-value storage backend                   | Limit for class with SQLite storage backend [^1] |
| ---------------------------------        | ---------------------------------------------------------------- | ----------------------------------------------- |
| Number of Objects                        | Unlimited (within an account or of a given class)                | Unlimited (within an account or of a given class)                |
| Maximum Durable Object namespaces        | 500 (identical to the [script limit](/workers/platform/limits/)) | 500 (identical to the [script limit](/workers/platform/limits/)) |
| Storage per account                      | 50 GB (can be raised by contacting Cloudflare) [^2]              | 50 GB (can be raised by contacting Cloudflare) [^2]              |
| Storage per class                        | Unlimited                                                        | Unlimited                                                        |
| Storage per Durable Object               | Unlimited                                                        | 1 GB [^3]                                                        |
| Key size                                 | 2 KiB (2048 bytes)                                               | Key and value combined cannot exceed 2 MB                        |
| Value size                               | 128 KiB (131072 bytes)                                           | Key and value combined cannot exceed 2 MB                        |
| WebSocket message size                   | 1 MiB (only for received messages)                               | 1 MiB (only for received messages)                               |
| CPU per request                          | 30s (including WebSocket messages) [^4]                          | 30s (including WebSocket messages) [^4]                          |

[^1]: The new beta version of Durable Objects is available where each Durable Object has a private, embedded SQLite database. When creating a Durable Object class, users can [opt-in to using SQL storage](/durable-objects/reference/durable-objects-migrations/#enable-sqlite-storage-backend-on-new-durable-object-class-migration).

[^2]: Durable Objects both bills and measures storage based on a gigabyte <br/> (1 GB = 1,000,000,000 bytes) and not a gibibyte (GiB). <br/>

[^3]: Will be raised to 10 GB for general availability.

[^4]: Each incoming HTTP request or WebSocket *message* resets the remaining available CPU time to 30 seconds. This allows the Durable Object to consume up to 30 seconds of compute after each incoming network request, with each new network request resetting the timer. If you consume more than 30 seconds of compute between incoming network requests, there is a heightened chance that the individual Durable Object is evicted and reset.

For Durable Object classes with [SQLite storage backend](/durable-objects/best-practices/access-durable-objects-storage/#sqlite-storage-backend) these SQL limits apply:

| SQL                                                      | Limit |
| -------------------------------------------------------- | ----- |
| Maximum number of columns per table                      | 100 |
| Maximum number of rows per table                         | Unlimited (excluding per-object storage limits) |
| Maximum string, `BLOB` or table row size                 | 2 MB |
| Maximum SQL statement length                             | 100 KB  |
| Maximum bound parameters per query                       | 100 |
| Maximum arguments per SQL function                       | 32 |
| Maximum characters (bytes) in a `LIKE` or `GLOB` pattern | 50 bytes |

<Render file="limits_increase" product="workers" />

## Frequently Asked Questions

### How much work can a single Durable Object do?

Durable Objects can scale horizontally across many Durable Objects. Each individual Object is inherently single-threaded.

- An individual Object has a soft limit of 1,000 requests per second. You can have an unlimited number of individual objects per namespace.
- A simple [storage](/durable-objects/api/storage-api/) `get()` on a small value that directly returns the response may realize a higher request throughput compared to a Durable Object that (for example) serializes and/or deserializes large JSON values.
- Similarly, a Durable Object that performs multiple `list()` operations may be more limited in terms of request throughput.

A Durable Object that receives too many requests will, after attempting to queue them, return an [overloaded](/durable-objects/observability/troubleshooting/#durable-object-is-overloaded) error to the caller.

### How many Durable Objects can I create?

Durable Objects are designed such that the number of individual objects in the system do not need to be limited, and can scale horizontally.

- You can create and run as many separate Durable Objects as you want within a given Durable Object <GlossaryTooltip term="namespace">namespace</GlossaryTooltip>.
- The main limit to your usage of Durable Objects is the total storage limit per account.
- If you need more storage, contact your account team or complete the [Limit Increase Request Form](https://forms.gle/ukpeZVLWLnKeixDu7) and we will contact you with next steps.

---

# Pricing

URL: https://developers.cloudflare.com/durable-objects/platform/pricing/

import { Render } from "~/components"

## Billing metrics

<Render file="durable_objects_pricing" product="workers" />

## Durable Objects billing examples

These examples exclude the costs for the Workers calling the Durable Objects. When modelling the costs of a Durable Object, note that:

* Inactive objects receiving no requests do not incur any duration charges.
* The [WebSocket Hibernation API](/durable-objects/best-practices/websockets/#websocket-hibernation-api) can dramatically reduce duration-related charges for Durable Objects communicating with clients over the WebSocket protocol, especially if messages are only transmitted occasionally at sparse intervals.

### Example 1

This example represents a simple Durable Object used as a co-ordination service invoked via HTTP.

* A single Durable Object was called by a Worker 1.5 million times
* It is active for 1,000,000 seconds in the month

In this scenario, the estimated monthly cost would be calculated as:

**Requests**:

* (1.5 million requests - included 1 million requests) x $0.15 / 1,000,000 = $0.075

**Compute Duration**:

* 1,000,000 seconds \* 128 MB / 1 GB = 128,000 GB-s
* (128,000 GB-s - included 400,000 GB-s) x $12.50 / 1,000,000 = $0.00

**Estimated total**: \~$0.075 (requests) + $0.00 (compute duration) + minimum $5/mo usage = $5.08 per month

### Example 2

This example represents a moderately trafficked Durable Objects based application using WebSockets to broadcast game, chat or real-time user state across connected clients:

* 100 Durable Objects have 50 WebSocket connections established to each of them.
* Clients send approximately one message a minute for eight active hours a day, every day of the month.

In this scenario, the estimated monthly cost would be calculated as:

**Requests**:

* 50 WebSocket connections \* 100 Durable Objects to establish the WebSockets = 5,000 connections created each day \* 30 days = 150,000 WebSocket connection requests.
* 50 messages per minute \* 100 Durable Objects \* 60 minutes \* 8 hours \* 30 days = 72,000,000 WebSocket message requests.
* 150,000 + (72 million requests / 20 for WebSocket message billing ratio) = 3.75 million billing request.
* (3.75 million requests - included 1 million requests) x $0.15 / 1,000,000 = $0.41.

**Compute Duration**:

* 100 Durable Objects \* 60 seconds \* 60 minutes \* 8 hours \* 30 days = 86,400,000 seconds.
* 86,400,000 seconds \* 128 MB / 1 GB = 11,059,200 GB-s.
* (11,059,200 GB-s - included 400,000 GB-s) x $12.50 / 1,000,000 = $133.24.

**Estimated total**: $0.41 (requests) + $133.24 (compute duration) + minimum $5/mo usage = $138.65 per month.

### Example 3

This example represents a horizontally scaled Durable Objects based application using WebSockets to communicate user-specific state to a single client connected to each Durable Object.

* 100 Durable Objects each have a single WebSocket connection established to each of them.
* Clients sent one message every second of the month so that the Durable Objects were active for the entire month.

In this scenario, the estimated monthly cost would be calculated as:

**Requests**:

* 100 WebSocket connection requests.
* 1 message per second \* 100 connections \* 60 seconds \* 60 minutes \* 24 hours \* 30 days = 259,200,000 WebSocket message requests.
* 100 + (259.2 million requests / 20 for WebSocket billing ratio) = 12,960,100 requests.
* (12.9 million requests - included 1 million requests) x $0.15 / 1,000,000 = $1.79.

**Compute Duration**:

* 100 Durable Objects \* 60 seconds \* 60 minutes \* 24 hours \* 30 days = 259,200,000 seconds
* 259,200,000 seconds \* 128 MB / 1 GB = 33,177,600 GB-s
* (33,177,600 GB-s - included 400,000 GB-s) x $12.50 / 1,000,000 = $409.72

**Estimated total**: $1.79 (requests) + $409.72 (compute duration) + minimum $5/mo usage = $416.51 per month

### Example 4

This example represents a moderately trafficked Durable Objects based application using WebSocket Hibernation to broadcast game, chat or real-time user state across connected clients:

* 100 Durable Objects each have 100 Hibernatable WebSocket connections established to each of them.
* Clients send one message per minute, and it takes 10ms to process a single message in the `webSocketMessage()` handler. Since each Durable Object handles 100 WebSockets, cumulatively each Durable Object will be actively executing JS for 1 second each minute (100 WebSockets \* 10ms).

In this scenario, the estimated monthly cost would be calculated as:

**Requests**:

* 100 WebSocket connections \* 100 Durable Objects to establish the WebSockets = 10,000 initial WebSocket connection requests.
* 100 messages per minute<sup>1</sup> \* 100 Durable Objects \* 60 minutes \* 24 hours \* 30 days = 432,000,000 requests.
* 10,000 + (432 million requests / 20 for WebSocket billing ratio) = 21,610,000 million requests.
* (21.6 million requests - included 1 million requests) x $0.15 / 1,000,000 = $3.09.

**Compute Duration**:

* 100 Durable Objects \* 1 second<sup>2</sup> \* 60 minutes \* 24 hours \* 30 days = 4,320,000 seconds
* 4,320,000 seconds \* 128 MB / 1 GB = 552,960 GB-s
* (552,960 GB-s - included 400,000 GB-s) x $12.50 / 1,000,000 = $1.91

**Estimated total**: $3.09 (requests) + $1.91 (compute duration) + minimum $5/mo usage = $10.00 per month

<sup>1</sup> 100 messages per minute comes from the fact that 100 clients connect to each DO, and each sends 1 message per minute.

<sup>2</sup> The example uses 1 second because each Durable Object is active for 1 second per minute. This can also be thought of as 432 million requests that each take 10 ms to execute (4,320,000 seconds).

## Storage API billing

<Render file="storage_api_pricing" product="workers" />

## Frequently Asked Questions

### Does an empty table / SQLite database contribute to my storage?

Yes, although minimal. Empty tables can consume at least a few kilobytes, based on the number of columns (table width) in the table. An empty SQLite database consumes approximately 12 KB of storage.

### Does metadata stored in Durable Objects count towards my storage?

All writes to a SQLite-backed Durable Object stores nominal amounts of metadata in internal tables in the Durable Object, which counts towards your billable storage.

The metadata remains in the Durable Object until you call [`deleteAll()`](/durable-objects/api/storage-api/#deleteall).

---

# Data location

URL: https://developers.cloudflare.com/durable-objects/reference/data-location/

import { GlossaryTooltip } from "~/components";

## Restrict Durable Objects to a jurisdiction

Jurisdictions are used to create <GlossaryTooltip term="Durable Object">Durable Objects</GlossaryTooltip> that only run and store data within a region to comply with local regulations such as the [GDPR](https://gdpr-info.eu/) or [FedRAMP](https://blog.cloudflare.com/cloudflare-achieves-fedramp-authorization/).

Workers may still access Durable Objects constrained to a jurisdiction from anywhere in the world. The jurisdiction constraint only controls where the Durable Object itself runs and persists data. Consider using [Regional Services](/data-localization/regional-services/) to control the regions from which Cloudflare responds to requests.

:::note[Logging]

A [`DurableObjectId`](/durable-objects/api/id) will be logged outside of the specified jurisdiction for billing and debugging purposes.

:::

Durable Objects can be restricted to a specific jurisdiction either by creating a [`DurableObjectNamespace`](/durable-objects/api/namespace/) restricted to a jurisdiction, or by creating an individual [`DurableObjectId`](/durable-objects/api/id) restricted to a jurisdiction:

```js
const euSubnamespace = env.MY_DURABLE_OBJECT.jurisdiction("eu");
const euId = euSubnamespace.newUniqueId();
// or
const euId = env.MY_DURABLE_OBJECT.newUniqueId({ jurisdiction: "eu" });
```

Methods on a [`DurableObjectNamespace`](/durable-objects/api/namespace/) that take a [`DurableObjectId`](/durable-objects/api/id) as a parameter will throw an exception if the parameter is associated with a different jurisdiction. To achieve this, a [`DurableObjectId`](/durable-objects/api/id) encodes its jurisdiction. As a consequence, it is possible to have the same name represent different IDs in different jurisdictions.

```js
const euId1 = env.MY_DURABLE_OBJECT.idFromName("my-name");
const euId2 = env.MY_DURABLE_OBJECT.jurisdiction("eu").idFromName("my-name");
console.assert(!euId1.equal(euId2), "This should always be true");
```

Methods on a [`DurableObjectNamespace`](/durable-objects/api/namespace/) that take a [`DurableObjectId`](/durable-objects/api/id) as a parameter will throw an exception if the parameter is associated with a different jurisdiction. However, these methods will not throw an exception if the [`DurableObjectNamespace`](/durable-objects/api/namespace/) is not associated with a jurisdiction. The common case is that any valid [`DurableObjectId`](/durable-objects/api/id) can be used in the top-level namespace's methods.

```js
const euSubnamespace = env.MY_DURABLE_OBJECT.jurisdiction("eu");
const euId = euSubnamespace.idFromName(name);
const stub = env.MY_DURABLE_OBJECT.get(euId);
```

### Supported locations

| Parameter | Location                     |
| --------- | ---------------------------- |
| eu        | The European Union           |
| fedramp   | FedRAMP-compliant data centers |

## Provide a location hint

Durable Objects, as with any stateful API, will often add response latency as requests must be forwarded to the data center where the Durable Object, or state, is located.

Durable Objects do not currently change locations after they are created<sup>1</sup>. By default, a Durable Object is instantiated in a data center close to where the initial `get()` request is made. This may not be in the same data center that the `get()` request is made from, but in most cases, it will be in close proximity.

:::caution[Initial requests to Durable Objects]

It can negatively impact latency to pre-create Durable Objects prior to the first client request or when the first client request is not representative of where the majority of requests will come from. It is better for latency to create Durable Objects in response to actual production traffic or provide explicit location hints.

:::

Location hints are the mechanism provided to specify the location that a Durable Object should be located regardless of where the initial `get()` request comes from.

To manually create Durable Objects in another location, provide an optional `locationHint` parameter to `get()`. Only the first call to `get()` for a particular Object will respect the hint.

```js
let durableObjectStub = OBJECT_NAMESPACE.get(id, { locationHint: "enam" });
```

:::caution

Hints are a best effort and not a guarantee. Unlike with jurisdictions, Durable Objects will not necessarily be instantiated in the hinted location, but instead instantiated in a data center selected to minimize latency from the hinted location.
:::

### Supported locations

| Parameter | Location                   |
| --------- | -------------------------- |
| wnam      | Western North America      |
| enam      | Eastern North America      |
| sam       | South America <sup>2</sup> |
| weur      | Western Europe             |
| eeur      | Eastern Europe             |
| apac      | Asia-Pacific               |
| oc        | Oceania                    |
| afr       | Africa <sup>2</sup>        |
| me        | Middle East <sup>2</sup>   |

<sup>1</sup> Dynamic relocation of existing Durable Objects is planned for the
future.

<sup>2</sup> Durable Objects currently do not spawn in this location. Instead,
the Durable Object will spawn in a nearby location which does support Durable
Objects. For example, Durable Objects hinted to South America spawn in Eastern
North America instead.

---

# Data security

URL: https://developers.cloudflare.com/durable-objects/reference/data-security/

This page details the data security properties of Durable Objects, including:

* Encryption-at-rest (EAR).
* Encryption-in-transit (EIT).
* Cloudflare's compliance certifications.

## Encryption at Rest

All Durable Object data, including metadata, is encrypted at rest. Encryption and decryption are automatic, do not require user configuration to enable, and do not impact the effective performance of Durable Objects.

Encryption keys are managed by Cloudflare and securely stored in the same key management systems we use for managing encrypted data across Cloudflare internally.

Encryption at rest is implemented using the Linux Unified Key Setup (LUKS) disk encryption specification and [AES-256](https://www.cloudflare.com/learning/ssl/what-is-encryption/), a widely tested, highly performant and industry-standard encryption algorithm.

## Encryption in Transit

Data transfer between a Cloudflare Worker, and/or between nodes within the Cloudflare network and Durable Objects is secured using the same [Transport Layer Security](https://www.cloudflare.com/learning/ssl/transport-layer-security-tls/) (TLS/SSL).

API access via the HTTP API or using the [wrangler](/workers/wrangler/install-and-update/) command-line interface is also over TLS/SSL (HTTPS).

## Compliance

To learn more about Cloudflare's adherence to industry-standard security compliance certifications, visit the Cloudflare [Trust Hub](https://www.cloudflare.com/trust-hub/compliance-resources/).

---

# Durable Objects migrations

URL: https://developers.cloudflare.com/durable-objects/reference/durable-objects-migrations/

import { GlossaryTooltip, WranglerConfig, Steps, Details } from "~/components";

A migration is a mapping process from a class name to a runtime state. This process communicates the changes to the Workers runtime and provides the runtime with instructions on how to deal with those changes.

To apply a migration, you need to:

1. Edit your `wrangler.toml / wrangler.json` file, as explained below.
2. Re-deploy your Worker using `npx wrangler deploy`.

You must initiate a migration process when you:

- Create a new <GlossaryTooltip term="Durable Object class">Durable Object class</GlossaryTooltip>.
- Rename a Durable Object class.
- Delete a Durable Object class.
- Transfer an existing Durable Objects class.

:::note

Updating the code for an existing Durable Object class does not require a migration. To update the code for an existing Durable Object class, run [`npx wrangler deploy`](/workers/wrangler/commands/#deploy). This is true even for changes to how the code interacts with persistent storage. Because of [global uniqueness](/durable-objects/platform/known-issues/#global-uniqueness), you do not have to be concerned about old and new code interacting with the same storage simultaneously. However, it is your responsibility to ensure that the new code is backwards compatible with existing stored data.

:::

## Create migration

The most common migration performed is a new class migration, which informs the runtime that a new Durable Object class is being uploaded. This is also the migration you need when creating your first Durable Object class.

To apply a Create migration:

<Steps>
1. Add the following lines to your `wrangler.toml / wrangler.json` file:

	<WranglerConfig>
	```toml
	[[migrations]]
	tag = "<v1>" # Migration identifier. This should be unique for each migration entry
	new_classes = ["<NewDurableObjectClass>"] # Array of new classes
	# For SQLite storage backend use new_sqlite_classes=["<NewDurableObjectClass>"] instead
	```
	</WranglerConfig>
	The Create migration contains:

	- A `tag` to identify the migration.
	- The array `new_classes`, which contains the new Durable Object class.

2. Ensure you reference the correct name of the Durable Object class in your Worker code.
3. Deploy the Worker.
</Steps>

<Details header="Create migration example">

To create a new Durable Object binding `DURABLE_OBJECT_A`, your `wrangler.toml / wrangler.json` file should look like the following:

<WranglerConfig>
```toml
# Creating a new Durable Object class
[[durable_objects.bindings]]
name = "DURABLE_OBJECT_A"
class_name = "DurableObjectAClass"

# Add the lines below for a Create migration.
[[migrations]]
tag = "v1"
new_classes = ["DurableObjectAClass"]
```
</WranglerConfig>

</Details>

## Delete migration

Running a Delete migration will delete all Durable Objects associated with the deleted class, including all of their stored data.

- Do not run a Delete migration on a class without first ensuring that you are not relying on the Durable Objects within that Worker anymore, that is, first remove the binding from the Worker.
- Copy any important data to some other location before deleting.
- You do not have to run a Delete migration on a class that was renamed or transferred.

To apply a Delete migration:

<Steps>
1. Remove the binding for the class you wish to delete from the `wrangler.toml / wrangler.json` file.
2. Remove references for the class you wish to delete from your Worker code.
3. Add the following lines to your `wrangler.toml / wrangler.json` file.

	<WranglerConfig>
	```toml
	[[migrations]]
	tag = "<v2>" # Migration identifier. This should be unique for each migration entry
	deleted_classes = ["<ClassToDelete>"] # Array of deleted class names
	```
	</WranglerConfig>
	The Delete migration contains:

	- A `tag` to identify the migration.
	- The array `deleted_classes`, which contains the deleted Durable Object classes.
4. Deploy the Worker.
</Steps>

<Details header = "Delete migration example">
To delete a Durable Object binding `DEPRECATED_OBJECT`, your `wrangler.toml / wrangler.json` file should look like the following:

<WranglerConfig>
```toml
# Remove the binding for the DeprecatedObjectClass DO
#[[durable_objects.bindings]]
#name = "DEPRECATED_OBJECT"
#class_name = "DeprecatedObjectClass"

[[migrations]]
tag = "v3" # Should be unique for each entry
deleted_classes = ["DeprecatedObjectClass"] # Array of new classes
```
</WranglerConfig>
</Details>

## Rename migration

Rename migrations are used to transfer stored Durable Objects between two Durable Object classes in the same Worker code file.

To apply a Rename migration:

<Steps>
1. Update the previous class name to the new class name by editing your `wrangler.toml / wrangler.json` file in the following way:

	<WranglerConfig>
	```toml
	[[durable_objects.bindings]]
	name = "<MY_DURABLE_OBJECT>"
	class_name = "<UpdatedDurableObject>" # Update the class name to the new class name

	[[migrations]]
	tag = "<v3>" # Migration identifier. This should be unique for each migration entry
	renamed_classes = [{from = "<OldDurableObject>", to = "<UpdatedDurableObject>" }] # Array of rename directives
	```
	</WranglerConfig>

	The Rename migration contains:
	- A `tag` to identify the migration.
	- The `renamed_classes` array, which contains objects with `from` and `to` properties.
		- `from` property is the old Durable Object class name.
		- `to` property is the renamed Durable Object class name.
2. Reference the new Durable Object class name in your Worker code.
3. Deploy the Worker.
</Steps>

<Details header = "Rename migration example">

To rename a Durable Object class, from `OldName` to `UpdatedName`, your `wrangler.toml / wrangler.json` file should look like the following:

<WranglerConfig>
```toml
# Before deleting the `DeprecatedClass` remove the binding for the `DeprecatedClass`.
# Update the binding for the `DurableObjectExample` to the new class name `UpdatedName`.
[[durable_objects.bindings]]
name = "MY_DURABLE_OBJECT"
class_name = "UpdatedName"

# Renaming classes
[[migrations]]
tag = "v3"
renamed_classes = [{from = "OldName", to = "UpdatedName" }] # Array of rename directives
```
</WranglerConfig>

</Details>

## Transfer migration

Transfer migrations are used to transfer stored Durable Objects between two Durable Object classes in different Worker code files.

If you want to transfer stored Durable Objects between two Durable Object classes in the same Worker code file, use [Rename migrations](#rename-migration) instead.

:::note
Do not run a [Create migration](#create-migration) for the destination class before running a Transfer migration. The Transfer migration will create the destination class for you.
:::

To apply a Transfer migration:

<Steps>
1. Edit your `wrangler.toml / wrangler.json` file in the following way:

	<WranglerConfig>
	```toml
	[[durable_objects.bindings]]
	name = "<MY_DURABLE_OBJECT>"
	class_name = "<DestinationDurableObjectClass>"

	[[migrations]]
	tag = "<v4>" # Migration identifier. This should be unique for each migration entry
	transferred_classes = [{from = "<SourceDurableObjectClass>", from_script = "<SourceWorkerScript>", to = "<DestinationDurableObjectClass>" }]
	```
	</WranglerConfig>
	The Transfer migration contains:
	- A `tag` to identify the migration.
	- The `transferred_class` array, which contains objects with `from`, `from_script`, and `to` properties.
	  - `from` property is the name of the source Durable Object class.
	  - `from_script` property is the name of the source Worker script.
	  - `to` property is the name of the destination Durable Object class.
2. Ensure you reference the name of the new, destination Durable Object class in your Worker code.
3. Deploy the Worker.
</Steps>

<Details header = "Transfer migration example">

You can transfer stored Durable Objects from `DurableObjectExample` to `TransferredClass` from a Worker script named `OldWorkerScript`. The configuration of the `wrangler.toml / wrangler.json` file for your new Worker code (destination Worker code) would look like this:

<WranglerConfig>
```toml
# destination worker
[[durable_objects.bindings]]
name = "MY_DURABLE_OBJECT"
class_name = "TransferredClass"

# Transferring class
[[migrations]]
tag = "v4"
transferred_classes = [{from = "DurableObjectExample", from_script = "OldWorkerScript", to = "TransferredClass" }]
```
</WranglerConfig>

</Details>
## Migration Wrangler configuration

- Migrations are performed through the `[[migrations]]` configurations key in your `wrangler.toml` file or `migration` key in your `wrangler.json` file.

- Migrations require a migration tag, which is defined by the `tag` property in each migration entry.

- Migration tags are treated like unique names and are used to determine which migrations have already been applied. Once a given Worker code has a migration tag set on it, all future Worker code deployments must include a migration tag.

- The migration list is an ordered array of tables, specified as a top-level key in your `wrangler` configuration file. The migration list is inherited by all environments and cannot be overridden by a specific environment.

- All migrations are applied at deployment. Each migration can only be applied once per [environment](/durable-objects/reference/environments/).

- Each migration in the list can have multiple directives, and multiple migrations can be specified as your project grows in complexity.

:::caution[Important]
- The destination class (the class that stored Durable Objects are being transferred to) for a Rename or Transfer migration must be exported by the deployed Worker.

- You should not create the destination Durable Object class before running a Rename or Transfer migration. The migration will create the destination class for you.

- After a Rename or Transfer migration, requests to the destination Durable Object class will have access to the source Durable Object's stored data.

- After a migration, any existing bindings to the original Durable Object class (for example, from other Workers) will automatically forward to the updated destination class. However, any Workers bound to the updated Durable Object class must update their Durable Object binding configuration in the `wrangler` configuration file for their next deployment.
:::

:::note
Note that `.toml` files do not allow line breaks in inline tables (the `{key = "value"}` syntax), but line breaks in the surrounding inline array are acceptable.
:::

{/* ## Examples of Durable Object migration

To illustrate an example migrations workflow, the `DurableObjectExample` class can be initially defined with:

<WranglerConfig>

```toml
# Creating a new Durable Object class
[[migrations]]
tag = "v1" # Migration identifier. This should be unique for each migration entry
new_classes = ["DurableObjectExample"] # Array of new classes
```

</WranglerConfig>

You can rename the `DurableObjectExample` class to `UpdatedName` and delete an outdated `DeprecatedClass` entirely. You can create separate migrations for each operation, or combine them into a single migration as shown below. */}

## Enable SQLite storage backend on new Durable Object class migration

:::note[SQLite in Durable Objects Beta]

The new beta version of Durable Objects is available where each Durable Object has a private, embedded SQLite database. When deploying a new Durable Object class, users can opt-in to a SQLite storage backend in order to access new [SQL API](/durable-objects/api/sql-storage/#exec). Otherwise, a Durable Object class has a key-value storage backend.

:::

To allow a new Durable Object class to use a SQLite storage backend, use `new_sqlite_classes` on the migration in your Worker's `wrangler` configuration file:

<WranglerConfig>

```toml
[[migrations]]
tag = "v1" # Should be unique for each entry
new_sqlite_classes = ["MyDurableObject"] # Array of new classes
```

</WranglerConfig>

For an example of a new class migration, refer to [Get started: Configure Durable Object class with SQLite storage backend](/durable-objects/get-started/tutorial-with-sql-api/#6-configure-durable-object-class-with-sqlite-storage-backend).

You cannot enable a SQLite storage backend on an existing, deployed Durable Object class, so setting `new_sqlite_classes` on later migrations will fail with an error. Automatic migration of deployed classes from their key-value storage backend to SQLite storage backend will be available in the future.

---

# Environments

URL: https://developers.cloudflare.com/durable-objects/reference/environments/

import { WranglerConfig } from "~/components";

[Wrangler](/workers/wrangler/install-and-update/) allows you to deploy the same Worker application with different configuration for each [environment](/workers/wrangler/environments/).

If you are using Wrangler environments, you must specify any [Durable Object bindings](/workers/runtime-apis/bindings/) you wish to use on a per-environment basis.

Durable Object bindings are not inherited. For example, you can define an environment named `staging` as below:

<WranglerConfig>

```toml
[env.staging]
durable_objects.bindings = [
  {name = "EXAMPLE_CLASS", class_name = "DurableObjectExample"}
]
```

</WranglerConfig>

Because Wrangler appends the [environment name](/workers/wrangler/environments/) to the top-level name when publishing, for a Worker named `worker-name` the above example is equivalent to:



<WranglerConfig>

```toml
[env.staging]
durable_objects.bindings = [
  {name = "EXAMPLE_CLASS", class_name = "DurableObjectExample", script_name = "worker-name-staging"}
]
```

</WranglerConfig>

`"EXAMPLE_CLASS"` in the staging environment is bound to a different Worker code name compared to the top-level `"EXAMPLE_CLASS"` binding, and will therefore access different Durable Objects with different persistent storage.

If you want an environment-specific binding that accesses the same Objects as the top-level binding, specify the top-level Worker code name explicitly using `script_name`:



<WranglerConfig>

```toml
[env.another]
durable_objects.bindings = [
  {name = "EXAMPLE_CLASS", class_name = "DurableObjectExample", script_name = "worker-name"}
]
```

</WranglerConfig>

---

# Glossary

URL: https://developers.cloudflare.com/durable-objects/reference/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's Durable Objects documentation.

<Glossary product="durable-objects" />

---

# In-memory state in a Durable Object

URL: https://developers.cloudflare.com/durable-objects/reference/in-memory-state/

import { GlossaryTooltip } from "~/components";

In-memory state means that each <GlossaryTooltip term="Durable Object">Durable Object</GlossaryTooltip> has one active instance at any particular time. All requests sent to that Durable Object are handled by that same instance. You can store some state in memory.

Variables in a Durable Object will maintain state as long as your Durable Object is not evicted from memory.

A common pattern is to initialize a Durable Object from [persistent storage](/durable-objects/api/storage-api/) and set instance variables the first time it is accessed. Since future accesses are routed to the same Durable Object, it is then possible to return any initialized values without making further calls to persistent storage.

```js
export class Counter {
	constructor(state, env) {
		this.state = state;
		// `blockConcurrencyWhile()` ensures no requests are delivered until
		// initialization completes.
		this.state.blockConcurrencyWhile(async () => {
			let stored = await this.state.storage.get("value");
			// After initialization, future reads do not need to access storage.
			this.value = stored || 0;
		});
	}

	// Handle HTTP requests from clients.
	async fetch(request) {
		// use this.value rather than storage
	}
}
```

A given instance of a Durable Object may share global memory with other instances defined in the same Worker code.

In the example above, using a global variable `value` instead of the instance variable `this.value` would be incorrect. Two different instances of `Counter` will each have their own separate memory for `this.value`, but might share memory for the global variable `value`, leading to unexpected results. Because of this, it is best to avoid global variables.

:::note[Built-in caching]

The Durable Object's storage has a built-in in-memory cache of its own. If you use `get()` to retrieve a value that was read or written recently, the result will be instantly returned from cache. Instead of writing initialization code like above, you could use `get("value")` whenever you need it, and rely on the built-in cache to make this fast. Refer to the [Build a counter example](/durable-objects/examples/build-a-counter/) to learn more about this approach.

However, in applications with more complex state, explicitly storing state in your Object may be easier than making Storage API calls on every access. Depending on the configuration of your project, write your code in the way that is easiest for you.

:::

---

# Reference

URL: https://developers.cloudflare.com/durable-objects/reference/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Tutorials

URL: https://developers.cloudflare.com/durable-objects/tutorials/

import { GlossaryTooltip, ListTutorials } from "~/components"

View <GlossaryTooltip term="tutorial">tutorials</GlossaryTooltip> to help you get started with Durable Objects.

<ListTutorials />

---

# Demos

URL: https://developers.cloudflare.com/email-routing/email-workers/demos/

import { ExternalResources, GlossaryTooltip } from "~/components"

Learn how you can use Email Workers within your existing architecture.

## Demos

Explore the following <GlossaryTooltip term="demo application">demo applications</GlossaryTooltip> for Email Workers.

<ExternalResources type="apps" products={["Email Workers"]} />

---

# Edit Email Workers

URL: https://developers.cloudflare.com/email-routing/email-workers/edit-email-workers/

import { Render } from "~/components"

Adding or editing Email Workers is straightforward. You can rename, delete or edit Email Workers, as well as change the routes bound to a specific Email Worker.

## Add an Email worker

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.

2. Go to **Email** > **Email Routing** > **Email Workers**.

3. Select  **Create**.

<Render file="enable-create-worker" />

## Edit an Email Worker

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.

2. Go to **Email** > **Email Routing** > **Email Workers**.

3. Find the Email Worker you want to rename, and select the three-dot button next to it.

4. Select **Code editor**.

5. Make the appropriate changes to your code.

6. Select **Save and deploy** when you are finished editing.

## Rename Email Worker

When you rename an Email Worker, you will lose the route that was previously bound to it. You will need to configure the route again after renaming the Email Worker.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.

2. Go to **Email** > **Email Routing** > **Email workers**.

3. Find the Email Worker you want to rename, and select the three-dot button next to it.

4. From the drop-down menu, select **Manage Worker**.

5. Select **Manage Service** > **Rename service**, and fill in the new Email Workerâ€™s name.

6. Select **Continue** > **Move**.

7. Acknowledge the warning and select **Finish**.

8. Now, go back to **Email** > **Email Routing**.

9. In **Routes** find the custom address you previously had associated with your Email Worker, and select **Edit**.

10. In the **Destination** drop-down menu, select your renamed Email Worker.

11. Select **Save**.

## Edit route

The following steps show how to change a route associated with an Email Worker.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.

2. Go to **Email** > **Email Routing** > **Email workers**.

3. Find the Email Worker you want to change the associated route, and select  **route** on its card.

4. Select **Edit** to make the required changes.

5. Select **Save** to finish.

## Delete an Email Worker

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.

2. Go to **Email** > **Email Routing** > **Email workers**.

3. Find the Email Worker you want to delete, and select the three-dot button next to it.

4. From the drop-down menu, select  **Manage Worker**.

5. Select **Manage Service** > **Delete**.

6. Type the name of the Email Worker to confirm you want to delete it, and select **Delete**.

---

# Enable Email Workers

URL: https://developers.cloudflare.com/email-routing/email-workers/enable-email-workers/

import { Render } from "~/components"

Follow these steps to enable and add your first Email Worker. If you have never used Cloudflare Workers before, Cloudflare will create a subdomain for you, and assign you to the Workers [free pricing plan](/workers/platform/pricing/).

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.

2. Go to **Email** > **Email Routing** > **Email Workers**.

3. Select  **Get started**.

<Render file="enable-create-worker" />

---

# Email Workers

URL: https://developers.cloudflare.com/email-routing/email-workers/

With Email Workers you can leverage the power of Cloudflare Workers to implement any logic you need to process your emails and create complex rules. These rules determine what happens when you receive an email.

Creating your own rules with Email Workers is as easy or complex as you want. You can begin using one of the starter templates that are pre-populated with code for popular use-cases. These templates allow you to create a blocklist, allowlist, or send notifications to Slack.

If you prefer, you can skip the templates and use custom code. You can, for example, create logic that only accepts messages from a specific address, and then forwards them to one or more of your verified email addresses, while also alerting you on Slack.

The following is an example of an allowlist Email Worker:

```js
export default {
  async email(message, env, ctx) {
    const allowList = ["friend@example.com", "coworker@example.com"];
    if (allowList.indexOf(message.from) == -1) {
      message.setReject("Address not allowed");
    } else {
      await message.forward("inbox@corp");
    }
  }
}
```

Refer to the [Workers Languages](/workers/languages/) for more information regarding the languages you can use with Workers.

## How to use Email Workers

To use Email Routing with Email Workers there are three steps involved:

1. Creating the Email Worker.
2. Adding the logic to your Email Worker (like email addresses allowed or blocked from sending you emails).
3. Binding the Email Worker to a route. This is the email address that forwards emails to the Worker.

The route, or email address, bound to the Worker forwards emails to your Email Worker. The logic in the Worker will then decide if the email is forwarded to its final destination or dropped, and what further actions (if any) will be applied.

For example, say that you create an allowlist Email Worker and bind it to a `hello@my-company.com` route. This route will be the email address you share with the world, to make sure that only email addresses on your allowlist are forwarded to your destination address. All other emails will be dropped.

## Limits

If you encounter any allocation errors while using Email Workers, refer to [Limits](/email-routing/limits/#email-workers-size-limits) for more information.

---

# Reply to emails from Workers

URL: https://developers.cloudflare.com/email-routing/email-workers/reply-email-workers/

You can reply to incoming emails with another new message and implement smart auto-responders programmatically, adding any content and context in the main body of the message. Think of a customer support email automatically generating a ticket and returning the link to the sender, an out-of-office reply with instructions when you are on vacation, or a detailed explanation of why you rejected an email.

Reply to emails is a new method of the [`EmailMessage` object](/email-routing/email-workers/runtime-api/#emailmessage-definition) in the Runtime API. Here is how it works:

```js
import { EmailMessage } from "cloudflare:email";
import { createMimeMessage } from "mimetext";

export default {
  async email(message, env, ctx) {

    const ticket = createTicket(message);

    const msg = createMimeMessage();
    msg.setHeader("In-Reply-To", message.headers.get("Message-ID"));
    msg.setSender({ name: "Thank you for your contact", addr: "<SENDER>@example.com" });
    msg.setRecipient(message.from);
    msg.setSubject("Email Routing Auto-reply");
    msg.addMessage({
      contentType: 'text/plain',
      data: `We got your message, your ticket number is ${ ticket.id }`
    });

    const replyMessage = new EmailMessage(
      "<SENDER>@example.com",
      message.from,
      msg.asRaw()
    );

    await message.reply(replyMessage);
  }
}
```

To mitigate security risks and abuse, replying to incoming emails has a few requirements:

* The incoming email has to have valid [DMARC](https://www.cloudflare.com/learning/dns/dns-records/dns-dmarc-record/).
* The email can only be replied to once in the same `EmailMessage` event.
* The `In-Reply-To` header of the reply message must be set to the `Message-ID` of the incoming message.
* The recipient in the reply must match the incoming sender.
* The outgoing sender domain must match the same domain that received the email.

If these and other internal conditions are not met, then `reply()` will fail with an exception, otherwise you can freely compose your reply message and send it back to the original sender.

---

# Runtime API

URL: https://developers.cloudflare.com/email-routing/email-workers/runtime-api/

## Background

An `EmailEvent` is the event type to programmatically process your emails with a Worker. You can reject, forward, or drop emails according to the logic you construct in your Worker.

***

## Syntax: Service Worker

`EmailEvent` can be handled in Workers functions written using the Service Worker syntax by attaching to the `email` event with `addEventListener`:

```js
addEventListener("email", (event) => {
  event.message.forward("<YOUR_EMAIL>");
});
```

### Properties



* `event.message` EmailMessage

  * An [`EmailMessage` object](#emailmessage-definition).



***

## Syntax: ES modules

`EmailEvent` can be handled in Workers functions written using the [ES modules format](/workers/reference/migrate-to-module-workers/) by adding an `email` function to your module's exported handlers:

```js
export default {
  async email(message, env, ctx) {
    message.forward("<YOUR_EMAIL>");
  },
};
```

### Parameters



* `message` EmailMessage

  * An [`EmailMessage` object](#emailmessage-definition).

* `env` object

  * An object containing the bindings associated with your Worker using ES modules format, such as KV namespaces and Durable Objects.

* `ctx` object
  * An object containing the context associated with your Worker using ES modules format. Currently, this object just contains the `waitUntil` function.



***

## `EmailMessage` definition

```ts
 interface EmailMessage<Body = unknown> {
  readonly from: string;
  readonly to: string;
  readonly headers: Headers;
  readonly raw: ReadableStream;
  readonly rawSize: number;

  public constructor(from: string, to: string, raw: ReadableStream | string);

  setReject(reason: string): void;
  forward(rcptTo: string, headers?: Headers): Promise<void>;
  reply(message: EmailMessage): Promise<void>;
}
```



* `from` string

  * `Envelope From` attribute of the email message.

* `to` string

  * `Envelope To` attribute of the email message.

* `headers` Headers

  * A [`Headers` object](https://developer.mozilla.org/en-US/docs/Web/API/Headers).

* `raw` ReadableStream

  * [Stream](/workers/runtime-apis/streams/readablestream) of the email message content.

* `rawSize` number

  * Size of the email message content.

* <code>setReject(reasonstring)</code> : void

  * Reject this email message by returning a permanent SMTP error back to the connecting client, including the given reason.

* <code>forward(rcptTostring, headersHeadersoptional)</code> : Promise

  * Forward this email message to a verified destination address of the account. If you want, you can add extra headers to the email message. Only `X-*` headers are allowed.
  * When the promise resolves, the message is confirmed to be forwarded to a verified destination address.

* <code>reply(messageEmailMessage)</code> : Promise

  * Reply to the sender of this email message with a new EmailMessage object.
  * When the promise resolves, the message is confirmed to be replied.

---

# Send emails from Workers

URL: https://developers.cloudflare.com/email-routing/email-workers/send-email-workers/

import { Render, WranglerConfig } from "~/components"

<Render file="send-emails-workers-intro" params={{ one: "Then, create a new binding in the Wrangler configuration file:" }} />

<WranglerConfig>

```toml
send_email = [
    {name = "<NAME_FOR_BINDING>", destination_address = "<YOUR_EMAIL>@example.com"},
]
```

</WranglerConfig>

## Types of bindings

There are three types of bindings:

* **No attribute defined**: When you do not define an attribute, the binding has no restrictions in place. You can use it to send emails to any verified email address [through Email Routing](/email-routing/setup/email-routing-addresses/#destination-addresses).
* **`destination_address`**: When you define the `destination_address` attribute, you create a targeted binding. This means you can only send emails to the chosen email address. For example, `{type = "send_email", name = "<NAME_FOR_BINDING>", destination_address = "<YOUR_EMAIL>@example.com"}`. <br/> For this particular binding, when you call the `send_email` function you can pass `null` or `undefined` to your Worker and it will assume the email address specified in the binding.
* **`allowed_destination_addresses`**: When you specify this attribute, you create an allowlist, and can send emails to any email address on the list.

<Render file="types-bindings" />

## Example Worker

Refer to the example below to learn how to construct a Worker capable of sending emails. This example uses [MIMEText](https://www.npmjs.com/package/mimetext):

:::note
The sender has to be an email from the domain where you have Email Routing active.
:::

```js
import { EmailMessage } from "cloudflare:email";
import { createMimeMessage } from "mimetext";

export default {
 async fetch(request, env) {
   const msg = createMimeMessage();
   msg.setSender({ name: "GPT-4", addr: "<SENDER>@example.com" });
   msg.setRecipient("<RECIPIENT>@example.com");
   msg.setSubject("An email generated in a worker");
   msg.addMessage({
       contentType: 'text/plain',
       data: `Congratulations, you just sent an email from a worker.`
   });

   var message = new EmailMessage(
     "<SENDER>@example.com",
     "<RECIPIENT>@example.com",
     msg.asRaw()
   );
   try {
     await env.SEB.send(message);
   } catch (e) {
     return new Response(e.message);
   }

   return new Response("Hello Send Email World!");
 },
};
```

---

# Audit logs

URL: https://developers.cloudflare.com/email-routing/get-started/audit-logs/

Audit logs for Email Routing are available in the [Cloudflare dashboard](https://dash.cloudflare.com/?account=audit-log). The following changes to Email Routing will be displayed:

* Add/edit Rule
* Add address
* Address change status
* Enable/disable/unlock zone

Refer to [Review audit logs](/fundamentals/setup/account/account-security/review-audit-logs/) for more information.

---

# Analytics

URL: https://developers.cloudflare.com/email-routing/get-started/email-routing-analytics/

The Overview page shows you a summary of your account. You can check details such as how many custom and destination addresses you have configured, as well as the status of your routing service.

## Email Routing summary

In Email Routing summary you can check metrics related the number of emails received, forwarded, dropped, and rejected. To filter this information by time interval, select the drop-down menu. You can choose preset periods between the previous 30 minutes and 30 days, as well as a custom date range.

## Activity Log

This section allows you to sort through emails received, and check Email Routing actions - for example, `Forwarded`, `Dropped`, or `Rejected`. Select a specific email to expand its details and check information regarding the [SPF](https://datatracker.ietf.org/doc/html/rfc7208), [DKIM](https://datatracker.ietf.org/doc/html/rfc6376), and [DMARC](https://datatracker.ietf.org/doc/html/rfc7489) statuses. Depending on the information shown, you can opt to mark an email as spam or block the sender.

---

# Enable Email Routing

URL: https://developers.cloudflare.com/email-routing/get-started/enable-email-routing/

:::caution[Important]
Enabling Email Routing adds the appropriate `MX` records to the DNS settings of your zone in order for the service to work. You can [change these `MX` records](/email-routing/setup/email-routing-dns-records/) at any time. However, depending on how you configure them, Email Routing might stop working.
:::

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing**.
3. Review the records that will be added to your zone.
4. Select **Add records and enable**.
5. Go to **Routing rules**.
6. For **Custom addresses**, select **Create address**.
7. Enter the custom email address you want to use (for example, `my-new-email@example.com`).
8. In **Destination addresses**, enter the full email address you want your emails to be forwarded to â€” for example, `your-name@example.com`.

   :::note[Notes]

   If you have several destination addresses linked to the same custom email address (rule), Email Routing will only process the most recent rule. To avoid this, do not link several destination addresses to the same custom address.

   The current implementation of email forwarding only supports a single destination address per custom address. To forward a custom address to multiple destinations you must create a Workers script to redirect the email to each destination. All the destinations used in the Workers script must be already validated.

   :::

9. Select **Save**.
10. Cloudflare will send a verification email to the address provided in the **Destination address** field. You must verify your email address before being able to proceed.
11. In the verification email Cloudflare sent you, select **Verify email address** > **Go to Email Routing** to activate Email Routing.
12. Your Destination address should now show **Verified**, under **Status**. Select **Continue**.
13. Cloudflare needs to add the relevant `MX` and `TXT` records to DNS records for Email Routing to work. This step is automatic and is only needed the first time you configure Email Routing. It is meant to ensure you have the proper records configured in your zone. Select **Add records and finish**.

Email Routing is now enabled. You can add other custom addresses to your account.

:::note
When Email Routing is configured and running, no other email services can be active in the domain you are configuring. If there are other `MX` records already configured in DNS, Cloudflare will ask you if you wish to delete them. If you do not delete existing `MX` records, Email Routing will not be enabled.
:::

---

# Get started

URL: https://developers.cloudflare.com/email-routing/get-started/

import { DirectoryListing } from "~/components"

To enable Email Routing, start by creating a custom email address linked to a destination address or Email Worker. This forms an **email rule**. You can enable or disable rules from the Cloudflare dashboard. Refer to [Enable Email Routing](/email-routing/get-started/enable-email-routing) for more details.

Custom addresses you create with Email Routing work as forward addresses only. Emails sent to custom addresses are forwarded by Email Routing to your destination inbox. Cloudflare does not process outbound email, and does not have an SMTP server.

The first time you access Email Routing, you will see a wizard guiding you through the process of creating email rules. You can skip the wizard and add rules manually.

If you need to pause Email Routing or offboard to another service, refer to [Disable Email Routing](/email-routing/setup/disable-email-routing/).

<DirectoryListing />

---

# Disable Email Routing

URL: https://developers.cloudflare.com/email-routing/setup/disable-email-routing/

Email Routing provides two options for disabling the service:

* **Delete and Disable**: This option will immediately disable Email Routing and remove its `MX` records. Your custom email addresses will stop working, and your email will not be routed to its final destination.
* **Unlock and keep DNS records**: (Advanced) This option is recommended if you plan to migrate to another provider. It allows you to add new `MX` records before disabling the service. Email Routing will stop working when you change your `MX` records.

## Delete and disable Email Routing

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing** > **Settings**.
3. Select **Start disabling** > **Delete and Disable**. Email Routing will show you the list of records associated with your account that will be deleted.
4. Select **Delete records**.

Email Routing is now disabled for your account and will stop forwarding email. To enable the service again, select **Enable Email Routing** and follow the wizard.

## Unlock and keep DNS records

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing** > **Settings**.
3. Select **Start disabling** > **Unlock records and continue**.
4. Select **Edit records on DNS**.

You now have the option to edit your DNS records to migrate your service to another provider.

:::caution

Changing your DNS records will make Email Routing stop working. If you changed your mind and want to keep Email Routing working with your account, select **Lock DNS records**. 
:::

---

# Test Email Routing

URL: https://developers.cloudflare.com/email-routing/get-started/test-email-routing/

To test that your configuration is working properly, send an email to the custom address [you set up in the dashboard](/email-routing/get-started/enable-email-routing/). You should send your test email from a different address than the one you specified as the destination address.

For example, if you set up `your-name@gmail.com` as the destination address, do not send your test email from that same Gmail account. Send a test email to that destination address from another email account (for example, `your-name@outlook.com`).

The reason for this is that some email providers will discard what they interpret as an incoming duplicate email and will not show it in your inbox, making it seem like Email Routing is not working properly.

---

# Configure rules and addresses

URL: https://developers.cloudflare.com/email-routing/setup/email-routing-addresses/

An email rule is a pair of a custom email address and a destination address, or a custom email address with an Email Worker. This allows you to route emails to your preferred inbox, or apply logic through Email Workers before deciding what should happen to your emails. You can have multiple custom addresses, to route email from specific providers to specific mail inboxes.

## Custom addresses

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing** > **Routes**.
3. Select **Create address**.
4. In **Custom address**, enter the custom email address you want to use (for example, `my-new-email`).
5. In the **Action** drop-down menu, choose what this email rule should do. Refer to [Email rule actions](#email-rule-actions) for more information.
6. In **Destination**, choose the email address or Email Worker you want your emails to be forwarded to â€” for example, `your-name@gmail.com`. You can only choose a destination address you have already verified. To add a new destination address, refer to [Destination addresses](#destination-addresses).

:::note

If you have more than one destination address linked to the same custom address, Email Routing will only process the most recent rule. This means only the most recent pair of custom address and destination address (rule) will receive your forwarded emails. To avoid this, do not link more than one destination address to the same custom address. 
:::

### Email rule actions

When creating an email rule, you must specify an **Action**:

* *Send to an email*: Emails will be routed to your destination address. This is the default action.
* *Send to a Worker*: Emails will be processed by the logic in your [Email Worker](/email-routing/email-workers).
* *Drop*: Deletes emails sent to the custom address without routing them. This can be useful if you want to make an email address appear valid for privacy reasons.

:::note


To prevent spamming unintended recipients, all email rules are automatically disabled until the destination address is validated by the user.


:::

### Disable an email rule

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing** > **Routes**.
3. In **Custom addresses**, identify the email rule you want to pause, and toggle the status button to **Disabled**.

Your email rule is now disabled. It will not forward emails to a destination address or Email Worker. To forward emails again, toggle the email rule status button to **Active**.

### Edit custom addresses

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing** > **Routes**.
3. In **Custom addresses**, identify the email rule you want to edit, and select **Edit**.
4. Make the appropriate changes to this custom address.

## Catch-all address

When you enable this feature, Email Routing will catch variations of email addresses to make them valid for the specified domain. For example, if you created an email rule for `info@example.com` and a sender accidentally types `ifno@example.com`, the email will still be correctly handled if you have **Catch-all addresses** enabled.

To enable Catch-all addresses:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing** > **Routes**.
3. Enable **Catch-all address**, so it shows as **Active**.
4. In the **Action** drop-down menu, select what to do with these emails. Refer to [Email rule actions](#email-rule-actions) for more information.
5. Select **Save**.

## Destination addresses

This section lets you manage your destination addresses. It lists all email addresses already verified, as well as email addresses pending verification. You can resend verification emails or delete destination addresses.

Destination addresses are shared at the account level, and can be reused with any other domain in your account. This means the same destination address will be available to different domains in your account.

To prevent spam, email rules do not become active until after the destination address has been verified. Cloudflare sends a verification email to destination addresses specified in **Custom addresses**. You have to select **Verify email address** in that email to activate a destination address.

:::note


Deleting a destination address automatically disables all email rules that use that email address as destination.


:::

---

# DNS records

URL: https://developers.cloudflare.com/email-routing/setup/email-routing-dns-records/

You can check the status of your DNS records in the **Settings** section of Email Routing. This section also allows you to troubleshoot any potential problems you might have with DNS records.

## Email DNS records

Check the status of your account's DNS records in the **Email DNS records** card:

* **Email DNS records configured** - DNS records are properly configured.
* **Email DNS records misconfigured** - There is a problem with your accounts DNS records. Select **Enable Email Routing** to [start troubleshooting problems](/email-routing/troubleshooting/).

### Start disabling

When you successfully configure Email Routing, your DNS records will be locked and the dashboard will show a **Start disabling** button in the Email DNS records card. This locked status is the recommended setting by Cloudflare. It means that the DNS records required for Email Routing to work are locked and can only be changed if you disable Email Routing on your domain.

If you need to delete Email Routing or migrate to another provider, select **Start disabling**. Refer to [Disable Email Routing](/email-routing/setup/disable-email-routing/) for more information.

### Lock DNS records

Depending on your zone configuration, you might have your DNS records unlocked. This will also be true if, for some reason, you have unlocked your DNS records. Select **Lock DNS records** to lock your DNS records and protect them from being accidentally changed or deleted.

## View DNS records

Select **View DNS records** for a list of the required `MX` and sender policy framework (SPF) records Email Routing is using.

If you are having trouble with your account's DNS records, refer to the [Troubleshooting](/email-routing/troubleshooting/) section.

---

# Setup

URL: https://developers.cloudflare.com/email-routing/setup/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Configure MTA-STS

URL: https://developers.cloudflare.com/email-routing/setup/mta-sts/

MTA Strict Transport Security ([MTA-STS](https://datatracker.ietf.org/doc/html/rfc8461)) was introduced by email service providers including Microsoft, Google and Yahoo as a solution to protect against downgrade and man-in-the-middle attacks in SMTP sessions, as well as solving the lack of security-first communication standards in email.

Suppose that `example.com` is your domain and uses Email Routing. Here is how you can enable MTA-STS for it.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **DNS** > **Records** and create a new CNAME record with the name `_mta-sts` that points to Cloudflareâ€™s record `_mta-sts.mx.cloudflare.net`. Make sure to disable the proxy mode.

![MTA-STS CNAME record](~/assets/images/email-routing/mta-sts-record.png)

3. Confirm that the record was created:

```sh
dig txt _mta-sts.example.com
```

```sh output
_mta-sts.example.com. 300 IN  CNAME _mta-sts.mx.cloudflare.net.
_mta-sts.mx.cloudflare.net. 300 IN  TXT "v=STSv1; id=20230615T153000;"
```

This tells the other end client that is trying to connect to us that we support MTA-STS.

Next you need an HTTPS endpoint at `mta-sts.example.com` to serve your policy file. This file defines the mail servers in the domain that use MTA-STS. The reason why HTTPS is used here instead of DNS is because not everyone uses DNSSEC yet, so we want to avoid another MITM attack vector.

To do this you need to deploy a Worker that allows email clients to pull Cloudflareâ€™s Email Routing policy file using the â€œwell-knownâ€ URI convention.

4. Go to your **Account** > **Workers & Pages** and press **Create Application**. Pick the "MTA-STS" template from the list.

![MTA-STS Worker](~/assets/images/email-routing/mta-sts-worker.png)

5. This Worker proxies `https://mta-sts.mx.cloudflare.net/.well-known/mta-sts.txt` to your own domain. After deploying it, go to the Worker configuration, then **Triggers** > **Custom Domains** and **Add Custom Domain**. Type the subdomain `mta-sts.example.com`.

![MTA-STS Worker Custom Domain](~/assets/images/email-routing/mta-sts-domain.png)

You can then confirm that your policy file is working with the following:

```sh
curl https://mta-sts.example.com/.well-known/mta-sts.txt
```

```sh output
version: STSv1
mode: enforce
mx: *.mx.cloudflare.net
max_age: 86400
```

This says that you domain `example.com` enforces MTA-STS. Capable email clients will only deliver email to this domain over a secure connection to the specified MX servers. If no secure connection can be established the email will not be delivered.

Email Routing also supports MTA-STS upstream, which greatly improves security when forwarding your Emails to service providers like Gmail, Microsoft, and others.

While enabling MTA-STS involves a few steps today, we aim to simplify things for you and automatically configure MTA-STS for your domains from the Email Routing dashboard as a future improvement.

---

# Subdomains

URL: https://developers.cloudflare.com/email-routing/setup/subdomains/

Email Routing is a [zone-level](/fundamentals/setup/accounts-and-zones/#zones) feature. A zone has a top-level domain (the same as the zone name) and it can have subdomains (managed under the DNS feature.) As an example, you can have the `example.com` zone, and then the `mail.example.com` and `corp.example.com` sub-domains under it.

You can use Email Routing with any subdomain of any zone in your account. Follow these steps to add Email Routing features to a new subdomain:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and zone.
2. Go to **Email** > **Email Routing** > **Settings**, and select **Add subdomain**.

Once the subdomain is added and the DNS records are configured, you can see it in the **Settings** list under the **Subdomains** section.

Now you can go to **Email** > **Email Routing** > **Routing rules** and create new custom addresses that will show you the option of using either the top domain of the zone or any other configured subdomain.

---

# DNS records

URL: https://developers.cloudflare.com/email-routing/troubleshooting/email-routing-dns-records/

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing** > **Settings**. Email Routing will show you the status of your DNS records, such as `Missing`.
3. Select **Enable Email Routing**.
4. The next page will show you what kind of action is needed. For example, if you are missing DNS records, select **Add records and enable**.

If there is a problem with your SPF records, refer to [Troubleshooting SPF records](/email-routing/troubleshooting/email-routing-spf-records/).

---

# SPF records

URL: https://developers.cloudflare.com/email-routing/troubleshooting/email-routing-spf-records/

Having multiple [sender policy framework (SPF) records](https://www.cloudflare.com/learning/dns/dns-records/dns-spf-record/) on your account is not allowed, and will prevent Email Routing from working properly. If your account has multiple SPF records, follow these steps to solve the issue:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing**. Email Routing will warn you that you have multiple SPF records.
3. Under **View DNS records**, select **Fix records**.
4. Delete the incorrect SPF record.

You should now have your SPF records correctly configured. If you are unsure of which SPF record to delete:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and domain.
2. Go to **Email** > **Email Routing**. Email Routing will warn you that you have multiple SPF records.
3. Under **View DNS records**, select **Fix records**.
4. Delete all SPF records.
5. Select **Add records and enable**.

---

# Troubleshooting

URL: https://developers.cloudflare.com/email-routing/troubleshooting/

import { DirectoryListing } from "~/components"

Email Routing warns you when your DNS records are not properly configured. In Email Routing's **Overview** page, you will see a message explaining what type of problem your account's DNS records have.

Refer to Email Routing's **Settings** tab on the dashboard for more information. Email Routing will list missing DNS records or warn you about duplicate sender policy framework (SPF) records, for example.

<DirectoryListing />

---

# Create a mTLS rule

URL: https://developers.cloudflare.com/firewall/cf-dashboard/create-mtls-rule/

Use the [Mutual TLS](/api-shield/security/mtls/configure/) Rule interface in the Cloudflare dashboard to create an mTLS rule that requires requests to your API or web application to present a valid client certificate.

---

# Create, edit, and delete rules

URL: https://developers.cloudflare.com/firewall/cf-dashboard/create-edit-delete-rules/

import { Render } from "~/components"

A firewall rule has two main attributes: an [expression](/ruleset-engine/rules-language/expressions/) and an [action](/firewall/cf-firewall-rules/actions/).

<Render file="deprecation-notice" />

When an incoming HTTP request matches a firewall rule expression, Cloudflare performs the specified action. For more information, refer to [Expressions](/ruleset-engine/rules-language/expressions/) and [Actions](/firewall/cf-firewall-rules/actions/).

:::note


<Render file="max-expression-length" product="ruleset-engine" />


:::

## Create a firewall rule

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/), and select your account and website.

2. Go to **Security** > **WAF** > **Firewall rules**.

3. Select **Create a firewall rule**.

4. In the **Create firewall rule** page that displays, use the **Rule name** input to supply a descriptive name.

5. Under **When incoming requests match**, use the **Field** drop-down list to choose an HTTP property (refer to the [Fields reference](/ruleset-engine/rules-language/fields/reference/) for details). For each request, the value of the property you choose for **Field** is compared to the value you specify for **Value**.

   Alternatively, use the [Expression Editor](/ruleset-engine/rules-language/expressions/edit-expressions/#expression-editor) to define the rule expression.

   ![Example firewall rule expression with a selected field, operator, and value](~/assets/images/firewall/firewall-rules-expression-builder-value.png)

6. Use the **Operator** drop-down list to choose a comparison operator. For an expression to match, the value of the request **Field** and the value specified in the **Value** input must satisfy the comparison operator.

7. Next, specify the value to match. If the value is an enumeration, then the **Value** control will be a drop-down list. Otherwise, it will be a text input.

8. To add a new sub-expression to the rule expression, select **And** or **Or** next to **Value**.

9. Select an action for your rule in the **Action** drop-down list.

10. To save and deploy your rule, select **Deploy**. If you are not ready to deploy your rule, select **Save as draft**.

After you choose an option, you return to the rules list, which displays your new rule.

## Manage rules

Use the available options in the rules list to manage firewall rules.

![The rules list interface in the dashboard where you can manage firewall rules](~/assets/images/firewall/cf-firewall-rules-list.png)

### Edit rule

Select **Edit** (wrench icon) located on the right of your rule in the rules list to open the **Edit firewall rule** panel and make the changes you want.

### Enable or disable rule

Use the toggle switch associated with a firewall rule to enable or disable it.

### Delete rule

1. Next to the rule you want to delete, select **Delete** (**X** icon).
2. In the confirmation dialog, select **Delete** to complete the operation.

### Order rules

By default, Cloudflare evaluates firewall rules in **list order**, where rules are evaluated in the order they appear in the rules list. When list ordering is enabled, the rules list allows you to drag and drop firewall rules into position, as shown below.

![Animation of a user dragging and dropping a rule in the rules list to reorder it](/images/firewall/firewall-rules-expression-builder-10.gif)

Once there are more than 200 total rules (including inactive rules), you must manage evaluation using **priority ordering**, in which Cloudflare evaluates firewall rules in order of their **priority number**, starting with the lowest. When you cross this threshold, the firewall rules interface automatically switches to priority ordering. For more on working with priority ordering, refer to [Order and priority](/firewall/cf-firewall-rules/order-priority/).

## Test firewall rules with Rule Preview

Rule Preview allows customers on an Enterprise plan to understand the potential impact of a new firewall rule, by testing it against a sample of requests drawn from the last 72 hours of traffic.

Rule Preview is built into the **Create firewall rule** and **Edit firewall rule** panels so that you can test a rule as you edit it. For more information, refer to [Preview rules](/firewall/cf-dashboard/rule-preview/).

---

# Preview rules

URL: https://developers.cloudflare.com/firewall/cf-dashboard/rule-preview/

The expression of a firewall rule can become quite complex. In this situation, you should test your firewall rule before deploying it to ensure that the rule will behave as expected.

Rule Preview helps you understand the potential impact of a firewall rule, by testing the rule against a sample drawn from the last 72 hours of traffic. Rule Preview is built into the firewall rules Expression Editor so that you can test a rule as you edit it.

:::caution
Rule Preview is only available to customers on an Enterprise plan.
:::

## Test a firewall rule with Rule Preview

1. Locate the desired rule in the rules list and select **Edit** (wrench icon).
2. Select **Test rule** to trigger the test.

![The Test Rule button next to the Action drop-down list allows you to check the traffic that would be affected by the current firewall rule](~/assets/images/firewall/firewall-rules-preview-1.png)

The results of the test are displayed in a plot that simulates how many of the total requests in the last 72 hours would have matched the tested expression.

In this screenshot, a rule that matches all User-Agents that contain the string `Mozilla` would block about 8% of requests to the zone:

![Example chart of a rule preview operation, stating that about 8% of the zone requests would be blocked by the current rule](~/assets/images/firewall/cf-firewall-rules-preview-rule-plot-chart.png)

## Important notes

**Consider the results of Firewall Preview an _indication_ of traffic levels**, not an exact calculation. The sample rate can be as little as 1% of your total traffic.

**Rule Preview does not take into account other firewall rules** that you have already configured. In effect, Rule Preview tests a single firewall rule in isolation. Security events or any other rules with a higher priority that may have blocked or challenged a request are ignored.

**You cannot test firewall rules that reference [IP lists](/waf/tools/lists/custom-lists/#lists-with-ip-addresses-ip-lists)**.

**Cloudflare does not store the entirety of requests, so only a limited number of fields are available to Rule Preview**. The table below lists the fields that Rule Preview supports (green cells), broken down by operator. Fields and operators that are not supported are not included in this table.

---

# Call sequence

URL: https://developers.cloudflare.com/firewall/api/call-sequence/

import { Render } from "~/components"

The API call examples in this site illustrate the **recommended sequence** of calling the two APIs (the [Cloudflare Filters API](/firewall/api/cf-filters/) and the [Firewall Rules API](/firewall/api/cf-firewall-rules/)).

<Render file="deprecation-notice" />

The image below depicts this sequence, which can be applied for creating and editing rules. The reverse would apply for delete operations.

![Recommended flow for calling the Cloudflare Filters API and Firewall Rules API when creating or editing rules](~/assets/images/firewall/recommended-flow.png)

Cloudflare recommends this sequence because it facilitates filter reusability and allows working with either API independently. Thanks to the standalone nature of Cloudflare Filters, the same filter can be shared in multiple firewall rules and in other future Cloudflare products and features.

For example, a filter that matches all traffic for your API (that is, `http.request.uri.path matches "^/api/.*$"`) may disable caching, disable human CAPTCHAs, configure JSON custom errors, and appear in a firewall rule. With the recommended sequence above, you would repeat steps 3-6 for every Cloudflare feature to configure against the same filter created in steps 1-2.

However, for a `POST` operation, the **simplified sequence** â€” shown below â€” allows you to create both a filter and rule in the same call. In this case, the filter and rule only refer to each other.

![Basic flow for invoking the Firewall Rules API to create both a filter and a rule in a single call](~/assets/images/firewall/simple-flow.png)

In this sequence, a single `POST` request to the `/firewall/rules` endpoint takes the filter object in the JSON to create the filter in the Filters API (also via a `POST` request). If successful, the firewall rule is created.

Below is an example call and response using this method:

```bash title="Request"
curl "https://api.cloudflare.com/client/v4/zones/{zone_id}/firewall/rules" \
--header "X-Auth-Email: <EMAIL>" \
--header "X-Auth-Key: <API_KEY>" \
--header "Content-Type: application/json" \
--data '[
  {
    "filter": {
      "expression": "http.request.uri.path contains \"/api/\" and ip.src eq 93.184.216.34"
    },
    "action": "block"
  }
]'
```

```json title="Response"
{
  "result": [
    {
      "id": "<RULE_ID>",
      "paused": false,
      "action": "block",
      "priority": null,
      "filter": {
        "id": "<FILTER_ID>",
        "expression": "http.request.uri.path contains \"/api/\" and ip.src eq 93.184.216.34",
        "paused": false
      }
    }
  ],
  "success": true,
  "errors": [],
  "messages": []
}
```

However, this approach has some disadvantages:

* The firewall rules client has to implement error and exception handling for every potential failure occurring in both the firewall rules and the filters APIs.
* To protect against accidentally modifying or deleting filters used by other Cloudflare features, the `PUT` or `DELETE` operations are not allowed.

By default, if either the filter or rule is invalid, neither will be created.

However, one exception applies. If you are about to exceed your rule quota, Cloudflare may create the filter but not the firewall rule. This happens because the rule is only created after the filter in the sequence diagram.

After you resolve the issue of exceeding your quota or requesting a feature that is unavailable to your zone, return to the recommended flow to create a rule that references the filter.

In summary, Cloudflare strongly recommends the sequence with the two API calls. Limit your rule and filter creation using the simplified sequence for emergency situations, and only via `curl` requests.

---

# Manage rules in the dashboard

URL: https://developers.cloudflare.com/firewall/cf-dashboard/

import { Render } from "~/components"

To configure firewall rules in the Cloudflare dashboard, open **Firewall rules** in **Security** > **WAF**.

<Render file="deprecation-notice" />

As depicted in the image below, the **Firewall rules** dashboard interface lets you:

* View the list of existing rules, both enabled and disabled.
* [Create, edit, and delete](/firewall/cf-dashboard/create-edit-delete-rules/) firewall rules.
* Enable or disable rules.
* Search and filter the list of existing rules.
* Reorder firewall rules when using [list ordering](/firewall/cf-firewall-rules/order-priority/#managing-rule-evaluation-by-list-order).

![The Firewall rules interface, displaying a list of three example firewall rules. This interface allows you to create new firewall rules, manage existing rules, and search and filter the list of rules.](~/assets/images/firewall/cf-firewall-rules-panel.png)

---

# Manage rules via the APIs

URL: https://developers.cloudflare.com/firewall/api/

import { Render } from "~/components"

Cloudflare offers APIs that work together to achieve the same effect as the UI-based **Firewall rules** feature under **Security** > **WAF**.

<Render file="deprecation-notice" />

These APIs are the following:

* [**Firewall Rules API**](/firewall/api/cf-firewall-rules/): Manage firewall rules and their actions, based on criteria separately defined through filters.
* [**Filters API**](/firewall/api/cf-filters/): Manage the filters that enable rule matching.
* [**Lists API**](/waf/tools/lists/lists-api/): Manage named lists of items (such as IP addresses) that you can use in the rules of different Cloudflare products.

---

# Actions

URL: https://developers.cloudflare.com/firewall/cf-firewall-rules/actions/

import { Render } from "~/components"

The action of a firewall rule tells Cloudflare how to handle HTTP requests that have matched the rule expression.

<Render file="deprecation-notice" />

## Supported actions

The table below lists the actions available in firewall rules. These actions are listed in order of precedence. If the same request matches two different rules which have the same priority, precedence determines the action to take.

For example, the *Allow* action takes precedence over the *Block* action. In a case where a request matches a rule with the *Allow* action and another with the *Block* action, precedence resolves the tie, and Cloudflare allows the request.

There are two exceptions to this behavior: the *Log* and *Bypass* actions. Unlike other actions, *Log* and *Bypass* do not terminate further evaluation within firewall rules. This means that if a request matches two different rules and one of those rules specifies the *Log* or *Bypass* action, the second action will be triggered instead, even though *Log*/*Bypass* has precedence.

:::note

For reference information on rule actions available for Cloudflare products powered by the Ruleset Engine, refer to [Rules language: Actions reference](/ruleset-engine/rules-language/actions/).

:::

<table style="width: 100%">
  <thead>
    <tr>
      <th>Action</th>
      <th>Description</th>
      <th>Order of precedence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        <strong>Log</strong>
        <br />
        <br />
        API value:
        <br />
        <code class="InlineCode">log</code>
      </td>
      <td>
        <ul>
          <li>Records matching requests in the Cloudflare Logs.</li>
          <li>Only available for Enterprise plans.</li>
          <li>
            Recommended for validating rules before committing to a more severe
            action.
          </li>
        </ul>
      </td>
      <td>1</td>
    </tr>
    <tr>
      <td>
        <strong>Bypass</strong>
        <br />
        <br />
        API value:
        <br />
        <code class="InlineCode">bypass</code>
      </td>
      <td>
        <ul>
          <li>
            Allows user to dynamically disable Cloudflare security features for
            a request.
          </li>
          <li>Available to all plans.</li>
          <li>
            <p>
              Matching requests exempt from evaluation by a user-defined list
              containing one or more of the following Cloudflare security
              features:
            </p>
            <ul>
              <li>
                <a href="/waf/tools/user-agent-blocking/">
                  User Agent Blocking
                </a>
              </li>
              <li>
                <a href="/waf/tools/browser-integrity-check/">
                  Browser Integrity Check
                </a>
              </li>
              <li>
                <a href="/waf/tools/scrape-shield/hotlink-protection/">
                  Hotlink Protection
                </a>
              </li>
              <li>
                <a href="/waf/tools/security-level/">
                  Security Level (IP Reputation)
                </a>
              </li>
              <li>
                <a href="/waf/reference/legacy/old-rate-limiting/">
                  Rate Limiting
                </a>{" "}
                (previous version, deprecated)
              </li>
              <li>
                <a href="/waf/tools/zone-lockdown/">Zone Lockdown</a>
              </li>
              <li>
                <a href="/waf/reference/legacy/old-waf-managed-rules/">
                  WAF managed rules
                </a>{" "}
                (previous version, deprecated)
              </li>
            </ul>
            <p>
              <strong>Notes:</strong>
            </p>
            <ul>
              <li>
                Currently, you cannot bypass Bot Fight Mode. For more
                information on this product, refer to{" "}
                <a href="/bots/">Cloudflare bot solutions</a>.
              </li>
              <li>
                You cannot bypass the new{" "}
                <a href="/waf/managed-rules/">WAF managed rules</a> using this
                action, only the previous version of WAF managed rules. To skip
                one or more managed rules in the new WAF for specific requests,{" "}
                <a href="/waf/managed-rules/waf-exceptions/">
                  create an exception
                </a>
                .
              </li>
            </ul>
            <p></p>
          </li>
          <li>
            Requests which match the <em>Bypass</em> action are still subject to
            evaluation (and thus a challenge or block) within Firewall Rules,
            based on the order of execution.
          </li>
        </ul>
      </td>
      <td>2</td>
    </tr>
    <tr>
      <td>
        <strong>Allow</strong>
        <br />
        <br />
        API value:
        <br />
        <code class="InlineCode">allow</code>
      </td>
      <td>
        <ul>
          <li>
            Matching requests are exempt from <em>Bypass</em>, <em>Block</em>,
            and challenge actions triggered by other firewall rules.
          </li>
          <li>
            The scope of the <em>Allow</em> action is limited to firewall rules;
            matching requests are <strong>not</strong> exempt from action by
            other Cloudflare security products such as Bot Fight Mode, IP Access
            Rules, and WAF Managed Rules.
          </li>
          <li>
            Matched requests will be mitigated if they are part of a DDoS
            attack.
          </li>
        </ul>
      </td>
      <td>3</td>
    </tr>
    <tr>
      <td>
        <strong>Interactive Challenge</strong>
        <br />
        <br />
        API value:
        <br />
        <code class="InlineCode">challenge</code>
      </td>
      <td>
        <ul>
          <li>
            This option is not recommended. Instead, choose{" "}
            <strong>Managed Challenge (Recommended)</strong>, which issues
            interactive challenges to visitors only when necessary.
          </li>
          <li>
            The client that made the request must pass an interactive challenge.
          </li>
          <li>
            If successful, Cloudflare accepts the matched request; otherwise, it
            is blocked.
          </li>
          <li>
            For additional information, refer to{" "}
            <a href="#notes-about-challenge-actions">
              Notes about challenge actions
            </a>
            .
          </li>
        </ul>
      </td>
      <td>4</td>
    </tr>
    <tr>
      <td>
        <strong>
          Managed Challenge
          <br />
          (Recommended)
        </strong>
        <br />
        <br />
        API value:
        <br />
        <code class="InlineCode">managed_challenge</code>
      </td>
      <td>
        <ul>
          <li>
            Helps reduce the lifetimes of human time spent solving interactive
            challenges across the Internet.
          </li>
          <li>
            Depending on the characteristics of a request, Cloudflare will
            dynamically choose the appropriate type of challenge from the
            following actions based on specific criteria:
            <ul>
              <li>
                Show a non-interactive challenge page (similar to the current JS
                Challenge).
              </li>
              <li>
                Show an interactive challenge (such as requiring the visitor to
                click a button or to perform a task).
              </li>
            </ul>
          </li>
          <li>
            For additional information, refer to{" "}
            <a href="#notes-about-challenge-actions">
              Notes about challenge actions
            </a>
            .
          </li>
        </ul>
      </td>
      <td>5</td>
    </tr>
    <tr>
      <td>
        <strong>JS Challenge</strong>
        <br />
        <br />
        API value:
        <br />
        <code class="InlineCode">js_challenge</code>
      </td>
      <td>
        <ul>
          <li>
            Useful for ensuring that bots and spam cannot access the requested
            resource; browsers, however, are free to satisfy the challenge
            automatically.
          </li>
          <li>
            The client that made the request must pass a Cloudflare JavaScript
            Challenge before proceeding.
          </li>
          <li>
            If successful, Cloudflare accepts the matched request; otherwise, it
            is blocked.
          </li>
          <li>
            For additional information, refer to{" "}
            <a href="#notes-about-challenge-actions">
              Notes about challenge actions
            </a>
            .
          </li>
        </ul>
      </td>
      <td>6</td>
    </tr>
    <tr>
      <td>
        <strong>Block</strong>
        <br />
        <br />
        API value:
        <br />
        <code class="InlineCode">block</code>
      </td>
      <td>Matching requests are denied access to the site.</td>
      <td>7</td>
    </tr>
  </tbody>
</table>

## Notes about challenge actions

When you configure a firewall rule with one of the challenge actions â€” *Managed Challenge*, *JS Challenge*, or *Interactive Challenge* â€” and a request matches the rule, one of two things can happen:

* The request is blocked if the visitor fails the challenge
* The request is allowed if the visitor passes the challenge

In this last case, no further firewall rules will be processed. This means that the action of any later rules with a challenge or *Block* action also matching the request will not be applied, and the request will be allowed.

---

# About

URL: https://developers.cloudflare.com/firewall/cf-firewall-rules/

import { Render } from "~/components";

Cloudflare Firewall Rules is a flexible and intuitive framework for filtering HTTP requests. It gives you fine-grained control over which requests reach your applications, proactively inspecting incoming site traffic and automatically responding to threats.

<Render file="deprecation-notice" />

In a firewall rule you define an [expression](/ruleset-engine/rules-language/expressions/) that tells Cloudflare what to look for in a request, and specify the appropriate [action](/firewall/cf-firewall-rules/actions/) to take when those conditions are met. Expressions can reference [IP lists](/waf/tools/lists/custom-lists/#lists-with-ip-addresses-ip-lists) - groups of IP addresses that you can reference collectively by name.

To write firewall rule expressions, use the [Rules language](/ruleset-engine/rules-language/), a powerful expression language inspired in the Wireshark Display Filter language.

---

# Order and priority

URL: https://developers.cloudflare.com/firewall/cf-firewall-rules/order-priority/

Cloudflare Firewall Rules, now deprecated, is part of a larger evaluation chain for HTTP requests, as illustrated in the diagram below. For example, Firewall Rules only evaluates requests that first clear IP Access rules. If a request is blocked by a rule at any stage in the chain, Cloudflare does not evaluate the request further.

![Flow chart of request evaluation at Cloudflare for security products that are not powered by the Ruleset Engine](~/assets/images/firewall/firewall-rules-order-and-priority-1.png)

:::caution

- You can use [IP Access rules](/waf/tools/ip-access-rules/) to allowlist requests under certain conditions, effectively excluding these requests from all security checks. However, allowing a given country code will not bypass [WAF Managed Rules](/waf/managed-rules/) or [WAF managed rules (previous version)](/waf/reference/legacy/old-waf-managed-rules/).

- The execution order diagram does not include products powered by the [Ruleset Engine](/ruleset-engine/) like the [WAF](/waf/) or [Transform Rules](/rules/transform/).

:::

By default, Cloudflare evaluates firewall rules in **list order**, where rules are evaluated in the order they appear in the firewall rules list. List ordering is convenient when working with small numbers of rules because you can manage their order by dragging and dropping them into position. However, as the number of rules grows, managing rules in list order becomes difficult. This is where priority order comes into play.

When **priority ordering** is enabled, Cloudflare evaluates firewall rules in order of their **priority number**, starting with the lowest. If a request matches two rules with the same priority, action precedence is used to resolve the tie. In this case, only the action of the rule with the highest precedence is executed, unless that action is _Log_ or _Bypass_ (refer to [Firewall rules actions](/firewall/cf-firewall-rules/actions/#supported-actions) for details). Priority ordering makes it a lot easier to manage large numbers of firewall rules, and once the number of rules passes 200, Cloudflare requires it.

## Managing rule evaluation by list order

Users with relatively small numbers of firewall rules (no more than 200) will find that list ordering is enabled by default. When list ordering is enabled, the rules list allows you to drag and drop firewall rules into position, as shown below:

![Animation of a firewall rule being moved into a new position in the rules list to reorder it](~/assets/images/firewall/firewall-rules-order-and-priority-2.gif)

Once there are more than 200 total rules, including inactive rules, you must manage evaluation using priority ordering. When you cross this threshold, the firewall rules interface automatically switches to priority ordering.

## Managing rule evaluation by priority order

Although priority ordering is enabled automatically when the number of active and inactive firewall rules exceeds 200, you can manually enable priority ordering at any time from the rules list.

Cloudflare Firewall Rules does not impose default priorities, and you are not required to set a priority for every rule.

### Enable priority ordering

To manually enable priority ordering:

1. Above the rules list, select **Ordering**.
2. Select _Priority Numbers_.

Once priority ordering is enabled, you can set a priority number for each firewall rule.

### Set rule priority

To set the priority number for a firewall rule:

1. Locate the desired rule in the rules list and select **Edit** (wrench icon).

2. In the **Edit firewall rule** panel, enter a positive integer value in **Priority**.

   ![Editing a firewall rule in the dashboard to define its Priority value](~/assets/images/firewall/firewall-rules-order-and-priority-4.png)

3. Select **Save**.

The **Priority** column in the rules list displays the priority value for each rule.

![When using priority order, the Firewall rules tab displays the priority of each rule (if any) in the first column of the rules list](~/assets/images/firewall/firewall-rules-order-and-priority-5.png)

## Working with priority ordering

Cloudflare has designed priority ordering to be extremely flexible. This flexibility is particularly useful for managing large rulesets programmatically via the Cloudflare API. Use the Update firewall rules command to set the `priority` property. Refer to [Cloudflare API: Firewall rules](/api/resources/firewall/subresources/rules/methods/list/) for details.

While your priority numbering scheme can be arbitrary, keep the following in mind:

- **The evaluation sequence starts from the lowest priority number** and goes to the highest.
- **Rules without a priority number are evaluated last**, in order of their action precedence. For example, a rule with the _Log_ action is evaluated before a rule that has the _Block_ action. For more on action precedence, refer to [Firewall rules actions](/firewall/cf-firewall-rules/actions/).
- **Avoid using the number `1` as a priority** to make rule order modification easier in the future.
- **Consider grouping ranges of priority numbers into categories** that have some meaning for your deployment. Here are some examples:

  - 5000-9999: Trusted IP addresses
  - 10000-19999: Blocking rules for bad crawlers
  - 20000-29999: Blocking rules for abusive users/spam

---

# Troubleshooting

URL: https://developers.cloudflare.com/firewall/troubleshooting/

import { DirectoryListing } from "~/components";

The following topics are useful for troubleshooting firewall issues.

<DirectoryListing />

---

# Required firewall rule changes to enable URL normalization

URL: https://developers.cloudflare.com/firewall/troubleshooting/required-changes-to-enable-url-normalization/

import { Render } from "~/components";

<Render file="deprecation-notice" />

On 2021-04-08, Cloudflare announced [URL normalization](/rules/normalization/), a feature that protects zones by normalizing HTTP request URI paths.

Malicious users can craft specific URIs that could be interpreted differently by firewall systems and origin systems. When you enable **Normalize incoming URLs**, all rules filtering on the URI path will receive the URL in a canonical form, which provides an extra layer of protection against these malicious users.

Cloudflare gradually enabled URL normalization for all Cloudflare zones except for those that could be impacted by this change. We determined the impacted zones by analyzing all firewall rules, looking for patterns in HTTP fields that would no longer match when using URL normalization techniques.

These fields are the following:

- `http.request.uri.path`
- `http.request.full_uri`
- `http.request.uri`

Cloudflare did not enable URL normalization automatically for zones that would be impacted by these changes to prevent any change in behavior of your existing firewall rules.

## Why URL normalization is important

Cloudflare strongly recommends that you enable **Normalize incoming URLs** in **Rules** > **Overview** > **URL Normalization** to strengthen your zone's security posture. Not doing so leaves your zone at greater risk of a successful attack. Malicious parties could craft the URL in a way that the rules are not accounting for.

For example, a firewall rule with an expression such as `http.request.uri.path contains "/login"` could be bypassed if the malicious actor has encoded the `l` character as `%6C`. In this scenario, and with URL normalization disabled, traffic would not be matched by the firewall rule.

Refer to [How URL normalization works](/rules/normalization/how-it-works/) for more information and additional examples.

---

## Recommended procedure

It is recommended that you:

1. Update any firewall rules impacted by the URL normalization changes.
2. Enable URL normalization.

These steps will ensure a stronger security posture on your zone(s).

### 1. Review and update firewall rules

Before enabling URL normalization, you should review the affected firewall rules on your zone(s) and take one of the following approaches:

- Edit these firewall rules to remove the parts which will no longer trigger once normalized â€” for example, any rules that look for `//` or `../` in URL paths. Administrators previously created these rules to perform a limited URL normalization, and these rules can now be safely disabled and then deleted.

- If you wish to identify visitors with non-normalized URI paths with these firewall rules, you should update them to use the original (or raw) non-normalized fields. These fields are the following:

  - `raw.http.request.uri.path`
  - `raw.http.request.full_uri`
  - `raw.http.request.uri`

### 2. Enable URL normalization

Once you have updated the affected firewall rules, enable URL normalization in **Rules** > **Overview** > **URL Normalization**.

A Cloudflare user must have the [Firewall role](/fundamentals/setup/manage-members/roles/) or one of the Administrator roles to access URL normalization settings in the dashboard.

---

## Related resources

- [URL normalization](/rules/normalization/)
- [Transform Rules](/rules/transform/)

---

# Service accounts

URL: https://developers.cloudflare.com/email-security/api/service-accounts/

A **service account** allows admins to create and maintain API credentials separate from a single username and password combination. It also allows you to create and control additional API access for different use cases.

When you connect to the [Email Security (formerly Area 1) API](/email-security/api/), the **Public Key** is used for the *username* and the **Private Key** for the *password*.

## Create service account

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. Go to **Service Accounts**.
4. Select **Add Service Account**.
5. Add a **Name**.
6. Select **Create Service Account**.
7. You will see your account's **Private Key** in a pop-up message (which will never be displayed again) and **Public Key** in the list of service accounts. Make sure to copy both values and store in a secure location.

***

## Rotate private key

If you lose your private key or need to rotate it for security reasons, you can generate a new private key:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. Go to **Service Accounts**.
4. On a specific account, select **...** > **Refresh key**.

---

# API

URL: https://developers.cloudflare.com/email-security/api/

import { GlossaryTooltip, Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

Email Security offers Application Programming Interfaces (APIs) to expose our <GlossaryTooltip term="phishing">phishing</GlossaryTooltip> campaign rulesets. These APIs both aid research and provide a set of indicators to block using network security edge devices.

All API requests are initiated using normal HTTP requests (`GET`/`POST`/`DELETE`) and responses are returned in JSON. Authentication to the APIs uses HTTP Basic Authentication over HTTPS.

For more details, refer to our [API documentation (PDF)](/email-security/static/api_documentation_1.38.1.pdf).

---

# Escalation contacts

URL: https://developers.cloudflare.com/email-security/account-setup/escalation-contacts/

import { GlossaryTooltip, Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

Whenever Email Security (formerly Area 1) finds an exceptional <GlossaryTooltip term="phishing">phishing</GlossaryTooltip> threat or Email Service irregularity behavior (compromised email servers at a partner or vendor, wire fraud tactics, and more), we try to reach out to our customers.

There are four types of contacts available to configure, each with a priority type:

* **SOC Contact**: P1 priority.
* **Triage Analyst**: P2 priority.
* **In-Depth Analyst**: P3 priority.
* **Executive Contact**: P4 priority.

Email Security will start by reaching out to P1-level contacts. If they do not respond, we will then try reaching out to the other contacts down the list until we receive a reply from one of these groups.

You can enable these special notifications through an opt-in process:

1. Log in to the [Email Security (formerly Area 1) dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. Go to **Subscriptions** > **Escalation Contacts**.
4. Select **Add Contact**.
5. Fill out the form.
6. Select **Save**.

:::note


If you select **Critical Service Events**, the contact will be sent a text and/or an email message. They will need to select the link to confirm the subscriptions.


:::

---

# Account setup

URL: https://developers.cloudflare.com/email-security/account-setup/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Manage parent permissions

URL: https://developers.cloudflare.com/email-security/account-setup/manage-parent-permissions/

import { Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

When you set up Email Security through a [partner](/email-security/partners/), that partner's account is the **parent** account to your **child** account.

Each child account can set the level of access allowed to their account from the parent. You may want to update this setting if you are receiving troubleshooting support from your parent account.

To update parent permissions:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).

2. Go to **Settings** (the gear icon).

3. Go to **Delegated Accounts**.

4. Select a permission level:

   * **No external account access**: Shuts off all access from the parent account (including Email Security).
   * **Allow external account view-only access** (default): Allows a parent user to view the customer's portal, including settings.
   * **Allow external account Super Admin access**: Allows a parent user to administer the customer account on their behalf. By selecting this option the customer is acknowledging consent for outside administration of their account.

5. Select **Save**.

---

# Manage account members

URL: https://developers.cloudflare.com/email-security/account-setup/manage-account-members/

import { GlossaryTooltip, Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

If your account is a **Super Admin**, you have the ability to add, edit, and delete users and - if those users lose their <GlossaryTooltip term="two-factor authentication (2FA)">two-factor authentication (2FA)</GlossaryTooltip> device - reset their 2FA.

## Add user

To add a user:

<Render file="add-user" params={{ one: " " }} />

## Edit user

To edit a user's settings:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. Go to **Users and Actions**.
4. On a specific user, select **...** > **Edit**.
5. Update any needed information.
6. Select **Update User**.

## Delete user

To delete a user:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. Go to **Users and Actions**.
4. On a specific user, select **...** > **Delete**.

## Reset two-factor authentication

To reset a user's two-factor authentication (2FA):

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. Go to **Users and Actions**.
4. On a specific user, select **...** > **Reset 2FA**.

---

# Permissions

URL: https://developers.cloudflare.com/email-security/account-setup/permissions/

import { Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

When you [create a user](/email-security/account-setup/manage-account-members/#add-user), the available options for permissions depend on whether your account is a **parent** account or a **child** account.

## Parent accounts

Parent accounts are treated as containers with no services provisioned. User accounts created at the parent level will allow them to access any child account.

These accounts are only required for administrators who manage multiple accounts, most commonly associated with our [partners](/email-security/partners/).

Parent users can have one of the following roles:

* **Viewer**: Can enter child accounts but is prevented from making any settings changes, regardless of the customer account settings.
* **SOC Analyst**: Can enter child accounts and make changes on behalf of the customer.

If your account has [parent permissions](/email-security/account-setup/manage-parent-permissions/) that conflict with a parent user's permissions, the parent permissions set on your account take precedence.

## Child accounts

Child accounts control settings and services associated with an Email Security instance.

### Child users

Users created at child level will only have access to the assigned child account. These users can have one of the following roles:

* **Super Admin**: Has full access to the account and can make any configuration changes. Can access **Settings** (the gear icon).
* **Configuration Admin**: Can make configuration changes and manage users, except for Super Admin. Has no ability to review messages.
* **SOC Analyst**: Can search, review and retract messages. Has no admin capabilities or access to **Settings** (the gear icon).
* **Viewer**: Only has access to metrics within the system. No access to **Settings** (the gear icon).

<table-wrap>

| Account area              | Super Admin | Configuration Admin | SOC Analyst | Viewer |
| ------------------------- | ----------- | ------------------- | ----------- | ------ |
| All Settings              | âœ…           | âœ…                   | âŒ           | âŒ      |
| User Profile              | âœ…           | âœ…                   | âœ…           | âœ…      |
| Global Search             | âœ…           | âœ…                   | âœ…           | âœ…      |
| Detection Search          | âœ…           | âœ…                   | âœ…           | âœ…      |
| Detection Search Actions  | âœ…           | âœ…                   | âœ…           | âŒ      |
| Mail Trace                | âœ…           | âœ…                   | âœ…           | âŒ      |
| Home                      | âœ…           | âœ…                   | âœ…           | âœ…      |
| Email                     | âœ…           | âœ…                   | âœ…           | âœ…      |
| Web                       | âœ…           | âœ…                   | âœ…           | âœ…      |
| Accountability            | âœ…           | âœ…                   | âœ…           | âœ…      |
| Announcements and Support | âœ…           | âœ…                   | âœ…           | âœ…      |
| Landscape                 | âœ…           | âœ…                   | âœ…           | âœ…      |
| Message Preview           | âœ…           | âŒ                   | âœ…           | âŒ      |
| Message Retraction        | âœ…           | âŒ                   | âœ…           | âŒ      |
| Admin Quarantine          | âœ…           | âŒ                   | âœ…           | âŒ      |

</table-wrap>

### Parent users

Depending on the [parent permissions](/email-security/account-setup/manage-parent-permissions/) of your child account, you can delegate access to parent users of your account. This configuration will allow a parent user to view and change settings associated with your account.

---

# Admin Quarantine

URL: https://developers.cloudflare.com/email-security/email-configuration/admin-quarantine/

import { GlossaryTooltip } from "~/components"

Admin Quarantine allows you to automatically prevent incoming messages from reaching a recipient's inbox based on the <GlossaryTooltip term="disposition" link="/email-security/reference/dispositions-and-attributes/">disposition</GlossaryTooltip> assigned by Email Security.

The messages sent to Admin Quarantine are determined by your [domain settings](/email-security/email-configuration/domains-and-routing/domains/).

## Quarantine emails by disposition

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).

2. Go to **Settings** (the gear icon).

3. Select **Email Configuration** > **Domains**.

4. Select the three dots on the domain that you want to configure admin quarantine for, and choose **Edit**.

5. In **Quarantine Policy** choose the dispositions you want to enable quarantine for that domain.

6. Select **Update Domain**.

:::note[Note]

Quarantine by disposition needs to be configured manually per domain. 
:::

## Access Admin Quarantine

You can view and potentially release emails that were sent to **Admin Quarantine**:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).

2. Go to **Email** > **Admin Quarantine**.

   ![Access Admin Quarantine to review emails](~/assets/images/email-security/admin-quarantine/access-quarantine.png)

3. Review emails as needed.

## Release quarantined emails

From **Admin Quarantine**, you can also release quarantined emails by selecting one or more messages:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).

2. Go to **Email** > **Admin Quarantine**.

3. Find the email you want to release.

4. Select **...** > **Release**.

   ![Select release to remove emails from quarantine](~/assets/images/email-security/admin-quarantine/release-emails.png)

5. Select **Release** to confirm that you want to release the selected email.

6. (Optional) You can also release multiple messages, by selecting the box next to each message you want to release.

:::note
After being released from quarantine, Email Security forwards the original email messages to their destination. These emails will arrive at email inboxes from the original sender, not Email Security.
:::

---

# Email configuration

URL: https://developers.cloudflare.com/email-security/email-configuration/

import { DirectoryListing, Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

Once you have [set up Email Security](/email-security/deployment/), you have several options to customize and fine-tune email behavior.

<DirectoryListing />

---

# Setup

URL: https://developers.cloudflare.com/email-security/deployment/

import { DirectoryListing, Render } from "~/components"

<Render file="deployment/setup-overview" />

Select a deployment method to get started.

<DirectoryListing />

---

# Cloudflare SSO

URL: https://developers.cloudflare.com/email-security/reference/cloudflare-sso/

import { Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

You can use your Cloudflare account as the single sign-on (SSO) authentication scheme to log in to the Email Security dashboard:

1. Log in to the [Email Security (formerly Area 1) dashboard](https://horizon.area1security.com/).
2. Select **Sign in with Cloudflare**. You will be redirected to your Cloudflare account to log in.
3. Select **Allow** to allow Email Security to make changes to your Cloudflare account. You will be redirected to the Email Security dashboard.
4. Enter your Email Security's email address to log in.

You can now use your Cloudflare account as a single sign-on authentication scheme to log in to Email Security. The next time you access the Email Security dashboard, just select **Sign in with Cloudflare** to log in.

---

# Dispositions and attributes

URL: https://developers.cloudflare.com/email-security/reference/dispositions-and-attributes/

import { GlossaryTooltip, Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

Email Security uses a variety of factors to determine whether a given email message, domain, URL, or packet is part of a <GlossaryTooltip term="phishing">phishing</GlossaryTooltip> campaign. These small pattern assessments are dynamic in nature and â€” in many cases â€” no single pattern will determine the final verdict.

Based on these patterns, Email Security may add `X-Headers` to each email message that passes through our system.

## Dispositions

Any traffic that flows through Email Security is given a final disposition, which represents our evaluation of that specific message. Each message will only receive one disposition header so your organization can take clear and specific actions on different message types.

You can use disposition values when [creating your quarantine policy](/email-security/email-configuration/domains-and-routing/domains/) or [setting up auto-retract](/email-security/email-configuration/retract-settings/).

### Available values

| Disposition | Description | Recommendation |
| --- | --- | --- |
| `MALICIOUS` | Traffic invoked multiple phishing verdict triggers, met thresholds for bad behavior, and is associated with active campaigns. | Block |
| `SUSPICIOUS` | Traffic associated with phishing campaigns (and is under further analysis by our automated systems). | Research these messages internally to evaluate legitimacy. |
| `SPOOF` | Traffic associated with phishing campaigns that is either non-compliant with your email authentication policies (SPF, DKIM, DMARC) or has mismatching `Envelope From` and `Header From` values. | Block after investigating (can be triggered by third-party mail services). |
| `SPAM` | Traffic associated with non-malicious, commercial campaigns. | Route to existing Spam quarantine folder. |
| `BULK` (dashboard only) | Traffic associated with [Graymail](https://en.wikipedia.org/wiki/Graymail_\(email\)), that fall in between the definitions of `SPAM` and `SUSPICIOUS`. For example, a marketing email that intentionally obscures its unsubscribe link. | Monitor or tag |

### Header structure

When Email Security adds a disposition header to an email message, that header matches the following format:

```txt
X-Area1Security-Disposition: [Value]
```

Note that emails with a disposition of `SPAM` will be tagged with `UCE` (unsolicited commercial emails) in their headers:

```txt
X-Area1Security-Disposition: UCE
```

## Attributes

Traffic that flows through Email Security can also receive one or more **Attributes**, which indicate that a specific condition has been met.

### Available values

| Attribute                               | Notes                                                                                                                                                                                                                               |
| --------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `CUSTOM_BLOCK_LIST`                     | This message matches a value you have defined in your custom block list.                                                                                                                                                            |
| `NEW_DOMAIN_SENDER=<REGISTRATION_DATE>` | Alerts to mail from a newly registered domain. Formatted as yyyy-MM-dd HH:mm:ss ZZZ.                                                                                                                                                |
| `NEW_DOMAIN_LINK=<REGISTRATION_DATE>`   | Alerts to mail with links pointing out to a newly registered domain. Formatted as yyyy-MM-dd HH:mm:ss ZZZ.                                                                                                                          |
| `ENCRYPTED`                             | Email message is encrypted.                                                                                                                                                                                                         |
| `EXECUTABLE`                            | Email message contains an executable file.                                                                                                                                                                                          |
| `BEC`                                   | Indicates that email address was contained in your [business email compromise (BEC)](/email-security/email-configuration/enhanced-detections/business-email-compromise/) list. Associated with `MALICIOUS` or `SPOOF` dispositions. |

### Header structure

When Email Security adds a disposition header to an email message, that header matches the following format.

```txt
X-Area1Security-Attribute: [Value]
X-Area1Security-Attribute: [Value2]
```

---

# How we detect phish

URL: https://developers.cloudflare.com/email-security/reference/how-we-detect-phish/

import { Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

<Render file="reference-detect-phish" />

---

# Reference

URL: https://developers.cloudflare.com/email-security/reference/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Language support

URL: https://developers.cloudflare.com/email-security/reference/language-support/

import { Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

## Email evaluation

The scanning service and verdict engines used by Email Security are language agnostic. We provide support for Double Byte Character sets (DBCS) and UTF-8, UTF-16, and UTF-32 encoding.

## Dashboard

The Email Security email dashboard is localized to several languages. To update your language settings:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).

2. At the bottom of the page, select the language icon.

    <div class="medium-img">

   ![Select the language icon to toggle your dashboard between English and Japanese.](~/assets/images/email-security/language-switcher.png)

    </div>

3. Select your preferred language.

---

# MS Office 365 GCC

URL: https://developers.cloudflare.com/email-security/reference/office365-gcc/

import { Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

Microsoft 365 Government Community Cloud (GCC) is designed to meet the requirements of the US government. GCC Low and GCC High are two tiers of GCC, each with different security and compliance requirements.

GCC Low is intended for use by US government organizations that handle sensitive but unclassified data, and have less stringent compliance requirements.

Email Security supports GCC Low environments.

---

# Timestamps

URL: https://developers.cloudflare.com/email-security/reference/timestamps/

import { Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

<br /><Render file="timestamp" />

The example below shows timestamps for [Audit logs](/email-security/reporting/audit-logs/). However, note that the same applies to all sections in the Email Security dashboard that show timestamps.

![How timestamps are localized to the user's current time zone.](~/assets/images/email-security/timestamps.png)

---

# Reporting

URL: https://developers.cloudflare.com/email-security/reporting/

import { DirectoryListing, Render } from "~/components"

:::caution[Area 1 has been renamed]


<Render file="rename-area1-to-ces" />


:::

Email Security offers a variety of ways for you to better examine and understand your message traffic:

<DirectoryListing />

* The [Email Security API](/email-security/api/) also allows you to download historical records as needed.

:::note
&#x20;<Render file="timestamp" /> 
:::

---

# Audit Logs

URL: https://developers.cloudflare.com/email-security/reporting/audit-logs/

import { Render } from "~/components"

You can use Email Security logs to review actions performed on your account:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. Go to **Users and Actions** > **Audit Log**.
4. Review the logs. You can also filter by type of log from the dropdown menu.

## Logs preview

You can use one of the Preview logs to preview how Email Security handles post delivery retractions. With Audit logs Preview, Email Security shows you the emails that would have been retracted with Post Delivery Response (PDR) or Phish Submissions Response (PSR) enabled.

Refer to **Post delivery retractions for new threats** for [Gmail](/email-security/deployment/api/setup/gsuite-bcc-setup/add-retraction/#post-delivery-retractions-for-new-threats) or [Office 365](/email-security/email-configuration/retract-settings/office365-retraction/#post-delivery-retractions-for-new-threats) to learn more about this feature.

To review preview logs:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. Go to **Users and Actions** > **Audit Log**.
4. From the dropdown, select one of the **Preview** logs. This will show you what would have been retracted with Post Delivery Response or Phish Submission Response enabled.

:::note
&#x20;<Render file="timestamp" /> 
:::

---

# Phish reports

URL: https://developers.cloudflare.com/email-security/reporting/phish-reports/

import { GlossaryTooltip } from "~/components"

Email Security automatically generates <GlossaryTooltip term="phishing">phish</GlossaryTooltip> reports to provide an overview of your email traffic. The report only includes malicious emails. Spam and bulk emails are not included.

## In the dashboard

To view phishing reports in the Email Security dashboard, [log in](https://horizon.area1security.com/) and explore the non-Settings areas of the Email Security dashboard (**Home**, **Email**, **Web**, and **Detection Details**).

## Through an email subscription

The same reports that are visible through the dashboard can also be delivered through an email.

To subscribe an email address to daily or weekly reports:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/).
2. Go to **Settings** (the gear icon).
3. Go to **Subscriptions** > **Email Subscriptions**.
4. Select **Add Subscriber**.
5. Enter an **Email** and choose the desired reports.
6. Select **Add Subscriber**.

---

# Statistics overview

URL: https://developers.cloudflare.com/email-security/reporting/statistics-overview/

To access an overview of your account, total number of emails processed, a breakdown of types of threads detected, among other types of information:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/users/login).
2. Make sure you are in the Home section to review information regarding your account:

| Field                           | Description                                                                                                                                                              |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **System stats**                | <ul><li>Status of Area 1â€™s services</li> <li>Uptime of Area 1â€™s services as well as any downtime</li> <li>Number of processed emails and attacks prevented</li></ul>     |
| **Detection stats**             | Statistics regarding the total number of detections made, and emails processed.                                                                                          |
| **Retractions**                 | Shows the distribution of messages removed from your user's mailboxes.                                                                                                   |
| **Phish Submissions Stats**     | Statistics regarding the number of phish emails submitted by your users and security operations center (SOC)                                                             |
| **Threat Origins**              | Top geographical threat origins to your organization.                                                                                                                    |
| **Org Spoofs**                  | Shows attacks where names in envelopes differ from the header, as well as spoofed domains.                                                                               |
| **Domain Proximity**            | List of domains similar to your own.                                                                                                                                     |
| **Malicious Threat Type**       | Breakdown of malicious threat types.                                                                                                                                     |
| **Email Link Isolation**        | How many email were processed by [Email Link Isolation](/email-security/email-configuration/email-policies/link-actions/#email-link-isolation).                          |
| **Top BEC Targets**             | What email addresses are the top targets on the [Business Email Compromise feature](/email-security/email-configuration/enhanced-detections/business-email-compromise/). |

---

# Types of malicious detections

URL: https://developers.cloudflare.com/email-security/reporting/types-malicious-detections/

To review the number and type of malicious detections made on your account:

1. Log in to the [Email Security dashboard](https://horizon.area1security.com/users/login).
2. Select the **Email** tab.
3. The **Overview** section will show you graphs with the total number of emails processed, as well as how many of those pertain to different threat categories - such as Malicious or Spam, among others. Refer to [Dispositions and attributes](/email-security/reference/dispositions-and-attributes/) for more information. Select **View Details**.
4. You will open the **Detections** page. This page breaks down the information regarding the various types of threats detected. You have access to:

| Field                           | Description                                                                                                                                                                     |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Attachments**                 | <ul><li>How many of the malicious emails received have an attachment.</li> <li>Of these, what are the top types of malicious files received (for example, PDF files).</li></ul> |
| **Senders**                     | <ul><li>Total number of malicious senders, as well as a graph showing how they are distributed throughout the month.</li> <li>Top malicious domains.</li></ul>                  |
| **Targets**                     | Top email targets on the [BEC feature](/email-security/email-configuration/enhanced-detections/business-email-compromise/).                                                     |
| **New domains**                 | <ul><li>Total number of malicious domains registered in the past month.</li> <li>Most common top level malicious domains.</li></ul>                                             |
| **Links**                       | <ul><li>Total number of malicious links and their distribution throughout the month.</li> <li>Top threat types (for example, credential harvester).</li></ul>                   |
| **Threat types**                | Top malicious threat types, and their percentage relatively to the total amount of threats received.                                                                            |
| **Threat origins**              | A graph representing where in the world are your top threat origins.                                                                                                            |

---

# Cloudflare's API

URL: https://developers.cloudflare.com/fundamentals/api/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Troubleshooting

URL: https://developers.cloudflare.com/fundamentals/api/troubleshooting/

## The token is not verified

Ensure the token has been verified by running the following `curl` command and confirming that the response returns `"status": "active"`.

```bash
curl "https://api.cloudflare.com/client/v4/user/tokens/verify" \
--header "Authorization: Bearer <API_TOKEN>"
```

```json
{
  "success": true,
  "errors": [],
  "messages": [],
  "result": {
    "id": "f267e341f3dd4697bd3b9f71dd96247f",
    "status": "active",
    "not_before": "2018-07-01T05:20:00Z",
    "expires_on": "2020-01-01T00:00:00Z"
  }
}
```

## The token has incorrect permissions

Review the permissions groups for your token in the [Cloudflare dashboard](https://dash.cloudflare.com/profile/api-tokens). Refer to [API token permissions](/fundamentals/api/reference/permissions/) for more information.

## The incorrect syntax is used

Occasionally customers will attempt to use an API token with an API key syntax. Ensure you are using the Bearer option rather than the email and API key pair.

## You have the incorrect user permissions

You cannot create a token that exceeds the permission granted to you on your account. For example, if you have been granted an **Admin (Read only)** role, you would need your Super Administrator to update your role so that you could create a token for yourself.

---

# Cloudflare IP addresses

URL: https://developers.cloudflare.com/fundamentals/concepts/cloudflare-ip-addresses/

import { Render } from "~/components"

<Render file="cloudflare-ips" product="fundamentals" />

## Allow Cloudflare IP addresses

<Render file="allow-cloudflare-ips" product="fundamentals" />

## Configure origin server

### Allowlist Cloudflare IP addresses

<Render file="allow-cloudflare-ips-tactical" product="fundamentals" />

### Block other IP addresses (recommended)

<Render file="block-cloudflare-ips-tactical" product="fundamentals" />

## Review external tools

To avoid blocking Cloudflare IP addresses unintentionally, review your external tools to check that:

* Any security plugins â€” such as those for WordPress â€” allow Cloudflare IP addresses.
* The [ModSecurity](https://github.com/SpiderLabs/ModSecurity) plugin is up to date.

### Additional recommendations

#### Further protection

For further recommendations on securing your origin server, refer to our guide on [protecting your origin server](/fundamentals/security/protect-your-origin-server/).

### Customize Cloudflare IP addresses

<Render file="customize-cloudflare-ips" product="fundamentals" />

### IP range updates

Cloudflare's IP ranges do not change frequently. When they do change, they are added to our [list of IP ranges](https://www.cloudflare.com/en-in/ips/) before being put into production. You can also use the Cloudflare API to programmatically keep your configuration updated.

---

# Concepts

URL: https://developers.cloudflare.com/fundamentals/concepts/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Performance

URL: https://developers.cloudflare.com/fundamentals/performance/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# How Cloudflare CDN works

URL: https://developers.cloudflare.com/fundamentals/concepts/how-cloudflare-works/

import { GlossaryTooltip } from "~/components"

To optimize your website or web application, Cloudflare provides [DNS](https://www.cloudflare.com/learning/dns/what-is-dns/) and [CDN](https://www.cloudflare.com/learning/cdn/what-is-a-cdn/) services, so we can [reverse proxy](https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/) the web traffic to and from your domain.


## DNS explained

The Domain Name System (DNS) acts as the Internet's phonebook, translating domain names (for example, `cloudflare.com`) into numerical Internet Protocol (IP) addresses (for example, `103.21.244.0`).

The IP address is like a home address of where a website lives, and the domain name is the human-readable name.

A DNS query is like asking for directions to a place, and the DNS records are the source-of-truth for what exists where. DNS records live in authoritative [DNS servers](https://www.cloudflare.com/learning/dns/dns-server-types/) and provide information about a domain, such as the [IP addresses](https://www.cloudflare.com/learning/dns/glossary/what-is-my-ip-address/) of the servers that host the web content and services on that domain. With this information, Internet browsers know where to find a website or app, so they can render it for visitors using [HTTP](https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/).

## Cloudflare as a DNS provider

When you onboard your website or application to Cloudflare, Cloudflare becomes the primary authoritative DNS provider for your domain. As the primary authoritative DNS provider, Cloudflare responds to DNS queries for your domain, and you manage your domain's DNS records via the Cloudflare dashboard or API.

:::note
Cloudflare only becomes the primary authoritative DNS provider when you use the default, full DNS setup. For alternative options, refer to [DNS setups](/dns/zone-setups/).
:::

If your [domain's status](/dns/zone-setups/reference/domain-status/) is active and the queried DNS record is set to `proxied`, Cloudflare responds with an [anycast IP address](/fundamentals/concepts/cloudflare-ip-addresses/), instead of the origin IP address defined in your DNS table.

Your domain status is active when your [nameservers are updated](/dns/nameservers/update-nameservers/) to point to Cloudflare and have been authenticated. The [proxy status](/dns/proxy-status/) defines how Cloudflare treats queries for specific DNS records. The [anycast IP address](/fundamentals/concepts/cloudflare-ip-addresses/) is used to distribute traffic amongst Cloudflare's network, which protects your website or app from [DDoS](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/) and other attacks, while optimizing site speed.

## Cloudflare as a reverse proxy

A reverse proxy is a network of servers that sits in front of web servers and either forwards requests to those web servers, or handles requests on behalf of the web servers. Reverse proxies are typically implemented to help increase security, performance, and reliability of websites and web applications.

![The flow of a request from a server through Cloudflare to the origin server when Cloudflare is a reverse proxy.](~/assets/images/fundamentals/reverse-proxy.png)

When Cloudflare receives a DNS query for your domain, the response is determined by the configuration [set in your DNS table](/dns/manage-dns-records/how-to/create-dns-records/), including the [type of the record](/dns/manage-dns-records/reference/dns-record-types/), the record's [proxy eligibility](/dns/proxy-status/limitations/#proxy-eligibility), and its [proxy status](/dns/proxy-status/#proxied-records).

When DNS records in your DNS table have a `proxied` status, the record's HTTP/HTTPS traffic will route through Cloudflare on its way between the client and the origin server. If the domain's status is active, all HTTP/HTTPS requests for proxied DNS records route through Cloudflare.

Using Cloudflare as a reverse proxy has several benefits, including:

- **Load balancing** A reverse proxy can provide a load balancing solution which distributes incoming traffic evenly among different servers to prevent any single server from becoming overloaded. In the event that a server fails completely, other servers can step up to handle the traffic.
- **Protection from attacks.** With a reverse proxy in place, a web site or service never needs to reveal the IP address of their origin servers, which makes it much harder for attackers to leverage a targeted attack against them, such as a DDoS attack. Instead the attackers will only be able to target the reverse proxy, such as Cloudflare's CDN, which will have tighter security and more resources to fend off a cyber attack.
- **Caching.** A reverse proxy can also cache content, resulting in faster performance. For example, if a user in Paris visits a reverse-proxied website with web servers in Los Angeles, the user might actually connect to a local reverse proxy server in Paris, which will then have to communicate with an origin server in L.A. The proxy server can then cache (or temporarily save) the response data. Subsequent Parisian users who browse the site will then get the locally cached version from the Parisian reverse proxy server, resulting in much faster performance.
- **SSL encryption.** SSL/TLS is essential. Without an SSL/TLS certificate, your visitors will find a warning on their browser stating your website or application is not secure. However, encrypting and decrypting SSL (or TLS) communications for each client can be computationally expensive for an origin server. A reverse proxy can be configured to decrypt all incoming requests and encrypt all outgoing responses, freeing up valuable resources on the origin server.

---

# Improve SEO

URL: https://developers.cloudflare.com/fundamentals/performance/improve-seo/

The goal of Search Engine Optimization (SEO) is to get your website to rank higher on various search engine providers (Google, Bing, etc.).

In practice, SEO is primarily about quality content, user experience, and not making things more difficult for search engine crawlers. While Cloudflare cannot write quality content for you, our service can help with user experience â€” especially related to [site speed](https://www.cloudflare.com/learning/performance/how-website-speed-boosts-seo/) â€” and search crawlers.

:::note[Tip:]


For general guidelines around SEO, refer to [Google's recommendations](https://developers.google.com/search/docs/advanced/guidelines/overview).


:::

## SEO improvements with Cloudflare

Several Cloudflare features improve Search Engine site rankings. However, meaningful and regularly updated site content is still crucial to improving SEO.

### Increase site speed

Since at least 2010, Google has publicly stated that [site speed affects your Google ranking](https://webmasters.googleblog.com/2010/04/using-site-speed-in-web-search-ranking.html).

Cloudflare offers multiple features to [optimize site performance](/speed/).

### Enable HTTPS

Since search engines use HTTPS as [a ranking signal](https://webmasters.googleblog.com/2014/08/https-as-ranking-signal.html), HTTPS is vital for SEO.

To make sure your domain is accessible over HTTPS:

1. Get an [SSL/TLS certificate](/ssl/get-started/) for your domain.
2. [Redirect visitors](/ssl/edge-certificates/encrypt-visitor-traffic/) to the HTTPS version of your domain.

### Enable Crawler Hints

With [Crawler Hints](/cache/advanced-configuration/crawler-hints/), search engines and other bot-powered experiences have the freshest version of your content, translating into happier users and ultimately influencing search rankings.

### Enable Automatic Signed Exchanges (SXGs)

Automatic Signed Exchanges (SXGs) is an open web platform specification developed by Google to verify a cached version of a website.

When you enable [signed exchanges](/speed/optimization/other/signed-exchanges/), your site will load faster when linked to from a site supporting SXG. Since many search engines use page load times to determine search result ranking, SXGs can provide an SEO boost.

## Troubleshooting

Depending on your domain's security settings, you might accidentally block search engine crawlers.

If you notice SEO issues, make sure your:

* [WAF custom rules](/waf/troubleshooting/faq/#caution-about-potentially-blocking-bots) are allowing **Verified Bots**.
* [Rate limiting rules](/waf/rate-limiting-rules/) are allowing **Verified Bots**.
* [Bot protection](/bots/concepts/bot/#verified-bots) settings are not blocking **Verified Bots**.

If you still notice issues with search engine crawlers, refer to our [Troubleshooting guide](/support/troubleshooting/general-troubleshooting/troubleshooting-crawl-errors/).

## Common misconceptions

The following characteristics do not affect your domain's SEO:

* **Changing your nameservers**: Using Cloudflare's nameservers does not affect your domain's SEO.
* **Server location**: According to Google, [server location](http://www.seroundtable.com/seo-geo-location-server-google-17468.html) is not important for SEO.
* **Sites sharing IP addresses**: Search engines do not generally penalize domains using shared IP addresses unless several of these sites are malicious or spammy.
* **Cloudflare caching**: When Cloudflare caches your content, it actually speeds up content delivery and only improves SEO. Our caching does not create duplicate content, rewrite URLs, or create additional subdomains.

---

# Maintenance mode

URL: https://developers.cloudflare.com/fundamentals/performance/maintenance-mode/

If you need to make large changes to your website, you may want to make your site temporarily unavailable.

## With code

If you are familiar with code, [create a Worker](/workers/get-started/guide/) that returns an [HTML page](/workers/examples/return-html/) to any site visitors.

![Workers maintenance page returned instead of your website](~/assets/images/fundamentals/workers-page.png)

## Without code

### Business and Enterprise

For a maintenance page without code, Business and Enterprise uses can create a [Cloudflare Waiting Room](/waiting-room/how-to/create-waiting-room/).

Certain customization and queue options depend on your [plan](/waiting-room/plans/).

![Waiting room page returned instead of your website](~/assets/images/fundamentals/waiting-room-page.png)

### All plans

Users on all plans can [create an Access application](/cloudflare-one/applications/configure-apps/self-hosted-public-app/). Make sure to limit your [Access policy](/cloudflare-one/policies/access/policy-management/#create-a-policy) to only include yourself and any collaborators.

If needed, you can also further [customize the login page](/cloudflare-one/applications/login-page).

![Example Access login page](~/assets/images/fundamentals/access-page.png)

---

# Minimize downtime

URL: https://developers.cloudflare.com/fundamentals/performance/minimize-downtime/

import { Render } from "~/components"

<Render file="minimize-downtime" product="fundamentals" />

---

# Prepare for surges or spikes in web traffic

URL: https://developers.cloudflare.com/fundamentals/performance/preparing-for-surges-or-spikes-in-web-traffic/

## Use Cloudflare Cache features to optimize caching

By default, CloudflareÂ [caches static content](/cache/concepts/default-cache-behavior/) such as images, CSS, and JavaScript. However, you can extend Cloudflare caching to work with HTMLÂ by creating customÂ [Cache Rules](/cache/how-to/cache-rules/).

### Cache more requests

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login), and select your account and domain.

2. Go to **Caching** >Â **Cache Rules** and selectÂ **Create rule**.

3. For **When incoming requests match**, enter either your entire website or a specific path on your application, based on the **Hostname** or **URI Path**. Refer to the [available fields](/cache/how-to/cache-rules/settings/#fields).

4. For **Cache eligibility**, define how these requests should be cached and for how long. Refer to the available [cache eligibility settings](/cache/how-to/cache-rules/settings/#eligible-for-cache-settings).

5. You can then monitor the effectiveness of your cache settings using [Cache Analytics](/cache/performance-review/cache-analytics/) and update your configuration according to our [Cache performance guide](/cache/performance-review/cache-performance/).

### Advanced cache optimizations

* [Custom Cache Keys](/cache/how-to/cache-keys/) allows you to precisely set the cacheability setting for any resource.

* [Origin Cache Control](/cache/concepts/cache-control/) can be used to let the `Cache-Control` headers tell Cloudflare how to handle content from the origin server.

### Use Tiered Cache

[Tiered Cache](/cache/how-to/tiered-cache/) uses the size of Cloudflare's network to reduce requests to customer origin servers by dramatically increasing cache hit ratios.

It works by dividing Cloudflare's data centers into a hierarchy of lower-tiers and upper-tiers. If content is not cached in lower-tier data centers (generally the ones closest to a visitor), the lower-tier requests an upper-tier for the content. If the upper-tier does not have the content, only the upper-tier will initiate a request to the origin. This practice improves bandwidth efficiency by limiting the number of Cloudflare data centers that can ask the origin for content.

Refer to [Enable Tiered Cache](/cache/how-to/tiered-cache/#enable-tiered-cache) to get started.

### Use Cache Reserve

[Cache Reserve](/cache/advanced-configuration/cache-reserve/) is a large, persistent data store implemented on top of [R2](/r2/).

With a single click in the dashboard, your cacheable content will be written to Cache Reserve. In the same way that Tiered Cache builds a hierarchy of caches between your visitors and your origin, Cache Reserve serves as the ultimate [upper-tier cache](/cache/how-to/tiered-cache/) that will reserve storage space for your assets for as long as you want.

This ensures that your content is served from cache longer, shielding your origin from unneeded egress fees.

## Understand the limits of your hosting plan

Cloudflare offsets most of the load to your website via caching and request filtering, but some traffic will still pass through to your origin. Knowing the limits of your hosting plan can help prevent a bottleneck from your host.Â 

Once you are aware of your plan limits, you can useÂ [Rate Limiting](/waf/rate-limiting-rules/)Â to restrict how many times a requesting entity can make a request to your website.

To help you define the best rate limiting setting for your use case, refer to [How Cloudflare determines the request rate article](/waf/rate-limiting-rules/request-rate/).

## Cloudflare Waiting Room

[Cloudflare Waiting Room](/waiting-room/) allows you to route excess users of your website to a customized waiting room, helping preserve customer experience and protect origin servers from being overwhelmed with requests.

## Use Cloudflare IP addresses to your advantage

Take action to prevent attacks to your application during peak season by configuring your firewall to only accept traffic from Cloudflare IP addresses. By only allowing [Cloudflare IPs](https://www.cloudflare.com/ips), you can prevent attackers from bypassing Cloudflare and sending requests directly to your origin.

Refer to [Cloudflare IP addresses](/fundamentals/concepts/cloudflare-ip-addresses/) for more information.

## Monitor traffic in your Cloudflare dashboard

You can use the Cloudflare dashboard to closely monitor the traffic on your domain and fine-tune your cache and security settings accordingly.

### Zone and Account analytics

[Cloudflare zone analytics](/analytics/account-and-zone-analytics/zone-analytics/) gives you access to a wide range of metrics, collected at the website or domain level.

[Cloudflare account analytics](/analytics/account-and-zone-analytics/account-analytics/) lets you access a wide range of aggregated metrics from all the sites under a specific Cloudflare account.

### Security Analytics and Security Events

[Security Analytics](/waf/analytics/security-analytics/) displays information about all incoming HTTP requests for your domain, including requests not handled by Cloudflare security products.

You can also use the [Security Events](/waf/analytics/security-events/) to review mitigated requests and tailor your security configurations.

### Cache Analytics

You can use [Cache Analytics](/cache/performance-review/cache-analytics/) to improve site performance or reduce origin web server traffic.
Cache Analytics helps determine if resources are missing from cache, expired, or ineligible for caching.

---

# Test speed

URL: https://developers.cloudflare.com/fundamentals/performance/test-speed/

Cloudflare offers several tools to test the speed of your website, as well as the speed of your Internet connection.

***

## Test website speed

### Using Cloudflare

Once your domain is [active on Cloudflare](/fundamentals/setup/manage-domains/add-site/), you can run speed tests within the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/:zone/speed).

This speed test will provide information about critical loading times, performance with and without [Cloudflare's proxy](/fundamentals/concepts/how-cloudflare-works/), and recommended optimizations.

If you experience any issues, make sure you are not blocking specific [user agents](/fundamentals/reference/cloudflare-site-crawling/#other-situations).

### Using third-party tools

If your domain is not yet active on Cloudflare or you want to measure the before and after improvements of using Cloudflare, Cloudflare recommends using the following third-party tools:

* [GTmetrix](https://gtmetrix.com/)
* [DebugBear](https://www.debugbear.com/test/website-speed)
* [Lighthouse](https://developer.chrome.com/docs/lighthouse/)
* [WebPageTest](https://www.webpagetest.org/)

If you use these third-party tools, you should do the following to test website speed:

1. [Pause Cloudflare](/fundamentals/setup/manage-domains/pause-cloudflare/) to remove performance and caching benefits.
2. Run a speed test.
3. Unpause Cloudflare.
4. Run a speed test[^1].
5. Run a second speed test to get your baseline performance with Cloudflare.

[^1]: The results of your first speed test with Cloudflare will likely contain uncached results, which will provide inaccurate results.<br/><br/>One of the key ways Cloudflare speeds up your site is through [caching](/learning-paths/get-started/concepts/how-cloudflare-works/#performance), which will appear in the results of the second test.

### Improve speed

Based on the results of these speed tests, you may want to explore other ways to [optimize your site speed](/speed/) using Cloudflare.

:::note


Cloudflare does not consider Time to First Byte (TTFB) the most important measure of page load speed. If you are concerned about a slower TTFB while using Cloudflare, refer to our blog post about [Cloudflare and TTFB](http://blog.cloudflare.com/ttfb-time-to-first-byte-considered-meaningles/).


:::

***

## Test Internet speed

To test the speed of your home network connection (download, update, packet loss, ping measurements, and more), visit [speed.cloudflare.com](https://speed.cloudflare.com).

---

# /cdn-cgi/ endpoint

URL: https://developers.cloudflare.com/fundamentals/reference/cdn-cgi-endpoint/

When you [add a domain to Cloudflare](/fundamentals/setup/manage-domains/add-site/), Cloudflare adds a `/cdn-cgi/` endpoint (`www.example.com/cdn-cgi/`) to that domain.

This endpoint is managed and served by Cloudflare. It cannot be modified or customized. The endpoint is not used by every Cloudflare product, but you may find some products use the endpoint in its URL.

A few examples include (but are not limited to):

* [Identify the Cloudflare data center serving your request](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/#identify-the-cloudflare-data-center-serving-your-request), which is helpful for troubleshooting (`https://<YOUR_DOMAIN>/cdn-cgi/trace`).
* [JavaScript detection](/bots/reference/javascript-detections/) used by Cloudflare bot products (`example.com/cdn-cgi/challenge-platform/`)
* [Image transformations](/images/transform-images) in the new URLs you would use for images (`example.com/cdn-cgi/image/`)
* [Email address obfuscation](/waf/tools/scrape-shield/email-address-obfuscation/) used to hide email addresses from malicious bots (`example.com/cdn-cgi/l/email-protection`)
* [Web analytics](/web-analytics/get-started/#sites-proxied-through-cloudflare) for a website proxied through Cloudflare (`example.com/cdn-cgi/rum`). This endpoint returns a `204` HTTP status code.
* [Speed Brain](/speed/optimization/content/speed-brain/) adds an HTTP header called `Speculation-Rules` to web page responses. This header contains a URL that hosts an opinionated Speculation-Rules configuration, which instructs the browser to initiate prefetch requests for anticipated future navigations.

## Recommended exclusions

### Exclude from security scanners

Some scanners may display an error because certain `/cdn-cgi/` endpoints do not have an [HSTS setting](/ssl/edge-certificates/additional-options/http-strict-transport-security/) applied to it or for similar reasons. Because the endpoint is managed by Cloudflare, you can ignore the error and do not need to worry about it.

To prevent scanner errors, omit the `/cdn-cgi/` endpoint from your security scans.

### Disallow using robots.txt

`/cdn-cgi/` also can cause issues with various web crawlers.

Search engine crawlers can encounter [errors when crawling these endpoints](/support/troubleshooting/general-troubleshooting/troubleshooting-crawl-errors/) and â€” though these errors do not impact site rankings â€” they may surface in your webmaster dashboard.

SEO and other web crawlers may also mistakenly crawl these endpoints, thinking that they are part of your site's content.

As a best practice, update your `robots.txt` file to include `Disallow: /cdn-cgi/`.

---

# Cloudflare Ray ID

URL: https://developers.cloudflare.com/fundamentals/reference/cloudflare-ray-id/

A **Cloudflare Ray ID** is an identifier given to every request that goes through Cloudflare.

Ray IDs are particularly useful when evaluating Security Events for patterns or false positives or more generally understanding your application traffic.

:::caution
Ray IDs are not guaranteed to be unique for every request. In some situations, different requests may have the same Ray ID.
:::

## Look up Ray IDs

### Security events

All customers can view Ray IDs and associated information â€” IP address, user agent, ASN, etc. â€” by looking through [sampled logs](/waf/analytics/security-events/#sampled-logs) in Security Events.

![Example list of events in sampled logs, with one of the events expanded to show its details](~/assets/images/waf/events-sampled-logs.png)

Additionally, you can [add filters](/waf/analytics/security-events/#adjust-displayed-data) to look for specific Ray IDs.

![Example of adding a new filter in Security Events for the Block action](~/assets/images/waf/events-add-filter.png)

Please note that Security Events may use sampled data to improve performance. If sampled data is applied to your search, you might not see all events, and filters might not return the expected results. To display more events, select a smaller timeframe.

### Logs

Enterprise customers can enable Ray ID as a field in their [Cloudflare Logs](/logs/).

### Server logs

For more details about sending Ray IDs to your server logs, refer to the [Cf-Ray](/fundamentals/reference/http-headers/#cf-ray) header.

---

# Connection limits

URL: https://developers.cloudflare.com/fundamentals/reference/connection-limits/

import { GlossaryTooltip } from "~/components"

When HTTP/HTTPS traffic is [proxied through Cloudflare](/fundamentals/concepts/how-cloudflare-works/#cloudflare-as-a-reverse-proxy), there are often two established [TCP connections](/fundamentals/reference/tcp-connections/): the first is between the requesting client to Cloudflare and the second is between Cloudflare and the origin server. Each connection has their own set of TCP and HTTP limits, which are documented below.

## Between client and Cloudflare

| Type                           | Limit (seconds) | HTTP status code at limit | Configurable |
| ------------------------------ | --------------- | ------------------------- | ------------ |
| Connection Keep-Alive HTTP/1.1 | 400             | TCP connection closed     | No           |
| Connection Idle HTTP/2         | 400             | TCP connection closed     | No           |

## Between Cloudflare and origin server

:::note

If you are using [Cloudflare tunnels](/cloudflare-one/connections/connect-networks/), refer to [Origin configuration](/cloudflare-one/connections/connect-networks/configure-tunnels/cloudflared-parameters/origin-parameters/) to view or modify your connection settings.
:::

| Type                                                                                      | Limit (seconds) | HTTP status code at limit                                                                                                              | [Configurable](/fundamentals/reference/connection-limits/#configurable-limits) |
| ----------------------------------------------------------------------------------------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| <GlossaryTooltip term="TCP three-way handshake">Complete TCP Connection</GlossaryTooltip> | 15              | [522](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-522-connection-timed-out)                | No                                                                             |
| <GlossaryTooltip term="ACK (Acknowledge)">TCP ACK</GlossaryTooltip> Timeout               | 90              | [522](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-522-connection-timed-out)                | No                                                                             |
| <GlossaryTooltip term="TCP Keep-Alive">TCP Keep-Alive</GlossaryTooltip> Interval          | 30              | [520](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-520-web-server-returns-an-unknown-error) | No                                                                             |
| <GlossaryTooltip term="idle connection">Proxy Idle</GlossaryTooltip> Timeout              | 900             | [520](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-520-web-server-returns-an-unknown-error) | No                                                                             |
| <GlossaryTooltip term="proxy read timeout">Proxy Read Timeout</GlossaryTooltip>           | 100             | [524](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-524-a-timeout-occurred)                  | [Yes](/api/resources/zones/subresources/settings/methods/edit/)                       |
| <GlossaryTooltip term="proxy write timeout">Proxy Write Timeout</GlossaryTooltip>         | 30              | [524](/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-524-a-timeout-occurred)                  | No                                                                             |
| HTTP/2 Pings to Origin                                                                    | Off             | -                                                                                                                                      | Yes                                                                            |
| <GlossaryTooltip term="idle connection">HTTP/2 Connection Idle</GlossaryTooltip>          | 900             | No                                                                                                                                     | No                                                                             |

## Configurable limits

Some TCP connections can be customized for Enterprise customers. Reach out to your account team for more details.

## Keep-Alives

Cloudflare maintains keep-alive connections to improve performance and reduce cost of recurring TCP connects in the request transaction as Cloudflare proxies customer traffic from its global network to the site's origin server.

Ensure HTTP keep-alive connections are enabled on your origin. Cloudflare reuses open TCP connections up to the `Proxy Idle Timeout` limit after the last HTTP request. Origin web servers close TCP connections if too many are open. HTTP keep-alive helps avoid connection resets for requests proxied by Cloudflare.

---

# Cloudflare crawlers

URL: https://developers.cloudflare.com/fundamentals/reference/cloudflare-site-crawling/

Cloudflare may crawl or make HTTP requests to your site to make sure its protected and performing properly.

## Crawling situations

### Specific products

Cloudflare will crawl your site when you have specific products enabled:

* [**Always Online**](/cache/how-to/always-online/)
  * *User-Agent*: `Mozilla/5.0 (compatible; CloudFlare-AlwaysOnline/1.0; +http://www.cloudflare.com/always-online)`
* [**Health checks**](/health-checks/)
  * *User-Agent*: `Mozilla/5.0 (compatible; Cloudflare-Healthchecks/1.0; +https://www.cloudflare.com/; healthcheck-id: <HEALTHCHECK_ID>)`
  * `HEALTHCHECK_ID` is a 16-character string associated with the health check ID.
* [**Load balancing monitors**](/load-balancing/monitors/)
  * *User-Agent*: `Mozilla/5.0 (compatible; Cloudflare-Traffic-Manager/1.0; +https://www.cloudflare.com/traffic-manager/; pool-id: <POOL_ID>)`
  * `POOL_ID` is a 16-character string associated with the load balancing pool ID being monitored.
* [**Prefetch URLs**](/speed/optimization/content/prefetch-urls/)
  * *User-Agent*: `Mozilla/5.0 (compatible; CloudFlare-Prefetch/0.1; +http://www.cloudflare.com/)`
* [**SSL/TLS recommender**](/ssl/origin-configuration/ssl-tls-recommender/)
  * *User-Agent*: `Cloudflare-SSLDetector`
  * This crawler ignores your `robots.txt` file unless there are rules explicitly targeting the user agent.
* [**Security Insights**](/security-center/security-insights/review-insights/)
  * *User-Agent*: `Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36 (compatible; +https://developers.cloudflare.com/security-center/)`

### Other situations

Cloudflare will also crawl your site in other, specific situations:

* **Speed tests**
  * *User-Agent*: `Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36 PTST/190628.140653`
  * *Triggered when*: You launch a speed test from within [the Cloudflare dashboard](/speed/speed-test/run-speed-test/).
* **Support diagnostics**:
  * *User-Agent*: `Cloudflare-diagnostics`
  * *Triggered when*: Cloudflare Support Engineers perform error checks and by continuous monitoring used to raise intelligent alerts in the Cloudflare dashboard.
* **Custom Hostname validation**:
  * *User-Agent*: `Cloudflare Custom Hostname Verification`
  * *Triggered when*: You choose to validate a custom hostname with an [HTTP ownership token](/cloudflare-for-platforms/cloudflare-for-saas/domain-support/hostname-validation/pre-validation/#http-tokens).

---

# Cryptographic Attestation of Personhood

URL: https://developers.cloudflare.com/fundamentals/reference/cryptographic-personhood/

Cloudflare developed an [alternative](https://blog.cloudflare.com/introducing-cryptographic-attestation-of-personhood/) to CAPTCHA authentication, the Cryptographic Attestation of Personhood (CAP).

CAP lets you prove that you are a legitimate website visitor by touching a hardware key, instead of solving a CAPTCHA puzzle.

This article provides answers to common questions about usability and privacy concerns.

You can also test CAP by going to the [demo site](https://cloudflarechallenge.com/).

## Privacy questions

The answer to most privacy concerns are summarized in this table:

| Property                                              | Cloudflare could                                 | Cloudflare does     |
| ----------------------------------------------------- | ------------------------------------------------ | ------------------- |
| Collect biometrics (fingerprints or face pictures)    | No                                               | N/A                 |
| Collect information about your hardware authenticator | Yes, limited to the number of keys in your batch | Yes, when available |

No, Cloudflare cannot collect biometrics. Our CAP process uses the WebAuthn API, which prevents the collection of [biometrics by default](https://www.w3.org/TR/webauthn-2/#sctn-biometric-privacy). When your device asks for a biometric authentication â€” such as via a fingerprint sensor â€” it all happens locally.Â 

As such, we never see your biometric data: that remains on your device. Once your device confirms a match, it sends only a basic attestation message. In effect, your device sends a message proving â€œyes, someone correctly entered a fingerprint on this trustworthy deviceâ€ and never sends the fingerprint itself.

Yes, Cloudflare does collect a limited amount of data about your key. We store the manufacturer of your key and batch identifier ([minimum of 100,000](https://fidoalliance.org/specs/fido-uaf-v1.1-ps-20170202/fido-uaf-protocol-v1.1-ps-20170202.html#full-basic-attestation) keys per batch) for verification purposes. From our perspective, your key looks like all other keys in the batch.

Some self-signed keys and keys from certain manufacturers have been found to [not meet this requirement](https://www.chromium.org/security-keys) and should be avoided if you are minimizing your online privacy risk.

***

For more details on how we set up Cryptographic Attestation of Personhood, refer to the [introductory blog post](https://blog.cloudflare.com/introducing-cryptographic-attestation-of-personhood/).

***

## What devices are and are not allowed?

### Allowed devices

CAP supports a wide variety of hardware authenticators:

* **Roaming (cross-platform) authenticators**:
  * *Supported*: All security keys found in the [FIDO Metadata Service 3.0](https://fidoalliance.org/metadata/), unless they have been revoked for security reasons.
  * *Examples*: YubiKeys, HyperFIDO keys, Thetis FIDO U2F keys
* **Platform authenticators:**
  * *Examples*: Apple Touch ID and Face ID on iOS mobile devices and macOS laptops; Android mobile devices with fingerprint readers; Windows Hello

### Known limitations

Most combinations of of web browsers and WebAuthn-capable authenticators will work, but there are some known compatibility issues with WebAuthn attestation that may prevent CAP from working successfully:

* **Basic CAP**:
  * *macOS desktop*: For TouchID, browser must be Safari
  * *Android*: Browser must be Chrome
* **CAP with Zero-Knowledge Proof**:
  * *Apple platform authenticators* (e.g., iPhone with Touch ID/Face ID) are incompatible with the [zero-knowledge proof system](https://blog.cloudflare.com/introducing-zero-knowledge-proofs-for-private-web-attestation-with-cross-multi-vendor-hardware/). If this fails, you will immediately be redirected to basic CAP route without having to take any further action. Since Apple uses a privacy-preserving [Apple Anonymous Attestation](https://www.w3.org/TR/webauthn/#sctn-apple-anonymous-attestation) to show that an authenticator is valid while blocking tracking, this method maintains a high standard of privacy.

We are updating this list as the ecosystem evolves and as we continue to test different combinations.

## Can hackers bypass the Cryptographic Attestation of Personhood?

CAP is one of many techniques to identify and block bots. To date, we have seen some attempts to test CAPâ€™s security system, such as [one thoughtfully-executed, well-documented test](https://betterappsec.com/building-a-webauthn-click-farm-are-captchas-obsolete-bfab07bb798c). The blog post discussing the test specifically calls out that this method does not break the Cloudflare threat model.

This does not mean that CAP is broken, but rather shows that it raises the cost of an attack over the current CAPTCHA model.

## What happens if I lose my key?

If you do not have the necessary hardware (such as a Yubikey), you can still solve a regular CAPTCHA challenge (e.g., selecting pictures).

## What are the common error codes and what do they mean?

* **Unsupported\_att\_fmt**:
  * *Cause*: Your authenticator is using an unsupported attestation format (combination of browser and key). Also occurs when you use *Firefox* and select the option to "anonymise your key".
  * *Solution:* If this error occurs during [zero-knowledge version of CAP](https://blog.cloudflare.com/introducing-zero-knowledge-proofs-for-private-web-attestation-with-cross-multi-vendor-hardware/), you will automatically be redirected to the basic CAP flow. If basic CAP fails, try a different combination of supported hardware device and browser or opt for a CAPTCHA.
* **Unsupported\_issuer**:
  * *Cause*: Your key is currently not supported.
  * *Solution*: Use a [supported key](#allowed-devices).

## Related resources

* [https://cloudflarechallenge.com](https://cloudflarechallenge.com/) (demo site)
* [Introducing Cryptographic Attestation of Personhood](https://blog.cloudflare.com/introducing-cryptographic-attestation-of-personhood/) (blog)
* [Expanding Crypotgraphic Attestation of Personhood](https://blog.cloudflare.com/cap-expands-support/) (blog)
* [Introducing Zero-Knowledge Proofs](https://blog.cloudflare.com/introducing-zero-knowledge-proofs-for-private-web-attestation-with-cross-multi-vendor-hardware/) (blog)

---

# Glossary

URL: https://developers.cloudflare.com/fundamentals/reference/glossary/

import { Glossary } from "~/components"

Review the definitions for terms used across Cloudflare's documentation.

<Glossary />

---

# Cloudflare and Google Analytics

URL: https://developers.cloudflare.com/fundamentals/reference/google-analytics/

Using Cloudflare does not affect Google Analytics (GA) tracking if it is added to the website [in one of ways recommended by Google](https://support.google.com/analytics/answer/9304153#add-tag).

## Standard GA setup

Cloudflare proxies traffic to your origin web server, but the GA JavaScript code never actually sends traffic to your server. Instead, it executes directly in a user's browser and does not interact with Cloudflare.

Cloudflare only affects analytics tools that read logs directly from your web server (like awstats).

:::note


To troubleshoot potential issues with Google Analytics, refer to [Common GA setup mistakes](https://support.google.com/analytics/answer/1009683).


:::

## Zaraz

As an alternative to the standard setup of Google Analytics with tag/snippet, Cloudflare offers a way to use Google Analytics with [Zaraz](/zaraz/). Zaraz is a solution that allows Google Analytics to collect data without its script loaded on the website. If GA is set up this way, then not all features may be available.

:::note


Details about features of Google Analytics that are unavailable with Zaraz can be found in [Zaraz FAQ](/zaraz/faq/#tools)


:::

---

# Cloudflare HTTP headers

URL: https://developers.cloudflare.com/fundamentals/reference/http-headers/

import { Render } from "~/components";

## Request headers

Cloudflare passes all HTTP request headers to your origin web server and adds additional headers as specified below.

:::note
Cloudflare may remove HTTP request headers with names considered invalid [according to NGINX](https://nginx.org/en/docs/http/ngx_http_core_module.html#ignore_invalid_headers) â€” for example, header names containing a `.` (dot) character.
:::

### Accept-Encoding

For incoming requests, the value of this header will always be set to `accept-encoding: br, gzip`. If the client set a different value, such as `accept-encoding: deflate`, it will be overwritten and the original value will be available in `request.cf.clientAcceptEncoding`.

### CF-Connecting-IP

`CF-Connecting-IP` provides the client IP address connecting to Cloudflare to the origin web server.
This header will only be sent on the traffic from Cloudflare's edge to your origin web server.

For guidance on logging your visitor's original IP address, refer to [Restoring original visitor IPs](/support/troubleshooting/restoring-visitor-ips/restoring-original-visitor-ips/).

Alternatively, if you do not wish to receive the `CF-Connecting-IP` header or any HTTP header that may contain the visitor's IP address, [enable the **Remove visitor IP headers** Managed Transform](/rules/transform/managed-transforms/configure/).

#### CF-Connecting-IP in Worker subrequests

In same-zone Worker subrequests, the value of `CF-Connecting-IP` reflects the value of `x-real-ip` (the client's IP). `x-real-ip` can be altered by the user in their Worker script.

In cross-zone subrequests from one Cloudflare zone to another Cloudflare zone, the `CF-Connecting-IP` value will be set to the Worker client IP address `'2a06:98c0:3600::103'` for security reasons.

For Worker subrequests destined for a non-Cloudflare customer zone, the `CF-Connecting-IP` and `x-real-ip` headers will both reflect the client's IP address, with only the `x-real-ip` header able to be altered.

When no Worker subrequest is triggered, `cf-connecting-ip` reflects the client's IP address and the `x-real-ip` header is stripped.

### CF-Connecting-IPv6

Cloudflare provides [free IPv6 support](/network/ipv6-compatibility/) to all domains without requiring additional configuration or hardware. To support migrating to IPv6, Cloudflare's [Pseudo IPv4](/network/pseudo-ipv4/) provides an IPv6 to IPv4 translation service for all Cloudflare domains.

<Render file="pseudo-ipv4-warning" /> <br />

### CF-EW-Via

This header is used for loop detection, similar to the `CDN-Loop` [header](https://blog.cloudflare.com/preventing-request-loops-using-cdn-loop/).

### CF-Pseudo-IPv4

If [Pseudo IPv4](/network/pseudo-ipv4/) is set to `Add Header` - Cloudflare automatically adds the `CF-Pseudo-IPv4` header with a Class E IPv4 address hashed from the original IPv6 address.

### True-Client-IP (Enterprise plan only)

`True-Client-IP` provides the original client IP address to the origin web server. `True-Client-IP` is only available on an Enterprise plan. In the example below, `203.0.113.1` is the original visitor IP address. For example: `True-Client-IP: 203.0.113.1`

There is no difference between the `True-Client-IP` and `CF-Connecting-IP` headers besides the name of the header. Some Enterprise customers with legacy devices need `True-Client-IP` to avoid updating firewalls or load-balancers to read a custom header name.

To add a `True-Client-IP` HTTP header to requests, [enable the **Add "True-Client-IP" header** Managed Transform](/rules/transform/managed-transforms/configure/).

Alternatively, if you do not wish to receive the `True-Client-IP` header or any HTTP header that may contain the visitor's IP address, [enable the **Remove visitor IP headers** Managed Transform](/rules/transform/managed-transforms/configure/).

:::caution
If you are using Cloudflare in a stacked CDN and authenticating HTTP requests based on the IP address value in the `True-Client-IP` header, you must add a `True-Client-IP` header to your requests. If you do not add this header, its value can be spoofed to any value.
:::

### X-Forwarded-For

`X-Forwarded-For` maintains proxy server and original visitor IP addresses. If there was no existing `X-Forwarded-For`header in the request sent to Cloudflare, `X-Forwarded-For` has an identical value to the `CF-Connecting-IP` header.

For example, if the original visitor IP address is `203.0.113.1` and the request sent to Cloudflare does not contain an `X-Forwarded-For` header, then Cloudflare will send `X-Forwarded-For: 203.0.113.1` to the origin.

If, on the other hand, an `X-Forwarded-For` header was already present in the request to Cloudflare, Cloudflare will append the IP address of the HTTP proxy connecting to Cloudflare to the header. For example, if the original visitor IP address is `203.0.113.1` and a request is proxied through two proxies: proxy A with an IP address of `198.51.100.101` and proxy B with an IP address of `198.51.100.102` before being proxied to Cloudflare, then Cloudflare will send `X-Forwarded-For: 203.0.113.1,198.51.100.101,198.51.100.102` to the origin. Proxy A will append the original visitor's IP address (`203.0.113.1`) to `X-Forwarded-For` before proxying the request to proxy B which, in turn, will append Proxy A's IP address (`198.51.100.101`) to `X-Forwarded-For` before proxying the request to Cloudflare. And finally, Cloudflare will append proxy B's IP address (`198.51.100.102`) to `X-Forwarded-For` before proxying the request to the origin.

If you do not wish to receive the visitor's IP address in the `X-Forwarded-For` header, or any HTTP header that may contain the visitor's IP address, [enable the **Remove visitor IP headers** Managed Transform](/rules/transform/managed-transforms/configure/).

:::note
To restore the original visitor IP address at your origin web server, Cloudflare recommends that your logs or applications look at `CF-Connecting-IP` or `True-Client-IP` instead of `X-Forwarded-For`. `CF-Connecting-IP` and `True-Client-IP` both have a consistent format containing only one IP address.
:::

### X-Forwarded-Proto

`X-Forwarded-Proto` is used to identify the protocol (HTTP or HTTPS) that a visitor used to connect to Cloudflare. By default, the protocol used is `https`, unless the visitor selected a different [encryption mode](/ssl/origin-configuration/ssl-modes/#custom-ssltls).

For incoming requests, the value of this header will be set to the protocol the client used (`http` or `https`). If the client set a different value, it will be overwritten.

### Cf-Ray

The `Cf-Ray` header (otherwise known as a [Ray ID](/fundamentals/reference/cloudflare-ray-id/)) is a hashed value that encodes information about the data center and the visitor's request. For example: `Cf-Ray: 230b030023ae2822-SJC`.

Add the [`Cf-Ray` header to your origin web server logs](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/#add-the-cf-ray-header-to-your-logs) to match requests proxied to Cloudflare to requests in your server logs.

Enterprise customers can also see all requests via [Cloudflare Logs](/logs/).

### CF-IPCountry

The `CF-IPCountry` header contains a two-character country code of the originating visitor's country.

Besides the [ISO-3166-1 alpha-2 codes](https://www.iso.org/iso-3166-country-codes.html), Cloudflare uses the following special country codes:

- `XX` - Used for clients without country code data.
- `T1` - Used for clients using the Tor network.

To add this header to requests, along with other HTTP headers with location information for the visitor's IP address, [enable the **Add visitor location headers** Managed Transform](/rules/transform/managed-transforms/configure/).

:::note
The `CF-IPCountry` header is removed from requests made from a Worker to an origin that is not proxied behind Cloudflare.
:::

### CF-Visitor

Currently, this header is a JSON object, containing only one key called `scheme`. The header will be either HTTP or HTTPS, and it is only relevant if you need to enable Flexible SSL in your Cloudflare settings. For example: `CF-Visitor: { \"scheme\":\"https\"}`.

### CDN-Loop

`CDN-Loop` allows Cloudflare to specify how many times a request can enter Cloudflare's network before it is blocked as a looping request. For example: `CDN-Loop: cloudflare`.

### CF-Worker

The `CF-Worker` request header is added to an edge Worker subrequest that identifies the host that spawned the subrequest. For example: `CF-Worker: example.com`.

You can add `CF-Worker` header on server logs similar to the way you add the [`CF-RAY`](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/#add-the-cf-ray-header-to-your-logs) header. To do that, add `$http_cf_worker` in the log format file: `log_format cf_custom "CF-Worker:$http_cf_worker"'`

`CF-Worker` is added to all Worker subrequests sent via `fetch()`. It is set to the name of the zone which owns the Worker making the subrequest. For example, a Worker script on route for `foo.example.com/*` from `example.com` will have all subrequests with the header:

```txt
CF-Worker: example.com
```

The intended purpose of this header is to provide a means for recipients (for example, origins, load balancers, other Workers) to recognize, filter, and route traffic generated by Workers on specific zones.

:::note
When configuring WAF custom rules, do not match on this header. These rules are applied before Cloudflare adds the `CF-Worker` header. Instead, use the [`cf.worker.upstream_zone`](/ruleset-engine/rules-language/fields/reference/cf.worker.upstream_zone/) field, which contains the same value and exists for the same purpose.

To block a specific Worker, add a `Block` action triggered by the expression `cf.worker.upstream_zone eq "example.com"`.

To block all Worker subrequests except those from your own zone's Worker, add a `Block` action triggered by the expression `not (cf.worker.upstream_zone in {"" "customer-zone.com"})`.
:::

### Connection

For incoming requests, the value of this header will always be set to `Keep-Alive`. If the client set a different value, such as `close`, it will be overwritten. Note that is also the case when the client uses HTTP/2 or HTTP/3 to connect.

### Considerations for Spectrum

When using Spectrum with a TCP application, these headers are not visible at the origin as they are HTTP headers. If you wish to utilize these in your application, there are two options:

- Use an HTTP or HTTPS Spectrum app instead of TCP
- Use the [Proxy Protocol feature](/spectrum/how-to/enable-proxy-protocol/)

## Response headers

Cloudflare will remove some HTTP headers from the response sent back to the visitor and add some Cloudflare-specific HTTP headers.

### Removed response headers

Cloudflare passes all HTTP headers in the response from the origin server back to the visitor with the exception of the following headers:

- `X-Accel-Buffering`
- `X-Accel-Charset`
- `X-Accel-Limit-Rate`
- `X-Accel-Redirect`

### Added response headers

Cloudflare adds the HTTP headers specified below to the response sent to the visitor.

#### Cf-Ray

The `Cf-Ray` value returned to the visitor will be the same `Cf-Ray` value that was sent to the origin server.

#### Cf-Cache-Status

A list of all possible `Cf-Cache-Status` values is contained in [Cloudflare cache responses](/cache/concepts/cache-responses/).

---

# Reference

URL: https://developers.cloudflare.com/fundamentals/reference/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Network Layers

URL: https://developers.cloudflare.com/fundamentals/reference/network-layers/

import { Render } from "~/components"

<Render file="network-layers" product="fundamentals" />

---

# Network ports

URL: https://developers.cloudflare.com/fundamentals/reference/network-ports/

import { Details, Render, RuleID } from "~/components";

Learn which network ports Cloudflare proxies by default and how to enable Cloudflare's proxy for additional ports.

## Network ports compatible with Cloudflare's proxy

By default, Cloudflare proxies traffic destined for the HTTP/HTTPS ports listed below.

<Details header="HTTP ports supported by Cloudflare">

- 80
- 8080
- 8880
- 2052
- 2082
- 2086
- 2095

</Details>

<Details header="HTTPS ports supported by Cloudflare">

- 443
- 2053
- 2083
- 2087
- 2096
- 8443

</Details>

<Details header="Ports supported by Cloudflare, but with caching disabled">

- 2052
- 2053
- 2082
- 2083
- 2086
- 2087
- 2095
- 2096
- 8880
- 8443

:::note

Enterprise customers that want to enable caching on these ports can do so by creating a [cache rule](/cache/how-to/cache-rules/settings/#caching-on-port-enterprise-only).

:::

</Details>

## How to enable Cloudflare's proxy for additional ports

If traffic for your domain is destined for a different port than the ones listed above, for example you have an SSH server that listens for incoming connections on port 22, either:

- Change your subdomain to be [gray-clouded](/dns/proxy-status/), via your Cloudflare DNS app, to bypass the Cloudflare network and connect directly to your origin.
- Configure a [Spectrum application](/spectrum/get-started/) for the hostname running the server. Spectrum supports all ports. Spectrum for all TCP and UDP ports is only available on the Enterprise plan. If you would like to know more about Cloudflare plans, please reach out to your Cloudflare account team.

## How to block traffic on additional ports

Block traffic on ports other than 80 and 443 in Cloudflare paid plans by doing one of the following:

- If you are using [WAF managed rules (previous version)](/waf/reference/legacy/old-waf-managed-rules/), enable rule ID `100015` (`Anomaly:Port - Non Standard Port (not 80 or 443)`).
- If you are using the new [Cloudflare Web Application Firewall (WAF)](/waf/), enable rule ID <RuleID id="8e361ee4328f4a3caf6caf3e664ed6fe" /> (`Anomaly:Port - Non Standard Port (not 80 or 443)`), which is disabled by default. This rule is part of the Cloudflare Managed Ruleset.

Ports 80 and 443 are the only ports compatible with:

- HTTP/HTTPS traffic within China data centers for domains that have the **China Network** enabled, and
- Proxying of [Cloudflare Apps](https://cloudflareapps.com/apps/developer/docs/getting-started)

<Render file="open-ports-blocked-traffic" product="waf" /> <br />

The WAF's [Cloudflare Managed Ruleset](/waf/managed-rules/reference/cloudflare-managed-ruleset/) includes a rule that will block traffic at the application layer (layer 7 in the [OSI model](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/)), preventing HTTP/HTTPS requests over non-standard ports from reaching the origin server.

:::note

[Cloudflare Access](/cloudflare-one/) does not support port numbers in URLs. Port numbers are stripped from requests for URLs protected through Cloudflare Access.

:::

## Related resources

- [Managing DNS records at Cloudflare](/dns/manage-dns-records/how-to/create-dns-records/)

---

# Partners

URL: https://developers.cloudflare.com/fundamentals/reference/partners/

import { DirectoryListing } from "~/components"

[Cloudflare Technology Partners](https://www.cloudflare.com/partners/technology-partners/) offer purpose-built integrations with our products, providing expanded functionality for our users. Learn how to configure these integrations with our tutorials and how-to guides.

## Analytics integrations

Learn how to configure a variety of products with Cloudflare Analytics:

<DirectoryListing folder="analytics/analytics-integrations" />

## Cloudflare Network Interconnect

Connect your network infrastructure with Cloudflare network fabric partners for increased reliability and security.

<DirectoryListing folder="network-interconnect/classic-cni/set-up/partners" />

## Cloudflare Zero Trust Technology Partners

Our third-party integrations allow you to deploy the WARP client application and configure devices remotely.

<DirectoryListing folder="cloudflare-one/connections/connect-devices/warp/deployment/mdm-deployment/partners" />

## Cloudflare Logs

Enterprise customers have access to detailed logs of the metadata generated by our products, and logs from Cloudflare solutions can be pushed to a variety of log management providers and storage services.

<DirectoryListing folder="logs/get-started/enable-destinations" />

## â€‹â€‹Cloudflare Technology Partners for Magic WAN

Cloudflare Magic WAN integrates with a number of third-party partners, which enables our users to securely route their Internet traffic.

<DirectoryListing folder="magic-wan/configuration/manually/third-party" />

---

# Redirects

URL: https://developers.cloudflare.com/fundamentals/reference/redirects/

import { GlossaryTooltip } from "~/components"

Cloudflare offers a variety of ways to perform <GlossaryTooltip term="redirect">URL redirects</GlossaryTooltip>, which tell a visitor's browser that the location of a page has been changed.

Use the following table to determine when to use each option.

| Option                                                      | Use when                                                  |
| ----------------------------------------------------------- | --------------------------------------------------------- |
| [Single redirects](/rules/url-forwarding/single-redirects/) | As a default option.                                      |
| [Bulk redirects](/rules/url-forwarding/bulk-redirects/)     | When you have a large number of static redirects.         |
| [Pages redirects](/pages/configuration/redirects/)          | If you have a Pages project.                              |
| [Workers redirect](/workers/examples/redirect/)             | When the other redirects do not meet your needs.          |
| [Page Rules](/rules/page-rules/how-to/url-forwarding/)      | If you already rely on Page Rules for other requirements. |

---

# Scans and penetration testing policy

URL: https://developers.cloudflare.com/fundamentals/reference/scans-penetration/

Customers may conduct scans and penetration tests (with certain restrictions) on application and network-layer aspects of their own assets, such as their [zones](/fundamentals/setup/accounts-and-zones/#zones) within their Cloudflare accounts, provided they adhere to Cloudflare's policy.

- **Permitted targets** - all scans or testing must be limited to the following:

  	- Customer-owned IPs,
  	- Cloudflare's designated public IPs, or
 	- The customer's registered DNS entries.

Targets like `*.cloudflare.com` or other Cloudflare-owned destinations are only allowed as part of Cloudflare's Public Bug Bounty program. Refer to the [Additional Resources](#additional-resources) section for more information.

### Scanning

- **Throttling**: Scans should be throttled to a reasonable rate to prevent disruptions and ensure stable system performance.
- **Scope and intent**: Scans should identify the presence of vulnerabilities without attempting to actively exploit any detected weaknesses.
- **Exclusions**: It is recommended to exclude [`/cdn-cgi/` endpoints](/fundamentals/reference/cdn-cgi-endpoint/) from scans to avoid false positives or irrelevant results.
- **Compliance checks**: Customers may conduct [PCI compliance scans](/fundamentals/security/pci-scans/) or verify that [known vulnerabilities](/ssl/reference/compliance-and-vulnerabilities/#known-vulnerabilities-mitigations) have been addressed.

### Penetration testing

- **Network behavior**:
  - Cloudflare's [anycast network](/fundamentals/concepts/how-cloudflare-works/) will report ports other than `80` and `443` as open due to its shared infrastructure and the nature of Cloudflare's proxy. This is expected behavior and does not indicate a vulnerability.
  - Tools like Netcat may list [non-standard HTTP ports](/fundamentals/reference/network-ports/) as open; however, these ports are open solely for Cloudflare's routing purposes and do not necessarily indicate that a connection can be established with the customer's origin over those ports.
- **Known false positives**: Any findings related to the [ROBOT vulnerability](/ssl/reference/compliance-and-vulnerabilities/#return-of-bleichenbachers-oracle-threat-robot) are false positives when the customer's assets are behind Cloudflare.
- **Customer security review**: During penetration testing, customers should be aware of the Cloudflare security and performance features, configurations, and rules active on their account or zone. After completing the test, it is recommended that customers review their security posture and make any necessary adjustments based on the findings.

Customers can download the latest Penetration Test Report of Cloudflare via the [Dashboard](/fundamentals/reference/policies-compliances/compliance-docs/).

### Denial-of-Service (DoS) testing

For guidelines on required notification and necessary information, refer to Cloudflare's documentation [Simulating DDoS Attacks](/ddos-protection/reference/simulate-ddos-attack). Customers should also familiarize themselves with Cloudflare's [DDoS protection best practices](/ddos-protection/best-practices/).

### Additional Resources

For information about Cloudflare's Public Bug Bounty program, visit [HackerOne](https://hackerone.com/cloudflare).

---

# SDK ecosystem support policy

URL: https://developers.cloudflare.com/fundamentals/reference/sdk-ecosystem-support-policy/

## Lifecycle

Unless otherwise stated in the code repository, Cloudflare only provides active support for the latest major version of a library or tool. The exception to this policy is for critical security fixes, which will be reviewed on a case-by-case basis and take the vulnerability, impact, and mitigation required into consideration.

We provide three primary stages of development: early access, active support, and end of life.

:::note

These lifecycle stages may be referred to in different terms across Cloudflare products, but the underlying principles are the same.
:::

### Early access

During this stage, Cloudflare makes SDK changes available that we are seeking feedback on prior to releasing for general usage. Early access will  often include warning labels or caveats on functionality that is subject to change without notice. In general, early access SDKs are not suitable for production systems unless explicitly mentioned.

### Active support

During the active support stage, planned changes and support are offered for the library or tool.

### End of life

During the end of life stage, a new major version of the library or tool is released and Cloudflare marks the previous major version as no longer receiving improvements or bug fixes. If you continue to run end of life versions, support will be very limited.

![All lifecycle stages and their relation to one another](~/assets/images/fundamentals/support-policy.png "All lifecycle stages and their relation to one another")

## Previous or end of life versions

While Cloudflare cannot provide support for all older versions of our libraries or tools, we do not remove those versions so they can continued to be used without direct support.

## Versioning

The SDK ecosystem follows semantic versioning, which defines versions as follows:

* MAJOR version when there are backward-incompatible changes made.
* MINOR version when functionality is added in a backward compatible-manner.
* PATCH version for backward-compatible bug fixes (without any improvements).

:::caution

As Cloudflare has recently swapped to [automatically generating our libraries using OpenAPI](https://blog.cloudflare.com/lessons-from-building-an-automated-sdk-pipeline), we have relaxed the strict versioning requirements on the libraries (Terraform is not changing). Minor releases *may* contain breaking changes in the forms of method, structure, or type renames as the service owners stabilize their schemas and iterate on usability improvements.

If this is not suitable for your use case, pin to a known good version or use the previous major version of the library.
:::

Depending on your needs, you should ensure your application's package manager versioning is configured correctly. At a minimum, restrict installation to the current major version of the library or tool you are using to prevent any major version upgrades occurring automatically.

## Migration

Where possible, Cloudflare provides an automated approach to performing major version upgrades to limit the disruption using codemods. Review the library or tool-specific release notes for how to use these migration tools.

Alongside the automatic migration approach, we provide documentation on the changes that have taken place in case you need to make the changes manually.

## Related resources

* [Semantic versioning definitions](https://semver.org/)
* [Cloudflare's Terraform documentation](/terraform/)

---

# TCP connections

URL: https://developers.cloudflare.com/fundamentals/reference/tcp-connections/

The following section explains how Cloudflare directs traffic efficiently with anycast routing and serves as an intermediary between users and origin servers. The second part covers TCP connections and keep-alives for performance optimization, and lastly, TCP Fast Open (TFO), a protocol extension that enhances the speed of TCP connections.

## How Cloudflare connects user to origin

Users connect to Cloudflare by sending requests from their devices to Cloudflare's global network. Cloudflare connects to the origin server by acting as an intermediary between the user and the origin.

```mermaid
flowchart LR
accTitle: Connections with Cloudflare
A[Visitor] <-- Connection --> B[Cloudflare global network] <-- Connection --> C[Origin server]
```

<br/>

User traffic is routed to the nearest Cloudflare data center based on the shortest [Border Gateway Protocol](https://www.cloudflare.com/learning/security/glossary/what-is-bgp/) (BGP) path, thanks to [anycast](https://www.cloudflare.com/learning/cdn/glossary/anycast-network/) routing. Cloudflare then processes the request. In case a request is not served from Cloudflareâ€™s data centers, Cloudflare will open a connection to the origin server to forward the request.

## TCP connections and keep-alives

HTTP (Hypertext Transfer Protocol) is a [Layer 7](https://en.wikipedia.org/wiki/OSI_model) application protocol that operates over TCP. By default, HTTP opens a new TCP connection for each request-response cycle, which can lead to performance overhead due to the repeated connection establishment and teardown.

Keep-Alives are a mechanism that bridges TCP and HTTP, and allow a single TCP connection to remain open for multiple HTTP requests and responses. This minimizes the connection overhead and latency associated with establishing new TCP connections for each web resource. Keep-Alives improve the efficiency and responsiveness of web applications by facilitating the reuse of existing connections, reducing network traffic, and enhancing user experience.

TCP connections can persist even after HTTP requests have concluded. However, to manage resources efficiently, idle connections are typically terminated after a certain period of inactivity. To enhance connection reuse and minimize connection overhead, keep-alives are employed. These mechanisms collectively optimize the performance and reliability of web applications while conserving network resources.

If either a user or an origin does not respond to two keep-alives, Cloudflare will sever the connection by sending a TCP Reset (RST) packet.

For connections to users, Cloudflare has a default idle timeout of 400 seconds. After the 400 seconds, Cloudflare will start sending keep-alive probes every 75 seconds. If nine consecutive probes are unanswered, Cloudflare will sever the connection by sending an RST packet.

:::note


Be aware that even if there are keep-alives, Cloudflare cannot guarantee to keep a connection, since besides idleness, there are other reasons, like capacity balancing, data center maintenance or node restarts that can cause disconnections. Having this in mind, applications should be structured to handle disconnections gracefully.


:::

TCP connection settings between the user and Cloudflare, and between Cloudflare and Origin can be customized for Enterprise customers. Reach out to your account team for more details.

## TCP Fast Open (TFO)

[TCP Fast Open](https://en.wikipedia.org/wiki/TCP_Fast_Open) (TFO) is a protocol extension that can significantly improve the speed of establishing TCP connections by allowing data to be sent in the initial SYN packet, rather than requiring a separate handshake before data transmission begins. TFO can reduce latency and improve website and application performance, particularly on high-latency networks. Cloudflare supports TFO on user connections.

When a client initiates a connection to a web server protected by Cloudflare, it sends a TCP SYN packet to request a connection. Cloudflare, acting as a reverse proxy, intercepts the SYN packet and responds with a SYN-ACK packet to establish the connection. With TFO enabled, Cloudflare can also send initial data (such as HTTP request data) in the SYN-ACK packet, eliminating the need for an additional round-trip for data transmission. The client receives the SYN-ACK packet with data and acknowledges it with an ACK packet. This fast tracks the connection setup.

---

# Under Attack mode

URL: https://developers.cloudflare.com/fundamentals/reference/under-attack-mode/

import { Example } from "~/components"

Cloudflare's **I'm Under Attack Mode** performs additional security checks to help mitigate layer 7 DDoS attacks.Â Validated users access your website and suspicious traffic is blocked. It is designed to be used as one of the last resorts when a zone is under attack (and will temporarily pause access to your site and impact your site analytics).

When enabled, visitors receive an interstitial page.

## Enable Under Attack mode

**I'm Under Attack Mode** is disabled by default for your zone.

### Globally

To put your entire zone in **I'm Under Attack Mode**:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com).
2. Select your account and zone.
3. Go to **Security** > **Settings**.
4. For **Security Level**, choose **I'm Under Attack!**.

### Selectively

To enable **I'm Under Attack Mode** for specific pages or sections of your site, use a [Configuration Rule](/rules/configuration-rules/) to adjust the **Security Level**.

<Example>

**When incoming requests match**

* **Field:** *URI Path*
* **Operator:** *starts with*
* **Value:** `/admin`

If you are using the Expression Editor, enter the following expression:<br/>
`(starts_with(http.request.uri.path, "/admin"))`

**Then the settings are**

1. For **I'm Under Attack**, select **Add**.
2. Switch the toggle to **On**.

</Example>

To turn it on for specific ASNs (hosts/ISPs that own IP addresses), countries, or IP ranges, use [IP Access Rules](/waf/tools/ip-access-rules/).

***

## Preview Under Attack mode

To preview what **I'm Under Attack** mode looks like for your visitors:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com).
2. Select your account.
3. Go to **Manage Account** > **Configurations**.
4. Go to **Custom Pages**.
5. For **Managed Challenge / I'm Under Attack Modeâ„¢**, select **Custom Pages** > **View default**.

The "Checking your browser before accessing..." challenge determines whether to block or allow a visitor within five seconds.Â After passing the challenge, the visitor does not observe another challenge until the duration configured in [**Challenge Passage**](/waf/tools/challenge-passage/).

***

## Potential issues

Since the Under Attack mode requires your browser to support JavaScript to display and pass the interstitial page, it is expected to observe impact on third party analytics tools.

---

# Security

URL: https://developers.cloudflare.com/fundamentals/security/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Protect your origin server

URL: https://developers.cloudflare.com/fundamentals/security/protect-your-origin-server/

import { Render } from "~/components"

<Render file="origin-health-overview" />

## Secure origin connections

<Render file="origin-secure-dns" />

### Application layer

<Render file="limit-external-connections-application" product="learning-paths" />

### Transport Layer

<Render file="limit-external-connections-transport" product="learning-paths" />

### Network Layer

<Render file="limit-external-connections-network" product="learning-paths" />

<Render file="ent-only-network-security" product="learning-paths" />

## Monitor origin health

For passive monitoring, [create notifications](/notifications/get-started/#create-a-notification) for **Origin Error Rate Alerts** to receive alerts when your origin returns 5xx codes above a configurable threshold and **Passive Origin Monitoring** to see when Cloudflare is unable to reach your origin for a few minutes.

<Render file="origin-health-check" />

<Render file="origin-lb-alert" />

### Zero Downtime Failover

<Render file="dns-zero-downtime-failover" />

## Reduce origin traffic

### Block traffic

For more details, refer to [Secure your website](/learning-paths/application-security/account-security/).

### Increase caching

<Render file="origin-caching" />

### Distribute traffic

<Render file="origin-load-balancing" />

\
<Render file="origin-waiting-room" />

---

# Scan for PCI compliance

URL: https://developers.cloudflare.com/fundamentals/security/pci-scans/

:::note


Cloudflare is PCI certified as a Data Processor. Refer to [PCI compliance and vulnerabilities mitigation](/ssl/reference/compliance-and-vulnerabilities) and Cloudflare's PCI DSS Responsibility Matrix for more information.


:::

PCI scanners are tools used to identify security weaknesses. When a business undergoes a compliance audit, PCI scan results are used for compliance verification.

## Initiate a scan

1. Identify which server your scan should target. Are you scanning against your origin server, where your applications are hosted, or at a proxy server sitting in front of your origin, such as Cloudflare?

2. On your scanner tool, enter a public URL or an IP address. If you enter a public website URL, the scanner will resolve the hostname and scan the resulting the IP address. To scan your origin server, be sure to enter your origin server's IP address or a hostname that resolves to the origin server's IP, not a proxy server.

3. Start the scan and analyze the results.

4. (Optional) Run another scan for a different origin server.

### Open ports versus blocked traffic

There is a difference between open ports and blocked traffic. Due to the nature of how Cloudflareâ€™s anycast network works, ports other than 80 and 443 are always open so that Cloudflare can serve traffic for other customers on these ports. However, customers can easily block all unwanted traffic to these ports by using Cloudflare [WAF Managed Rules](/fundamentals/reference/network-ports/#how-to-block-traffic-on-additional-ports) or [custom rules](/waf/custom-rules/). The PCI scan will show the ports being open, but the traffic would not reach your origin server. This is an often misunderstood concern.

## Additional resources

You can find all our public compliance resources in the following pages:

* [Certifications and compliance resources](https://www.cloudflare.com/trust-hub/compliance-resources/)
* [Compliance documentation](/fundamentals/reference/policies-compliances/compliance-docs/)

You can access Compliance documents in the Cloudflare dashboard by selecting your account where you are a Super Administrator and then navigating to **Support** > **Compliance Documents**.

---

# Under a DDoS attack?

URL: https://developers.cloudflare.com/fundamentals/security/under-ddos-attack/

import { Render } from "~/components"

<Render file="ddos-definition-and-diagram" product="learning-paths" />

## Common signs of an attack

Common signs that you are under DDoS attack include:

* Your site is offline or slow to respond to requests.
* There are unexpected spikes in the graph ofÂ **Requests Through Cloudflare**Â orÂ **Bandwidth**Â in your CloudflareÂ **Analytics**Â app.
* There are strange requests in your origin web server logs that do not match normal visitor behavior.

:::note

If you are currently under DDoS attack, refer to our guide on [responding to a DDoS attack](/ddos-protection/best-practices/respond-to-ddos-attacks/).
:::

---

# Recovering from a hacked site

URL: https://developers.cloudflare.com/fundamentals/security/recovering-from-hacked-site/

If your website has been hacked recently, review the recommended steps below to recover a hacked website and prevent future hacks.

## Recovering from an attack

To recover from an attack, reach out to your hosting provider to request:

* Details about the hack, including how they believe the site was hacked.
* That your hosting provider remove any malicious content placed on your website.

Once the hack has been resolved, you should resolve site warnings in [Google Webmaster Tools](https://www.google.com/webmasters/tools)Â and resubmit your site for Googleâ€™s review.

***

## Preventing and mitigating the risks of a future hack

To prevent the risk of a hacked site:

* Activate Cloudflare's [WAF managed rules](/waf/managed-rules/) so they can challenge or block known malicious behavior.
* If you use a Content Management System (CMS), make sure you have the most recent version installed (CMS platforms push out updates to address known vulnerabilities).
* If you use plugins, make sure they are updated.
* If you have an admin login page, protect it with Cloudflare's [Rate limiting rules](/waf/rate-limiting-rules/) or a [Cloudflare Access policy](/cloudflare-one/policies/access/).
* Use a backup service so you can avoid losing valid content.

---

# Changelog

URL: https://developers.cloudflare.com/fundamentals/trace-request/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/trace.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Use Cloudflare Trace

URL: https://developers.cloudflare.com/fundamentals/trace-request/how-to/

import { GlossaryTooltip } from "~/components"

## Use Trace in the dashboard

### 1. Configure one or more Cloudflare products

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com), and select your account.
2. Set configuration settings at the account level, or select a domain and configure settings for one or more Cloudflare products.

### 2. Build a trace

1. In the [Cloudflare dashboard](https://dash.cloudflare.com), go to Account Home > **Trace**.

2. Enter a URL to trace. The URL must include a hostname that belongs to your account.

3. Select an HTTP method. If you select *POST*, *PUT*, or *PATCH*, you should enter a value in **Request body**.

4. (Optional) Define any custom request properties to simulate the conditions of a specific HTTP/S request. You can customize the following request properties:

   * **Protocol** (HTTP protocol version)
   * **Request headers**
   * **Cookies**
   * **Geolocation** (request source [country](/ruleset-engine/rules-language/fields/reference/ip.src.country/), [region](/ruleset-engine/rules-language/fields/reference/ip.src.region/), and [city](/ruleset-engine/rules-language/fields/reference/ip.src.city/))
   * [**Bot score**](/bots/concepts/bot-score/)
   * <GlossaryTooltip term="threat score" link="/ruleset-engine/rules-language/fields/reference/cf.threat_score/">**Threat score**</GlossaryTooltip>
   * **Request body** (for `POST`, `PUT`, and `PATCH` requests)
   * **Skip challenge** (skips a Cloudflare-issued [challenge](/waf/reference/cloudflare-challenges/), if any, allowing the trace to continue)

5. Select **Send trace**.

### 3. Assess results

The **Trace results** page shows all evaluated and executed configurations from different Cloudflare products, in evaluation order. Any inactive rules are not evaluated.

1. Analyze the different [steps](#steps-in-trace-results) with evaluated and executed configurations for the current trace. Trace results include matches for all active rules and configurations, whether configured at the account level or for a specific domain or subdomain.

   To show all configurations, including the ones that did not match the request, select *All configurations* in the **Results shown** dropdown.

2. (Optional) Update your Cloudflare configuration (at the account or at the domain/subdomain level) and create a new trace to check the impact of your changes.

### 4. (Optional) Save the trace configuration

To run a trace later with the same configuration:

1. Copy the JSON shown in the dashboard with the current trace configuration.
2. When creating a new trace, paste it in the JSON box to define all the settings of the new trace.

## Use Trace via API

Use the [Request Trace](/api/resources/request_tracers/subresources/traces/methods/create/) operation to perform a trace using the Cloudflare API.

***

## Steps in trace results

* Execution of one or more rules of Cloudflare products built on the [Ruleset Engine](/ruleset-engine/). Refer to the Ruleset Engine's [Phases list](/ruleset-engine/reference/phases-list/) for a list of such products.
* [Page Rules](/rules/page-rules/): Execution of one or more rules.
* [Workers](/workers/): Execution of one or more scripts.

---

# Trace a request

URL: https://developers.cloudflare.com/fundamentals/trace-request/

import { DirectoryListing, Plan } from "~/components"

<Plan type="all" />

Cloudflare Trace (beta) follows an HTTP/S request through Cloudflareâ€™s reverse proxy to your origin. Use this tool to understand how different Cloudflare configurations interact with an HTTP/S request for one of your hostnames. If the hostname you are testing is not [proxied by Cloudflare](/dns/proxy-status/), Cloudflare Trace will still return all the configurations that Cloudflare would have applied to the request.

You can define specific request properties to simulate different conditions for an HTTP/S request. Inactive rules configured in Cloudflare products will not be evaluated.

Cloudflare Trace is available to users with an Administrator or Super Administrator role.

## Resources

<DirectoryListing />

---

# Limitations

URL: https://developers.cloudflare.com/fundamentals/trace-request/limitations/

Trace currently does not support:

* Hostnames using [Data Localization Suite](/data-localization/)
* [Spectrum](/spectrum/) applications

Additionally, the following products will not appear in trace results:

* [Firewall rules (deprecated)](/firewall/)
* [Load Balancing](/load-balancing/) and [Load Balancer Custom Rules](/load-balancing/additional-options/load-balancing-rules/)
* [IP Access rules](/waf/tools/ip-access-rules/)
* [Rate limiting rules (previous version)](/waf/reference/legacy/old-rate-limiting/)
* [WAF managed rules (previous version)](/waf/reference/legacy/old-waf-managed-rules/)

---

# Accounts, zones, and profiles

URL: https://developers.cloudflare.com/fundamentals/setup/accounts-and-zones/

Within the Cloudflare ecosystem, there are three organizing concepts that control where specific settings live: user profiles, accounts, and zones.

```mermaid
flowchart LR
accTitle: Accounts contain zones and user profiles contain user settings
subgraph Account
    subgraph Zone - example.com
        A[WAF]
        B[DNS]
    end
    subgraph Zone - example2.com
        C[Cache rules]
        D[Waiting Room]
    end
    Workers
    K[Account members]
end
subgraph User profile
    G[Email address]
    H[Language]
    I[Communication preferences]
end
```

***

## User profiles

Each user has a profile that contains several settings, such as [Communication preferences](/fundamentals/setup/account/customize-account/communication-preference/) and [Language preferences](/fundamentals/setup/account/customize-account/language-preference/).

### Navigation

To access your profile, select the user icon and then **My Profile** from any page within the [Cloudflare dashboard](https://dash.cloudflare.com).

![Use the Profile icon to access your profile settings](~/assets/images/fundamentals/get-started/profile-navigation.png)

***

## Accounts

An account refers to an organization account. Accounts contain one or more users and can contain one or more zones. A user can be part of one or more accounts.

There are also several account-level products - such as [Workers](/workers/), [Pages](/pages/), [Security Center](/security-center/), and [Bulk redirects](/rules/url-forwarding/bulk-redirects/) - that can affect some or all zones contained within that account.

After you [log in](https://dash.cloudflare.com) and select an account - but before you select a zone - the sidebar will list account-level products.

Accounts also have their own settings, including [account billing profiles](/fundamentals/subscriptions-and-billing/create-billing-profile/), [account members](/fundamentals/setup/manage-members/), [lists](/waf/tools/lists/), and more.

### Navigation

When you log into the [Cloudflare dashboard](https://dash.cloudflare.com), you can access all accounts where your user is a member.

To access account settings and account-level products from within a zone, use the **Account Home** option from the **Profile** dropdown.

![Use the Account Home option in the Profile dropdown to access account settings and products](~/assets/images/fundamentals/get-started/account-navigation-profile.png)

You can also use the back button near the zone name.

![Use the back button near the account name to move from a zone to your account](~/assets/images/fundamentals/get-started/account-navigation.png)

***

## Zones

Domains (or [subdomains](/dns/zone-setups/subdomain-setup/)) that are added to Cloudflare become zones[^1], which have a direct impact on the security and performance of your website, application, or API. Use your zone to monitor security and performance, update configurations, and apply zone-level products and services.

Zone-level services - such as [Load Balancers](/load-balancing/) and [Cache rules](/cache/how-to/cache-rules/) - only affect your website, application, or API for that zone and not other zones, even if they are contained within the same account.

### Navigation

When you log into the [Cloudflare dashboard](https://dash.cloudflare.com) and choose an account, there will be a list of all zones within that account.

Once you are within a zone, items within the sidebar will be zone-related products.

If you need to change to another zone, use the forward arrow next to the zone name or by go back to the homepage of your account.

![Use the forward button near the account name to switch between zones in an account](~/assets/images/fundamentals/get-started/zone-navigation.png)

[^1]: Similar to [DNS zones](https://www.cloudflare.com/learning/dns/glossary/dns-zone/), but with additional capabilities.

---

# Find zone and account IDs

URL: https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/

Once you [set up a new account](/fundamentals/setup/account/) and [add your domain](/fundamentals/setup/manage-domains/add-site/) to Cloudflare, you may need access to your zone and account IDs for API operations.

To find your zone and account IDs:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login).

2. Select your account and domain.

3. On the **Overview** page (the landing page for your domain), find the **API** section.

   ![Screenshot of the Overview page with the API section highlighted](~/assets/images/fundamentals/get-started/dash-overview-api-highlighted.png)

4. The **API** section contains your **Zone ID** and **Account ID**. To copy these values for API commands or other tasks, select **Click to copy**.

   ![Screenshot of the API section on the Overview page of a zone](~/assets/images/fundamentals/get-started/dash-overview-api-close-view.png)

For more guidance on setting up API tokens and using the API, refer to our [Cloudflare API Documentation](/fundamentals/api/).

## Find account ID (Workers and Pages)

You can also find your account ID within the **Workers & Pages** section of your account:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login).
2. Select your account.
3. Go to **Workers & Pages**.
4. The **Account details** section contains your **Account ID**. To copy these values for API commands or other tasks, select **Click to copy**.

   ![Screenshot of the Workers & Pages Overview page with the account ID section highlighted](~/assets/images/fundamentals/get-started/workers-account-id.png)

---

# How to use Cloudflare

URL: https://developers.cloudflare.com/fundamentals/setup/

import { DirectoryListing, Render } from "~/components"

<DirectoryListing />

<Render file="pointer-to-workers-zt-docs" />

---

# Interacting with Cloudflare

URL: https://developers.cloudflare.com/fundamentals/setup/interact-with-cloudflare/

Once you [set up an account](/fundamentals/setup/account/), you have several ways to interact with Cloudflare.

## Without code

If you prefer working without code, you can manage your account and domain settings through the [Cloudflare dashboard](https://dash.cloudflare.com/login).

:::note

If your domain was added to Cloudflare by a hosting partner, manage your DNS records via the hosting partner.

:::

## With code

For those who prefer to interact with Cloudflare programmatically, you can use several methods:

| Resource                                                                               | Docs                                                                 | Description                                                                    |
| -------------------------------------------------------------------------------------- | -------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| [Cloudflare API](/fundamentals/api/)                                                   | [API docs](/api/)                                                    | RESTful API based on HTTPS requests and JSON responses.                        |
| [Terraform](https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs) | [Terraform docs](/terraform/)                                        | Configure Cloudflare using HashiCorpâ€™s Infrastructure as Code tool, Terraform. |
| [cloudflare-go](https://github.com/cloudflare/cloudflare-go)                           | [README](https://github.com/cloudflare/cloudflare-go#readme)         | The official Go library for the Cloudflare API.                                |
| [cloudflare-typescript](https://github.com/cloudflare/cloudflare-typescript)           | [README](https://github.com/cloudflare/cloudflare-typescript#readme) | The official TypeScript library for the Cloudflare API.                        |
| [cloudflare-python](https://github.com/cloudflare/cloudflare-python)                   | [README](https://github.com/cloudflare/cloudflare-python#readme)     | The official Python library for the Cloudflare API.                            |

---

# Use Cloudflare without changing nameservers

URL: https://developers.cloudflare.com/fundamentals/setup/use-cloudflare-without-changing-nameservers/

If you cannot [change your domain nameservers](/dns/zone-setups/full-setup/), you can still use Cloudflare on your website by:

* Activating Cloudflare through a [certified hosting partner](https://www.cloudflare.com/hosting-partners).
* Using a [partial (CNAME) setup](/dns/zone-setups/partial-setup/).

---

# Troubleshooting

URL: https://developers.cloudflare.com/fundamentals/setup/troubleshooting/

When you [set up Cloudflare](/fundamentals/setup/), you may experience the following issues or error messages.

## Error messages

* [`ERR_TOO_MANY_REDIRECTS`](/ssl/troubleshooting/too-many-redirects/)
* [`525` or `526` errors](/ssl/troubleshooting/too-many-redirects/)
* [Cannot add DNS records with the same name](/dns/manage-dns-records/troubleshooting/records-with-same-name/)
* [`ERR_SSL_VERSION_OR_CIPHER_MISMATCH` or `SSL_ERROR_NO_CYPHER_OVERLAP`](/ssl/troubleshooting/version-cipher-mismatch/)
* [`DNS_PROBE_FINISHED_NXDOMAIN`](/dns/troubleshooting/dns-probe-finished-nxdomain/)
* [Record exposing origin server IP address](/dns/manage-dns-records/troubleshooting/exposed-ip-address/)
* [Mixed content errors](/ssl/troubleshooting/mixed-content-errors/)
* [SSL errors in appear in my browser](/ssl/troubleshooting/general-ssl-errors/)

## Behavior

* [Why are Cloudflare's IPs in my origin web server logs?](/support/troubleshooting/restoring-visitor-ips/restoring-original-visitor-ips/)
* [Is Cloudflare attacking me?](#is-cloudflare-attacking-me)
* [Cannot add domain to Cloudflare](/dns/zone-setups/troubleshooting/cannot-add-domain/)
* [My domainâ€™s email stopped working](/dns/troubleshooting/email-issues/)
* [Why is my site served over HTTP instead of HTTPS?](/ssl/edge-certificates/encrypt-visitor-traffic/)
* [SSL is not working for my second-level subdomain, such as `dev.www.example.com`](/ssl/troubleshooting/general-ssl-errors/#only-some-of-your-subdomains-return-ssl-errors)
* [Why was my domain deleted from Cloudflare?](/dns/zone-setups/troubleshooting/domain-deleted/)

## Cloudflare

* [Gather information to troubleshoot site issues](/support/troubleshooting/general-troubleshooting/gathering-information-for-troubleshooting-sites/)
* [Contact Cloudflare support](/support/contacting-cloudflare-support/)
* [Login and account issues](/fundamentals/setup/account/account-security/login-and-account-issues/)
* [Manage email notifications](/fundamentals/setup/account/manage-email-notifications/)

## General resources

* [DNS FAQ](/dns/troubleshooting/faq/)
* [SSL/TLS FAQ](/ssl/troubleshooting/faq/)

## Is Cloudflare attacking me?

There are two common scenarios where Cloudflare is falsely perceived to attack your site:

* Unless youÂ [restore the original visitor IP addresses](/support/troubleshooting/restoring-visitor-ips/restoring-original-visitor-ips/), Cloudflare IP addresses appear in your server logs for all proxied requests.
* The attacker is spoofing Cloudflare's IPs. Cloudflare onlyÂ [sends traffic to your origin web server over a few specific ports](/fundamentals/reference/network-ports/)Â unless you useÂ [Cloudflare Spectrum](/spectrum/).

Ideally, because Cloudflare is a reverse proxy, your hosting provider observes attack traffic connecting fromÂ [Cloudflare IP addresses](https://www.cloudflare.com/ips/). In contrast, if you notice connections from IP addresses that do not belong to Cloudflare, the attack is direct to your origin web server. Cloudflare cannot stop attacks directly to your origin IP address because the traffic bypasses Cloudflareâ€™s network.

:::note

If an attacker is directly targeting your origin web server, refer to [Respond to DDoS attacks](/ddos-protection/best-practices/respond-to-ddos-attacks/).
:::

---

# Cancel Cloudflare subscriptions

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/cancel-subscription/

Cancellations are not processed until the end of the billing period, so you can continue using the add-on or subscription until the new billing period begins. To avoid unwanted charges, start the cancellation process before the end of your billing period.

If you are making a subscription downgrade including a change from a yearly plan to a monthly plan or a change from a paid plan to a free plan, be aware that downgrades and cancellations do not take immediate effect and that you will continue to be billed and have access to your service for the remainder of your contracted service period. You should not contact Cloudflare to ask for exceptions to this policy or refunds for early downgrades and cancellations.

In addition, you may not always see a cancel button present for each subscription and often it is a switch from a paid plan to a free plan that will act as the cancellation of your paid subscription.

:::note


All billing period dates on invoices and the dashboard are in UTC/GMT timezone, not your local timezone. We recommend any downgrades or changes to subscriptions be made at least 24 hours before stated billing date to avoid any timezone confusion.


:::

***

## Step 1: Disable the Cloudflare subscription

To disable a subscription:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com), and select your account and website.

2. Select the feature you want to disable under **Active Subscriptions** (Free or Pro customers) or **Plan Extensions** (Enterprise customers).

3. Follow the instructions to disable the feature. Each feature has a different process which could include toggling a switch, clicking a button, or choosing *Disable* from a drop-down list.

## Step 2: Cancel the subscription in your billing profile

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.

2. Select **Manage account** > **Billing**.

3. Select **Subscriptions**.

4. Find the subscription you want to disable and select **Cancel**.

5. You will be asked to share feedback with us. Choose all that apply.

6. Select **Confirm** to finish.

:::note


To alter your plan, select **Change** under **Active Subscriptions** and choose a new plan type. If you would like to cancel your paid plan, select **Free**.


:::

---

# Change password or email

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/change-password-or-email/

## Change password

To change your Cloudflare password:

1. Go to your [Profile](https://dash.cloudflare.com/?to=/:account/profile).
2. Select your account.
3. Select **Authentication**.
4. On **Password**, select **Change**.
5. Change your password and select **Save**.

For added account security, consider changing your [API tokens](/fundamentals/api/how-to/roll-token/) as well.

:::note


If your administrator has [enabled Single sign-on (SSO)](/cloudflare-one/applications/configure-apps/dash-sso-apps/), you cannot change your **Authentication** settings.


:::

***

## Change email address

To change the email address associated with your Cloudflare account:

1. Go to your [Profile](https://dash.cloudflare.com/?to=/:account/profile).
2. Select your account.
3. In the Email Address panel, select **Change Email Address**.
4. In the dialog, enter your new email address in **New email** and **Confirm email**.
5. Enter your current password.
6. Select **Save**.

:::note


If your administrator has [enabled Single sign-on (SSO)](/cloudflare-one/applications/configure-apps/dash-sso-apps/), you cannot change the email address associated with your account.


:::

---

# Change Super Administrator

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/change-super-admin/

If you or someone in your organization leaves or loses access to email, you can add another Super Administrator.

First, [add a member](/fundamentals/setup/manage-members/manage/) to your account and assign the **Super Administrator** role.

Then, if needed, remove the previous Super Administrator.

---

# Change domain plan

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/change-plan/

import { TabItem, Tabs } from "~/components";

Occasionally, you may want to upgrade or downgrade the plan associated with a specific Cloudflare domain.

## Limitations

Only Super Administrators can manage changes to domain plans.

If you decide to downgrade or remove a domain, Cloudflare does not issue refunds. Refer to our [billing policy](/support/account-management-billing/billing-cloudflare-plans/cloudflare-billing-policy/) for more information.

Upgrades are processed immediately, but downgrades are not processed until the end of the billing period. You will not be able to upgrade if you have an unpaid invoice. When downgrading, you are allowed to continue using the higher plans' products until the new billing period begins.

If you downgrade your plan, your plan may have access to [fewer Page Rules](/rules/page-rules/). If you continue to use more page rules than is allowed by your plan limit, you may be charged for additional rules. Remove excess rules and [cancel additional subscriptions](/fundamentals/subscriptions-and-billing/cancel-subscription/) if you do not want to be charged.

The Enterprise App Sec Advanced and Enterprise App Sec Core plans cannot be downgraded without [contacting Cloudflare](/support/contacting-cloudflare-support/).

For additional help, refer to [this Community thread](https://community.cloudflare.com/t/communitytip-page-rules-best-practices-when-downgrading-pro-to-free/305725).

## Change plan type

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To change the Cloudflare plan for a domain in the dashboard:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account and domain.

2. Go to **Overview**.

3. For Plan Extensions, select **Change**.

   ![Screenshot of the Overview page with the Plan extension section highlighted](~/assets/images/fundamentals/get-started/change-plan.png)

4. Choose the appropriate plan type, then select **Continue**.

5. Select **Confirm**.

</TabItem> <TabItem label="API">

To change the Cloudflare plan for a domain using the API, first send a [`GET`](/api/resources/zones/subresources/plans/methods/list/) request to review available subscriptions.

Then, send a [`PUT`](/api/resources/zones/subresources/subscriptions/methods/update/) request with your desired plan type in the `rate_plan` object.

</TabItem> </Tabs>

:::note

If you are an Enterprise customer and cannot change your plan type, contact your Customer Success Manager.

:::

## Change plan duration

<Tabs syncKey="dashPlusAPI"> <TabItem label="Dashboard">

To change the duration of your Cloudflare plan in the dashboard:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account and domain.

2. Go to **Overview**.

3. For Plan Extensions, select **Change**.

   ![Screenshot of the Overview page with the Plan extension section highlighted](~/assets/images/fundamentals/get-started/change-plan.png)

4. Switch the toggle between **Monthly** or **Annual**.

   ![Screenshot of the Plan choice with the annual or monthly toggle highlighted](~/assets/images/fundamentals/get-started/plan-duration.png)

5. Choose the appropriate plan type, then select **Continue**.

6. Select **Confirm**.

</TabItem> <TabItem label="API">

To change the duration of a Cloudflare plan for a domain using the API, send a [`PUT`](/api/resources/zones/subresources/subscriptions/methods/update/) request with an updated value for the `frequency` parameter.

</TabItem> </Tabs>

---

# Create billing profile

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/create-billing-profile/

import { Render } from "~/components"

After you [create a new account](/fundamentals/setup/account/create-account/), you might want to create your billing profile.


## Add primary payment method

<Render file="billing-add-payment-method" />


## Optional - Add backup payment method

A backup payment method is used if the primary payment method fails. To add a backup payment method:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com).
2. Go to **Manage account** > **Billing**.
3. Select **Payment Info**.
4. In the **Payment Method** card, select **Manage**.
5. In the **Backup** card, select **Add** to enter a backup payment method.
6. Enter the required information based on your preferred payment method (credit card or PayPal) and select **Confirm**.
7. If you would like to make the backup payment method the primary method, select **Make primary payment method** in the **Backup** card.


## Set up billing notifications


If you have a [usage-based product](https://www.cloudflare.com/plans/) like Rate Limiting or Load Balancing, set up Billing notifications to monitor usage and avoid surprises on your bill.

These notifications do not set a cap on usage, but rather alert you when your usage might be reaching a threshold.

To set up billable usage notifications:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to **Notifications**.
3. Select **Add**.
4. On the **Billing** notification, click **Select**.
5. Enter a name and description.
6. Select a **Product**. This value affects the usage threshold specified in the next step.
7. Enter a usage threshold.
8. Add a **Notification email**.
9. Select **Create**.

To disable, edit, or delete this notification, return to **Notifications** and find your notification.


## Enable email invoices


To receive invoices via email â€” which are sent when you add or remove subscriptions from your account â€” you can opt-in within the Billing section of the Cloudflare dashboard. Once enabled, you will receive invoices via email:

* Within one business day of initial setup.
* Every month at the end of your billing period.
* Within one business day for all new purchases.

To enable Cloudflare invoice emails:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to **Manage Account** > **Billing**.
3. Go to **Invoices & Documents**.
4. For **Billing email preference**, switch the value to **On**. You will receive an invoice via billing email address on file within one business day.

---

# Delete your Cloudflare account

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/delete-account/

:::note
These steps do not apply to accounts under contract. Contact your account team for more information.
:::

***

## Who can delete their account

If your account uses [Single-Sign On (SSO)](/cloudflare-one/applications/configure-apps/dash-sso-apps/), your super administrator may need to delete your account on your behalf.

If your account does not use SSO, you can delete your account on your own.

## Prerequisites

Before Cloudflare can cancel your account and delete your personal information, you will need to follow the process below for each domain associated with your Cloudflare account:

* [Cancel your subscriptions or add-on services](/fundamentals/subscriptions-and-billing/cancel-subscription/)

* [Remove your domain from Cloudflare](/fundamentals/setup/manage-domains/remove-domain/)

* [Remove Cloudflare nameservers at your domain registrar](/dns/zone-setups/full-setup/setup/)

* [Disable auto-renew for your Registrar domain(s)](/registrar/account-options/renew-domains#set-up-automatic-renewals)

* If you are using a Cloudflare [CNAME setup](/dns/zone-setups/partial-setup/), [update your DNS records](/dns/manage-dns-records/how-to/create-dns-records/#edit-dns-records) at your DNS provider to point to your website IPs or hostnames instead of Cloudflare.

* [Delete payment information](/fundamentals/subscriptions-and-billing/update-billing-info/#delete-your-current-payment-method)

* (*Optional*) [Download a copy of your invoices](/fundamentals/subscriptions-and-billing/understand-invoices/#download-invoice). Once deleted, the invoices will no longer be accessible and cannot be re-sent to you.

## Delete your Cloudflare account

Deleting your account is permanent. Any accounts where you are the primary owner will also be deleted and any other users on those accounts will be removed.

All domains, subscriptions, and billing information on your account will be removed from Cloudflare.

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com).

2. Select **My Profile**.

3. Select **Delete this user**.

   ![To delete your account, select Delete this user on your profile page.](~/assets/images/fundamentals/get-started/delete-account.png)

4. Select **Continue to delete user**.

5. Follow the prompts to finish deleting your account.

:::note


Cloudflare will purge your personal information within a year of a deletion request unless required to retain it for legal obligations (such as ongoing abuse investigations or pending litigation). Refer to the [Cloudflare Data Processing Addendum](https://www.cloudflare.com/cloudflare-customer-dpa/) for further information about the deletion of personal information following the cancellation of your account.


:::

---

# Our Free plan

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/free-plan/

At Cloudflare, our mission is to help build a better Internet. We believe the web should be open and free, and that all websites and web users, no matter how small, should be safe, secure, and fast. Cloudflare always has and always will offer a generous free plan for [many reasons](https://webmasters.stackexchange.com/questions/88659/how-can-cloudflare-offer-a-free-cdn-with-unlimited-bandwidth).

We work hard to minimize the cost of running our network so we can offer huge value in our Free plan. On top of this, protecting more sites means we get better data about the types of attacks on our network so we can offer better security and protection for all.

As a privacy-first company, one thing we do not do is sell your data. In fact, Cloudflare recognizes privacy in personal data as a fundamental human right and has taken a [number of steps](https://blog.cloudflare.com/certifying-our-commitment-to-your-right-to-information-privacy/) to demonstrate our commitment to privacy.

---

# Subscriptions and billing

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Preview services

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/preview-services/

Contracted customers can try certain products and features for 30 days.

Once you enable the product or feature, it will not be an officially contracted service until you purchase it. Cloudflare will reach out in case you have any questions or feedback, and provide you with a sales quote if you are enjoying the product.

You may cancel at any time before the 30 days ends by reaching out to your sales team for assistance.

## Available features and products

To enable a preview service, use the **Dashboard link** in the following table. Then, select **Enable**. And that is it. You will have instant access to the product or feature you selected so you can begin determining if it is right for your businessâ€™ needs.

| Name                               | Dashboard link                                                                                              | Docs                                                                       | Community                                                                      |
| ---------------------------------- | ----------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| Advanced Certificate Manager (ACM) | [ACM dashboard](https://dash.cloudflare.com/?to=/:account/:zone/ssl-tls/edge-certificates)                  | [ACM docs](/ssl/edge-certificates/advanced-certificate-manager/)           | [ACM community](https://community.cloudflare.com/c/security/6)                 |
| API Shield                         | [API Shield dashboard](https://dash.cloudflare.com/?to=/:account/:zone/security/api-shield)                 | [API Shield docs](/api-shield/)                                            | [API Shield community](https://community.cloudflare.com/)                      |
| Argo Smart Routing                 | [Argo dashboard](https://dash.cloudflare.com/?to=/:account/:zone/traffic)                                   | [Argo docs](/argo-smart-routing/)                                          | [Argo community](https://community.cloudflare.com/c/performance/argo/45)       |
| Bot Management                     | [Bot Management dashboard](https://dash.cloudflare.com/?to=/:account/:zone/security/bots)                   | [Bot Management docs](/bots/plans/bm-subscription/)                        | [Bot Management community](https://community.cloudflare.com/c/security/6)      |
| Cloudflare for SaaS                | [Cloudflare for SaaS dashboard](https://dash.cloudflare.com/?to=/:account/:zone/ssl-tls/custom-hostnames)   | [Cloudflare for SaaS docs](/cloudflare-for-platforms/cloudflare-for-saas/) | [Cloudflare for SaaS community](https://discord.cloudflare.com)                |
| Images                             | [Images dashboard](https://dash.cloudflare.com/?to=/:account/images)                                        | [Images docs](/images/)                                                    | [Images community](https://community.cloudflare.com/c/developers/images/63)    |
| Load Balancing                     | [Load Balancing dashboard](https://dash.cloudflare.com/?to=/:account/:zone/traffic/load-balancing)          | [Load Balancing docs](/load-balancing/)                                    | [Load Balancing community](https://community.cloudflare.com/tag/loadbalancing) |
| Advanced Rate Limiting             | [Rate Limiting dashboard](https://dash.cloudflare.com/?to=/:account/:zone/security/waf/rate-limiting-rules) | [Rate Limiting docs](/waf/rate-limiting-rules/)                            | [Rate Limiting community](https://community.cloudflare.com/c/security/6)       |
| Stream                             | [Stream dashboard](https://dash.cloudflare.com/?to=/:account/stream)                                        | [Stream docs](/stream/)                                                    | [Stream community](https://community.cloudflare.com/tag/cloudflarestream)      |
| Waiting Room                       | [Waiting Room Room dashboard](https://dash.cloudflare.com/?to=/:account/:zone/traffic/waiting-rooms)        | [Waiting Room docs](/waiting-room/)                                        | [Waiting Room community](https://community.cloudflare.com/)                    |
| Web3                               | [Web3 dashboard](https://dash.cloudflare.com/?to=/:account/:zone/web3)                                      | [Web3 docs](/web3/)                                                        | [Web3 discord](https://discord.cloudflare.com)                                 |
| Workers                            | [Workers dashboard](https://dash.cloudflare.com/?to=/:account/workers)                                      | [Workers docs](/workers/)                                                  | [Workers discord](https://discord.com/invite/cloudflaredev)                    |
| Zero Trust                         | [Zero Trust dashboard](https://one.dash.cloudflare.com/)                                                    | [Zero Trust docs](/cloudflare-one/)                                        | [Zero Trust community](https://community.cloudflare.com/c/security/access/51)  |

## Recommendations

Since these services are not yet part of your contract, we recommend that you use them on staging or other, non-production environments.

## View enabled products

To view which products you have previously enabled, go to your [Account Subscriptions](https://dash.cloudflare.com/?to=/:account/billing/subscriptions) page and look for items that with **Terms** of **NOT IN CONTRACT**.

---

# Troubleshoot failed payments

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/troubleshooting-failed-payments/

## Overview

If payment for a new Cloudflare plan, add-on, or subscription fails, you may need to ask your bank to remove a hold, update your Cloudflare billing information, or confirm that your account has sufficient funds

If payment for a recurring charge for a Cloudflare plan, add-on, or subscription is unsuccessful after five (5) days, your account is automatically downgraded to a Free plan. Downgrading to a Free plan does not suspend your website, but you will lose any subscriptions or add-on services associated with the Pro, Business, or Enterprise plan.

:::caution

The five-day grace period for failed payments on recurring Cloudflare charges includes weekends and holidays.
:::

After confirming your payment method information, Cloudflare suggests [manually updating your plan type](/fundamentals/subscriptions-and-billing/change-plan/)Â and subscriptions using the Cloudflare dashboard.

***

## Payment method validation

All changes to payment methods must now pass validation before being accepted. An error will result if the payment details are incorrect.

Below are common validation methods as well as next steps to rectify the issue:

### Address validation

The address entered is checked against a database to verify if it is correct. The verification will fail if there is a mismatch.

### Card address validation

The address entered on the Cloudflare Dashboard is validated against the address that the card is registered with at the card provider. The verification will fail if there is a mismatch.

### CVC validation

The CVC code is verified against the card entered. The verification will fail if there is a mismatch. The card will also be rejected if it is not accepted for use on our website.

### Card type validation

The card type will be validated, which means that card types that we do not support will fail. Not all card types are supported by Cloudflare. As a result, some Visa and Mastercard methods may not be supported.

### Email validation

A verification email is sent to the address on file for your PayPal account.

***

## Common reasons for failed payments

### Bank hold

The bank or card issuer has placed a hold or other similar measure that prevents Cloudflare from charging the card on file. The email you received alerting you about the failed payment lists the specific reason for the decline.

Contact your bank to check for any restriction that would prevent Cloudflare from successfully processing your payment.

### Incorrect billing information

The billing information that Cloudflare has on file is inaccurate or incomplete. Also, the credit card on file might have expired.

:::note

Gift cards and pre-payment cards may not be accepted for payment as they are not associated with a billing address.
:::

To ensure all billing information is current and accurate, refer to [Updating your Cloudflare billing information](/fundamentals/subscriptions-and-billing/update-billing-info/).

### Insufficient funds

Cloudflare was unable to collect payment due to insufficient funds in the account on file. Contact your financial institution to ensure fund availability.

### Cloudflare Registrar domain purchases

Cloudflare processes payments for Registrar domain transfers one at a time, which may trigger a failed payment for users that purchase multiple domains at a time.

For example, if you register five domains in one day, you will be charged five times. This may be flagged as fraud by your credit card company.

Contact your financial institution to ensure that this payment will be processed.

***

## Related resources

* [Cloudflare billing policy](/support/account-management-billing/billing-cloudflare-plans/cloudflare-billing-policy/)
* [Understand Cloudflare invoices](/fundamentals/subscriptions-and-billing/understand-invoices/)
* [Update your Cloudflare billing information](/fundamentals/subscriptions-and-billing/update-billing-info/)

---

# Understand Cloudflare invoices

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/understand-invoices/

You will receive an invoice in the **Billing** section of your Cloudflare account when you:

* Change your Cloudflare plan type
* Upgrade or downgrade to or from a paid plan
* Add a new domain to a Cloudflare account
* Enable or renew a subscription or add-on service

For any historical invoices not included in the **Billing** section, [contact Cloudflare support](/support/contacting-cloudflare-support/).

:::note


Enterprise customers receive invoices directly from the Cloudflare accounting team.


:::

***

## Enable email invoices from Cloudflare

To enable Cloudflare invoice emails, which are sent when you add or remove subscriptions from your account:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com), and select your account.

2. Select **Manage Account** > **Billing**.

3. Select **Invoices & Documents**.

4. In the **Billing email preferences** card, toggle the notification switch to **On**.

Once enabled, you will receive invoices via email:

* within one business day of initial set-up
* every month at the end of your billing period
* within one business day for all new purchases

## Download invoice

To download your Cloudflare invoice:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com), and select your account.

2. Select **Manage Account** > **Billing**.

3. Select **Invoices & Documents**.

4. Find the invoice you want to download and select the icon next to the invoice number.

:::note


Invoices cannot be re-generated once issued. Any pending billing updates or changes appear in the next month's invoice.


:::

***

## Billing cycles

Monthly and annual billing subscriptions run on different billing cycles.

The first monthly purchase on a Cloudflare account sets the billing date for the following monthly subscriptions. The same behaviour occurs for annual subscriptions.

You can have two different billing cycles on your account, one for a monthly subscription and another for an annual subscription.

***

## Troubleshooting your invoice

### Change in billing contact information

If your billing contact information has changed, [update your Cloudflare email address](/fundamentals/subscriptions-and-billing/change-password-or-email/#change-email-address) as soon as possible.

### Change in Cloudflare subscription or account

The invoice data corresponds to the date your Cloudflare account changed. You are charged immediately for the plan, additional domain, or add-on service. An invoice will be available on the Cloudflare dashboard within 24 hours of the account change.

Billing periods are 30 days. Payments for all recurring monthly costs are processed on the last day of the billing period. Invoices are generated the same day and will appear in the **Billing** section of the [Cloudflare dashboard](https://dash.cloudflare.com) within 24 hours.

### Cloudflare invoice without company name

To add your business or company name, VAT ID, or Tax ID/EIN on an invoice, add the company name when [updating billing information](/fundamentals/subscriptions-and-billing/update-billing-info/).

### Inconsistent invoice and payment amounts

If your Cloudflare payment is past due and you order additional services, the past due amount will be added to your invoice. This may cause inconsistencies between the invoice and what you see in the [Cloudflare dashboard](https://dash.cloudflare.com). Once the account is current, the amounts in the [Cloudflare dashboard](https://dash.cloudflare.com) will update.

:::note


It may take up to one billing period for updates to appear in your Cloudflare invoice.


:::

---

# Update billing information

URL: https://developers.cloudflare.com/fundamentals/subscriptions-and-billing/update-billing-info/

import { Render } from "~/components"

To avoid potential disruptions in your Cloudflare services, make sure your billing information is current and accurate. It is also important to understand [how Cloudflare plans and add-ons are priced](https://www.cloudflare.com/plans/#overview).

If Cloudflare is unable to process your payment, review [Troubleshooting failed payments](/fundamentals/subscriptions-and-billing/troubleshooting-failed-payments/).

***

## Access payment methods

To access your current payment methods (primary and backup):

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com).

2. Select **Manage account** > **Billing**.

3. Select **Payment Info**.

## Add or update payment methods

<Render file="billing-add-payment-method" />

### Payment options

Enterprise customers can submit payments via the following payment options:

#### ACH

(preferred method)

* **Bank**: Citibank, One Pennâ€™s Way, New Castle, DE 19720 USA
* **Account name**: CLOUDFLARE INC
* **Account number**: 31460181
* **ABA/Routing number**: 031100209

#### Wire transfer

* **Bank**: Citibank, One Pennâ€™s Way, New Castle, DE 19720 USA
* **Account name**: CLOUDFLARE INC
* **Account number**: 31460181
* **ABA/Routing number**: 031100209
* **SWIFT**: CITIUS33

#### PayPal

Log in to your PayPal account and send your payment to [ar@cloudflare.com](mailto:ar@cloudflare.com). The payment must include the invoice number and customer name.

#### International payments

* **Bank**: Citibank, One Pennâ€™s Way, New Castle, DE 19720 USA
* **Account name**: CLOUDFLARE INC
* **Account number**: 31460181
* **SWIFT**: CITIUS33

:::note


US banks do not participate in International Bank Account Numbers (IBAN).


:::

## Delete your current payment method

Before removing your payment method from file, you must cancel all Cloudflare paid services.

:::caution


If you currently subscribe to any [add-on services](/support/account-management-billing/billing-add-on-service/), Cloudflare must always have a payment method on file. If you need to remove a payment method, then you must enter a new one to replace it. Otherwise, you will see an error.

You also cannot delete your payment method if a payment fails or if there is an outstanding balance. Until we process payment, you can only add or edit your payment method.


:::

To delete your current payment method:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com).

2. On the left sidebar, select **Manage account** > **Billing**.

3. Select **Payment Info**.

4. In the **Payment Method** card, select **Manage**.

5. Next to your current payment method, select **Delete**.

6. Select **Confirm** to finish.

## Update your billing address

The billing address is associated with your payment method and is used to calculate your sales tax. If you need to update your billing address, you must also enter you payment method. The process for updating your billing address depends on the payment method.

If paying by credit card:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com).

2. On the left sidebar, select **Manage account** > **Billing**.

3. Select **Edit** in the **Billing Address** card and input your information.

4. Review the suggested address in the pop-up window. If the information is correct, select **Confirm**.

If paying by PayPal, refer to PayPal's [billing address documentation](https://www.paypal.com/ai/smarthelp/article/how-do-i-edit-the-billing-address-linked-to-my-credit-card-faq680).

## Update billing email address

Your billing email address is particularly important if you have [opted into email invoices](/fundamentals/subscriptions-and-billing/create-billing-profile/#enable-email-invoices).

To update your billing email address:

1. Log into the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to **Manage Account** > **Billing**.
3. Go to **Invoices & Documents**.
4. For **Billing email preference**, select **Change email address**.
5. Enter and confirm your new email and select **Save**.

---

# Health Checks regions

URL: https://developers.cloudflare.com/health-checks/concepts/health-checks-regions/

Cloudflare has data centers in [hundreds of cities worldwide](https://www.cloudflare.com/network/). Health checks do not run from every single of these data centers as this would result in numerous requests to your servers. Instead, you are able to choose between one and thirteen regions from which to run health checks. Cloudflare will run Health Checks from three data centers in each region that you select.

:::note


The exact location of these data centers are subject to change at any moment.


:::

The Internet is not the same everywhere around the world and your users may not have the same experience on your application according to where they are. Running Health Checks from different regions lets you know the health of your application from the point of view of the Cloudflare network in each of these regions.

If you select multiple regions or choose **All Regions** (Business & Enterprise Only), you may increase traffic to your servers. Each region sends individual health checks from three data centers.

---

# Concepts

URL: https://developers.cloudflare.com/health-checks/concepts/

import { DirectoryListing } from "~/components"

Review the following topics to learn more about the basic concepts involved in Cloudflare Health Checks.

<DirectoryListing />

---

# Health Checks notifications

URL: https://developers.cloudflare.com/health-checks/how-to/health-checks-notifications/

You can [configure notification emails](/health-checks/how-to/health-checks-notifications/#configure-notifications) to be alerted when the Health Check detects that there is a change in the status of your origin server. Cloudflare will send you an email within seconds so you can take the necessary action before customers are impacted.

The email provides information to determine what caused the health status change. You can evaluate when the change happened, the status of the origin server, if and why it is unhealthy, the expected response code, and the received response code.

## Configure notifications

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account and domain.
2. Navigate to **Traffic** > **Health Checks**.
3. Select **Configure an alert**.
4. Fill out the **Notification name** and **Description**.
5. Add a Notification email.
6. Select **Next**.
7. Add health checks to include in your alerts.
8. Choose the **Notification trigger**, which determines when you receive alerts.
9. Select **Create**.

:::note


A notification is only sent after a change of status in the majority of all selected region(s).

For a single region, this will be 2 of 3 data centers. With 13 regions selected, this will be 7 of 13 regions.


:::

See [common error codes](/health-checks/health-checks-analytics/#common-error-codes) for more information regarding the cause of any changes to your Health Check.

Cloudflare encourages you to view your [Health Checks Analytics](/health-checks/health-checks-analytics/#common-error-codes) to get more context about the health of your servers over time.

---

# How to

URL: https://developers.cloudflare.com/health-checks/how-to/

import { DirectoryListing } from "~/components"

See the following pages for more information about standalone Health Checks:

<DirectoryListing />

---

# Zone Lockdown

URL: https://developers.cloudflare.com/health-checks/how-to/zone-lockdown/

Currently, any Cloudflare customer on a paid plan can configure Health Checks against any host or IP. [Zone Lockdown](/waf/tools/zone-lockdown/) specifies a list of one or more IP addresses, CIDR ranges, or networks that are the only IPs allowed to access a domain, subdomain, or URL. It allows multiple destinations in a single rule as well as IPv4 and IPv6 addresses. IP addresses not specified in the Zone Lockdown rule are denied access to the specified resources.

When a customer enables zone lockdown, any Health Checks targeting that zone regardless of ownership will still get through because Cloudflare's ASN is on an allow-list.

Cloudflare's ASN is on an allow-list. This allows health checks to bypass zone lockdown. However, this creates a vulnerability and that behavior will change, resulting in Health Checks no longer being allowed through zone lockdown by default. Customers who use zone lockdown and want their health checks to continue passing can follow the guide below to bypass zone lockdown.

## Bypass zone lockdown

To bypass zone lockdown using a WAF custom rule:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account and domain.
2. Go to **Security** > **WAF** > **Custom rules**.
3. Select **Create rule**.
4. Create a custom rule matching on **user agent**.
5. Set the action to *Skip* and the corresponding feature to **Zone Lockdown** under **More components to skip**.

Cloudflare Health Checks have a user agent of the following format:
`Mozilla/5.0 (compatible;Cloudflare-Healthchecks/1.0;+https://www.cloudflare.com/; healthcheck-id: XXX)` where `XXX` is replaced with the first 16 characters of the Health Check ID.

To allow a specific Health Check, verify if the user agent contains the first 16 characters of the Health Check ID.

### Via the API

This example adds a new WAF custom rule to the ruleset with ID `{ruleset_id}` that skips zone lockdown for incoming requests with a user agent containing `1234567890abcdef`:

```bash
curl "https://api.cloudflare.com/client/v4/{zone_id}/rulesets/{ruleset_id}/rules" \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{
  "action": "skip",
  "action_parameters": {
    "products": [
      "zoneLockdown"
    ]
  },
  "expression": "http.user_agent contains \"1234567890abcdef\"",
  "description": "bypass zone lockdown - specific healthcheck"
}'
```

---

# Connect to PostgreSQL

URL: https://developers.cloudflare.com/hyperdrive/configuration/connect-to-postgres/

import { TabItem, Tabs, Render, WranglerConfig } from "~/components";

Hyperdrive supports PostgreSQL and PostgreSQL-compatible databases, [popular drivers](#supported-drivers) and Object Relational Mapper (ORM) libraries that use those drivers.

## Create a Hyperdrive

:::note

New to Hyperdrive? Refer to the [Get started guide](/hyperdrive/get-started/) to learn how to set up your first Hyperdrive.

:::

To create a Hyperdrive that connects to an existing PostgreSQL database, use the [wrangler](/workers/wrangler/install-and-update/) CLI or the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive).

When using wrangler, replace the placeholder value provided to `--connection-string` with the connection string for your database:

```sh
# wrangler v3.11 and above required
npx wrangler hyperdrive create my-first-hyperdrive --connection-string="postgres://user:password@database.host.example.com:5432/databasenamehere"
```

The command above will output the ID of your Hyperdrive, which you will need to set in the [Wrangler configuration file](/workers/wrangler/configuration/) for your Workers project:

<WranglerConfig>

```toml
# required for database drivers to function
compatibility_flags = ["nodejs_compat"]
compatibility_date = "2024-09-23"

[[hyperdrive]]
binding = "HYPERDRIVE"
id = "<your-hyperdrive-id-here>"
```

</WranglerConfig>

This will allow Hyperdrive to generate a dynamic connection string within your Worker that you can pass to your existing database driver. Refer to [Driver examples](#driver-examples) to learn how to set up a database driver with Hyperdrive.

Refer to the [Examples documentation](/hyperdrive/examples/) for step-by-step guides on how to set up Hyperdrive with several popular database providers.

## Supported drivers

Hyperdrive uses Workers [TCP socket support](/workers/runtime-apis/tcp-sockets/#connect) to support TCP connections to databases. The following table lists the supported database drivers and the minimum version that works with Hyperdrive:

| Driver                                                     | Documentation                                                            | Minimum Version Required | Notes                                                                                                                                                                                                                                                                                |
| ---------------------------------------------------------- | ------------------------------------------------------------------------ | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Postgres.js (**recommended**)                              | [Postgres.js documentation](https://github.com/porsager/postgres)        | `postgres@3.4.4`         | Supported in both Workers & Pages.                                                                                                                                                                                                                                                   |
| node-postgres - `pg`                                       | [node-postgres - `pg` documentation](https://node-postgres.com/)         | `pg@8.13.0`              | `8.11.4` introduced a bug with URL parsing and will not work. `8.11.5` fixes this. Requires `compatibility_flags = ["nodejs_compat"]` and `compatibility_date = "2024-09-23"` - refer to [Node.js compatibility](/workers/runtime-apis/nodejs). Requires wrangler `3.78.7` or later. |
| Drizzle                                                    | [Drizzle documentation](https://orm.drizzle.team/)                       | `0.26.2`^                |                                                                                                                                                                                                                                                                                      |
| Kysely                                                     | [Kysely documentation](https://kysely.dev/)                              | `0.26.3`^                |                                                                                                                                                                                                                                                                                      |
| [rust-postgres](https://github.com/sfackler/rust-postgres) | [rust-postgres documentation](https://docs.rs/postgres/latest/postgres/) | `v0.19.8`                | Use the [`query_typed`](https://docs.rs/postgres/latest/postgres/struct.Client.html#method.query_typed) method for best performance.                                                                                                                                                 |

^ _The marked libraries use `node-postgres` as a dependency._

Other drivers and ORMs not listed may also be supported: this list is not exhaustive.

### Database drivers and Node.js compatibility

[Node.js compatibility](/workers/runtime-apis/nodejs/) is required for database drivers, including Postgres.js, and needs to be configured for your Workers project.

<Render file="nodejs_compat" product="workers" />

## Supported TLS (SSL) modes

Hyperdrive supports the following [PostgreSQL TLS (SSL)](https://www.postgresql.org/docs/current/libpq-ssl.html) connection modes when connecting to your origin database:

| Mode          | Supported                       | Details                                                                                                                                   |
| ------------- | ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| `none`        | No                              | Hyperdrive does not support insecure plain text connections.                                                                              |
| `prefer`      | No (use `require`)              | Hyperdrive will always use TLS.                                                                                                           |
| `require`     | Yes (default)                   | TLS is required, and server certificates are validated (based on WebPKI).                                                                 |
| `verify-ca`   | Not currently supported in beta | Verifies the server's TLS certificate is signed by a root CA on the client. This ensures the server has a certificate the client trusts.  |
| `verify-full` | Not currently supported in beta | Identical to `verify-ca`, but also requires the database hostname must match a Subject Alternative Name (SAN) present on the certificate. |

:::caution

Hyperdrive does not currently support uploading client CA certificates. In the future, you will be able to provide the client CA to Hyperdrive as part of your database configuration.

:::

## Driver examples

The following examples show you how to:

1. Create a database client with a database driver.
2. Pass the Hyperdrive connection string and connect to the database.
3. Query your database via Hyperdrive.

### Postgres.js

The following Workers code shows you how to use [Postgres.js](https://github.com/porsager/postgres) with Hyperdrive.

<Render file="use-postgresjs-to-make-query" product="hyperdrive" />

### node-postgres / pg

Install the `node-postgres` driver:

```sh
npm install pg
```

**Ensure you have `compatibility_flags` and `compatibility_date` set in your [Wrangler configuration file](/workers/wrangler/configuration/)** as shown below:

<Render file="nodejs-compat-wrangler-toml" product="workers" />

Create a new `Client` instance and pass the Hyperdrive parameters:

```ts
import { Client } from "pg";

export interface Env {
	// If you set another name in the Wrangler configuration file as the value for 'binding',
	// replace "HYPERDRIVE" with the variable name you defined.
	HYPERDRIVE: Hyperdrive;
}

export default {
	async fetch(request, env, ctx): Promise<Response> {
		const client = new Client({
			connectionString: env.HYPERDRIVE.connectionString,
		});

		try {
			// Connect to your database
			await client.connect();

			// A very simple test query
			const result = await client.query({ text: "SELECT * FROM pg_tables" });

			// Clean up the client, ensuring we don't kill the worker before that is
			// completed.
			ctx.waitUntil(client.end());

			// Return result rows as JSON
			return Response.json({ result: result });
		} catch (e) {
			console.log(e);
			return Response.json({ error: e.message }, { status: 500 });
		}
	},
} satisfies ExportedHandler<Env>;
```

## Identify connections from Hyperdrive

To identify active connections to your Postgres database server from Hyperdrive:

- Hyperdrive's connections to your database will show up with `Cloudflare Hyperdrive` as the `application_name` in the `pg_stat_activity` table.
- Run `SELECT DISTINCT usename, application_name FROM pg_stat_activity WHERE application_name = 'Cloudflare Hyperdrive'` to show whether Hyperdrive is currently holding a connection (or connections) open to your database.

## Next steps

- Refer to the list of [supported database integrations](/workers/databases/connecting-to-databases/) to understand other ways to connect to existing databases.
- Learn more about how to use the [Socket API](/workers/runtime-apis/tcp-sockets) in a Worker.
- Understand the [protocols supported by Workers](/workers/reference/protocols/).

---

# How Hyperdrive works

URL: https://developers.cloudflare.com/hyperdrive/configuration/how-hyperdrive-works/

Connecting to traditional centralized databases from Cloudflare's global network which consists of over [300 data center locations](https://www.cloudflare.com/network/) presents a few challenges as queries can originate from any of these locations.

If your database is centrally located, queries can take a long time to get to the database and back. Queries can take even longer in situations where you have to establish a connection and make multiple round trips.

Traditional databases usually handle a maximum number of connections. With any reasonably large amount of distributed traffic, it becomes easy to exhaust these connections.

Hyperdrive solves these challenges by managing the number of global connections to your origin database, selectively parsing and choosing which query response to cache while reducing loading on your database and accelerating your database queries.

![Hyperdrive connection](~/assets/images/hyperdrive/configuration/hyperdrive-comparison.svg)

## Edge connection setup

When a database driver connects to a database from a Cloudflare Worker (illustrated above on the right half of the diagram in **Direct (without Hyperdrive)**) it will first go through the connection setup. This may require multiple round trips to the database in order to verify and establish a secure connection. This can incur additional network latency due to the distance between your Cloudflare Worker and your database.

**With Hyperdrive** (on the left half of the above diagram), this connection setup occurs between your Cloudflare Worker and Hyperdrive on the edge, as close to your Worker as possible. This incurs significantly less latency, since the connection setup is completed within the same location.

## Connection Pooling

Hyperdrive creates a pool of connections to your database that can be reused as your application executes queries against your database.

The pool of database connections is placed in one or more regions closest to your origin database. This minimizes the latency incurred by roundtrips between your Cloudflare Workers and database to establish new connections. This also ensures that as little network latency is incurred for uncached queries.

When a query hits Hyperdrive, the request is routed to the nearest connection pool.

If the connection pool has pre-existing connections, the connection pool will try and reuse that connection.

If the connection pool does not have pre-existing connections, it will establish a new connection to your database and use that to route your query. This aims at reusing and creating the least number of connections possible as required to operate your application.

:::note

Hyperdrive automatically manages the connection pool properties for you, including limiting the total number of connections to your origin database. Refer to [Limits](/hyperdrive/platform/limits/) to learn more.
:::

## Pooling mode

The Hyperdrive connection pooler operates in transaction mode, where the client that executes the query communicates through a single connection for the duration of a transaction. When that transaction has completed, the connection is returned to the pool.

Hyperdrive supports [`SET` statements](https://www.postgresql.org/docs/current/sql-set.html) for the duration of a transaction or a query. For instance, if you manually create a transaction with `BEGIN`/`COMMIT`, `SET` statements within the transaction will take effect. Moreover, a query that includes a `SET` command (`SET X; SELECT foo FROM bar;`) will also apply the `SET` command. When a connection is returned to the pool, the connection is `RESET` such that the `SET` commands will not take effect on subsequent queries.

This implies that a single Worker invocation may obtain multiple connections to perform its database operations and may need to `SET` any configurations for every query or transaction. It is not recommended to wrap multiple database operations with a single transaction to maintain the `SET` state. Doing so will affect the performance and scaling of Hyperdrive as the connection cannot be reused by other Worker isolates for the duration of the transaction.

Hyperdrive supports named prepared statements as implemented in the `postgres.js` and `node-postgres` drivers. Named prepared statements in other drivers may have worse performance.

## Unsupported PostgreSQL features:

Hyperdrive does not support the following PostgreSQL features:

- SQL-level management of prepared statements, such as using `PREPARE`, `DISCARD`, `DEALLOCATE`, or `EXECUTE`.
- Advisory locks ([PostgreSQL documentation](https://www.postgresql.org/docs/current/explicit-locking.html#ADVISORY-LOCKS)).
- `LISTEN` and `NOTIFY`.
- `PREPARE` and `DEALLOCATE`.
- Any modification to per-session state not explicitly documented as supported elsewhere.

In cases where you need to issue these unsupported statements from your application, the Hyperdrive team recommends setting up a second, direct client without Hyperdrive.

## Query Caching

Hyperdrive supports caching of non-mutating (read) queries to your database.

When queries are sent via Hyperdrive, Hyperdrive parses the query and determines whether the query is a mutating (write) or non-mutating (read) query.

For non-mutating queries, Hyperdrive will cache the response for the configured `max_age`, and whenever subsequent queries are made that match the original, Hyperdrive will return the cached response, bypassing the need to issue the query back to the origin database.

Caching reduces the burden on your origin database and accelerates the response times for your queries.

## Related resources

- [Query caching](/hyperdrive/configuration/query-caching/)

---

# Connect to a private database using Tunnel

URL: https://developers.cloudflare.com/hyperdrive/configuration/connect-to-private-database/

import { TabItem, Tabs, Render } from "~/components";

Hyperdrive can securely connect to your private databases using [Cloudflare Tunnel](/cloudflare-one/connections/connect-networks/) and [Cloudflare Access](/cloudflare-one/policies/access/).

## How it works

When your database is isolated within a private network (such as a [virtual private cloud](https://www.cloudflare.com/learning/cloud/what-is-a-virtual-private-cloud) or an on-premise network), you must enable a secure connection from your network to Cloudflare.

- [Cloudflare Tunnel](/cloudflare-one/connections/connect-networks/) is used to establish the secure tunnel connection.
- [Cloudflare Access](/cloudflare-one/policies/access/) is used to restrict access to your tunnel such that only specific Hyperdrive configurations can access it.

A request from the Cloudflare Worker to the origin database goes through Hyperdrive, Cloudflare Access, and the Cloudflare Tunnel established by `cloudflared`. `cloudflared` must be running in the private network in which your database is accessible.

The Cloudflare Tunnel will establish an outbound bidirectional connection from your private network to Cloudflare. Cloudflare Access will secure your Cloudflare Tunnel to be only accessible by your Hyperdrive configuration.

![A request from the Cloudflare Worker to the origin database goes through Hyperdrive, Cloudflare Access and the Cloudflare Tunnel established by `cloudflared`.](~/assets/images/hyperdrive/configuration/hyperdrive-private-database-architecture.png)

<Render file="tutorials-before-you-start" product="workers" />

:::caution[Warning]

If your organization also uses [Super Bot Fight Mode](/bots/get-started/pro/), keep **Definitely Automated** set to **Allow**. Otherwise, tunnels might fail with a `websocket: bad handshake` error.
:::

## Prerequisites

- A database in your private network, [configured to use TLS/SSL](/hyperdrive/configuration/connect-to-postgres/#supported-tls-ssl-modes).
- A hostname on your Cloudflare account, which will be used to route requests to your database.

## 1. Create a tunnel in your private network

### 1.1. Create a tunnel

First, create a [Cloudflare Tunnel](/cloudflare-one/connections/connect-networks/) in your private network to establish a secure connection between your network and Cloudflare. Your network must be configured such that the tunnel has permissions to egress to the Cloudflare network and access the database within your network.

<Render file="tunnel/create-tunnel" product="cloudflare-one" />

### 1.2. Connect your database using a public hostname

Your tunnel must be configured to use a public hostname so that Hyperdrive can route requests to it. If you don't have a hostname on Cloudflare yet, you will need to [register a new hostname](/registrar/get-started/register-domain/) or [add a zone](/dns/zone-setups/) to Cloudflare to proceed.

1. In the **Public Hostnames** tab, choose a **Domain** and specify any subdomain or path information. This will be used in your Hyperdrive configuration to route to this tunnel.

2. In the **Service** section, specify **Type** `TCP` and the URL and configured port of your database, such as `localhost:5432` or `my-database-host.database-provider.com:5432`. This address will be used by the tunnel to route requests to your database.

3. Select **Save tunnel**.

:::note
If you are setting up the tunnel through the CLI instead ([locally-managed tunnel](/cloudflare-one/connections/connect-networks/do-more-with-tunnels/local-management/)), you will have to complete these steps manually. Follow the Cloudflare Zero Trust documentation to [add a public hostname to your tunnel](/cloudflare-one/connections/connect-networks/routing-to-tunnel/dns/) and [configure the public hostname to route to the address of your database](/cloudflare-one/connections/connect-networks/do-more-with-tunnels/local-management/configuration-file/).
:::

## 2. Create and configure Hyperdrive to connect to the Cloudflare Tunnel

To restrict access to the Cloudflare Tunnel to Hyperdrive, a [Cloudflare Access application](/cloudflare-one/applications/) must be configured with a [Policy](/cloudflare-one/policies/) that requires requests to contain a valid [Service Auth token](/cloudflare-one/policies/access/#service-auth).

The Cloudflare dashboard can automatically create and configure the underlying [Cloudflare Access application](/cloudflare-one/applications/), [Service Auth token](/cloudflare-one/policies/access/#service-auth), and [Policy](/cloudflare-one/policies/) on your behalf. Alternatively, you can manually create the Access application and configure the Policies.

<Tabs> <TabItem label="Automatic creation">

### 2.1 Create a Hyperdrive configuration in the Cloudflare dashboard

Create a Hyperdrive configuration in the Cloudflare dashboard to automatically configure Hyperdrive to connect to your Cloudflare Tunnel.

1. In the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive), navigate to **Storage & Databases > Hyperdrive** and click **Create configuration**.
2. Select **Private database**.
3. In the **Networking details** section, select the tunnel you are connecting to.
4. In the **Networking details** section, select the hostname associated to the tunnel. If there is no hostname for your database, return to step [1.2. Connect your database using a public hostname](/hyperdrive/configuration/connect-to-private-database/#12-connect-your-database-using-a-public-hostname).
5. In the **Access Service Authentication Token** section, select **Create new (automatic)**.
6. In the **Access Application** section, select **Create new (automatic)**.
7. In the **Database connection details** section, enter the database **name**, **user**, and **password**.

</TabItem>
<TabItem label="Manual creation">
### 2.1 Create a service token

The service token will be used to restrict requests to the tunnel, and is needed for the next step.

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Access** > **Service auth** > **Service Tokens**.

2. Select **Create Service Token**.

3. Name the service token. The name allows you to easily identify events related to the token in the logs and to revoke the token individually.

4. Set a **Service Token Duration** of `Non-expiring`. This prevents the service token from expiring, ensuring it can be used throughout the life of the Hyperdrive configuration.

5. Select **Generate token**. You will see the generated Client ID and Client Secret for the service token, as well as their respective request headers.

6. Copy the Access Client ID and Access Client Secret. These will be used when creating the Hyperdrive configuration.

   :::caution
   This is the only time Cloudflare Access will display the Client Secret. If you lose the Client Secret, you must regenerate the service token.
   :::

### 2.2 Create an Access application to secure the tunnel

[Cloudflare Access](/cloudflare-one/policies/access/) will be used to verify that requests to the tunnel originate from Hyperdrive using the service token created above.

1. In [Zero Trust](https://one.dash.cloudflare.com), go to **Access** > **Applications**.

2. Select **Add an application**.

3. Select **Self-hosted**.

4. Enter any name for the application.

5. In **Session Duration**, select `No duration, expires immediately`.

6. Select **Add public hostname**. and enter the subdomain and domain that was previously set for the tunnel application.

7. Select **Create new policy**.

8. Enter a **Policy name** and set the **Action** to _Service Auth_.

9. Create an **Include** rule. Specify a **Selector** of _Service Token_ and the **Value** of the service token you created in step [2. Create a service token](#21-create-a-service-token).

10. Save the policy.

11. Go back to the application configuration and add the newly created Access policy.

12. In **Login methods**, turn off _Accept all available identity providers_ and clear all identity providers.

13. Select **Next**.

14. In **Application Appearance**, turn off **Show application in App Launcher**.

15. Select **Next**.

16. Select **Next**.

17. Save the application.

### 2.3 Create a Hyperdrive configuration

To create a Hyperdrive configuration for your private database, you'll need to specify the Access application and Cloudflare Tunnel information upon creation.

<Tabs> <TabItem label="Wrangler">

```sh
# wrangler v3.65 and above required
npx wrangler hyperdrive create <NAME-OF-HYPERDRIVE-CONFIGURATION-FOR-DB-VIA-TUNNEL> --host=<HOSTNAME-FOR-THE-TUNNEL> --user=<USERNAME-FOR-YOUR-DATABASE> --password=<PASSWORD-FOR-YOUR-DATABASE> --database=<DATABASE-TO-CONNECT-TO> --access-client-id=<YOUR-ACCESS-CLIENT-ID> --access-client-secret=<YOUR-SERVICE-TOKEN-CLIENT-SECRET>
```

</TabItem> <TabItem label="Terraform">

```terraform
resource "cloudflare_hyperdrive_config"  "<TERRAFORM_VARIABLE_NAME_FOR_CONFIGURATION>" {
  account_id = "<YOUR_ACCOUNT_ID>"
  name       = "<NAME_OF_HYPERDRIVE_CONFIGURATION>"
  origin     = {
    host     = "<HOSTNAME_OF_TUNNEL>"
    database = "<NAME_OF_DATABASE>"
    user     = "<NAME_OF_DATABASE_USER>"
    password = "<DATABASE_PASSWORD>"
    scheme   = "postgres"
    access_client_id     = "<ACCESS_CLIENT_ID>"
    access_client_secret = "<ACCESS_CLIENT_SECRET>"
  }
  caching = {
    disabled = false
  }
}
```

</TabItem> </Tabs>

This will create a Hyperdrive configuration using the usual database information (database name, database host, database user, and database password).

In addition, it will also set the Access Client ID and the Access Client Secret of the Service Token. When Hyperdrive makes requests to the tunnel, requests will be intercepted by Access and validated using the credentials of the Service Token.

:::note
When creating the Hyperdrive configuration for the private database, you must enter the `access-client-id` and the `access-client-id`, and omit the `port`. Hyperdrive will route database messages to the public hostname of the tunnel, and the tunnel will rely on its service configuration (as configured in [1.2. Connect your database using a public hostname](#12-connect-your-database-using-a-public-hostname)) to route requests to the database within your private network.
:::

</TabItem> </Tabs>

## 3. Query your Hyperdrive configuration from a Worker (optional)

To test your Hyperdrive configuration to the database using Cloudflare Tunnel and Access, use the Hyperdrive configuration ID in your Worker and deploy it.

### Create a Hyperdrive binding

<Render file="create-hyperdrive-binding" product="hyperdrive" />

### Query your database using Postgres.js

Use Postgres.js to send a test query to validate that the connection has been successful.

<Render file="use-postgresjs-to-make-query" product="hyperdrive" />

Now, deploy your Worker:

```bash
npx wrangler deploy
```

If you successfully receive the list of `pg_tables` from your database when you access your deployed Worker, your Hyperdrive has now been configured to securely connect to a private database using [Cloudflare Tunnel](/cloudflare-one/connections/connect-networks/) and [Cloudflare Access](/cloudflare-one/policies/access/).

## Troubleshooting

If you encounter issues when setting up your Hyperdrive configuration with tunnels to a private database, consider these common solutions, in addition to [general troubleshooting steps](/hyperdrive/observability/troubleshooting/) for Hyperdrive:

- Ensure your database is configured to use TLS (SSL). Hyperdrive requires TLS (SSL) to connect.

---

# Configuration

URL: https://developers.cloudflare.com/hyperdrive/configuration/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Query caching

URL: https://developers.cloudflare.com/hyperdrive/configuration/query-caching/

Hyperdrive automatically caches the most popular queries executed against your database, reducing the need to go back to your database (incurring latency and database load) for every query.

## What does Hyperdrive cache?

Because Hyperdrive uses database protocols, it can differentiate between a mutating query (a query that writes to the database) and a non-mutating query (a read-only query), allowing Hyperdrive to safely cache read-only queries.

Besides determining the difference between a `SELECT` and an `INSERT`, Hyperdrive also parses the database wire-protocol and uses it to differentiate between a mutating or non-mutating query.

For example, a read query that populates the front page of a news site would be cached:

```sql
-- Cacheable
SELECT * FROM articles
WHERE DATE(published_time) = CURRENT_DATE()
ORDER BY published_time DESC
LIMIT 50
```

Mutating queries (including `INSERT`, `UPSERT`, or `CREATE TABLE`) and queries that use [functions designated as `volatile` by PostgreSQL](https://www.postgresql.org/docs/current/xfunc-volatility.html) are not cached:

```sql
-- Not cached
INSERT INTO users(id, name, email) VALUES(555, 'Matt', 'hello@example.com');

SELECT LASTVAL(), * FROM articles LIMIT 50;
```

## Default cache settings

The default caching behaviour for Hyperdrive is defined as below:

- `max_age` = 60 seconds (1 minute)
- `stale_while_revalidate` = 15 seconds

The `max_age` setting determines the maximum lifetime a query response will be served from cache. Cached responses may be evicted from the cache prior to this time if they are rarely used.

The `stale_while_revalidate` setting allows Hyperdrive to continue serving stale cache results for an additional period of time while it is revalidating the cache. In most cases, revalidation should happen rapidly.

You can set a maximum `max_age` of 1 hour.

## Disable caching

Disable caching on a per-Hyperdrive basis by using the [Wrangler](/workers/wrangler/install-and-update/) CLI to set the `--caching-disabled` option to `true`.

For example:

```sh
# wrangler v3.11 and above required
npx wrangler hyperdrive update my-hyperdrive-id --origin-password my-db-password --caching-disabled true
```

You can also configure multiple Hyperdrive connections from a single application: one connection that enables caching for popular queries, and a second connection where you do not want to cache queries, but still benefit from Hyperdrive's latency benefits and connection pooling.

For example, using the [Postgres.js](/hyperdrive/configuration/connect-to-postgres/) driver:

```ts
const client = new Client({
	connectionString: env.HYPERDRIVE.connectionString,
});
// ...
const noCachingClient = new Client({
	// This represents a Hyperdrive configuration with the cache disabled
	connectionString: env.HYPERDRIVE_CACHE_DISABLED.connectionString,
});
```

## Next steps

- Learn more about [How Hyperdrive works](/hyperdrive/configuration/how-hyperdrive-works/).
- Learn how to [Connect to PostgreSQL](/hyperdrive/configuration/connect-to-postgres/) from Hyperdrive.
- Review [Troubleshooting common issues](/hyperdrive/observability/troubleshooting/) when connecting a database to Hyperdrive.

---

# Local development

URL: https://developers.cloudflare.com/hyperdrive/configuration/local-development/

import { WranglerConfig } from "~/components";

Hyperdrive can be used when developing and testing your Workers locally by connecting to any local database instance running on your machine directly. Local development uses [Wrangler](/workers/wrangler/install-and-update/), the command-line interface for Workers, to manage local development sessions and state.

## Configure local development

:::note

This guide assumes you are using `wrangler` version `3.27.0` or later.

If you are new to Hyperdrive and/or Cloudflare Workers, refer to [Hyperdrive tutorial](/hyperdrive/get-started/) to install `wrangler` and deploy their first database.

:::

To specify a database to connect to when developing locally, you can:

- **Recommended** Create a `WRANGLER_HYPERDRIVE_LOCAL_CONNECTION_STRING_<BINDING_NAME>` environmental variable with the connection string of your database. `<BINDING_NAME>` is the name of the binding assigned to your Hyperdrive in your [Wrangler configuration file](/workers/wrangler/configuration/) or Pages configuration. This allows you to avoid committing potentially sensitive credentials to source control in your Wrangler configuration file, if your test/development database is not ephemeral. If you have configured multiple Hyperdrive bindings, replace `<BINDING_NAME>` with the unique binding name for each.
- Set `localConnectionString` in the Wrangler configuration file.

If both the `WRANGLER_HYPERDRIVE_LOCAL_CONNECTION_STRING_<BINDING_NAME>` environmental variable and `localConnectionString` in the Wrangler configuration file are set, `wrangler dev` will use the environmental variable instead. Use `unset WRANGLER_HYPERDRIVE_LOCAL_CONNECTION_STRING_<BINDING_NAME>` to unset any existing environmental variables.

For example, to use the environmental variable, export the environmental variable before running `wrangler dev`:

```sh
# Your configured Hyperdrive binding is "TEST_DB"
export WRANGLER_HYPERDRIVE_LOCAL_CONNECTION_STRING_TEST_DB="postgres://user:password@localhost:5432/databasename"
# Start a local development session referencing this local instance
npx wrangler dev
```

To configure a `localConnectionString` in the [Wrangler configuration file](/workers/wrangler/configuration/), ensure your Hyperdrive bindings have a `localConnectionString` property set:

<WranglerConfig>

```toml
[[hyperdrive]]
binding = "TEST_DB"
id = "c020574a-5623-407b-be0c-cd192bab9545"
localConnectionString = "postgres://user:password@localhost:5432/databasename"
```

</WranglerConfig>

## Use `wrangler dev`

The following example shows you how to check your wrangler version, set a `WRANGLER_HYPERDRIVE_LOCAL_CONNECTION_STRING_TEST_DB` environmental variable, and run a `wrangler dev` session:

```sh
# Confirm you are using wrangler v3.0+
npx wrangler --version
```

```sh output
â›…ï¸ wrangler 3.27.0
```

```sh
# Set your environmental variable: your configured Hyperdrive binding is "TEST_DB".
export WRANGLER_HYPERDRIVE_LOCAL_CONNECTION_STRING_TEST_DB="postgres://user:password@localhost:5432/databasename"
```

```sh
# Start a local dev session:
npx wrangler dev
```

```sh output
------------------
Found a non-empty WRANGLER_HYPERDRIVE_LOCAL_CONNECTION_STRING_TEST_DB variable. Hyperdrive will connect to this database
during local development.

wrangler dev now uses local mode by default, powered by ðŸ”¥ Miniflare and ðŸ‘· workerd.
To run an edge preview session for your Worker, use wrangler dev --remote
Your worker has access to the following bindings:
- Hyperdrive configs:
  - TEST_DB: c020574a-5623-407b-be0c-cd192bab9545
âŽ” Starting local server...

[mf:inf] Ready on http://127.0.0.1:8787/
[b] open a browser, [d] open Devtools, [l] turn off local mode, [c] clear console, [x] to exit
```

`wrangler dev` separates local and production (remote) data. A local session does not have access to your production data by default. To access your production (remote) Hyperdrive configuration, pass the `--remote` flag when calling `wrangler dev`. Any changes you make when running in `--remote` mode cannot be undone.

Refer to the [`wrangler dev` documentation](/workers/wrangler/commands/#dev) to learn more about how to configure a local development session.

## Related resources

- Use [`wrangler dev`](/workers/wrangler/commands/#dev) to run your Worker and Hyperdrive locally and debug issues before deploying.
- Learn [how Hyperdrive works](/hyperdrive/configuration/how-hyperdrive-works/).
- Understand how to [configure query caching in Hyperdrive](/hyperdrive/configuration/query-caching/).

---

# Rotating database credentials

URL: https://developers.cloudflare.com/hyperdrive/configuration/rotate-credentials/

import { TabItem, Tabs, Render, WranglerConfig } from "~/components";

You can change the connection information and credentials of your Hyperdrive configuration in one of two ways:

1. Create a new Hyperdrive configuration with the new connection information, and update your Worker to use the new Hyperdrive configuration.
2. Update the existing Hyperdrive configuration with the new connection information and credentials.

## Use a new Hyperdrive configuration

Creating a new Hyperdrive configuration to update your database credentials allows you to keep your existing Hyperdrive configuration unchanged, gradually migrate your Worker to the new Hyperdrive configuration, and easily roll back to the previous configuration if needed.

To create a Hyperdrive configuration that connects to an existing PostgreSQL database, use the [Wrangler](/workers/wrangler/install-and-update/) CLI or the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive).

```sh
# wrangler v3.11 and above required
npx wrangler hyperdrive create my-updated-hyperdrive --connection-string="<YOUR_CONNECTION_STRING>"
```

The command above will output the ID of your Hyperdrive. Set this ID in the [Wrangler configuration file](/workers/wrangler/configuration/) for your Workers project:

<WranglerConfig>

```toml
# required for database drivers to function
compatibility_flags = [ "nodejs_compat" ]
compatibility_date = "2024-09-23"

[[hyperdrive]]
binding = "HYPERDRIVE"
id = "<your-hyperdrive-id-here>"
```

</WranglerConfig>

To update your Worker to use the new Hyperdrive configuration, redeploy your Worker or use [gradual deployments](/workers/configuration/versions-and-deployments/gradual-deployments/).

## Update the existing Hyperdrive configuration

You can update the configuration of an existing Hyperdrive configuration using the [wrangler CLI](/workers/wrangler/install-and-update/).

```sh
# wrangler v3.11 and above required
npx wrangler hyperdrive update <HYPERDRIVE_CONFIG_ID> --origin-host <YOUR_ORIGIN_HOST> --origin-password <YOUR_ORIGIN_PASSWORD> --origin-user <YOUR_ORIGIN_USERNAME> --database <YOUR_DATABASE> --origin-port <YOUR_ORIGIN_PORT>
```

:::note
Updating the settings of an existing Hyperdrive configuration does not purge Hyperdrive's cache and does not tear down the existing database connection pool. New connections will be established using the new connection information.
:::

---

# Connect to AWS RDS and Aurora

URL: https://developers.cloudflare.com/hyperdrive/examples/aws-rds-aurora/

import { Render } from "~/components";

This example shows you how to connect Hyperdrive to an Amazon Relational Database Service (Amazon RDS) Postgres or Amazon Aurora database instance.

## 1. Allow Hyperdrive access

To allow Hyperdrive to connect to your database, you will need to ensure that Hyperdrive has valid user credentials and network access.

<Render file="public-connectivity" />

### AWS Console

When creating or modifying an instance in the AWS console:

1. Configure a **DB cluster identifier** and other settings you wish to customize.
2. Under **Settings** > **Credential settings**, note down the **Master username** and **Master password**.
3. Under the **Connectivity** header, ensure **Public access** is set to **Yes**.
4. Select an **Existing VPC security group** that allows public Internet access from `0.0.0.0/0` to the port your database instance is configured to listen on (default: `5432` for PostgreSQL instances).
5. Select **Create database**.

:::caution

You must ensure that the [VPC security group](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html) associated with your database allows public IPv4 access to your database port.

Refer to AWS' [database server rules](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules-reference.html#sg-rules-db-server) for details on how to configure rules specific to your RDS or Aurora database.

:::

### Retrieve the database endpoint (Aurora)

To retrieve the database endpoint (hostname) for Hyperdrive to connect to:

1. Go to **Databases** view under **RDS** in the AWS console.
2. Select the database you want Hyperdrive to connect to.
3. Under the **Endpoints** header, note down the **Endpoint name** with the type `Writer` and the **Port**.

### Retrieve the database endpoint (RDS PostgreSQL)

For regular RDS instances (non-Aurora), you will need to fetch the endpoint and port of the database:

1. Go to **Databases** view under **RDS** in the AWS console.
2. Select the database you want Hyperdrive to connect to.
3. Under the **Connectivity & security** header, note down the **Endpoint** and the **Port**.

The endpoint will resemble `YOUR_DATABASE_NAME.cpuo5rlli58m.AWS_REGION.rds.amazonaws.com` and the port will default to `5432`.

## 2. Create your user

Once your database is created, you will need to create a user for Hyperdrive to connect as. Although you can use the **Master username** configured during initial database creation, best practice is to create a less privileged user.

To create a new user, log in to the database and use the `CREATE ROLE` command:

```sh
# Log in to the database
psql postgresql://MASTER_USERNAME:MASTER_PASSWORD@ENDPOINT_NAME:PORT/database_name
```

Run the following SQL statements:

```sql
-- Create a role for Hyperdrive
CREATE ROLE hyperdrive;

-- Allow Hyperdrive to connect
GRANT CONNECT ON DATABASE postgres TO hyperdrive;

-- Grant database privileges to the hyperdrive role
GRANT ALL PRIVILEGES ON DATABASE postgres to hyperdrive;

-- Create a specific user for Hyperdrive to log in as
CREATE ROLE hyperdrive_user LOGIN PASSWORD 'sufficientlyRandomPassword';

-- Grant this new user the hyperdrive role privileges
GRANT hyperdrive to hyperdrive_user;
```

Refer to AWS' [documentation on user roles in PostgreSQL](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.PostgreSQL.CommonDBATasks.Roles.html) for more details.

With a database user, password, database endpoint (hostname and port) and database name (default: `postgres`), you can now set up Hyperdrive.

## 3. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Connect to CockroachDB

URL: https://developers.cloudflare.com/hyperdrive/examples/cockroachdb/

import { Render } from "~/components"

This example shows you how to connect Hyperdrive to a [CockroachDB](https://www.cockroachlabs.com/) database cluster. CockroachDB is a PostgreSQL-compatible distributed SQL database with strong consistency guarantees.

## 1. Allow Hyperdrive access

To allow Hyperdrive to connect to your database, you will need to ensure that Hyperdrive has valid user credentials and network access.

### CockroachDB Console

The steps below assume you have an [existing CockroachDB Cloud account](https://www.cockroachlabs.com/docs/cockroachcloud/quickstart) and database cluster created.

To create and/or fetch your database credentials:

1. Go to the [CockroachDB Cloud console](https://cockroachlabs.cloud/clusters) and select the cluster you want Hyperdrive to connect to.
2. Select **SQL Users** from the sidebar on the left, and select **Add User**.
3. Enter a username (for example, \`hyperdrive-user), and select **Generate & Save Password**.
4. Note down the username and copy the password to a temporary location.

To retrieve your database connection details:

1. Go to the [CockroachDB Cloud console](https://cockroachlabs.cloud/clusters) and select the cluster you want Hyperdrive to connect to.
2. Select **Connect** in the top right.
3. Choose the user you created, for example,`hyperdrive-user`.
4. Select the database, for example `defaultdb`.
5. Select **General connection string** as the option.
6. In the text box below, select **Copy** to copy the connection string.

By default, the CockroachDB cloud enables connections from the public Internet (`0.0.0.0/0`). If you have changed these settings on an existing cluster, you will need to allow connections from the public Internet for Hyperdrive to connect.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Connect to Azure Database

URL: https://developers.cloudflare.com/hyperdrive/examples/azure/

import { Render } from "~/components";

This example shows you how to connect Hyperdrive to an Azure Database for PostgreSQL instance.

## 1. Allow Hyperdrive access

To allow Hyperdrive to connect to your database, you will need to ensure that Hyperdrive has valid credentials and network access.

<Render file="public-connectivity" />

### Azure Portal

#### Public access networking

To connect to your Azure Database for PostgreSQL instance using public Internet connectivity:

1. In the [Azure Portal](https://portal.azure.com/), select the instance you want Hyperdrive to connect to.
2. Expand **Settings** > **Networking** > ensure **Public access** is enabled > in **Firewall rules** add `0.0.0.0` as **Start IP address** and `255.255.255.255` as **End IP address**.
3. Select **Save** to persist your changes.
4. Select **Overview** from the sidebar and note down the **Server name** of your instance.

With the username, password, server name, and database name (default: `postgres`), you can now create a Hyperdrive database configuration.

#### Private access networking

To connect to a private Azure Database for PostgreSQL instance, refer to [Connect to a private database using Tunnel](/hyperdrive/configuration/connect-to-private-database/).

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Connect to Digital Ocean

URL: https://developers.cloudflare.com/hyperdrive/examples/digital-ocean/

import { Render } from "~/components";

This example shows you how to connect Hyperdrive to a Digital Ocean database instance.

## 1. Allow Hyperdrive access

To allow Hyperdrive to connect to your database, you will need to ensure that Hyperdrive has valid user credentials and network access.

### DigitalOcean Dashboard

1. Go to the DigitalOcean dashboard and select the database you wish to connect to.
2. Go to the **Overview** tab.
3. Under the **Connection Details** panel, select **Public network**.
4. On the dropdown menu, select **Connection string** > **show-password**.
5. Copy the connection string.

With the connection string, you can now create a Hyperdrive database configuration.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

:::note
If you see a DNS-related error, it is possible that the DNS for your vendor's database has not yet been propagated. Try waiting 10 minutes before retrying the operation. Refer to [DigitalOcean support page](https://docs.digitalocean.com/support/why-does-my-domain-fail-to-resolve/) for more information.
:::

---

# Connect to Google Cloud SQL

URL: https://developers.cloudflare.com/hyperdrive/examples/google-cloud-sql/

import { Render } from "~/components";

This example shows you how to connect Hyperdrive to a Google Cloud SQL PostgreSQL database instance.

## 1. Allow Hyperdrive access

To allow Hyperdrive to connect to your database, you will need to ensure that Hyperdrive has valid user credentials and network access.

<Render file="public-connectivity" />

### Cloud Console

When creating the instance or when editing an existing instance in the [Google Cloud Console](https://console.cloud.google.com/sql/instances):

To allow Hyperdrive to reach your instance:

1. In the [Cloud Console](https://console.cloud.google.com/sql/instances), select the instance you want Hyperdrive to connect to.
2. Expand **Connections** > ensure **Public IP** is enabled > **Add a Network** and input `0.0.0.0/0`.
3. Select **Done** > **Save** to persist your changes.
4. Select **Overview** from the sidebar and note down the **Public IP address** of your instance.

To create a user for Hyperdrive to connect as:

1. Select **Users** in the sidebar.
2. Select **Add User Account** > select **Built-in authentication**.
3. Provide a name (for example, `hyperdrive-user`) > select **Generate** to generate a password.
4. Copy this password to your clipboard before selecting **Add** to create the user.

With the username, password, public IP address and (optional) database name (default: `postgres`), you can now create a Hyperdrive database configuration.

### gcloud CLI

The [gcloud CLI](https://cloud.google.com/sdk/docs/install) allows you to create a new user and enable Hyperdrive to connect to your database.

Use `gcloud sql` to create a new user (for example, `hyperdrive-user`) with a strong password:

```sh
gcloud sql users create hyperdrive-user --instance=YOUR_INSTANCE_NAME --password=SUFFICIENTLY_LONG_PASSWORD
```

Run the following command to enable [Internet access](https://cloud.google.com/sql/docs/postgres/configure-ip) to your database instance:

```sh
# If you have any existing authorized networks, ensure you provide those as a comma separated list.
# The gcloud CLI will replace any existing authorized networks with the list you provide here.
gcloud sql instances patch YOUR_INSTANCE_NAME --authorized-networks="0.0.0.0/0"
```

Refer to [Google Cloud's documentation](https://cloud.google.com/sql/docs/postgres/create-manage-users) for additional configuration options.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Examples

URL: https://developers.cloudflare.com/hyperdrive/examples/

import { GlossaryTooltip, ListExamples } from "~/components";

Explore the following <GlossaryTooltip term="code example">examples</GlossaryTooltip> for Hyperdrive.

<ListExamples directory="hyperdrive/examples/" />

---

# Connect to Neon

URL: https://developers.cloudflare.com/hyperdrive/examples/neon/

import { Render } from "~/components";

This example shows you how to connect Hyperdrive to a [Neon](https://neon.tech/) Postgres database.

## 1. Allow Hyperdrive access

You can connect Hyperdrive to any existing Neon database by creating a new user and fetching your database connection string.

### Neon Dashboard

1. Go to the [**Neon dashboard**](https://console.neon.tech/app/projects) and select the project (database) you wish to connect to.
2. Select **Roles** from the sidebar and select **New Role**. Enter `hyperdrive-user` as the name (or your preferred name) and **copy the password**. Note that the password will not be displayed again: you will have to reset it if you do not save it somewhere.
3. Select **Dashboard** from the sidebar > go to the **Connection Details** pane > ensure you have selected the **branch**, **database** and **role** (for example,`hyperdrive-user`) that Hyperdrive will connect through.
4. Select the `psql` and **uncheck the connection pooling** checkbox. Note down the connection string (starting with `postgres://hyperdrive-user@...`) from the text box.

With both the connection string and the password, you can now create a Hyperdrive database configuration.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Connect to Materialize

URL: https://developers.cloudflare.com/hyperdrive/examples/materialize/

import { Render } from "~/components"

This example shows you how to connect Hyperdrive to a [Materialize](https://materialize.com/) database. Materialize is a Postgres-compatible streaming database that can automatically compute real-time results against your streaming data sources.

## 1. Allow Hyperdrive access

To allow Hyperdrive to connect to your database, you will need to ensure that Hyperdrive has valid user credentials and network access to your database.

### Materialize Console

:::note


Read the Materialize [Quickstart guide](https://materialize.com/docs/get-started/quickstart/) to set up your first database. The steps below assume you have an existing Materialize database ready to go.


:::

You will need to create a new application user and password for Hyperdrive to connect with:

1. Log in to the [Materialize Console](https://console.materialize.com/).
2. Under the **App Passwords** section, select **Manage app passwords**.
3. Select **New app password** and enter a name, for example, `hyperdrive-user`.
4. Select **Create Password**.
5. Copy the provided password: it will only be shown once.

To retrieve the hostname and database name of your Materialize configuration:

1. Select **Connect** in the sidebar of the Materialize Console.
2. Select **External tools**.
3. Copy the **Host**, **Port** and **Database** settings.

With the username, app password, hostname, port and database name, you can now connect Hyperdrive to your Materialize database.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Connect to Nile

URL: https://developers.cloudflare.com/hyperdrive/examples/nile/

import { Render } from "~/components";

This example shows you how to connect Hyperdrive to a [Nile](https://thenile.dev) PostgreSQL database instance.

Nile is PostgreSQL re-engineered for multi-tenant applications. Nile's virtual tenant databases provide you with isolation, placement, insight, and other features for your tenant's data and embedding. Refer to [Nile documentation](https://www.thenile.dev/docs/getting-started/whatisnile) to learn more.

## 1. Allow Hyperdrive access

You can connect Cloudflare Hyperdrive to any Nile database in your workspace using its connection string - either with a new set of credentials, or using an existing set.

### Nile console

To get a connection string from Nile console:

1. Log in to [Nile console](https://console.thenile.dev), then select a database.
2. On the left hand menu, click **Settings** (the bottom-most icon) and then select **Connection**.
3. Select the PostgreSQL logo to show the connection string.
4. Select "Generate credentials" to generate new credentials.
5. Copy the connection string (without the "psql" part).

You will have obtained a connection string similar to the following:

```txt
    postgres://0191c898-...:4d7d8b45-...@eu-central-1.db.thenile.dev:5432/my_database
```

With the connection string, you can now create a Hyperdrive database configuration.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Connect to pgEdge Cloud

URL: https://developers.cloudflare.com/hyperdrive/examples/pgedge/

import { Render } from "~/components";

This example shows you how to connect Hyperdrive to a [pgEdge](https://pgedge.com/) Postgres database. pgEdge Cloud provides easy deployment of fully-managed, fully-distributed, and secure Postgres.

## 1. Allow Hyperdrive access

You can connect Hyperdrive to any existing pgEdge database with the default user and password provided by pgEdge.

### pgEdge dashboard

To retrieve your connection string from the pgEdge dashboard:

1. Go to the [**pgEdge dashboard**](https://app.pgedge.com) and select the database you wish to connect to.
2. From the **Connect to your database** section, note down the connection string (starting with `postgres://app@...`) from the **Connection String** text box.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Connect to Supabase

URL: https://developers.cloudflare.com/hyperdrive/examples/supabase/

import { Render } from "~/components"

This example shows you how to connect Hyperdrive to a [Supabase](https://supabase.com/) Postgres database.

## 1. Allow Hyperdrive access

You can connect Hyperdrive to any existing Supabase database as the Postgres user which is set up during project creation.
Alternatively, to create a new user for Hyperdrive, run these commands in the [SQL Editor](https://supabase.com/dashboard/project/_/sql/new).

```sql
CREATE ROLE hyperdrive_user LOGIN PASSWORD 'sufficientlyRandomPassword';

-- Here, you are granting it the postgres role. In practice, you want to create a role with lesser privileges.
GRANT postgres to hyperdrive_user;
```

The database endpoint can be found in the [database settings page](https://supabase.com/dashboard/project/_/settings/database).

With a database user, password, database endpoint (hostname and port) and database name (default: postgres), you can now set up Hyperdrive.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Connect to Timescale

URL: https://developers.cloudflare.com/hyperdrive/examples/timescale/

import { Render } from "~/components"

This example shows you how to connect Hyperdrive to a [Timescale](https://www.timescale.com/) time-series database. Timescale is built on PostgreSQL, and includes powerful time-series, event and analytics features.

You can learn more about Timescale by referring to their [Timescale services documentation](https://docs.timescale.com/getting-started/latest/services/).

## 1. Allow Hyperdrive access

You can connect Hyperdrive to any existing Timescale database by creating a new user and fetching your database connection string.

### Timescale Dashboard

:::note


Similar to most services, Timescale requires you to reset the password associated with your database user if you do not have it stored securely. You should ensure that you do not break any existing clients if when you reset the password.


:::

To retrieve your credentials and database endpoint in the [Timescale Console](https://console.cloud.timescale.com/):

1. Select the service (database) you want Hyperdrive to connect to.
2. Expand **Connection info**.
3. Copy the **Service URL**. The Service URL is the connection string that Hyperdrive will use to connect. This string includes the database hostname, port number and database name.

If you do not have your password stored, you will need to select **Forgot your password?** and set a new **SCRAM** password. Save this password, as Timescale will only display it once.

You will end up with a connection string resembling the below:

```txt
postgres://tsdbadmin:YOUR_PASSWORD_HERE@pn79dztyy0.xzhhbfensb.tsdb.cloud.timescale.com:31358/tsdb
```

With the connection string, you can now create a Hyperdrive database configuration.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Connect to Xata

URL: https://developers.cloudflare.com/hyperdrive/examples/xata/

import { Render } from "~/components";

This example shows you how to connect Hyperdrive to a Xata PostgreSQL database instance.

## 1. Allow Hyperdrive access

You can connect Hyperdrive to any existing Xata database with the default user and password provided by Xata.

### Xata dashboard

To retrieve your connection string from the Xata dashboard:

1. Go to the [**Xata dashboard**](https://app.xata.io/).
2. Select the database you want to connect to.
3. Select **Settings**.
4. Copy the connection string from the `PostgreSQL endpoint` section and add your API key.

## 2. Create a database configuration

<Render file="create-hyperdrive-config" />

---

# Observability

URL: https://developers.cloudflare.com/hyperdrive/observability/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Troubleshoot and debug

URL: https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/

Troubleshoot and debug errors commonly associated with connecting to a database with Hyperdrive.

## Configuration errors

When creating a new Hyperdrive configuration, or updating the connection parameters associated with an existing configuration, Hyperdrive performs a test connection to your database in the background before creating or updating the configuration.

Hyperdrive will also issue an empty test query, a `;` in PostgreSQL, to validate that it can pass queries to your database.

| Error Code | Details                                                                                          | Recommended fixes                                                                                                                                              |
| ---------- | ------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `2008`     | Bad hostname.                                                                                    | Hyperdrive could not resolve the database hostname. Confirm it exists in public DNS.                                                                           |
| `2009`     | The hostname does not resolve to a public IP address, or the IP address is not a public address. | Hyperdrive can only connect to public IP addresses. Private IP addresses, like `10.1.5.0` or `192.168.2.1`, are not currently supported.                       |
| `2010`     | Cannot connect to the host:port.                                                                 | Hyperdrive could not route to the hostname: ensure it has a public DNS record that resolves to a public IP address. Check that the hostname is not misspelled. |
| `2011`     | Connection refused.                                                                              | A network firewall or access control list (ACL) is likely rejecting requests from Hyperdrive. Ensure you have allowed connections from the public Internet.    |
| `2012`     | TLS (SSL) not supported by the database.                                                         | Hyperdrive requires TLS (SSL) to connect. Configure TLS on your database.                                                                                      |
| `2013`     | Invalid database credentials.                                                                    | Ensure your username is correct (and exists), and the password is correct (case-sensitive).                                                                    |
| `2014`     | The specified database name does not exist.                                                      | Check that the database (not table) name you provided exists on the database you are asking Hyperdrive to connect to.                                          |
| `2015`     | Generic error.                                                                                   | Hyperdrive failed to connect and could not determine a reason. Open a support ticket so Cloudflare can investigate.                                            |
| `2016`     | Test query failed.                                                                               | Confirm that the user Hyperdrive is connecting as has permissions to issue read and write queries to the given database.                                       |

## Connection errors

Hyperdrive may also return errors at runtime. This can happen during initial connection setup, or in response to a query or other wire-protocol command sent by your driver.

These errors are returned as `ErrorResponse` wire protocol messages, which are handled by most drivers by throwing from the responsible query or by triggering an error event.
Hyperdrive errors that do not map 1:1 with an error message code [documented by PostgreSQL](https://www.postgresql.org/docs/current/errcodes-appendix.html) use the `58000` error code.

Hyperdrive may also encounter `ErrorResponse` wire protocol messages sent by your database. Hyperdrive will pass these errors through unchanged when possible.

### Hyperdrive specific errors

| Error Message                                          | Details                                                                                         | Recommended fixes                                                                                                                                                                                                                                                                              |
| ------------------------------------------------------ | ----------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Internal error.`                                      | Something is broken on our side.                                                                | Check for an ongoing incident affecting Hyperdrive, and contact Cloudflare Support.  Retrying the query is appropriate, if it makes sense for your usage pattern.                                                                                                                              |
| `Failed to acquire a connection from the pool.`        | Hyperdrive timed out while waiting for a connection to your database, or cannot connect at all. | If you are seeing this error intermittently, your Hyperdrive pool is being exhausted because too many connections are being held open for too long by your worker. This can be caused by a myriad of different issues, but long-running queries/transactions are a common offender.            |
| `Server connection attempt failed: connection_refused` | Hyperdrive is unable to create new connections to your origin database.                         | A network firewall or access control list (ACL) is likely rejecting requests from Hyperdrive. Ensure you have allowed connections from the public Internet. Sometimes, this can be caused by your database host provider refusing incoming connections when you go over your connection limit. |

### Node errors

| Error Message                                    | Details                                                                                                               | Recommended fixes                                                                                                            |
| ------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| `Uncaught Error: No such module "node:<module>"` | Your Cloudflare Workers project or a library that it imports is trying to access a Node module that is not available. | Enable [Node.js compatibility](/workers/runtime-apis/nodejs/) for your Cloudflare Workers project to maximize compatibility. |

### Improve performance

Having query traffic written as transactions can limit performance. This is because in the case of a transaction, the connection must be held for the duration of the transaction, which limits connection multiplexing. If there are multiple queries per transaction, this can be particularly impactful on connection multiplexing. Where possible, we recommend not wrapping queries in transactions to allow the connections to be shared more aggressively.

---

# Metrics and analytics

URL: https://developers.cloudflare.com/hyperdrive/observability/metrics/

Hyperdrive exposes analytics that allow you to inspect query volume, query latency and cache ratios size across all and/or each Hyperdrive configuration in your account.

## Metrics

Hyperdrive currently exports the below metrics as part of the `hyperdriveQueriesAdaptiveGroups` GraphQL dataset:

| Metric             | GraphQL Field Name  | Description                                                                                                                                                                                                       |
| ------------------ | ------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Queries            | `count`             | The number of queries issued against your Hyperdrive in the given time period.                                                                                                                                    |
| Cache Status       | `cacheStatus`       | Whether the query was cached or not. Can be one of `disabled`, `hit`, `miss`, `uncacheable`, `multiplestatements`, `notaquery`, `oversizedquery`, `oversizedresult`, `parseerror`, `transaction`, and `volatile`. |
| Query Bytes        | `queryBytes`        | The size of your queries, in bytes.                                                                                                                                                                               |
| Result Bytes       | `resultBytes`       | The size of your query *results*, in bytes.                                                                                                                                                                       |
| Connection Latency | `connectionLatency` | The time (in milliseconds) required to establish new connections from Hyperdrive to your database, as measured from your Hyperdrive connection pool(s).                                                                                                                |
| Query Latency      | `queryLatency`      | The time (in milliseconds) required to query (and receive results) from your database, as measured from your Hyperdrive connection pool(s).                                                                                                                            |
| Event Status       | `eventStatus`       | Whether a query responded successfully (`complete`) or failed (`error`).                                                                                                                                          |

Metrics can be queried (and are retained) for the past 31 days.

## View metrics in the dashboard

Per-database analytics for Hyperdrive are available in the Cloudflare dashboard. To view current and historical metrics for a Hyperdrive configuration:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com) and select your account.
2. Go to [**Workers & Pages** > **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive).
3. Select an existing Hyperdrive configuration.
4. Select the **Metrics** tab.

You can optionally select a time window to query. This defaults to the last 24 hours.

## Query via the GraphQL API

You can programmatically query analytics for your Hyperdrive configurations via the [GraphQL Analytics API](/analytics/graphql-api/). This API queries the same datasets as the Cloudflare dashboard, and supports GraphQL [introspection](/analytics/graphql-api/features/discovery/introspection/).

Hyperdrives's GraphQL datasets require an `accountTag` filter with your Cloudflare account ID. Hyperdrive exposes the `hyperdriveQueriesAdaptiveGroups` dataset.

## Write GraphQL queries

Examples of how to explore your Hyperdrive metrics.

### Get the number of queries handled via your Hyperdrive config by cache status

```graphql
query HyperdriveQueries($accountTag: string!, $configId: string!, $datetimeStart: Time!, $datetimeEnd: Time!) {
  viewer {
    accounts(filter: {accountTag: $accountTag}) {
      hyperdriveQueriesAdaptiveGroups(
        limit: 10000
        filter: {
          configId: $configId
          datetime_geq: $datetimeStart
          datetime_leq: $datetimeEnd
        }
      ) {
        count
        dimensions {
          cacheStatus
        }
      }
    }
  }
}
```

### Get the average query and connection latency for queries handled via your Hyperdrive config within a range of time, excluding queries that failed due to an error

```graphql
query AverageHyperdriveLatencies($accountTag: string!, $configId: string!, $datetimeStart: Time!, $datetimeEnd: Time!) {
  viewer {
    accounts(filter: {accountTag: $accountTag}) {
      hyperdriveQueriesAdaptiveGroups(
        limit: 10000
        filter: {
          configId: $configId
          eventStatus: "complete"
          datetime_geq: $datetimeStart
          datetime_leq: $datetimeEnd
        }
      ) {
        avg {
          connectionLatency
          queryLatency
        }
      }
    }
  }
}
```

### Get the total amount of query and result bytes flowing through your Hyperdrive config

```graphql
query HyperdriveQueryAndResultBytesForSuccessfulQueries($accountTag: string!, $configId: string!, $datetimeStart: Date!, $datetimeEnd: Date!) {
  viewer {
    accounts(filter: {accountTag: $accountTag}) {
      hyperdriveQueriesAdaptiveGroups(
        limit: 10000
        filter: {
          configId: $configId
          datetime_geq: $datetimeStart
          datetime_leq: $datetimeEnd
        }
      ) {
        sum {
          queryBytes
          resultBytes
        }
      }
    }
  }
}
```

---

# Changelog

URL: https://developers.cloudflare.com/hyperdrive/platform/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/hyperdrive.yaml. Update the file there for new entries to appear here. For more details, refer to https://developers.cloudflare.com/style-guide/documentation-content-strategy/content-types/changelog/#yaml-file --> */}

<ProductReleaseNotes />

---

# Platform

URL: https://developers.cloudflare.com/hyperdrive/platform/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Limits

URL: https://developers.cloudflare.com/hyperdrive/platform/limits/

The following limits apply to Hyperdrive configuration, connections, and queries made to your configured origin databases.

| Feature                                        | Limit                                                                                 |
| ---------------------------------------------- | ------------------------------------------------------------------------------------- |
| Maximum configured databases                   | 25 per account                                                                        |
| Initial connection timeout                     | 15 seconds                                                                            |
| Idle connection timeout                        | 10 minutes                                                                            |
| Maximum cached query response size             | 50 MB                                                                                 |
| Maximum query (statement) duration             | 60 seconds                                                                            |
| Maximum username length                        | 63 characters (bytes) [^1]                                                            |
| Maximum database name length                   | 63 characters (bytes) [^1]                                                            |
| Maximum potential origin database connections  | approx. \~100 connections [^2]																													 |

:::note
Hyperdrive does not have a hard limit on the number of concurrent *client* connections made from your Workers.

As many hosted databases have limits on the number of unique connections they can manage, Hyperdrive attempts to keep number of concurrent pooled connections to your origin database lower.
:::

[^1]: This is a limit enforced by PostgreSQL. Some database providers may enforce smaller limits.

[^2]: Hyperdrive is a distributed system, so it is possible for a client to be unable to reach an existing pool. In this scenario, a new pool will be established, with its own allocation of connections. This favors availability over strictly enforcing limits, but does mean that it is possible in edge cases to overshoot the normal connection limit.

:::note
You can request adjustments to limits that conflict with your project goals by contacting Cloudflare. Not all limits can be increased. To request an increase, submit a [Limit Increase Request](https://forms.gle/ukpeZVLWLnKeixDu7) and we will contact you with next steps.
:::

---

# Pricing

URL: https://developers.cloudflare.com/hyperdrive/platform/pricing/

**Hyperdrive is free and included in every [Workers Paid](/workers/platform/pricing/#workers) plan**.

Hyperdrive is automatically enabled when subscribed to a Workers Paid plan, and does not require you to pay any additional fees to use. Hyperdrive's [connection pooling and query caching](/hyperdrive/configuration/how-hyperdrive-works/) do not incur any additional charges, and there are no hidden limits other than those [published](/hyperdrive/platform/limits/).

:::note

For questions about pricing, refer to the [pricing FAQs](/hyperdrive/reference/faq/#pricing).

:::

---

# FAQ

URL: https://developers.cloudflare.com/hyperdrive/reference/faq/

Below you will find answers to our most commonly asked questions regarding Hyperdrive.

## Connectivity

### Does Hyperdrive use specific IP addresses to connect to my database?

Hyperdrive connects to your database using [Cloudflare's IP address ranges](https://www.cloudflare.com/ips/). These are shared by all Hyperdrive configurations and other Cloudflare products.

You can use this to configure restrictions in your database firewall to restrict the IP addresses that can access your database.

## Pricing

### Does Hyperdrive charge for data transfer / egress?

No.

### Is Hyperdrive available on the [Workers Free](/workers/platform/pricing/#workers) plan?

Not at this time.

### Does Hyperdrive charge for additional compute?

Hyperdrive itself does not charge for compute (CPU) or processing (wall clock) time. Workers querying Hyperdrive and computing results: for example, serializing results into JSON and/or issuing queries, are billed per [Workers pricing](/workers/platform/pricing/#workers).

## Limits

### Are there any limits to Hyperdrive?

Refer to the published [limits](/hyperdrive/platform/limits/) documentation.

---

# Reference

URL: https://developers.cloudflare.com/hyperdrive/reference/

import { DirectoryListing } from "~/components";

<DirectoryListing />

---

# Supported databases

URL: https://developers.cloudflare.com/hyperdrive/reference/supported-databases/

## Database support

Details on which database engines and/or specific database providers are supported are detailed in the following table.

| Database Engine | Supported                | Known supported versions | Details                                                                                              |
| --------------- | ------------------------ | ------------------------ | ---------------------------------------------------------------------------------------------------- |
| PostgreSQL      | âœ…                       | `9.0` to `16.x`          | Both self-hosted and managed (AWS, Google Cloud, Oracle) instances are supported.                    |
| Neon            | âœ…                       | All                      | Neon currently runs Postgres 15.x                                                                    |
| Supabase        | âœ…                       | All                      | Supabase currently runs Postgres 15.x                                                                |
| Timescale       | âœ…                       | All                      | See the [Timescale guide](/hyperdrive/examples/timescale/) to connect.                               |
| Materialize     | âœ…                       | All                      | Postgres-compatible. Refer to the [Materialize guide](/hyperdrive/examples/materialize/) to connect. |
| CockroachDB     | âœ…                       | All                      | Postgres-compatible. Refer to the [CockroachDB](/hyperdrive/examples/cockroachdb/) guide to connect. |
| MySQL           | Coming soon              |                          |                                                                                                      |
| SQL Server      | Not currently supported. |                          |                                                                                                      |
| MongoDB         | Not currently supported. |                          |                                                                                                      |

## Supported PostgreSQL authentication modes

Hyperdrive supports the following [authentication modes](https://www.postgresql.org/docs/current/auth-methods.html) for connecting to PostgreSQL databases:

- Password Authentication (`md5`)
- Password Authentication (`password`) (clear-text password)
- SASL Authentication (`SCRAM-SHA-256`)

---

# Tutorials

URL: https://developers.cloudflare.com/hyperdrive/tutorials/

import { GlossaryTooltip, ListTutorials } from "~/components"

View <GlossaryTooltip term="tutorial">tutorials</GlossaryTooltip> to help you get started with Hyperdrive.

<ListTutorials />

---

# Apply blur

URL: https://developers.cloudflare.com/images/manage-images/blur-variants/

You can apply blur to image variants by creating a specific variant for this effect first or by editing a previously created variant. Note that you cannot blur an SVG file.

Refer to [Resize images](/images/manage-images/create-variants/) for help creating variants. You can also refer to the API to learn how to use blur using flexible variants.

To blur an image:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Select **Images** > **Variants**.
3. Find the variant you want to blur and select **Edit** > **Customization Options**.
4. Use the slider to adjust the blurring effect. You can use the preview image to see how strong the blurring effect will be.
5. Select **Save**.

The image should now display the blurred effect.

---

# Browser TTL

URL: https://developers.cloudflare.com/images/manage-images/browser-ttl/

Browser TTL controls how long an image stays in a browser's cache and specifically configures the `cache-control` response header.

### Default TTL

By default, an image's TTL is set to two days to meet user needs, such as re-uploading an image under the same [Custom ID](/images/upload-images/upload-custom-path/).

## Custom setting

You can use two custom settings to control the Browser TTL, an account or a named variant. To adjust how long a browser should keep an image in the cache, set the TTL in seconds, similar to how the `max-age` header is set. The value should be an interval between one hour to one year.

### Browser TTL for an account

Setting the Browser TTL per account overrides the default TTL.

```bash title="Example"
curl --request PATCH 'https://api.cloudflare.com/client/v4/accounts/{account_id}/images/v1/config' \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{
  "browser_ttl": 31536000
}'
```

When the Browser TTL is set to one year for all images, the response for the `cache-control` header is essentially `public`, `max-age=31536000`, `stale-while-revalidate=7200`.

### Browser TTL for a named variant

Setting the Browser TTL for a named variant is a more granular option that overrides all of the above when creating or updating an image variant, specifically the `browser_ttl` option in seconds.

```bash title="Example"
curl 'https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_TAG>/images/v1/variants' \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{
  "id":"avatar",
  "options": {
    "width":100,
    "browser_ttl": 86400
  }
}'
```

When the Browser TTL is set to one day for images requested with this variant, the response for the `cache-control` header is essentially `public`, `max-age=86400`, `stale-while-revalidate=7200`.

:::note


[Private images](/images/manage-images/serve-images/serve-private-images/) do not respect default or custom TTL settings. The private images cache time is set according to the expiration time and can be as short as one hour.


:::

---

# Configure webhooks

URL: https://developers.cloudflare.com/images/manage-images/configure-webhooks/

You can set up webhooks to receive notifications about your upload workflow. This will send an HTTP POST request to a specified endpoint when an image either successfully uploads or fails to upload.

Currently, webhooks are supported only for [direct creator uploads](/images/upload-images/direct-creator-upload/).

To receive notifications for direct creator uploads:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Go to **Notifications** > **Destinations**.
3. From the Webhooks card, select **Create**.
4. Enter information for your webhook and select **Save and Test**. The new webhook will appear in the **Webhooks** card and can be attached to notifications.
5. Next, go to **Notifications** > **All Notifications** and select **Add**.
6. Under the list of products, locate **Images** and select **Select**.
7. Give your notification a name and optional description.
8. Under the **Webhooks** field, select the webhook that you recently created.
9. Select **Save**.

---

# Create variants

URL: https://developers.cloudflare.com/images/manage-images/create-variants/

Variants let you specify how images should be resized for different use cases. By default, images are served with a `public` variant, but you can create up to 100 variants to fit your needs. Follow these steps to create a variant.

:::note

Cloudflare Images can deliver SVG files but will not resize them because it is an inherently scalable format.
Resize via the Cloudflare dashboard. 
:::

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Select **Images** > **Variants**.
3. Name your variant and select **Add New Variant**.
4. Define variables for your new variant, such as resizing options, type of fit, and specific metadata options.

## Resize via the API

Make a `POST` request to [create a variant](/api/resources/images/subresources/v1/subresources/variants/methods/create/).

```bash
curl "https://api.cloudflare.com/client/v4/accounts/{account_id}/images/v1/variants" \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{"id":"<NAME_OF_THE_VARIANT>","options":{"fit":"scale-down","metadata":"none","width":1366,"height":768},"neverRequireSignedURLs":true}
```

## Fit options

The `Fit` property describes how the width and height dimensions should be interpreted. The chart below describes each of the options.

| Fit Options | Behavior                                                                                                                                                                                                                                                                        |
| ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Scale down  | The image is shrunk in size to fully fit within the given width or height, but will not be enlarged.                                                                                                                                                                            |
| Contain     | The image is resized (shrunk or enlarged) to be as large as possible within the given width or height while preserving the aspect ratio.                                                                                                                                        |
| Cover       | The image is resized to exactly fill the entire area specified by width and height and will be cropped if necessary.                                                                                                                                                            |
| Crop        | The image is shrunk and cropped to fit within the area specified by the width and height. The image will not be enlarged. For images smaller than the given dimensions, it is the same as `scale-down`. For images larger than the given dimensions, it is the same as `cover`. |
| Pad         | The image is resized (shrunk or enlarged) to be as large as possible within the given width or height while preserving the aspect ratio. The extra area is filled with a background color (white by default).                                                                   |

## Metadata options

Variants allow you to choose what to do with your imageâ€™s metadata information. From the **Metadata** dropdown, choose:

* Strip all metadata
* Strip all metadata except copyright
* Keep all metadata

## Public access

When the **Always allow public access** option is selected, particular variants will always be publicly accessible, even when images are made private through the use of [signed URLs](/images/manage-images/serve-images/serve-private-images).

---

# Delete images

URL: https://developers.cloudflare.com/images/manage-images/delete-images/

You can delete an image from the Cloudflare Images storage using the dashboard or the API.

## Delete images via the Cloudflare dashboard

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Select **Images**.
3. Find the image you want to remove and select **Delete**.
4. (Optional) To delete more than one image, select the checkbox next to the images you want to delete and then **Delete selected**.

Your image will be deleted from your account.

## Delete images via the API

Make a `DELETE` request to the [delete image endpoint](/api/resources/images/subresources/v1/methods/delete/). `{image_id}` must be fully URL encoded in the API call URL.

```bash
curl --request DELETE https://api.cloudflare.com/client/v4/accounts/{account_id}/images/v1/{image_id} \
--header "Authorization: Bearer <API_TOKEN>"
```

After the image has been deleted, the response returns `"success": true`.

---

# Delete variants

URL: https://developers.cloudflare.com/images/manage-images/delete-variants/

You can delete variants via the Images dashboard or API. The only variant you cannot delete is public.

:::caution


Deleting a variant is a global action that will affect other images that contain that variant.


:::

## Delete variants via the Cloudflare dashboard

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Select **Images** > **Variants**.
3. Find the variant you want to remove and select **Delete**.

## Delete variants via the API

Make a `DELETE` request to the delete variant endpoint.

```bash
curl --request DELETE https://api.cloudflare.com/client/v4/account/{account_id}/images/v1/variants/{variant_name} \
--header "Authorization: Bearer <API_TOKEN>"
```

After the variant has been deleted, the response returns `"success": true.`

---

# Edit images

URL: https://developers.cloudflare.com/images/manage-images/edit-images/

The Edit option provides you available options to modify a specific image. After choosing to edit an image, you can:

* Require signed URLs to use with that particular image.
* Use a cURL command you can use as an example to access the image.
* Use fully-formed URLs for all the variants configured in your account.

To edit an image:

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. In **Account Home**, select **Images**.
3. Locate the image you want to modify and select **Edit**.

---

# Export images

URL: https://developers.cloudflare.com/images/manage-images/export-images/

Cloudflare Images supports image exports via the Cloudflare dashboard and API which allows you to get the original version of your image.

## Export images via the Cloudflare dashboard

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Select **Images**.
3. Find the image or images you want to export.
4. To export a single image, select **Export** from its menu. To export several images, select the checkbox next to each image and then select **Export selected**.

Your images are downloaded to your machine.

## Export images via the API

Make a `GET` request as shown in the example below. `<IMAGE_ID>` must be fully URL encoded in the API call URL.

`GET accounts/<ACCOUNT_ID>/images/v1/<IMAGE_ID>/blob`

---

# Enable flexible variants

URL: https://developers.cloudflare.com/images/manage-images/enable-flexible-variants/

Flexible variants allow you to create variants with dynamic resizing which can provide more options than regular variants allow. This option is not enabled by default.

## Enable flexible variants via the Cloudflare dashboard

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/login) and select your account.
2. Select **Images** > **Variants**.
3. Enable **Flexible variants**.

## Enable flexible variants via the API

Make a `PATCH` request to the [Update a variant endpoint](/api/resources/images/subresources/v1/subresources/variants/methods/edit/).

```bash
curl --request PATCH https://api.cloudflare.com/client/v4/accounts/{account_id}/images/v1/config \
--header "Authorization: Bearer <API_TOKEN>" \
--header "Content-Type: application/json" \
--data '{"flexible_variants": true}'
```

After activation, you can use [transformation parameters](/images/transform-images/transform-via-url/#options) on any Cloudflare image. For example,

`https://imagedelivery.net/{account_hash}/{image_id}/w=400,sharpen=3`

Note that flexible variants cannot be used for images that require a [signed delivery URL](/images/manage-images/serve-images/serve-private-images).

---

# Manage uploaded images

URL: https://developers.cloudflare.com/images/manage-images/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Changelog

URL: https://developers.cloudflare.com/images/platform/changelog/

import { ProductReleaseNotes } from "~/components";

{/* <!-- Actual content lives in /src/content/release-notes/images.yaml. --> */}

<ProductReleaseNotes />

---

# Platform

URL: https://developers.cloudflare.com/images/platform/

import { DirectoryListing } from "~/components"

<DirectoryListing />

---

# Activate Polish

URL: https://developers.cloudflare.com/images/polish/activate-polish/

import { Render } from "~/components"

Images in the [cache must be purged](/cache/how-to/purge-cache/) or expired before seeing any changes in Polish settings.

:::caution


Do not activate Polish and [image transformations](/images/transform-images/) simultaneously. Image transformations already apply lossy compression, which makes Polish redundant.


:::

1. Log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select the account and domain where you want to activate Polish.
2. Go to **Speed** > **Optimization** > **Image Optimization**.
3. Under **Polish**, select *Lossy* or *Lossless* from the drop-down menu. [*Lossy*](/images/polish/compression/#lossy) gives greater file size savings.
4. (Optional) Select **WebP**. Enable this option if you want to further optimize PNG and JPEG images stored in the origin server, and serve them as WebP files to browsers that support this format.

To ensure WebP is not served from cache to a browser without WebP support, disable any WebP conversion utilities at your origin web server when using Polish.

<Render file="configuration-rule-promotion" product="rules" />

---

# Polish compression

URL: https://developers.cloudflare.com/images/polish/compression/

With Lossless and Lossy modes, Cloudflare attempts to strip as much metadata as possible. However, Cloudflare cannot guarantee stripping all metadata because other factors, such as caching status, might affect which metadata is finally sent in the response.

:::caution[Warning]


Polish may not be applied to origin responses that contain a `Vary` header. The only accepted `Vary` header is `Vary: Accept-Encoding`.


:::

## Compression options

### Off

Polish is disabled and no compression is applied. Disabling Polish does not revert previously polished images to original, until they expire or are purged from the cache.

### Lossless

The Lossless option attempts to reduce file sizes without changing any of the image pixels, keeping images identical to the original. It removes most metadata, like EXIF data, and losslessly recompresses image data. JPEG images may be converted to progressive format. On average, lossless compression reduces file sizes by 21 percent compared to unoptimized image files.

The Lossless option prevents conversion of JPEG to WebP, because this is always a lossy operation.

### Lossy

The Lossy option applies significantly better compression to images than the Lossless option, at a cost of small quality loss. When uncompressed, some of the redundant information from the original image is lost. On average, using Lossy mode reduces file sizes by 48 percent.

This option also removes metadata from images. The Lossy option mainly affects JPEG images, but PNG images may also be compressed in a lossy way, or converted to JPEG when this improves compression.

### WebP

When enabled, in addition to other optimizations, Polish creates versions of images converted to 